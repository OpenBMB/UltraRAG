{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a8a828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-21 20:45:44.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mhome path: /mnt1/guodewen/research/UltraRAG\u001b[0m\n",
      "\u001b[32m2025-04-21 20:45:44.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1m设置推理卡片为1,2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-21 20:45:47 __init__.py:183] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt1/guodewen/research/UltraRAG/ultrarag/modules/database/__init__.py:8: UserWarning: failed to load MilvusIndex beacause not available qdrant_client, ignored it if you do not need it\n",
      "  warnings.warn(\"failed to load MilvusIndex beacause not available qdrant_client, ignored it if you do not need it\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import jsonlines\n",
    "from loguru import logger\n",
    "from pathlib import Path\n",
    "home_path = Path().absolute().parents[1]\n",
    "logger.info(f\"home path: {home_path.as_posix()}\")\n",
    "\n",
    "logger.info('设置推理卡片为1,2')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2'\n",
    "sys.path.append(home_path.as_posix())\n",
    "\n",
    "from ultrarag.modules.llm import OpenaiLLM\n",
    "from ultrarag.modules.database import QdrantIndex\n",
    "from ultrarag.modules.embedding import MiniCPMEmbServer\n",
    "\n",
    "\n",
    "prompt = \"请结合召回结果回答用户问题：\\n召回结果：\\n{recalls}\\n用户问题：\\n{query}\\n回答：\"\n",
    "\n",
    "corpus_path = home_path / 'examples' / 'workflow' / 'corpus.jsonl'\n",
    "data = list(jsonlines.open(corpus_path.as_posix(), 'r'))\n",
    "\n",
    "\n",
    "embedding_model_path = \"/mnt1/guodewen/models/MiniCPM-Embedding-Light\"\n",
    "qdrant_collection_path = (home_path / 'resource' / 'qdrant').as_posix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f283da3",
   "metadata": {},
   "source": [
    "#### 初始化函数说明\n",
    "\n",
    "> OpenaiLLM(model, base_url, api_key, **kargs) -> client\n",
    "> 初始化推理模型\n",
    "*入参*\n",
    "- model: 默认的模型名称，如果请求时不指定则使用这个 model 名称推理\n",
    "- base_url: 推理接口地址\n",
    "- api_key: 推理接口密钥\n",
    "- kargs: 字典结构，作为模型请求时的其他默认参数，如温度系数、topk, top-p等\n",
    "*输出*\n",
    "- client：输出一个 object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65bfcca",
   "metadata": {},
   "source": [
    "> MiniCPMEmbServer(url_or_path, batch_size, max_length, query_instruction, document_instruction, **kargs)\n",
    "> 初始化文本向量化模型\n",
    "\n",
    "**入参**\n",
    "- url_or_path: 加载推理模型时指定模型的路径\n",
    "- batch_size: 批量请求时限制推理的并发，避免显存 OOM\n",
    "- max_length: 推理请求时限制文本的长度，避免显存 OOM\n",
    "- query_instruction: 对于有些模型，需要在 encode 之前拼上特殊指令，例如 bge large\n",
    "- document_instruction: 对于有些模型，需要在 encode 之前拼上特殊指令，例如 bge large\n",
    "\n",
    "**输出**\n",
    "- client：输出一个 object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e73146d",
   "metadata": {},
   "source": [
    "> QdrantIndex(url, encoder)\n",
    "> 初始化数据库\n",
    "\n",
    "**入参**\n",
    "- url: 加载知识库时指定数据库的路径\n",
    "- encoder: 对于向量数据库，需要指定推理模型将文本映射成向量，对于全文检索数据库，可以为空"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aecd31b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-21 20:45:48.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36multrarag.modules.llm.openai_like\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mapi_key: EMPTY, base_url: http://localhost:8000/v1, kargs: {'model': 'QwQ-32B'}\u001b[0m\n",
      "\u001b[32m2025-04-21 20:45:49.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36multrarag.modules.embedding.minicpm_embedding\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1mThe parameters of colbert_linear and sparse linear is new initialize. Make sure the model is loaded for training, not inferencing\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------using 2*GPUs----------\n"
     ]
    }
   ],
   "source": [
    "llm = OpenaiLLM(\n",
    "    model=\"QwQ-32B\",\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"EMPTY\",\n",
    ")\n",
    "\n",
    "embed = MiniCPMEmbServer(url_or_path=embedding_model_path)\n",
    "\n",
    "index = QdrantIndex(url=qdrant_collection_path, encoder=embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd5585a",
   "metadata": {},
   "source": [
    "#### 构建知识库说明\n",
    "\n",
    "> index.get_collections()\n",
    "> 获取知识库列表\n",
    "\n",
    "> index.create(collection_name, dimension):\n",
    "> 创建新的索引\n",
    "- collection_name: 创建索引的名称\n",
    "- dimension: 对于向量数据库，需要指定初始化向量的维度\n",
    "\n",
    "> index.insert(collection, payloads, func, callback):\n",
    "> 插入数据库数据\n",
    "\n",
    "- collection: 待插入目标索引的名称\n",
    "- payloads: 待插入数据，是一个 list,每个元素是一个字典，或者说是一个 json [TODO:后面需要考虑大文件的情况，通过传入文件生成器减少内存占用]\n",
    "- func: 这个函数是一个回调函数，用来告诉数据库对哪些信息建索引；对于向量数据库来说，就是确定 mebedding 文本的拼接规则\n",
    "- callback: 对于大文件插入， 传输 callback 用于显示处理的进度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c9c2fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if \"demo\" not in index.get_collections():\n",
    "    await index.create(\"demo\")\n",
    "    \n",
    "await index.insert(collection=\"demo\", payloads=data, func=lambda x: x[\"title\"] + x[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b87e77",
   "metadata": {},
   "source": [
    "#### 检索生成\n",
    "\n",
    "> index.search(collection, query, topn, method)\n",
    "> 检索召回文档，由于检索耗时所以是异步调用\n",
    "\n",
    "**入参**\n",
    "- collection: 指定要检索的索引名称, 可以是一个或者是多个，按照得分排序取 topn\n",
    "- query: 待检索的 query \n",
    "- topn：控制召回的候选数目\n",
    "- method: 支持稠密检索和混合检索\n",
    "\n",
    "**出参**\n",
    "- return: 返回一个BaseNode 格式的列表，其中包含相似度得分、content 和原始的 payloads 信息\n",
    "\n",
    "> llm.arun(messages, stream, **kargs)\n",
    "> 生成回复\n",
    "\n",
    "**入参**\n",
    "- messages：对话流信息，格式和 openai 保持一致\n",
    "- stream: 用于标记是否流式输出\n",
    "- kargs: 推理模型请求所需要的其他参数，例如指定模型名称、温度值等等\n",
    "\n",
    "**出参**\n",
    "- return: 返回一个输出的字符串（非流式）,或者一个异步生成器(流式)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27ac9a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-21 20:45:51.672\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1m召回结果：\n",
      "[BaseNode(score=0.6624137391062277, content='9. **宫保鸡丁**\\\\n\\\\n**食材准备**：鸡胸肉250g、花生米一小碗、干辣椒10个、花椒适量、葱姜蒜各适量、生抽、老抽、醋、糖、淀粉。\\\\n\\\\n**做法详解**：  \\\\n鸡胸肉洗净切成1厘米见方的小丁，用料酒、生抽、淀粉腌制15分钟，帮助鸡肉保持嫩滑口感。干辣椒剪成小段，花生米提前用小火炒熟备用。调料汁：生抽1勺、老抽半勺、糖1勺、醋1.5勺、水适量、淀粉1勺，搅拌均匀成宫保汁。\\\\n\\\\n锅中加油烧热，下鸡丁快速翻炒至变色后盛出。锅中留底油，放入干辣椒和花椒，用小火炒出香味（不要糊），然后加入葱姜蒜炒香，接着倒入鸡丁快速翻炒。最后淋入宫保汁，翻炒至浓稠，再撒入炒熟的花生米，炒匀即可。\\\\n\\\\n**小贴士**：干辣椒先炒香但不能炒糊；宫保汁一次倒入，快速翻炒收汁；花生最后放入保持脆感。', payload={'title': '宫保鸡丁', 'content': '9. **宫保鸡丁**\\\\n\\\\n**食材准备**：鸡胸肉250g、花生米一小碗、干辣椒10个、花椒适量、葱姜蒜各适量、生抽、老抽、醋、糖、淀粉。\\\\n\\\\n**做法详解**：  \\\\n鸡胸肉洗净切成1厘米见方的小丁，用料酒、生抽、淀粉腌制15分钟，帮助鸡肉保持嫩滑口感。干辣椒剪成小段，花生米提前用小火炒熟备用。调料汁：生抽1勺、老抽半勺、糖1勺、醋1.5勺、水适量、淀粉1勺，搅拌均匀成宫保汁。\\\\n\\\\n锅中加油烧热，下鸡丁快速翻炒至变色后盛出。锅中留底油，放入干辣椒和花椒，用小火炒出香味（不要糊），然后加入葱姜蒜炒香，接着倒入鸡丁快速翻炒。最后淋入宫保汁，翻炒至浓稠，再撒入炒熟的花生米，炒匀即可。\\\\n\\\\n**小贴士**：干辣椒先炒香但不能炒糊；宫保汁一次倒入，快速翻炒收汁；花生最后放入保持脆感。'}), BaseNode(score=0.3389509456255837, content='2. **青椒土豆丝**\\\\n\\\\n**食材准备**：土豆2个、青椒1个、大蒜2瓣、白醋1勺、盐适量、食用油适量。\\\\n\\\\n**做法详解**：  \\\\n将土豆去皮，先切成薄片，再切成细丝，尽量保持粗细一致。切好的土豆丝用清水浸泡5分钟以上，以去除多余淀粉，这样炒出来不粘锅也更脆爽。可以多冲洗几遍，直到水变清为止。青椒去籽后切成丝，大蒜拍碎剁成蒜末备用。\\\\n\\\\n锅中倒油，油热后转中火，放入蒜末炒出香味，然后迅速放入控干水分的土豆丝。炒的时候注意快速翻动，使其受热均匀。大约炒2分钟后，加入青椒丝继续翻炒约1分钟。当土豆丝变得稍微透明但依然有脆感时，加盐调味，并加入一小勺白醋提味。继续翻炒几下后即可关火装盘。\\\\n\\\\n**小贴士**：白醋可以帮助保持土豆丝清脆的口感，也能提香去腥；炒土豆丝时全程不要加水，保持快火快炒；土豆丝一定要泡水、冲洗淀粉，否则容易炒糊。', payload={'title': '青椒土豆丝', 'content': '2. **青椒土豆丝**\\\\n\\\\n**食材准备**：土豆2个、青椒1个、大蒜2瓣、白醋1勺、盐适量、食用油适量。\\\\n\\\\n**做法详解**：  \\\\n将土豆去皮，先切成薄片，再切成细丝，尽量保持粗细一致。切好的土豆丝用清水浸泡5分钟以上，以去除多余淀粉，这样炒出来不粘锅也更脆爽。可以多冲洗几遍，直到水变清为止。青椒去籽后切成丝，大蒜拍碎剁成蒜末备用。\\\\n\\\\n锅中倒油，油热后转中火，放入蒜末炒出香味，然后迅速放入控干水分的土豆丝。炒的时候注意快速翻动，使其受热均匀。大约炒2分钟后，加入青椒丝继续翻炒约1分钟。当土豆丝变得稍微透明但依然有脆感时，加盐调味，并加入一小勺白醋提味。继续翻炒几下后即可关火装盘。\\\\n\\\\n**小贴士**：白醋可以帮助保持土豆丝清脆的口感，也能提香去腥；炒土豆丝时全程不要加水，保持快火快炒；土豆丝一定要泡水、冲洗淀粉，否则容易炒糊。'}), BaseNode(score=0.3087142905910696, content='5. **可乐鸡翅**\\\\n\\\\n**食材准备**：鸡翅中500g、可乐1罐（约330ml）、姜片3片、生抽1勺、老抽半勺、料酒1勺、食用油适量。\\\\n\\\\n**做法详解**：  \\\\n将鸡翅洗净，在两面各划两刀便于入味。锅中加冷水，放入鸡翅和几片姜，水开后撇去浮沫，焯水2分钟捞出备用。炒锅加少量油烧热，放入焯水后的鸡翅煎至两面金黄出香味。加入姜片一起翻炒，再加入1勺料酒、1勺生抽、半勺老抽提色。\\\\n\\\\n接着倒入整罐可乐（刚好能没过鸡翅），转中火烧开后调至小火，盖锅盖焖煮15分钟，让鸡翅充分吸收可乐的香甜。待汤汁渐少、变得浓稠时，转大火收汁，不断翻动鸡翅，防止糊锅。直到汤汁裹满鸡翅，表面油亮泛红即可出锅。\\\\n\\\\n**小贴士**：可乐要用原味，不要用零度型；焯水能去腥味；收汁阶段要不停翻动以防焦糊。', payload={'title': '可乐鸡翅', 'content': '5. **可乐鸡翅**\\\\n\\\\n**食材准备**：鸡翅中500g、可乐1罐（约330ml）、姜片3片、生抽1勺、老抽半勺、料酒1勺、食用油适量。\\\\n\\\\n**做法详解**：  \\\\n将鸡翅洗净，在两面各划两刀便于入味。锅中加冷水，放入鸡翅和几片姜，水开后撇去浮沫，焯水2分钟捞出备用。炒锅加少量油烧热，放入焯水后的鸡翅煎至两面金黄出香味。加入姜片一起翻炒，再加入1勺料酒、1勺生抽、半勺老抽提色。\\\\n\\\\n接着倒入整罐可乐（刚好能没过鸡翅），转中火烧开后调至小火，盖锅盖焖煮15分钟，让鸡翅充分吸收可乐的香甜。待汤汁渐少、变得浓稠时，转大火收汁，不断翻动鸡翅，防止糊锅。直到汤汁裹满鸡翅，表面油亮泛红即可出锅。\\\\n\\\\n**小贴士**：可乐要用原味，不要用零度型；焯水能去腥味；收汁阶段要不停翻动以防焦糊。'})]\u001b[0m\n",
      "/mnt1/guodewen/research/UltraRAG/ultrarag/modules/llm/openai_like.py:59: UserWarning: retry 0: Traceback (most recent call last):\n",
      "  File \"/mnt1/guodewen/research/UltraRAG/ultrarag/modules/llm/openai_like.py\", line 52, in arun\n",
      "    response = await self._generator_async.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt1/guodewen/miniconda3/envs/urag/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 2002, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt1/guodewen/miniconda3/envs/urag/lib/python3.11/site-packages/openai/_base_client.py\", line 1776, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt1/guodewen/miniconda3/envs/urag/lib/python3.11/site-packages/openai/_base_client.py\", line 1466, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt1/guodewen/miniconda3/envs/urag/lib/python3.11/site-packages/openai/_base_client.py\", line 1571, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `QwQ-32B` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n",
      "\n",
      "  warnings.warn(f\"retry {retry}: {traceback.format_exc()}\")\n",
      "/mnt1/guodewen/research/UltraRAG/ultrarag/modules/llm/openai_like.py:59: UserWarning: retry 1: Traceback (most recent call last):\n",
      "  File \"/mnt1/guodewen/research/UltraRAG/ultrarag/modules/llm/openai_like.py\", line 52, in arun\n",
      "    response = await self._generator_async.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt1/guodewen/miniconda3/envs/urag/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 2002, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt1/guodewen/miniconda3/envs/urag/lib/python3.11/site-packages/openai/_base_client.py\", line 1776, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt1/guodewen/miniconda3/envs/urag/lib/python3.11/site-packages/openai/_base_client.py\", line 1466, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt1/guodewen/miniconda3/envs/urag/lib/python3.11/site-packages/openai/_base_client.py\", line 1571, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `QwQ-32B` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n",
      "\n",
      "  warnings.warn(f\"retry {retry}: {traceback.format_exc()}\")\n",
      "/mnt1/guodewen/research/UltraRAG/ultrarag/modules/llm/openai_like.py:59: UserWarning: retry 2: Traceback (most recent call last):\n",
      "  File \"/mnt1/guodewen/research/UltraRAG/ultrarag/modules/llm/openai_like.py\", line 52, in arun\n",
      "    response = await self._generator_async.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt1/guodewen/miniconda3/envs/urag/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 2002, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt1/guodewen/miniconda3/envs/urag/lib/python3.11/site-packages/openai/_base_client.py\", line 1776, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt1/guodewen/miniconda3/envs/urag/lib/python3.11/site-packages/openai/_base_client.py\", line 1466, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt1/guodewen/miniconda3/envs/urag/lib/python3.11/site-packages/openai/_base_client.py\", line 1571, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `QwQ-32B` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n",
      "\n",
      "  warnings.warn(f\"retry {retry}: {traceback.format_exc()}\")\n",
      "/mnt1/guodewen/research/UltraRAG/ultrarag/modules/llm/openai_like.py:62: UserWarning: failed with 3 times\n",
      "  warnings.warn(f\"failed with {self.max_retries} times\")\n",
      "\u001b[32m2025-04-21 20:45:51.686\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1m推理结果：\n",
      "None\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query=\"宫保鸡丁的做法\"\n",
    "\n",
    "results = await index.search(collection=\"demo\", query=query, topn=3)\n",
    "logger.info(f\"召回结果：\\n{results}\")\n",
    "\n",
    "message = [\n",
    "    dict(role=\"user\", content=prompt.format(query=query, recalls=results)),\n",
    "]\n",
    "response = await llm.arun(messages=message, stream=False)\n",
    "logger.info(f\"推理结果：\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b94b0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除知识库\n",
    "await index.remove(\"demo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
