# servers/generation/parameter.yaml

backend: vllm # options: vllm, openai
backend_configs:
  vllm:
    model_name_or_path: openbmb/MiniCPM4-8B
    gpu_ids: "2,3"
    gpu_memory_utilization: 0.9
    dtype: auto
  openai:
    model_name: MiniCPM4-8B
    base_url: http://localhost:8000/v1
    api_key: ""
    concurrency: 8
    retries: 3
    base_delay: 1.0
  hf:
    model_name_or_path: openbmb/MiniCPM4-8B
    gpu_ids: '2,3'
    trust_remote_code: true

sampling_params:
  temperature: 0.7
  top_p: 0.8
  max_tokens: 2048
  presence_penalty: 1.5
  top_k: 20
  chat_template_kwargs:
    enable_thinking: false

system_prompt: ""
