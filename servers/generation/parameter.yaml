# servers/generation/parameter.yaml

# Default to OpenAI-compatible server mode.
# This avoids requiring `pip install vllm` inside the UltraRAG container:
# run vLLM as a separate service (see root docker-compose.yml `vllm`) and point here.
backend: openai # options: vllm, openai, hf
backend_configs:
  vllm:
    model_name_or_path: openbmb/MiniCPM4-8B
    gpu_ids: "2,3"
    gpu_memory_utilization: 0.9
    dtype: auto
    trust_remote_code: true
  openai:
    # Should match vLLM's served model name in docker-compose (VLLM_SERVED_MODEL_NAME)
    model_name: agentcpm-report
    # In docker-compose, use the service DNS name `vllm`
    base_url: http://vllm:8000/v1
    # vLLM usually ignores api_key but the SDK may require a non-empty string
    api_key: "sk-no-key"
    concurrency: 8
    retries: 3
    base_delay: 1.0
  hf:
    model_name_or_path: openbmb/MiniCPM4-8B
    gpu_ids: '2,3'
    trust_remote_code: true
    batch_size: 8

sampling_params:
  temperature: 0.7
  top_p: 0.8
  max_tokens: 2048

extra_params:
  chat_template_kwargs:
    enable_thinking: false

system_prompt: ""
image_tag: null
