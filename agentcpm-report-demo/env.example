OPENAI_API_KEY=
OPENAI_BASE_URL=https://api.openai.com/v1

VLLM_MODEL=OpenBMB/AgentCPM-Report
MILVUS_PORT=19531

# Generation backend (OpenAI-compatible):
# If you use docker-compose `vllm` service, UltraRAG will use `http://vllm:8000/v1` by default
# via `UltraRAG/servers/generation/parameter.yaml`. No need to install `vllm` inside UltraRAG.


# CPU GGUF generation (used by docker-compose.cpu.yml with llama.cpp server):
# GGUF_MODEL_HOST_PATH=
# Optional overrides:
# GGUF_MODEL_CONTAINER_PATH=/models/model.gguf
# LLAMA_CPP_PORT=8000
# LLM_MODEL_NAME=agentcpm-report_gguf


# (Optional) Use a local model path on host and mount into vLLM container:
# VLLM_MODEL_HOST_PATH=
# VLLM_MODEL_CONTAINER_PATH=/models/agentcpm-report
# VLLM_MODEL=/models/agentcpm-report
# VLLM_SERVED_MODEL_NAME=agentcpm-report

# Optional: run an external OpenAI-compatible embedding server (e.g. Infinity container) and point to it:
# INFINITY_IMAGE=michaelfeil/infinity:latest
# INFINITY_HOST_PORT=7997
# INFINITY_MODEL_ID=openbmb/MiniCPM-Embedding-Light
# INFINITY_DEVICE=cuda
# RETRIEVER_BACKEND=openai
# RETRIEVER_OPENAI_BASE_URL=http://infinity-embed:7997/v1
# RETRIEVER_OPENAI_MODEL=openbmb/MiniCPM-Embedding-Light
# RETRIEVER_OPENAI_API_KEY=sk-no-key
