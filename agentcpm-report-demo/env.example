OPENAI_API_KEY=
OPENAI_BASE_URL=https://api.openai.com/v1
ULTRARAG_UI_PORT=5050
ULTRARAG_ADMIN_UI_PORT=5051
# GPU-first default:
# - faiss-gpu-cu12: FAISS GPU backend
# - transformers==4.37.2 + sentencepiece: avoid tokenizer.model being mis-parsed as tiktoken BPE
PIP_EXTRA_PACKAGES=faiss-gpu-cu12 pymilvus chonkie tiktoken infinity-emb[torch] huggingface_hub==0.24.7 transformers==4.37.2 sentencepiece
VLLM_MODEL=OpenBMB/AgentCPM-Report
VLLM_SERVED_MODEL_NAME=agentcpm-report
VLLM_PORT=8000
MILVUS_PORT=19531
ULTRARAG_MILVUS_URI=http://milvus:19530

# Hugging Face mirror (for CN networks). huggingface_hub / transformers / infinity-emb will use this.
HF_ENDPOINT=https://hf-mirrors.com
HF_HUB_ENABLE_HF_TRANSFER=1

# PyTorch wheel selection (example: CUDA 12.1)
TORCH_CUDA_TAG=cu121
# Runtime torch index (used by UI auto-install when needed)
TORCH_INDEX_URL=https://download.pytorch.org/whl/cu121
# Build-time torch index (used by Dockerfile.v3-dev)
TORCH_INDEX_URL_BUILD=https://download.pytorch.org/whl/cu121

# Retriever embeddings backend:
# - Default: run embeddings in-process via infinity-emb (no extra Docker image pull needed)
RETRIEVER_BACKEND=infinity

# Generation backend (OpenAI-compatible):
# If you use docker-compose `vllm` service, UltraRAG will use `http://vllm:8000/v1` by default
# via `UltraRAG/servers/generation/parameter.yaml`. No need to install `vllm` inside UltraRAG.

LLM_BACKEND=openai
LLM_BASE_URL=http://llama-cpp:8000/v1
LLM_MODEL_NAME=agentcpm-report

# CPU GGUF generation (used by docker-compose.cpu.yml with llama.cpp server):
# GGUF_MODEL_HOST_PATH=
# Optional overrides:
# GGUF_MODEL_CONTAINER_PATH=/models/model.gguf
# LLAMA_CPP_PORT=8000
# LLM_MODEL_NAME=agentcpm-report_gguf


# (Optional) Use a local model path on host and mount into vLLM container:
# VLLM_MODEL_HOST_PATH=
# VLLM_MODEL=/models/agentcpm-report
# VLLM_SERVED_MODEL_NAME=agentcpm-report

# Optional: run an external OpenAI-compatible embedding server (e.g. Infinity container) and point to it:
# INFINITY_IMAGE=michaelfeil/infinity:latest
# INFINITY_HOST_PORT=7997
# INFINITY_MODEL_ID=openbmb/MiniCPM-Embedding-Light
# INFINITY_DEVICE=cuda
# RETRIEVER_BACKEND=openai
# RETRIEVER_OPENAI_BASE_URL=http://infinity-embed:7997/v1
# RETRIEVER_OPENAI_MODEL=openbmb/MiniCPM-Embedding-Light
# RETRIEVER_OPENAI_API_KEY=sk-no-key


# Force CPU (set to 1 on CPU-only hosts)
RETRIEVER_FORCE_CPU=0