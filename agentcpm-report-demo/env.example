OPENAI_API_KEY=
OPENAI_BASE_URL=https://api.openai.com/v1

VLLM_MODEL=OpenBMB/AgentCPM-Report
MILVUS_PORT=19531

# Generation backend (OpenAI-compatible):
# If you use docker-compose `vllm` service, UltraRAG will use `http://vllm:8000/v1` by default
# via `UltraRAG/servers/generation/parameter.yaml`. No need to install `vllm` inside UltraRAG.


# CPU GGUF generation (used by docker-compose.cpu.yml with llama.cpp server):
# GGUF auto-download directory on host (will be mounted into containers as /models):
GGUF_MODELS_HOST_DIR=./models
# Hugging Face repo containing GGUF files:
GGUF_REPO_ID=openbmb/AgentCPM-Report-GGUF
# Optional: pin the GGUF file name (recommended). If empty, auto-select (prefers Q4_K_M).
GGUF_FILENAME=
# Optional revision/tag/commit:
GGUF_REVISION=main
# Optional: if the repo is gated/private:
HF_TOKEN=

# (Legacy) If you prefer to mount a single GGUF file path directly, keep the old var:
# GGUF_MODEL_HOST_PATH=
# Optional overrides:
# GGUF_MODEL_CONTAINER_PATH=/models/model.gguf
# LLAMA_CPP_PORT=8000
# LLM_MODEL_NAME=agentcpm-report_gguf


# (Optional) Use a local model path on host and mount into vLLM container:
# VLLM_MODEL_HOST_PATH=
# VLLM_MODEL_CONTAINER_PATH=/models/agentcpm-report
# VLLM_MODEL=/models/agentcpm-report
# VLLM_SERVED_MODEL_NAME=agentcpm-report

# Optional: run an external OpenAI-compatible embedding server (e.g. Infinity container) and point to it:
# INFINITY_IMAGE=michaelfeil/infinity:latest
# INFINITY_HOST_PORT=7997
# INFINITY_MODEL_ID=openbmb/MiniCPM-Embedding-Light
# INFINITY_DEVICE=cuda
# RETRIEVER_BACKEND=openai
# RETRIEVER_OPENAI_BASE_URL=http://infinity-embed:7997/v1
# RETRIEVER_OPENAI_MODEL=openbmb/MiniCPM-Embedding-Light
# RETRIEVER_OPENAI_API_KEY=sk-no-key
