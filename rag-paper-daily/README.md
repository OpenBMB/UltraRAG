# ğŸ“š RAG Paper Daily

### ğŸ“… 2026-01-21
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2601.15161v1">Automated Rubrics for Reliable Evaluation of Medical Dialogue Systems</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) are increasingly used for clinical decision support, where hallucinations and unsafe suggestions may pose direct risks to patient safety. These risks are particularly challenging as they often manifest as subtle clinical errors that evade detection by generic metrics, while expert-authored fine-grained rubrics remain costly to construct and difficult to scale. In this paper, we propose a retrieval-augmented multi-agent framework designed to automate the generation of instance-specific evaluation rubrics. Our approach grounds evaluation in authoritative medical evidence by decomposing retrieved content into atomic facts and synthesizing them with user interaction constraints to form verifiable, fine-grained evaluation criteria. Evaluated on HealthBench, our framework achieves a Clinical Intent Alignment (CIA) score of 60.12%, a statistically significant improvement over the GPT-4o baseline (55.16%). In discriminative tests, our rubrics yield a mean score delta ($Î¼_Î” = 8.658$) and an AUROC of 0.977, nearly doubling the quality separation achieved by GPT-4o baseline (4.972). Beyond evaluation, our rubrics effectively guide response refinement, improving quality by 9.2% (from 59.0% to 68.2%). This provides a scalable and transparent foundation for both evaluating and improving medical LLMs. The code is available at https://anonymous.4open.science/r/Automated-Rubric-Generation-AF3C/.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºæ£€ç´¢å¢å¼ºçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨åŒ–ç”Ÿæˆé’ˆå¯¹ç‰¹å®šå®ä¾‹çš„ä¸´åºŠå†³ç­–æ”¯æŒè¯„ä¼°æ ‡å‡†ã€‚è¯¥æ¡†æ¶é€šè¿‡æ£€ç´¢æƒå¨åŒ»å­¦è¯æ®ã€åˆ†è§£å†…å®¹ä¸ºåŸå­äº‹å®å¹¶ç»“åˆç”¨æˆ·çº¦æŸï¼Œç”Ÿæˆå¯éªŒè¯çš„ç»†ç²’åº¦è¯„ä¼°å‡†åˆ™ï¼Œæ˜¾è‘—æå‡äº†ä¸´åºŠæ„å›¾å¯¹é½ï¼ˆCIAï¼‰å¾—åˆ†ï¼ˆ60.12% vs GPT-4oåŸºçº¿55.16%ï¼‰ï¼Œå¹¶åœ¨åŒºåˆ†æ€§æµ‹è¯•å’Œè´¨é‡æ”¹è¿›ï¼ˆ9.2%æå‡ï¼‰ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸ºåŒ»ç–—å¤§è¯­è¨€æ¨¡å‹çš„è¯„ä¼°å’Œä¼˜åŒ–æä¾›äº†å¯æ‰©å±•æ–¹æ¡ˆã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.15153v1">How to Build AI Agents by Augmenting LLMs with Codified Human Expert Domain Knowledge? A Software Engineering Framework</a></td><td><details><summary>å±•å¼€</summary>Critical domain knowledge typically resides with few experts, creating organizational bottlenecks in scalability and decision-making. Non-experts struggle to create effective visualizations, leading to suboptimal insights and diverting expert time. This paper investigates how to capture and embed human domain knowledge into AI agent systems through an industrial case study. We propose a software engineering framework to capture human domain knowledge for engineering AI agents in simulation data visualization by augmenting a Large Language Model (LLM) with a request classifier, Retrieval-Augmented Generation (RAG) system for code generation, codified expert rules, and visualization design principles unified in an agent demonstrating autonomous, reactive, proactive, and social behavior. Evaluation across five scenarios spanning multiple engineering domains with 12 evaluators demonstrates 206% improvement in output quality, with our agent achieving expert-level ratings in all cases versus baseline's poor performance, while maintaining superior code quality with lower variance. Our contributions are: an automated agent-based system for visualization generation and a validated framework for systematically capturing human domain knowledge and codifying tacit expert knowledge into AI agents, demonstrating that non-experts can achieve expert-level outcomes in specialized domains.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å¢å¼ºLLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰çš„AIæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºä»¿çœŸæ•°æ®å¯è§†åŒ–é¢†åŸŸï¼Œå…¶ä¸­æ•´åˆäº†RAGç³»ç»Ÿï¼ˆç”¨äºä»£ç ç”Ÿæˆï¼‰ã€ä¸“å®¶è§„åˆ™ç¼–ç å’Œå¯è§†åŒ–è®¾è®¡åŸåˆ™ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿæ˜¾è‘—æå‡äº†è¾“å‡ºè´¨é‡ï¼ˆ206%æ”¹è¿›ï¼‰ï¼Œä½¿éä¸“å®¶ç”¨æˆ·ä¹Ÿèƒ½è¾¾åˆ°ä¸“å®¶çº§æˆæœï¼Œå¹¶éªŒè¯äº†å°†é¢†åŸŸçŸ¥è¯†ç³»ç»ŸåŒ–åµŒå…¥AIæ™ºèƒ½ä½“çš„æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.15124v1">Overcoming In-Memory Bottlenecks in Graph Foundation Models via Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Graph Foundation Models (GFMs) have emerged as a frontier in graph learning, which are expected to deliver transferable representations across diverse tasks. However, GFMs remain constrained by in-memory bottlenecks: they attempt to encode knowledge into model parameters, which limits semantic capacity, introduces heavy lossy compression with conflicts, and entangles graph representation with the knowledge in ways that hinder efficient adaptation, undermining scalability and interpretability. In this work,we propose RAG-GFM, a Retrieval-Augmented Generation aided Graph Foundation Model that offloads knowledge from parameters and complements parameterized learning. To externalize graph knowledge, we build a dual-modal unified retrieval module, where a semantic store from prefix-structured text and a structural store from centrality-based motif. To preserve heterogeneous information, we design a dual-view alignment objective that contrasts both modalities to capture both content and relational patterns. To enable efficient downstream adaptation, we perform in-context augmentation to enrich supporting instances with retrieved texts and motifs as contextual evidence. Extensive experiments on five benchmark graph datasets demonstrate that RAG-GFM consistently outperforms 13 state-of-the-art baselines in both cross-domain node and graph classification, achieving superior effectiveness and efficiency.</details></td><td><details><summary>å±•å¼€</summary>  </details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.14952v1">CorpusQA: A 10 Million Token Benchmark for Corpus-Level Analysis and Reasoning</a></td><td><details><summary>å±•å¼€</summary>While large language models now handle million-token contexts, their capacity for reasoning across entire document repositories remains largely untested. Existing benchmarks are inadequate, as they are mostly limited to single long texts or rely on a "sparse retrieval" assumption-that answers can be derived from a few relevant chunks. This assumption fails for true corpus-level analysis, where evidence is highly dispersed across hundreds of documents and answers require global integration, comparison, and statistical aggregation. To address this critical gap, we introduce CorpusQA, a new benchmark scaling up to 10 million tokens, generated via a novel data synthesis framework. By decoupling reasoning from textual representation, this framework creates complex, computation-intensive queries with programmatically guaranteed ground-truth answers, challenging systems to perform holistic reasoning over vast, unstructured text without relying on fallible human annotation. We further demonstrate the utility of our framework beyond evaluation, showing that fine-tuning on our synthesized data effectively enhances an LLM's general long-context reasoning capabilities. Extensive experiments reveal that even state-of-the-art long-context LLMs struggle as input length increases, and standard retrieval-augmented generation systems collapse entirely. Our findings indicate that memory-augmented agentic architectures offer a more robust alternative, suggesting a critical shift is needed from simply extending context windows to developing advanced architectures for global information synthesis.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è·¨æ–‡æ¡£åº“æ¨ç†æ—¶çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†æ–°çš„è¯„æµ‹åŸºå‡†CorpusQAæ¥æµ‹è¯•æ¨¡å‹åœ¨å¤§è§„æ¨¡ï¼ˆå¤šè¾¾1000ä¸‡tokenï¼‰ä¸Šä¸‹æ–‡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ç°æœ‰çš„RAGç³»ç»Ÿåœ¨å¤„ç†é«˜åº¦åˆ†æ•£çš„è¯æ®å’Œéœ€è¦å…¨å±€æ•´åˆçš„é—®é¢˜æ—¶ä¼šå®Œå…¨å¤±æ•ˆï¼Œè€ŒåŸºäºè®°å¿†å¢å¼ºçš„æ™ºèƒ½ä½“æ¶æ„å¯èƒ½æˆä¸ºæ›´å¯é çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæš—ç¤ºéœ€è¦ä»ç®€å•æ‰©å±•ä¸Šä¸‹æ–‡çª—å£è½¬å‘å¼€å‘æ›´å…ˆè¿›çš„ä¿¡æ¯åˆæˆæ¶æ„ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.14949v1">What Should I Cite? A RAG Benchmark for Academic Citation Prediction</a></td><td><details><summary>å±•å¼€</summary>With the rapid growth of Web-based academic publications, more and more papers are being published annually, making it increasingly difficult to find relevant prior work. Citation prediction aims to automatically suggest appropriate references, helping scholars navigate the expanding scientific literature. Here we present \textbf{CiteRAG}, the first comprehensive retrieval-augmented generation (RAG)-integrated benchmark for evaluating large language models on academic citation prediction, featuring a multi-level retrieval strategy, specialized retrievers, and generators. Our benchmark makes four core contributions: (1) We establish two instances of the citation prediction task with different granularity. Task 1 focuses on coarse-grained list-specific citation prediction, while Task 2 targets fine-grained position-specific citation prediction. To enhance these two tasks, we build a dataset containing 7,267 instances for Task 1 and 8,541 instances for Task 2, enabling comprehensive evaluation of both retrieval and generation. (2) We construct a three-level large-scale corpus with 554k papers spanning many major subfields, using an incremental pipeline. (3) We propose a multi-level hybrid RAG approach for citation prediction, fine-tuning embedding models with contrastive learning to capture complex citation relationships, paired with specialized generation models. (4) We conduct extensive experiments across state-of-the-art language models, including closed-source APIs, open-source models, and our fine-tuned generators, demonstrating the effectiveness of our framework. Our open-source toolkit enables reproducible evaluation and focuses on academic literature, providing the first comprehensive evaluation framework for citation prediction and serving as a methodological template for other scientific domains. Our source code and data are released at https://github.com/LQgdwind/CiteRAG.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†\textbf{CiteRAG}ï¼Œä¸€ä¸ªåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨å­¦æœ¯å¼•ç”¨é¢„æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æ–‡ç« æå‡ºäº†å¤šçº§æ£€ç´¢ç­–ç•¥ã€ä¸“ç”¨æ£€ç´¢å™¨å’Œç”Ÿæˆå™¨ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªä»»åŠ¡ï¼ˆç²—ç²’åº¦åˆ—è¡¨çº§å¼•ç”¨é¢„æµ‹å’Œç»†ç²’åº¦ä½ç½®çº§å¼•ç”¨é¢„æµ‹ï¼‰çš„æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜å¼€å‘äº†ä¸€ä¸ªåŒ…å«554kè®ºæ–‡çš„å¤§è§„æ¨¡è¯­æ–™åº“ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ··åˆRAGæ–¹æ³•ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å¾®è°ƒåµŒå…¥æ¨¡å‹ä»¥æ•æ‰å¤æ‚çš„å¼•ç”¨å…³ç³»ã€‚å®éªŒéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºå­¦æœ¯æ–‡çŒ®é¢†åŸŸçš„å¼•ç”¨é¢„æµ‹æä¾›äº†é¦–ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.14896v1">Language-Coupled Reinforcement Learning for Multilingual Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Multilingual retrieval-augmented generation (MRAG) requires models to effectively acquire and integrate beneficial external knowledge from multilingual collections. However, most existing studies employ a unitive process where queries of equivalent semantics across different languages are processed through a single-turn retrieval and subsequent optimization. Such a ``one-size-fits-all'' strategy is often suboptimal in multilingual settings, as the models occur to knowledge bias and conflict during the interaction with the search engine. To alleviate the issues, we propose LcRL, a multilingual search-augmented reinforcement learning framework that integrates a language-coupled Group Relative Policy Optimization into the policy and reward models. We adopt the language-coupled group sampling in the rollout module to reduce knowledge bias, and regularize an auxiliary anti-consistency penalty in the reward models to mitigate the knowledge conflict. Experimental results demonstrate that LcRL not only achieves competitive performance but is also appropriate for various practical scenarios such as constrained training data and retrieval over collections encompassing a large number of languages. Our code is available at https://github.com/Cherry-qwq/LcRL-Open.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šè¯­è¨€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆMRAGï¼‰æ¡†æ¶LcRLï¼Œé€šè¿‡ç»“åˆè¯­è¨€è€¦åˆçš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–å’Œå¼ºåŒ–å­¦ä¹ ï¼Œè§£å†³äº†å¤šè¯­è¨€ç¯å¢ƒä¸‹çŸ¥è¯†åå·®å’Œå†²çªçš„é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒLcRLåœ¨æ€§èƒ½ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œé€‚ç”¨äºè®­ç»ƒæ•°æ®æœ‰é™å’Œè·¨å¤šè¯­è¨€æ£€ç´¢ç­‰å®é™…åœºæ™¯ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.14662v1">Query-Efficient Agentic Graph Extraction Attacks on GraphRAG Systems</a></td><td><details><summary>å±•å¼€</summary>Graph-based retrieval-augmented generation (GraphRAG) systems construct knowledge graphs over document collections to support multi-hop reasoning. While prior work shows that GraphRAG responses may leak retrieved subgraphs, the feasibility of query-efficient reconstruction of the hidden graph structure remains unexplored under realistic query budgets. We study a budget-constrained black-box setting where an adversary adaptively queries the system to steal its latent entity-relation graph. We propose AGEA (Agentic Graph Extraction Attack), a framework that leverages a novelty-guided exploration-exploitation strategy, external graph memory modules, and a two-stage graph extraction pipeline combining lightweight discovery with LLM-based filtering. We evaluate AGEA on medical, agriculture, and literary datasets across Microsoft-GraphRAG and LightRAG systems. Under identical query budgets, AGEA significantly outperforms prior attack baselines, recovering up to 90% of entities and relationships while maintaining high precision. These results demonstrate that modern GraphRAG systems are highly vulnerable to structured, agentic extraction attacks, even under strict query limits.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†åŸºäºå›¾çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿï¼ˆGraphRAGï¼‰ä¸­çŸ¥è¯†å›¾è°±çš„ç»“æ„å®‰å…¨é£é™©ï¼Œæå‡ºäº†ä¸€ç§åä¸ºAGEAçš„æ”»å‡»æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æŸ¥è¯¢é¢„ç®—å—é™çš„æ¡ä»¶ä¸‹é«˜æ•ˆçªƒå–ç³»ç»Ÿå†…éƒ¨çš„å®ä½“-å…³ç³»å›¾ç»“æ„ã€‚é€šè¿‡å®éªŒéªŒè¯ï¼ŒAGEAåœ¨åŒ»ç–—ã€å†œä¸šå’Œæ–‡å­¦æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ”»å‡»æ–¹æ³•ï¼Œæ­ç¤ºäº†ç°ä»£GraphRAGç³»ç»Ÿå¯¹ç»“æ„åŒ–ä»£ç†æ”»å‡»çš„é«˜åº¦è„†å¼±æ€§ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2026-01-20
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2601.14123v1">A Systematic Analysis of Chunking Strategies for Reliable Question Answering</a></td><td><details><summary>å±•å¼€</summary>We study how document chunking choices impact the reliability of Retrieval-Augmented Generation (RAG) systems in industry. While practice often relies on heuristics, our end-to-end evaluation on Natural Questions systematically varies chunking method (token, sentence, semantic, code), chunk size, overlap, and context length. We use a standard industrial setup: SPLADE retrieval and a Mistral-8B generator. We derive actionable lessons for cost-efficient deployment: (i) overlap provides no measurable benefit and increases indexing cost; (ii) sentence chunking is the most cost-effective method, matching semantic chunking up to ~5k tokens; (iii) a "context cliff" reduces quality beyond ~2.5k tokens; and (iv) optimal context depends on the goal (semantic quality peaks at small contexts; exact match at larger ones).</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†æ–‡æ¡£åˆ†å—ï¼ˆchunkingï¼‰ç­–ç•¥å¯¹å·¥ä¸šç•ŒRAGç³»ç»Ÿå¯é æ€§çš„å½±å“ï¼Œé€šè¿‡å®éªŒè¯„ä¼°ä¸åŒåˆ†å—æ–¹æ³•ï¼ˆå¦‚åˆ†è¯ã€å¥å­ã€è¯­ä¹‰ç­‰ï¼‰ã€å¤§å°åŠä¸Šä¸‹æ–‡çš„æ•ˆåº”ï¼Œå¹¶æå‡ºäº†æˆæœ¬ä¼˜åŒ–å»ºè®®ï¼Œä¾‹å¦‚å¥å­åˆ†å—æœ€ç»æµã€é‡å æ— æ˜¾è‘—ç›Šå¤„ç­‰ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.14053v1">LLMOrbit: A Circular Taxonomy of Large Language Models -From Scaling Walls to Agentic AI Systems</a></td><td><details><summary>å±•å¼€</summary>The field of artificial intelligence has undergone a revolution from foundational Transformer architectures to reasoning-capable systems approaching human-level performance. We present LLMOrbit, a comprehensive circular taxonomy navigating the landscape of large language models spanning 2019-2025. This survey examines over 50 models across 15 organizations through eight interconnected orbital dimensions, documenting architectural innovations, training methodologies, and efficiency patterns defining modern LLMs, generative AI, and agentic systems. We identify three critical crises: (1) data scarcity (9-27T tokens depleted by 2026-2028), (2) exponential cost growth ($3M to $300M+ in 5 years), and (3) unsustainable energy consumption (22x increase), establishing the scaling wall limiting brute-force approaches. Our analysis reveals six paradigms breaking this wall: (1) test-time compute (o1, DeepSeek-R1 achieve GPT-4 performance with 10x inference compute), (2) quantization (4-8x compression), (3) distributed edge computing (10x cost reduction), (4) model merging, (5) efficient training (ORPO reduces memory 50%), and (6) small specialized models (Phi-4 14B matches larger models). Three paradigm shifts emerge: (1) post-training gains (RLHF, GRPO, pure RL contribute substantially, DeepSeek-R1 achieving 79.8% MATH), (2) efficiency revolution (MoE routing 18x efficiency, Multi-head Latent Attention 8x KV cache compression enables GPT-4-level performance at <$0.30/M tokens), and (3) democratization (open-source Llama 3 88.6% MMLU surpasses GPT-4 86.4%). We provide insights into techniques (RLHF, PPO, DPO, GRPO, ORPO), trace evolution from passive generation to tool-using agents (ReAct, RAG, multi-agent systems), and analyze post-training innovations.</details></td><td><details><summary>å±•å¼€</summary></details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.13719v1">Hierarchical Long Video Understanding with Audiovisual Entity Cohesion and Agentic Search</a></td><td><details><summary>å±•å¼€</summary>Long video understanding presents significant challenges for vision-language models due to extremely long context windows. Existing solutions relying on naive chunking strategies with retrieval-augmented generation, typically suffer from information fragmentation and a loss of global coherence. We present HAVEN, a unified framework for long-video understanding that enables coherent and comprehensive reasoning by integrating audiovisual entity cohesion and hierarchical video indexing with agentic search. First, we preserve semantic consistency by integrating entity-level representations across visual and auditory streams, while organizing content into a structured hierarchy spanning global summary, scene, segment, and entity levels. Then we employ an agentic search mechanism to enable dynamic retrieval and reasoning across these layers, facilitating coherent narrative reconstruction and fine-grained entity tracking. Extensive experiments demonstrate that our method achieves good temporal coherence, entity consistency, and retrieval efficiency, establishing a new state-of-the-art with an overall accuracy of 84.1% on LVBench. Notably, it achieves outstanding performance in the challenging reasoning category, reaching 80.1%. These results highlight the effectiveness of structured, multimodal reasoning for comprehensive and context-consistent understanding of long-form videos.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†HAVENæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆè§†å¬å®ä½“è¿è´¯æ€§å’Œåˆ†å±‚è§†é¢‘ç´¢å¼•ä¸ä»£ç†æœç´¢ï¼Œè§£å†³äº†é•¿è§†é¢‘ç†è§£ä¸­çš„ä¿¡æ¯ç¢ç‰‡åŒ–å’Œå…¨å±€ä¸€è‡´æ€§é—®é¢˜ï¼Œå¹¶åœ¨LVBenchä¸Šè¾¾åˆ°äº†84.1%çš„å‡†ç¡®ç‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.13476v1">A Unified Variational Imputation Framework for Electric Vehicle Charging Data Using Retrieval-Augmented Language Model</a></td><td><details><summary>å±•å¼€</summary>The reliability of data-driven applications in electric vehicle (EV) infrastructure, such as charging demand forecasting, hinges on the availability of complete, high-quality charging data. However, real-world EV datasets are often plagued by missing records, and existing imputation methods are ill-equipped for the complex, multimodal context of charging data, often relying on a restrictive one-model-per-station paradigm that ignores valuable inter-station correlations. To address these gaps, we develop a novel PRobabilistic variational imputation framework that leverages the power of large lAnguage models and retrIeval-augmented Memory (PRAIM). PRAIM employs a pre-trained language model to encode heterogeneous data, spanning time-series demand, calendar features, and geospatial context, into a unified, semantically rich representation. This is dynamically fortified by retrieval-augmented memory that retrieves relevant examples from the entire charging network, enabling a single, unified imputation model empowered by variational neural architecture to overcome data sparsity. Extensive experiments on four public datasets demonstrate that PRAIM significantly outperforms established baselines in both imputation accuracy and its ability to preserve the original data's statistical distribution, leading to substantial improvements in downstream forecasting performance.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºPRAIMçš„æ–°å‹æ¦‚ç‡å˜åˆ†æ’è¡¥æ¡†æ¶ï¼Œç”¨äºè§£å†³ç”µåŠ¨æ±½è½¦å……ç”µæ•°æ®ä¸­çš„ç¼ºå¤±è®°å½•é—®é¢˜ã€‚PRAIMç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ£€ç´¢å¢å¼ºè®°å¿†æŠ€æœ¯ï¼Œé€šè¿‡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ç¼–ç å¼‚æ„æ•°æ®ï¼ˆå¦‚æ—¶é—´åºåˆ—éœ€æ±‚ã€æ—¥å†ç‰¹å¾å’Œåœ°ç†ç©ºé—´ä¸Šä¸‹æ–‡ï¼‰ï¼Œå¹¶åˆ©ç”¨æ£€ç´¢å¢å¼ºè®°å¿†ä»æ•´ä¸ªå……ç”µç½‘ç»œä¸­æ£€ç´¢ç›¸å…³ç¤ºä¾‹ï¼Œä»è€Œæå‡æ•°æ®æ’è¡¥çš„å‡†ç¡®æ€§å’Œç»Ÿè®¡åˆ†å¸ƒä¸€è‡´æ€§ï¼Œæœ€ç»ˆæ˜¾è‘—æ”¹å–„ä¸‹æ¸¸é¢„æµ‹æ€§èƒ½ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2026-01-19
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2601.14123v1">A Systematic Analysis of Chunking Strategies for Reliable Question Answering</a></td><td><details><summary>å±•å¼€</summary>We study how document chunking choices impact the reliability of Retrieval-Augmented Generation (RAG) systems in industry. While practice often relies on heuristics, our end-to-end evaluation on Natural Questions systematically varies chunking method (token, sentence, semantic, code), chunk size, overlap, and context length. We use a standard industrial setup: SPLADE retrieval and a Mistral-8B generator. We derive actionable lessons for cost-efficient deployment: (i) overlap provides no measurable benefit and increases indexing cost; (ii) sentence chunking is the most cost-effective method, matching semantic chunking up to ~5k tokens; (iii) a "context cliff" reduces quality beyond ~2.5k tokens; and (iv) optimal context depends on the goal (semantic quality peaks at small contexts; exact match at larger ones).</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†æ–‡æ¡£åˆ†å—ï¼ˆchunkingï¼‰é€‰æ‹©å¯¹å·¥ä¸šç•ŒRAGç³»ç»Ÿå¯é æ€§çš„å½±å“ï¼Œé€šè¿‡ç«¯åˆ°ç«¯è¯„ä¼°ä¸åŒåˆ†å—æ–¹æ³•ã€å¤§å°ã€é‡å å’Œä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå‘ç°å¥å­åˆ†å—æœ€å…·æˆæœ¬æ•ˆç›Šï¼Œå¹¶æ­ç¤ºäº†ä¸Šä¸‹æ–‡é•¿åº¦å¯¹ç”Ÿæˆè´¨é‡çš„å½±å“è§„å¾‹ï¼Œä¸ºé«˜æ•ˆéƒ¨ç½²æä¾›äº†å®ç”¨å»ºè®®ï¼ˆå¦‚é‡å æ— æ˜¾è‘—æ”¶ç›Šã€è¯­ä¹‰è´¨é‡ä¸ç²¾ç¡®åŒ¹é…çš„ä¸Šä¸‹æ–‡éœ€æ±‚å·®å¼‚ç­‰ï¼‰ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.14053v1">LLMOrbit: A Circular Taxonomy of Large Language Models -From Scaling Walls to Agentic AI Systems</a></td><td><details><summary>å±•å¼€</summary>The field of artificial intelligence has undergone a revolution from foundational Transformer architectures to reasoning-capable systems approaching human-level performance. We present LLMOrbit, a comprehensive circular taxonomy navigating the landscape of large language models spanning 2019-2025. This survey examines over 50 models across 15 organizations through eight interconnected orbital dimensions, documenting architectural innovations, training methodologies, and efficiency patterns defining modern LLMs, generative AI, and agentic systems. We identify three critical crises: (1) data scarcity (9-27T tokens depleted by 2026-2028), (2) exponential cost growth ($3M to $300M+ in 5 years), and (3) unsustainable energy consumption (22x increase), establishing the scaling wall limiting brute-force approaches. Our analysis reveals six paradigms breaking this wall: (1) test-time compute (o1, DeepSeek-R1 achieve GPT-4 performance with 10x inference compute), (2) quantization (4-8x compression), (3) distributed edge computing (10x cost reduction), (4) model merging, (5) efficient training (ORPO reduces memory 50%), and (6) small specialized models (Phi-4 14B matches larger models). Three paradigm shifts emerge: (1) post-training gains (RLHF, GRPO, pure RL contribute substantially, DeepSeek-R1 achieving 79.8% MATH), (2) efficiency revolution (MoE routing 18x efficiency, Multi-head Latent Attention 8x KV cache compression enables GPT-4-level performance at <$0.30/M tokens), and (3) democratization (open-source Llama 3 88.6% MMLU surpasses GPT-4 86.4%). We provide insights into techniques (RLHF, PPO, DPO, GRPO, ORPO), trace evolution from passive generation to tool-using agents (ReAct, RAG, multi-agent systems), and analyze post-training innovations.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†LLMOrbitï¼Œä¸€ä¸ªå…¨é¢çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åˆ†ç±»ä½“ç³»ï¼Œæ¶µç›–äº†2019-2025å¹´çš„æ¨¡å‹å‘å±•ï¼Œåˆ†æäº†50å¤šä¸ªæ¨¡å‹åœ¨æ¶æ„åˆ›æ–°ã€è®­ç»ƒæ–¹æ³•å’Œæ•ˆç‡æ¨¡å¼ç­‰æ–¹é¢çš„ç‰¹ç‚¹ã€‚æ–‡ç« æŒ‡å‡ºäº†æ•°æ®ç¨€ç¼ºã€æˆæœ¬å¢é•¿å’Œèƒ½æºæ¶ˆè€—ä¸‰å¤§å±æœºï¼Œå¹¶æå‡ºäº†å…­ç§çªç ´æ€§èŒƒå¼ï¼ˆå¦‚æµ‹è¯•æ—¶è®¡ç®—ã€é‡åŒ–ã€åˆ†å¸ƒå¼è¾¹ç¼˜è®¡ç®—ç­‰ï¼‰å’Œä¸‰å¤§èŒƒå¼è½¬å˜ï¼ˆå¦‚è®­ç»ƒåå¢ç›Šã€æ•ˆç‡é©å‘½å’Œæ°‘ä¸»åŒ–ï¼‰ã€‚å…¶ä¸­ç‰¹åˆ«æåˆ°äº†RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ä½œä¸ºä»è¢«åŠ¨ç”Ÿæˆåˆ°å·¥å…·ä½¿ç”¨ä»£ç†ï¼ˆå¦‚ReActã€å¤šä»£ç†ç³»ç»Ÿï¼‰æ¼”å˜çš„ä¸€éƒ¨åˆ†ï¼Œå±•ç¤ºäº†å…¶åœ¨LLMå‘å±•ä¸­çš„é‡è¦æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.13719v1">Hierarchical Long Video Understanding with Audiovisual Entity Cohesion and Agentic Search</a></td><td><details><summary>å±•å¼€</summary>Long video understanding presents significant challenges for vision-language models due to extremely long context windows. Existing solutions relying on naive chunking strategies with retrieval-augmented generation, typically suffer from information fragmentation and a loss of global coherence. We present HAVEN, a unified framework for long-video understanding that enables coherent and comprehensive reasoning by integrating audiovisual entity cohesion and hierarchical video indexing with agentic search. First, we preserve semantic consistency by integrating entity-level representations across visual and auditory streams, while organizing content into a structured hierarchy spanning global summary, scene, segment, and entity levels. Then we employ an agentic search mechanism to enable dynamic retrieval and reasoning across these layers, facilitating coherent narrative reconstruction and fine-grained entity tracking. Extensive experiments demonstrate that our method achieves good temporal coherence, entity consistency, and retrieval efficiency, establishing a new state-of-the-art with an overall accuracy of 84.1% on LVBench. Notably, it achieves outstanding performance in the challenging reasoning category, reaching 80.1%. These results highlight the effectiveness of structured, multimodal reasoning for comprehensive and context-consistent understanding of long-form videos.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†HAVENæ¡†æ¶ï¼Œé’ˆå¯¹é•¿è§†é¢‘ç†è§£ä¸­çš„ä¿¡æ¯ç¢ç‰‡åŒ–å’Œå…¨å±€è¿è´¯æ€§é—®é¢˜ï¼Œé€šè¿‡ç»“åˆè§†å¬å®ä½“å‡èšå’Œåˆ†å±‚è§†é¢‘ç´¢å¼•ä¸ä»£ç†æœç´¢ï¼Œæ”¹è¿›äº†ä¼ ç»Ÿçš„åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„åˆ†å—ç­–ç•¥ã€‚è¯¥æ¡†æ¶é€šè¿‡è·¨æ¨¡æ€å®ä½“çº§è¡¨ç¤ºå’Œå±‚æ¬¡åŒ–å†…å®¹ç»„ç»‡ï¼Œå®ç°äº†åŠ¨æ€æ£€ç´¢å’Œæ¨ç†ï¼Œæ˜¾è‘—æå‡äº†é•¿è§†é¢‘ç†è§£çš„è¿è´¯æ€§ã€å®ä½“ä¸€è‡´æ€§å’Œæ£€ç´¢æ•ˆç‡ï¼Œåœ¨LVBenchä¸Šè¾¾åˆ°äº†84.1%çš„æ•´ä½“å‡†ç¡®ç‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.13476v1">A Unified Variational Imputation Framework for Electric Vehicle Charging Data Using Retrieval-Augmented Language Model</a></td><td><details><summary>å±•å¼€</summary>The reliability of data-driven applications in electric vehicle (EV) infrastructure, such as charging demand forecasting, hinges on the availability of complete, high-quality charging data. However, real-world EV datasets are often plagued by missing records, and existing imputation methods are ill-equipped for the complex, multimodal context of charging data, often relying on a restrictive one-model-per-station paradigm that ignores valuable inter-station correlations. To address these gaps, we develop a novel PRobabilistic variational imputation framework that leverages the power of large lAnguage models and retrIeval-augmented Memory (PRAIM). PRAIM employs a pre-trained language model to encode heterogeneous data, spanning time-series demand, calendar features, and geospatial context, into a unified, semantically rich representation. This is dynamically fortified by retrieval-augmented memory that retrieves relevant examples from the entire charging network, enabling a single, unified imputation model empowered by variational neural architecture to overcome data sparsity. Extensive experiments on four public datasets demonstrate that PRAIM significantly outperforms established baselines in both imputation accuracy and its ability to preserve the original data's statistical distribution, leading to substantial improvements in downstream forecasting performance.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºPRAIMçš„æ–°å‹æ¦‚ç‡å˜åˆ†æ’è¡¥æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œæ£€ç´¢å¢å¼ºè®°å¿†æŠ€æœ¯ï¼ˆRAGï¼‰ï¼Œè§£å†³ç”µåŠ¨æ±½è½¦å……ç”µæ•°æ®ç¼ºå¤±é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ç¼–ç å¤šæ¨¡æ€æ•°æ®ï¼Œå¹¶é€šè¿‡æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡å¢å¼ºæ’è¡¥å‡†ç¡®æ€§ï¼Œæ˜¾è‘—æå‡äº†æ•°æ®å®Œæ•´æ€§å’Œä¸‹æ¸¸é¢„æµ‹æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.13388v1">Structured Insight from Unstructured Data: Large Language Models for SDOH-Driven Diabetes Risk Prediction</a></td><td><details><summary>å±•å¼€</summary>Social determinants of health (SDOH) play a critical role in Type 2 Diabetes (T2D) management but are often absent from electronic health records and risk prediction models. Most individual-level SDOH data is collected through structured screening tools, which lack the flexibility to capture the complexity of patient experiences and unique needs of a clinic's population. This study explores the use of large language models (LLMs) to extract structured SDOH information from unstructured patient life stories and evaluate the predictive value of both the extracted features and the narratives themselves for assessing diabetes control. We collected unstructured interviews from 65 T2D patients aged 65 and older, focused on their lived experiences, social context, and diabetes management. These narratives were analyzed using LLMs with retrieval-augmented generation to produce concise, actionable qualitative summaries for clinical interpretation and structured quantitative SDOH ratings for risk prediction modeling. The structured SDOH ratings were used independently and in combination with traditional laboratory biomarkers as inputs to linear and tree-based machine learning models (Ridge, Lasso, Random Forest, and XGBoost) to demonstrate how unstructured narrative data can be applied in conventional risk prediction workflows. Finally, we evaluated several LLMs on their ability to predict a patient's level of diabetes control (low, medium, high) directly from interview text with A1C values redacted. LLMs achieved 60% accuracy in predicting diabetes control levels from interview text. This work demonstrates how LLMs can translate unstructured SDOH-related data into structured insights, offering a scalable approach to augment clinical risk models and decision-making.</details></td><td><details><summary>å±•å¼€</summary>è¯¥ç ”ç©¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œä»65å2å‹ç³–å°¿ç—…æ‚£è€…çš„éç»“æ„åŒ–è®¿è°ˆä¸­æå–ç»“æ„åŒ–ç¤¾ä¼šå¥åº·å†³å®šå› ç´ ï¼ˆSDOHï¼‰ä¿¡æ¯ï¼Œç”Ÿæˆä¸´åºŠå¯è§£é‡Šçš„å®šæ€§æ‘˜è¦å’Œå®šé‡è¯„åˆ†ï¼Œå¹¶å°†å…¶ç”¨äºç³–å°¿ç—…æ§åˆ¶é£é™©é¢„æµ‹æ¨¡å‹ï¼ŒåŒæ—¶éªŒè¯äº†LLMsç›´æ¥åŸºäºè®¿è°ˆæ–‡æœ¬é¢„æµ‹ç³–å°¿ç—…æ§åˆ¶æ°´å¹³çš„æ½œåŠ›ï¼ˆå‡†ç¡®ç‡60%ï¼‰ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.13251v1">Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in a 15-Million Node Turkish Synonym Graph</a></td><td><details><summary>å±•å¼€</summary>Neural embeddings have a notorious blind spot: they can't reliably tell synonyms apart from antonyms. Consequently, increasing similarity thresholds often fails to prevent opposites from being grouped together. We've built a large-scale semantic clustering system specifically designed to tackle this problem head on. Our pipeline chews through 15 million lexical items, evaluates a massive 520 million potential relationships, and ultimately generates 2.9 million high-precision semantic clusters. The system makes three primary contributions. First, we introduce a labeled dataset of 843,000 concept pairs spanning synonymy, antonymy, and co-hyponymy, constructed via Gemini 2.5-Flash LLM augmentation and verified using human-curated dictionary resources. Second, we propose a specialized three-way semantic relation discriminator that achieves 90% macro-F1, enabling robust disambiguation beyond raw embedding similarity. Third, we introduce a novel soft-to-hard clustering algorithm that mitigates semantic drift preventing erroneous transitive chains (e.g., hot -> spicy -> pain -> depression) while simultaneously resolving polysemy. Our approach employs a topology-aware two-stage expansion-pruning procedure with topological voting, ensuring that each term is assigned to exactly one semantically coherent cluster. The resulting resource enables high-precision semantic search and retrieval-augmented generation, particularly for morphologically rich and low-resource languages where existing synonym databases remain sparse.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤§è§„æ¨¡è¯­ä¹‰èšç±»é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡æ„å»ºæ ‡æ³¨æ•°æ®é›†ã€å¼€å‘ä¸‰å‘è¯­ä¹‰å…³ç³»åˆ¤åˆ«å™¨ä»¥åŠæ–°é¢–çš„è½¯ç¡¬èšç±»ç®—æ³•ï¼Œæœ‰æ•ˆåŒºåˆ†åŒä¹‰è¯ä¸åä¹‰è¯å¹¶ç”Ÿæˆé«˜ç²¾åº¦è¯­ä¹‰èšç±»ã€‚å…¶æˆæœç‰¹åˆ«é€‚ç”¨äºå½¢æ€ä¸°å¯Œå’Œä½èµ„æºè¯­è¨€çš„é«˜ç²¾åº¦è¯­ä¹‰æœç´¢åŠæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.13240v1">KOCO-BENCH: Can Large Language Models Leverage Domain Knowledge in Software Development?</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) excel at general programming but struggle with domain-specific software development, necessitating domain specialization methods for LLMs to learn and utilize domain knowledge and data. However, existing domain-specific code benchmarks cannot evaluate the effectiveness of domain specialization methods, which focus on assessing what knowledge LLMs possess rather than how they acquire and apply new knowledge, lacking explicit knowledge corpora for developing domain specialization methods. To this end, we present KOCO-BENCH, a novel benchmark designed for evaluating domain specialization methods in real-world software development. KOCO-BENCH contains 6 emerging domains with 11 software frameworks and 25 projects, featuring curated knowledge corpora alongside multi-granularity evaluation tasks including domain code generation (from function-level to project-level with rigorous test suites) and domain knowledge understanding (via multiple-choice Q&A). Unlike previous benchmarks that only provide test sets for direct evaluation, KOCO-BENCH requires acquiring and applying diverse domain knowledge (APIs, rules, constraints, etc.) from knowledge corpora to solve evaluation tasks. Our evaluations reveal that KOCO-BENCH poses significant challenges to state-of-the-art LLMs. Even with domain specialization methods (e.g., SFT, RAG, kNN-LM) applied, improvements remain marginal. Best-performing coding agent, Claude Code, achieves only 34.2%, highlighting the urgent need for more effective domain specialization methods. We release KOCO-BENCH, evaluation code, and baselines to advance further research at https://github.com/jiangxxxue/KOCO-bench.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†KOCO-BENCHï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°é¢†åŸŸä¸“ä¸šåŒ–æ–¹æ³•ï¼ˆåŒ…æ‹¬RAGï¼‰çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–6ä¸ªæ–°å…´é¢†åŸŸå’Œå¤šä¸ªè½¯ä»¶æ¡†æ¶ï¼Œé€šè¿‡å¤šç²’åº¦ä»»åŠ¡ï¼ˆå¦‚ä»£ç ç”Ÿæˆå’ŒçŸ¥è¯†ç†è§£ï¼‰æµ‹è¯•å¤§è¯­è¨€æ¨¡å‹è·å–ä¸åº”ç”¨é¢†åŸŸçŸ¥è¯†çš„èƒ½åŠ›ï¼Œå¹¶æŒ‡å‡ºç°æœ‰æ–¹æ³•ï¼ˆå¦‚RAGï¼‰æ•ˆæœæœ‰é™ï¼ŒäºŸéœ€æ”¹è¿›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.13227v1">Insider Knowledge: How Much Can RAG Systems Gain from Evaluation Secrets?</a></td><td><details><summary>å±•å¼€</summary>RAG systems are increasingly evaluated and optimized using LLM judges, an approach that is rapidly becoming the dominant paradigm for system assessment. Nugget-based approaches in particular are now embedded not only in evaluation frameworks but also in the architectures of RAG systems themselves. While this integration can lead to genuine improvements, it also creates a risk of faulty measurements due to circularity. In this paper, we investigate this risk through comparative experiments with nugget-based RAG systems, including Ginger and Crucible, against strong baselines such as GPT-Researcher. By deliberately modifying Crucible to generate outputs optimized for an LLM judge, we show that near-perfect evaluation scores can be achieved when elements of the evaluation - such as prompt templates or gold nuggets - are leaked or can be predicted. Our results highlight the importance of blind evaluation settings and methodological diversity to guard against mistaking metric overfitting for genuine system progress.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æ¢è®¨äº†åŸºäºLLMè¯„ä¼°çš„RAGç³»ç»Ÿä¸­å­˜åœ¨çš„å¾ªç¯æ€§é£é™©ï¼Œé€šè¿‡å®éªŒï¼ˆå¦‚ä¿®æ”¹Crucibleç³»ç»Ÿä¼˜åŒ–LLMè¯„åˆ¤æŒ‡æ ‡ï¼‰æ­ç¤ºè¯„ä¼°åˆ†æ•°å¯èƒ½å› æç¤ºæ¨¡æ¿æˆ–é»„é‡‘ç‰‡æ®µæ³„éœ²è€Œè™šé«˜ï¼Œå¼ºè°ƒç›²æµ‹å’Œæ–¹æ³•å¤šæ ·æ€§å¯¹é¿å…æŒ‡æ ‡è¿‡æ‹Ÿåˆçš„é‡è¦æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.13222v1">Incorporating Q&A Nuggets into Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>RAGE systems integrate ideas from automatic evaluation (E) into Retrieval-augmented Generation (RAG). As one such example, we present Crucible, a Nugget-Augmented Generation System that preserves explicit citation provenance by constructing a bank of Q&A nuggets from retrieved documents and uses them to guide extraction, selection, and report generation. Reasoning on nuggets avoids repeated information through clear and interpretable Q&A semantics - instead of opaque cluster abstractions - while maintaining citation provenance throughout the entire generation process. Evaluated on the TREC NeuCLIR 2024 collection, our Crucible system substantially outperforms Ginger, a recent nugget-based RAG system, in nugget recall, density, and citation grounding.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRAGEï¼ˆåœ¨RAGä¸­é›†æˆè‡ªåŠ¨è¯„ä¼°æ€æƒ³ï¼‰çš„ç³»ç»Ÿï¼Œå…¶å…·ä½“å®ç°ä¸ºCrucibleã€‚è¯¥ç³»ç»Ÿé€šè¿‡ä»æ£€ç´¢æ–‡æ¡£ä¸­æ„å»ºé—®ç­”ä¿¡æ¯å—ï¼ˆnuggetï¼‰åº“æ¥ä¿ç•™æ˜¾å¼å¼•ç”¨æ¥æºï¼Œå¹¶åˆ©ç”¨è¿™äº›ä¿¡æ¯å—æŒ‡å¯¼æå–ã€é€‰æ‹©å’ŒæŠ¥å‘Šç”Ÿæˆã€‚ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•ï¼Œå®ƒåœ¨æ•´ä¸ªç”Ÿæˆè¿‡ç¨‹ä¸­é€šè¿‡æ¸…æ™°çš„é—®ç­”è¯­ä¹‰ä¿æŒå¯è§£é‡Šæ€§ï¼Œå¹¶åœ¨TREC NeuCLIR 2024æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†å—å¬å›ç‡ã€å¯†åº¦å’Œå¼•ç”¨å‡†ç¡®æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.13122v1">Responsible AI for General-Purpose Systems: Overview, Challenges, and A Path Forward</a></td><td><details><summary>å±•å¼€</summary>Modern general-purpose AI systems made using large language and vision models, are capable of performing a range of tasks like writing text articles, generating and debugging codes, querying databases, and translating from one language to another, which has made them quite popular across industries. However, there are risks like hallucinations, toxicity, and stereotypes in their output that make them untrustworthy. We review various risks and vulnerabilities of modern general-purpose AI along eight widely accepted responsible AI (RAI) principles (fairness, privacy, explainability, robustness, safety, truthfulness, governance, and sustainability) and compare how they are non-existent or less severe and easily mitigable in traditional task-specific counterparts. We argue that this is due to the non-deterministically high Degree of Freedom in output (DoFo) of general-purpose AI (unlike the deterministically constant or low DoFo of traditional task-specific AI systems), and there is a need to rethink our approach to RAI for general-purpose AI. Following this, we derive C2V2 (Control, Consistency, Value, Veracity) desiderata to meet the RAI requirements for future general-purpose AI systems, and discuss how recent efforts in AI alignment, retrieval-augmented generation, reasoning enhancements, etc. fare along one or more of the desiderata. We believe that the goal of developing responsible general-purpose AI can be achieved by formally modeling application- or domain-dependent RAI requirements along C2V2 dimensions, and taking a system design approach to suitably combine various techniques to meet the desiderata.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ç°ä»£é€šç”¨AIç³»ç»Ÿåœ¨è´Ÿè´£AIï¼ˆRAIï¼‰åŸåˆ™ä¸‹é¢ä¸´çš„é£é™©å’ŒæŒ‘æˆ˜ï¼Œå¦‚å¹»è§‰ã€æ¯’æ€§å’Œåˆ»æ¿å°è±¡ï¼Œå¹¶æå‡ºé€šè¿‡æ§åˆ¶ã€ä¸€è‡´æ€§ã€ä»·å€¼å’ŒçœŸå®æ€§ï¼ˆC2V2ï¼‰ç­‰æ ‡å‡†æ¥æ”¹è¿›ã€‚æ–‡ä¸­æåˆ°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­‰æŠ€æœ¯å¯ä½œä¸ºæ»¡è¶³éƒ¨åˆ†RAIéœ€æ±‚çš„è§£å†³æ–¹æ¡ˆä¹‹ä¸€ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.13115v1">Agentic Conversational Search with Contextualized Reasoning via Reinforcement Learning</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) have become a popular interface for human-AI interaction, supporting information seeking and task assistance through natural, multi-turn dialogue. To respond to users within multi-turn dialogues, the context-dependent user intent evolves across interactions, requiring contextual interpretation, query reformulation, and dynamic coordination between retrieval and generation. Existing studies usually follow static rewrite, retrieve, and generate pipelines, which optimize different procedures separately and overlook the mixed-initiative action optimization simultaneously. Although the recent developments in deep search agents demonstrate the effectiveness in jointly optimizing retrieval and generation via reasoning, these approaches focus on single-turn scenarios, which might lack the ability to handle multi-turn interactions. We introduce a conversational agent that interleaves search and reasoning across turns, enabling exploratory and adaptive behaviors learned through reinforcement learning (RL) training with tailored rewards towards evolving user goals. The experimental results across four widely used conversational benchmarks demonstrate the effectiveness of our methods by surpassing several existing strong baselines.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹å¯¹è¯ä»£ç†ï¼Œèƒ½åœ¨å¤šè½®å¯¹è¯ä¸­äº¤æ›¿è¿›è¡Œæ£€ç´¢å’Œç”Ÿæˆï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒé€‚åº”åŠ¨æ€ç”¨æˆ·æ„å›¾ï¼Œè¶…è¶Šé™æ€æ£€ç´¢ç”Ÿæˆæµç¨‹ï¼Œå¹¶åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰åŸºçº¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.13105v1">Leveraging Lora Fine-Tuning and Knowledge Bases for Construction Identification</a></td><td><details><summary>å±•å¼€</summary>This study investigates the automatic identification of the English ditransitive construction by integrating LoRA-based fine-tuning of a large language model with a Retrieval-Augmented Generation (RAG) framework.A binary classification task was conducted on annotated data from the British National Corpus. Results demonstrate that a LoRA-fine-tuned Qwen3-8B model significantly outperformed both a native Qwen3-MAX model and a theory-only RAG system. Detailed error analysis reveals that fine-tuning shifts the model's judgment from a surface-form pattern matching towards a more semantically grounded understanding based.</details></td><td><details><summary>å±•å¼€</summary>è¯¥ç ”ç©¶é€šè¿‡ç»“åˆLoRAå¾®è°ƒçš„å¤§è¯­è¨€æ¨¡å‹ä¸RAGæ¡†æ¶ï¼Œè‡ªåŠ¨è¯†åˆ«è‹±è¯­åŒåŠç‰©ç»“æ„ï¼Œå®éªŒè¡¨æ˜å¾®è°ƒåçš„Qwen3-8Bæ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¼˜äºåŸç”Ÿæ¨¡å‹å’Œçº¯ç†è®ºRAGç³»ç»Ÿï¼Œä¸”é”™è¯¯åˆ†ææ˜¾ç¤ºå¾®è°ƒä½¿æ¨¡å‹ä»è¡¨å±‚æ¨¡å¼åŒ¹é…è½¬å‘è¯­ä¹‰ç†è§£ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.12991v1">RAGExplorer: A Visual Analytics System for the Comparative Diagnosis of RAG Systems</a></td><td><details><summary>å±•å¼€</summary>The advent of Retrieval-Augmented Generation (RAG) has significantly enhanced the ability of Large Language Models (LLMs) to produce factually accurate and up-to-date responses. However, the performance of a RAG system is not determined by a single component but emerges from a complex interplay of modular choices, such as embedding models and retrieval algorithms. This creates a vast and often opaque configuration space, making it challenging for developers to understand performance trade-offs and identify optimal designs. To address this challenge, we present RAGExplorer, a visual analytics system for the systematic comparison and diagnosis of RAG configurations. RAGExplorer guides users through a seamless macro-to-micro analytical workflow. Initially, it empowers developers to survey the performance landscape across numerous configurations, allowing for a high-level understanding of which design choices are most effective. For a deeper analysis, the system enables users to drill down into individual failure cases, investigate how differences in retrieved information contribute to errors, and interactively test hypotheses by manipulating the provided context to observe the resulting impact on the generated answer. We demonstrate the effectiveness of RAGExplorer through detailed case studies and user studies, validating its ability to empower developers in navigating the complex RAG design space. Our code and user guide are publicly available at https://github.com/Thymezzz/RAGExplorer.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ä»‹ç»äº†RAGExplorerï¼Œä¸€ä¸ªç”¨äºç³»ç»Ÿæ¯”è¾ƒå’Œè¯Šæ–­RAGé…ç½®çš„å¯è§†åŒ–åˆ†æç³»ç»Ÿã€‚å®ƒé€šè¿‡å®è§‚åˆ°å¾®è§‚çš„åˆ†æå·¥ä½œæµå¸®åŠ©å¼€å‘è€…ç†è§£ä¸åŒè®¾è®¡é€‰æ‹©ï¼ˆå¦‚åµŒå…¥æ¨¡å‹å’Œæ£€ç´¢ç®—æ³•ï¼‰å¯¹RAGæ€§èƒ½çš„å½±å“ï¼Œæ”¯æŒæ€§èƒ½è¯„ä¼°ã€é”™è¯¯æ¡ˆä¾‹è¯Šæ–­åŠäº¤äº’å¼å‡è®¾æµ‹è¯•ï¼Œå¹¶é€šè¿‡æ¡ˆä¾‹å’Œç”¨æˆ·ç ”ç©¶éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.12974v1">Bridging the Knowledge-Action Gap by Evaluating LLMs in Dynamic Dental Clinical Scenarios</a></td><td><details><summary>å±•å¼€</summary>The transition of Large Language Models (LLMs) from passive knowledge retrievers to autonomous clinical agents demands a shift in evaluation-from static accuracy to dynamic behavioral reliability. To explore this boundary in dentistry, a domain where high-quality AI advice uniquely empowers patient-participatory decision-making, we present the Standardized Clinical Management & Performance Evaluation (SCMPE) benchmark, which comprehensively assesses performance from knowledge-oriented evaluations (static objective tasks) to workflow-based simulations (multi-turn simulated patient interactions). Our analysis reveals that while models demonstrate high proficiency in static objective tasks, their performance precipitates in dynamic clinical dialogues, identifying that the primary bottleneck lies not in knowledge retention, but in the critical challenges of active information gathering and dynamic state tracking. Mapping "Guideline Adherence" versus "Decision Quality" reveals a prevalent "High Efficacy, Low Safety" risk in general models. Furthermore, we quantify the impact of Retrieval-Augmented Generation (RAG). While RAG mitigates hallucinations in static tasks, its efficacy in dynamic workflows is limited and heterogeneous, sometimes causing degradation. This underscores that external knowledge alone cannot bridge the reasoning gap without domain-adaptive pre-training. This study empirically charts the capability boundaries of dental LLMs, providing a roadmap for bridging the gap between standardized knowledge and safe, autonomous clinical practice.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç‰™ç§‘ä¸´åºŠé¢†åŸŸçš„åº”ç”¨ï¼Œä»é™æ€çŸ¥è¯†è¯„ä¼°åˆ°åŠ¨æ€ä¸´åºŠå¯¹è¯è¡Œä¸ºå¯é æ€§çš„è½¬å˜ï¼Œå¹¶é€šè¿‡SCMPEåŸºå‡†æµ‹è¯•æ­ç¤ºäº†æ¨¡å‹åœ¨åŠ¨æ€ä¿¡æ¯æœé›†å’ŒçŠ¶æ€è¿½è¸ªæ–¹é¢çš„ç“¶é¢ˆã€‚ç ”ç©¶ç‰¹åˆ«é‡åŒ–äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„ä½œç”¨ï¼Œå‘ç°RAGè™½èƒ½åœ¨é™æ€ä»»åŠ¡ä¸­å‡å°‘å¹»è§‰ï¼Œä½†åœ¨åŠ¨æ€ä¸´åºŠå·¥ä½œæµä¸­æ•ˆæœæœ‰é™ä¸”ä¸ç¨³å®šï¼Œè¡¨æ˜ä»…ä¾èµ–å¤–éƒ¨çŸ¥è¯†æ— æ³•å¼¥è¡¥é¢†åŸŸé€‚åº”æ€§æ¨ç†çš„ä¸è¶³ï¼Œéœ€ç»“åˆé¢†åŸŸé¢„è®­ç»ƒæ‰èƒ½æå‡æ¨¡å‹ä¸´åºŠå®è·µçš„å®‰å…¨æ€§å’Œè‡ªä¸»æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.12945v1">A Component-Based Survey of Interactions between Large Language Models and Multi-Armed Bandits</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) have become powerful and widely used systems for language understanding and generation, while multi-armed bandit (MAB) algorithms provide a principled framework for adaptive decision-making under uncertainty. This survey explores the potential at the intersection of these two fields. As we know, it is the first survey to systematically review the bidirectional interaction between large language models and multi-armed bandits at the component level. We highlight the bidirectional benefits: MAB algorithms address critical LLM challenges, spanning from pre-training to retrieval-augmented generation (RAG) and personalization. Conversely, LLMs enhance MAB systems by redefining core components such as arm definition and environment modeling, thereby improving decision-making in sequential tasks. We analyze existing LLM-enhanced bandit systems and bandit-enhanced LLM systems, providing insights into their design, methodologies, and performance. Key challenges and representative findings are identified to help guide future research. An accompanying GitHub repository that indexes relevant literature is available at https://github.com/bucky1119/Awesome-LLM-Bandit-Interaction.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸å¤šè‡‚è€è™æœºï¼ˆMABï¼‰ç®—æ³•çš„åŒå‘äº¤äº’ï¼Œå…¶ä¸­MABç®—æ³•è¢«ç”¨äºè§£å†³LLMsåœ¨é¢„è®­ç»ƒã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åŠä¸ªæ€§åŒ–ç­‰å…³é”®ç¯èŠ‚çš„æŒ‘æˆ˜ï¼ŒåŒæ—¶LLMsä¹Ÿä¼˜åŒ–äº†MABç³»ç»Ÿçš„å†³ç­–ç»„ä»¶ã€‚ç ”ç©¶ç³»ç»Ÿåˆ†æäº†ç°æœ‰åŒå‘å¢å¼ºç³»ç»Ÿçš„è®¾è®¡ä¸æ€§èƒ½ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶æ–¹å‘ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.12921v1">Injecting Knowledge from Social Science Journals to Improve Indonesian Cultural Understanding by LLMs</a></td><td><details><summary>å±•å¼€</summary>Recently there have been intensifying efforts to improve the understanding of Indonesian cultures by large language models (LLMs). An attractive source of cultural knowledge that has been largely overlooked is local journals of social science, which likely contain substantial cultural studies from a native perspective. We present a novel text dataset of journal article passages, created from 151 open-source Indonesian social science journals, called IndoSoSci. We demonstrate an effective recipe for injecting Indonesian cultural knowledge therein into LLMs: extracting the facts related to Indonesian culture, and apply retrieval-augmented generation (RAG) with LLM-generated hypothetical documents as queries during retrieval. The proposed recipe yields strong performance gains over several strong baselines on the IndoCulture benchmark. Additionally, by combining IndoSoSci with Indonesian Wikipedia, we set a new state-of-the-art accuracy on the IndoCulture benchmark.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºIndoSoSciçš„å°å°¼ç¤¾ä¼šç§‘å­¦æœŸåˆŠæ–‡æœ¬æ•°æ®é›†ï¼Œå¹¶é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œç»“åˆLLMç”Ÿæˆçš„å‡è®¾æ–‡æ¡£ä½œä¸ºæ£€ç´¢æŸ¥è¯¢ï¼Œæœ‰æ•ˆå°†å°å°¼æ–‡åŒ–çŸ¥è¯†æ³¨å…¥å¤§è¯­è¨€æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†åœ¨IndoCultureåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.12904v1">From Prefix Cache to Fusion RAG Cache: Accelerating LLM Inference in Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation enhances Large Language Models by integrating external knowledge, which reduces hallucinations but increases prompt length. This increase leads to higher computational costs and longer Time to First Token (TTFT). To mitigate this issue, existing solutions aim to reuse the preprocessed KV cache of each retrieved chunk to accelerate RAG. However, the lack of cross-chunk contextual information leads to a significant drop in generation quality, leaving the potential benefits of KV cache reuse largely unfulfilled. The challenge lies in how to reuse the precomputed KV cache of chunks while preserving generation quality. We propose FusionRAG, a novel inference framework that optimizes both the preprocessing and reprocessing stages of RAG. In the offline preprocessing stage, we embed information from other related text chunks into each chunk, while in the online reprocessing stage, we recompute the KV cache for tokens that the model focuses on. As a result, we achieve a better trade-off between generation quality and efficiency. According to our experiments, FusionRAG significantly improves generation quality at the same recomputation ratio compared to previous state-of-the-art solutions. By recomputing fewer than 15% of the tokens, FusionRAG achieves up to 70% higher normalized F1 scores than baselines and reduces TTFT by 2.66x-9.39x compared to Full Attention.</details></td><td><details><summary>å±•å¼€</summary></details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.12779v1">Open Vocabulary Panoptic Segmentation With Retrieval Augmentation</a></td><td><details><summary>å±•å¼€</summary>Given an input image and set of class names, panoptic segmentation aims to label each pixel in an image with class labels and instance labels. In comparison, Open Vocabulary Panoptic Segmentation aims to facilitate the segmentation of arbitrary classes according to user input. The challenge is that a panoptic segmentation system trained on a particular dataset typically does not generalize well to unseen classes beyond the training data. In this work, we propose RetCLIP, a retrieval-augmented panoptic segmentation method that improves the performance of unseen classes. In particular, we construct a masked segment feature database using paired image-text data. At inference time, we use masked segment features from the input image as query keys to retrieve similar features and associated class labels from the database. Classification scores for the masked segment are assigned based on the similarity between query features and retrieved features. The retrieval-based classification scores are combined with CLIP-based scores to produce the final output. We incorporate our solution with a previous SOTA method (FC-CLIP). When trained on COCO, the proposed method demonstrates 30.9 PQ, 19.3 mAP, 44.0 mIoU on the ADE20k dataset, achieving +4.5 PQ, +2.5 mAP, +10.0 mIoU absolute improvement over the baseline.</details></td><td><details><summary>å±•å¼€</summary></details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.12658v1">Augmenting Question Answering with A Hybrid RAG Approach</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has emerged as a powerful technique for enhancing the quality of responses in Question-Answering (QA) tasks. However, existing approaches often struggle with retrieving contextually relevant information, leading to incomplete or suboptimal answers. In this paper, we introduce Structured-Semantic RAG (SSRAG), a hybrid architecture that enhances QA quality by integrating query augmentation, agentic routing, and a structured retrieval mechanism combining vector and graph based techniques with context unification. By refining retrieval processes and improving contextual grounding, our approach improves both answer accuracy and informativeness. We conduct extensive evaluations on three popular QA datasets, TruthfulQA, SQuAD and WikiQA, across five Large Language Models (LLMs), demonstrating that our proposed approach consistently improves response quality over standard RAG implementations.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºStructured-Semantic RAG (SSRAG)çš„æ··åˆæ¶æ„ï¼Œé€šè¿‡ç»“åˆæŸ¥è¯¢å¢å¼ºã€æ™ºèƒ½è·¯ç”±ä»¥åŠèåˆå‘é‡å’Œå›¾æ£€ç´¢æŠ€æœ¯çš„ç»“æ„åŒ–æ£€ç´¢æœºåˆ¶ï¼Œæ”¹è¿›äº†RAGåœ¨é—®ç­”ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œæå‡äº†å›ç­”çš„å‡†ç¡®æ€§å’Œä¿¡æ¯ä¸°å¯Œåº¦ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.12641v1">STEP-LLM: Generating CAD STEP Models from Natural Language with Large Language Models</a></td><td><details><summary>å±•å¼€</summary>Computer-aided design (CAD) is vital to modern manufacturing, yet model creation remains labor-intensive and expertise-heavy. To enable non-experts to translate intuitive design intent into manufacturable artifacts, recent large language models-based text-to-CAD efforts focus on command sequences or script-based formats like CadQuery. However, these formats are kernel-dependent and lack universality for manufacturing. In contrast, the Standard for the Exchange of Product Data (STEP, ISO 10303) file is a widely adopted, neutral boundary representation (B-rep) format directly compatible with manufacturing, but its graph-structured, cross-referenced nature poses unique challenges for auto-regressive LLMs. To address this, we curate a dataset of ~40K STEP-caption pairs and introduce novel preprocessing tailored for the graph-structured format of STEP, including a depth-first search-based reserialization that linearizes cross-references while preserving locality and chain-of-thought(CoT)-style structural annotations that guide global coherence. We integrate retrieval-augmented generation to ground predictions in relevant examples for supervised fine-tuning, and refine generation quality through reinforcement learning with a specific Chamfer Distance-based geometric reward. Experiments demonstrate consistent gains of our STEP-LLM in geometric fidelity over the Text2CAD baseline, with improvements arising from multiple stages of our framework: the RAG module substantially enhances completeness and renderability, the DFS-based reserialization strengthens overall accuracy, and the RL further reduces geometric discrepancy. Both metrics and visual comparisons confirm that STEP-LLM generates shapes with higher fidelity than Text2CAD. These results show the feasibility of LLM-driven STEP model generation from natural language, showing its potential to democratize CAD design for manufacturing.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–‡æœ¬åˆ°CADæ¨¡å‹ç”Ÿæˆæ–¹æ³•STEP-LLMï¼Œé’ˆå¯¹é€šç”¨çš„STEPæ–‡ä»¶æ ¼å¼ï¼ˆISO 10303ï¼‰ï¼Œé€šè¿‡å¼•å…¥æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ä»ç›¸å…³ç¤ºä¾‹ä¸­æ£€ç´¢ä¿¡æ¯ä»¥æå‡æ¨¡å‹é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œå¹¶ç»“åˆæ·±åº¦ä¼˜å…ˆæœç´¢ï¼ˆDFSï¼‰é‡åºåˆ—åŒ–å’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å‡ ä½•ä¸€è‡´æ€§ï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å‡ ä½•ä¿çœŸåº¦å’Œå¯æ¸²æŸ“æ€§ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹Text2CADã€‚</details></td></tr></tbody></table>

### ğŸ“… 2026-01-18
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2601.13388v1">Structured Insight from Unstructured Data: Large Language Models for SDOH-Driven Diabetes Risk Prediction</a></td><td><details><summary>å±•å¼€</summary>Social determinants of health (SDOH) play a critical role in Type 2 Diabetes (T2D) management but are often absent from electronic health records and risk prediction models. Most individual-level SDOH data is collected through structured screening tools, which lack the flexibility to capture the complexity of patient experiences and unique needs of a clinic's population. This study explores the use of large language models (LLMs) to extract structured SDOH information from unstructured patient life stories and evaluate the predictive value of both the extracted features and the narratives themselves for assessing diabetes control. We collected unstructured interviews from 65 T2D patients aged 65 and older, focused on their lived experiences, social context, and diabetes management. These narratives were analyzed using LLMs with retrieval-augmented generation to produce concise, actionable qualitative summaries for clinical interpretation and structured quantitative SDOH ratings for risk prediction modeling. The structured SDOH ratings were used independently and in combination with traditional laboratory biomarkers as inputs to linear and tree-based machine learning models (Ridge, Lasso, Random Forest, and XGBoost) to demonstrate how unstructured narrative data can be applied in conventional risk prediction workflows. Finally, we evaluated several LLMs on their ability to predict a patient's level of diabetes control (low, medium, high) directly from interview text with A1C values redacted. LLMs achieved 60% accuracy in predicting diabetes control levels from interview text. This work demonstrates how LLMs can translate unstructured SDOH-related data into structured insights, offering a scalable approach to augment clinical risk models and decision-making.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œä»è€å¹´2å‹ç³–å°¿ç—…æ‚£è€…çš„éç»“æ„åŒ–ç”Ÿæ´»æ•…äº‹ä¸­æå–ç»“æ„åŒ–ç¤¾ä¼šå¥åº·å†³å®šå› ç´ ï¼ˆSDOHï¼‰ä¿¡æ¯ï¼Œå¹¶å°†å…¶ç”¨äºç³–å°¿ç—…æ§åˆ¶çš„é£é™©é¢„æµ‹æ¨¡å‹ï¼ŒåŒæ—¶è¯„ä¼°äº†LLMsç›´æ¥ä»æ–‡æœ¬é¢„æµ‹ç³–å°¿ç—…æ§åˆ¶æ°´å¹³çš„èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.13251v1">Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in a 15-Million Node Turkish Synonym Graph</a></td><td><details><summary>å±•å¼€</summary>Neural embeddings have a notorious blind spot: they can't reliably tell synonyms apart from antonyms. Consequently, increasing similarity thresholds often fails to prevent opposites from being grouped together. We've built a large-scale semantic clustering system specifically designed to tackle this problem head on. Our pipeline chews through 15 million lexical items, evaluates a massive 520 million potential relationships, and ultimately generates 2.9 million high-precision semantic clusters. The system makes three primary contributions. First, we introduce a labeled dataset of 843,000 concept pairs spanning synonymy, antonymy, and co-hyponymy, constructed via Gemini 2.5-Flash LLM augmentation and verified using human-curated dictionary resources. Second, we propose a specialized three-way semantic relation discriminator that achieves 90% macro-F1, enabling robust disambiguation beyond raw embedding similarity. Third, we introduce a novel soft-to-hard clustering algorithm that mitigates semantic drift preventing erroneous transitive chains (e.g., hot -> spicy -> pain -> depression) while simultaneously resolving polysemy. Our approach employs a topology-aware two-stage expansion-pruning procedure with topological voting, ensuring that each term is assigned to exactly one semantically coherent cluster. The resulting resource enables high-precision semantic search and retrieval-augmented generation, particularly for morphologically rich and low-resource languages where existing synonym databases remain sparse.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§å¤§è§„æ¨¡è¯­ä¹‰èšç±»ç³»ç»Ÿï¼Œé€šè¿‡æ„å»ºæ ‡æ³¨æ•°æ®é›†ã€å¼€å‘ä¸‰å‘è¯­ä¹‰å…³ç³»åˆ¤åˆ«å™¨å’Œæ–°é¢–çš„è½¯ç¡¬èšç±»ç®—æ³•ï¼Œè§£å†³äº†ç¥ç»åµŒå…¥åœ¨åŒºåˆ†åŒä¹‰è¯å’Œåä¹‰è¯æ–¹é¢çš„å±€é™æ€§ã€‚å…¶ç”Ÿæˆçš„è¯­ä¹‰èšç±»èµ„æºå¯ç”¨äºé«˜ç²¾åº¦è¯­ä¹‰æœç´¢å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ï¼Œå°¤å…¶åœ¨å½¢æ€ä¸°å¯Œå’Œèµ„æºç¨€ç¼ºçš„è¯­è¨€ä¸­è¡¨ç°å‡ºè‰²ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.13240v1">KOCO-BENCH: Can Large Language Models Leverage Domain Knowledge in Software Development?</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) excel at general programming but struggle with domain-specific software development, necessitating domain specialization methods for LLMs to learn and utilize domain knowledge and data. However, existing domain-specific code benchmarks cannot evaluate the effectiveness of domain specialization methods, which focus on assessing what knowledge LLMs possess rather than how they acquire and apply new knowledge, lacking explicit knowledge corpora for developing domain specialization methods. To this end, we present KOCO-BENCH, a novel benchmark designed for evaluating domain specialization methods in real-world software development. KOCO-BENCH contains 6 emerging domains with 11 software frameworks and 25 projects, featuring curated knowledge corpora alongside multi-granularity evaluation tasks including domain code generation (from function-level to project-level with rigorous test suites) and domain knowledge understanding (via multiple-choice Q&A). Unlike previous benchmarks that only provide test sets for direct evaluation, KOCO-BENCH requires acquiring and applying diverse domain knowledge (APIs, rules, constraints, etc.) from knowledge corpora to solve evaluation tasks. Our evaluations reveal that KOCO-BENCH poses significant challenges to state-of-the-art LLMs. Even with domain specialization methods (e.g., SFT, RAG, kNN-LM) applied, improvements remain marginal. Best-performing coding agent, Claude Code, achieves only 34.2%, highlighting the urgent need for more effective domain specialization methods. We release KOCO-BENCH, evaluation code, and baselines to advance further research at https://github.com/jiangxxxue/KOCO-bench.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†KOCO-BENCHï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç‰¹å®šé¢†åŸŸè½¯ä»¶å¼€å‘ä¸­è·å–å’Œåº”ç”¨æ–°çŸ¥è¯†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†åŒ…å«å¤šä¸ªæ–°å…´é¢†åŸŸã€è½¯ä»¶æ¡†æ¶å’Œé¡¹ç›®ï¼Œé€šè¿‡æä¾›çŸ¥è¯†åº“å’Œå¤šç²’åº¦è¯„ä¼°ä»»åŠ¡ï¼ˆå¦‚ä»£ç ç”Ÿæˆå’ŒçŸ¥è¯†ç†è§£ï¼‰ï¼Œè¦æ±‚æ¨¡å‹ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢å¹¶åº”ç”¨é¢†åŸŸçŸ¥è¯†ï¼ˆå¦‚APIã€è§„åˆ™ç­‰ï¼‰æ¥è§£å†³é—®é¢˜ã€‚è®ºæ–‡æåˆ°ç°æœ‰é¢†åŸŸä¸“ä¸šåŒ–æ–¹æ³•ï¼ˆåŒ…æ‹¬RAGï¼‰åœ¨è¯¥åŸºå‡†ä¸Šçš„è¡¨ç°ä»æœ‰é™ï¼Œçªæ˜¾äº†å¯¹æ›´æœ‰æ•ˆæ–¹æ³•çš„éœ€æ±‚ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.13227v1">Insider Knowledge: How Much Can RAG Systems Gain from Evaluation Secrets?</a></td><td><details><summary>å±•å¼€</summary>RAG systems are increasingly evaluated and optimized using LLM judges, an approach that is rapidly becoming the dominant paradigm for system assessment. Nugget-based approaches in particular are now embedded not only in evaluation frameworks but also in the architectures of RAG systems themselves. While this integration can lead to genuine improvements, it also creates a risk of faulty measurements due to circularity. In this paper, we investigate this risk through comparative experiments with nugget-based RAG systems, including Ginger and Crucible, against strong baselines such as GPT-Researcher. By deliberately modifying Crucible to generate outputs optimized for an LLM judge, we show that near-perfect evaluation scores can be achieved when elements of the evaluation - such as prompt templates or gold nuggets - are leaked or can be predicted. Our results highlight the importance of blind evaluation settings and methodological diversity to guard against mistaking metric overfitting for genuine system progress.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æ¢è®¨äº†åŸºäºLLMæ³•å®˜è¯„ä¼°RAGç³»ç»Ÿæ—¶å­˜åœ¨çš„å¾ªç¯é£é™©ï¼Œé€šè¿‡å®éªŒè¯æ˜å½“è¯„ä¼°å…ƒç´ ï¼ˆå¦‚æç¤ºæ¨¡æ¿æˆ–é»„é‡‘ç‰‡æ®µï¼‰æ³„éœ²æ—¶å¯å¯¼è‡´è™šå‡é«˜åˆ†ï¼Œå¼ºè°ƒäº†ç›²è¯„ä¼°å’Œæ–¹æ³•å¤šæ ·æ€§çš„é‡è¦æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.13222v1">Incorporating Q&A Nuggets into Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>RAGE systems integrate ideas from automatic evaluation (E) into Retrieval-augmented Generation (RAG). As one such example, we present Crucible, a Nugget-Augmented Generation System that preserves explicit citation provenance by constructing a bank of Q&A nuggets from retrieved documents and uses them to guide extraction, selection, and report generation. Reasoning on nuggets avoids repeated information through clear and interpretable Q&A semantics - instead of opaque cluster abstractions - while maintaining citation provenance throughout the entire generation process. Evaluated on the TREC NeuCLIR 2024 collection, our Crucible system substantially outperforms Ginger, a recent nugget-based RAG system, in nugget recall, density, and citation grounding.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†RAGEç³»ç»Ÿï¼ˆç»“åˆè‡ªåŠ¨è¯„ä¼°çš„RAGï¼‰ï¼Œå…·ä½“æå‡ºäº†Crucibleç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé€šè¿‡æ„å»ºQ&AçŸ¥è¯†ç‰‡æ®µåº“æ¥ä¿ç•™æ˜ç¡®çš„å¼•ç”¨æ¥æºï¼Œå¹¶åˆ©ç”¨è¿™äº›ç‰‡æ®µæŒ‡å¯¼ä¿¡æ¯æå–ã€é€‰æ‹©å’ŒæŠ¥å‘Šç”Ÿæˆï¼Œä»è€Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é¿å…é‡å¤ä¿¡æ¯ä¸”ä¿æŒå¯è§£é‡Šæ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒCrucibleåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºä¹‹å‰çš„åŸºäºç‰‡æ®µçš„RAGç³»ç»Ÿã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.13122v1">Responsible AI for General-Purpose Systems: Overview, Challenges, and A Path Forward</a></td><td><details><summary>å±•å¼€</summary>Modern general-purpose AI systems made using large language and vision models, are capable of performing a range of tasks like writing text articles, generating and debugging codes, querying databases, and translating from one language to another, which has made them quite popular across industries. However, there are risks like hallucinations, toxicity, and stereotypes in their output that make them untrustworthy. We review various risks and vulnerabilities of modern general-purpose AI along eight widely accepted responsible AI (RAI) principles (fairness, privacy, explainability, robustness, safety, truthfulness, governance, and sustainability) and compare how they are non-existent or less severe and easily mitigable in traditional task-specific counterparts. We argue that this is due to the non-deterministically high Degree of Freedom in output (DoFo) of general-purpose AI (unlike the deterministically constant or low DoFo of traditional task-specific AI systems), and there is a need to rethink our approach to RAI for general-purpose AI. Following this, we derive C2V2 (Control, Consistency, Value, Veracity) desiderata to meet the RAI requirements for future general-purpose AI systems, and discuss how recent efforts in AI alignment, retrieval-augmented generation, reasoning enhancements, etc. fare along one or more of the desiderata. We believe that the goal of developing responsible general-purpose AI can be achieved by formally modeling application- or domain-dependent RAI requirements along C2V2 dimensions, and taking a system design approach to suitably combine various techniques to meet the desiderata.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ç°ä»£é€šç”¨AIç³»ç»Ÿçš„é£é™©ä¸è„†å¼±æ€§ï¼Œå¯¹æ¯”äº†ä¼ ç»Ÿä»»åŠ¡ä¸“ç”¨AIç³»ç»Ÿçš„ä¼˜åŠ¿ï¼Œå¹¶æå‡ºäº†C2V2ï¼ˆControl, Consistency, Value, Veracityï¼‰åŸåˆ™ä»¥æ»¡è¶³æœªæ¥é€šç”¨AIç³»ç»Ÿçš„è´Ÿè´£ä»»è¦æ±‚ã€‚æ–‡ä¸­æåˆ°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­‰è¿‘æœŸæŠ€æœ¯è¿›å±•åœ¨æ»¡è¶³C2V2åŸåˆ™æ–¹é¢å‘æŒ¥äº†ä½œç”¨ï¼Œå¼ºè°ƒäº†é€šè¿‡ç»“åˆå¤šç§æŠ€æœ¯æ¥å®ç°è´Ÿè´£ä»»é€šç”¨AIçš„é‡è¦æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.13115v1">Agentic Conversational Search with Contextualized Reasoning via Reinforcement Learning</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) have become a popular interface for human-AI interaction, supporting information seeking and task assistance through natural, multi-turn dialogue. To respond to users within multi-turn dialogues, the context-dependent user intent evolves across interactions, requiring contextual interpretation, query reformulation, and dynamic coordination between retrieval and generation. Existing studies usually follow static rewrite, retrieve, and generate pipelines, which optimize different procedures separately and overlook the mixed-initiative action optimization simultaneously. Although the recent developments in deep search agents demonstrate the effectiveness in jointly optimizing retrieval and generation via reasoning, these approaches focus on single-turn scenarios, which might lack the ability to handle multi-turn interactions. We introduce a conversational agent that interleaves search and reasoning across turns, enabling exploratory and adaptive behaviors learned through reinforcement learning (RL) training with tailored rewards towards evolving user goals. The experimental results across four widely used conversational benchmarks demonstrate the effectiveness of our methods by surpassing several existing strong baselines.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§æ”¯æŒå¤šè½®å¯¹è¯çš„æ··åˆæœç´¢ä¸æ¨ç†çš„å¯¹è¯æ™ºèƒ½ä½“ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ£€ç´¢ä¸ç”Ÿæˆçš„åŠ¨æ€ååŒï¼Œè§£å†³ç°æœ‰é™æ€æµç¨‹åœ¨å¤„ç†ä¸Šä¸‹æ–‡ç›¸å…³ç”¨æˆ·æ„å›¾æ—¶çš„ä¸è¶³ï¼Œå¹¶åœ¨å¤šè½®äº¤äº’åœºæ™¯ä¸­è¶…è¶ŠåŸºå‡†è¡¨ç°ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.13105v1">Leveraging Lora Fine-Tuning and Knowledge Bases for Construction Identification</a></td><td><details><summary>å±•å¼€</summary>This study investigates the automatic identification of the English ditransitive construction by integrating LoRA-based fine-tuning of a large language model with a Retrieval-Augmented Generation (RAG) framework.A binary classification task was conducted on annotated data from the British National Corpus. Results demonstrate that a LoRA-fine-tuned Qwen3-8B model significantly outperformed both a native Qwen3-MAX model and a theory-only RAG system. Detailed error analysis reveals that fine-tuning shifts the model's judgment from a surface-form pattern matching towards a more semantically grounded understanding based.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ç ”ç©¶é€šè¿‡ç»“åˆLoRAå¾®è°ƒçš„å¤§è¯­è¨€æ¨¡å‹å’ŒRAGæ¡†æ¶ï¼Œå®ç°äº†è‹±è¯­åŒåŠç‰©ç»“æ„çš„è‡ªåŠ¨è¯†åˆ«ï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ä¼˜äºåŸç”Ÿæ¨¡å‹å’Œçº¯ç†è®ºRAGç³»ç»Ÿï¼Œä¸”åˆ†ææ˜¾ç¤ºå¾®è°ƒä½¿æ¨¡å‹ä»è¡¨å±‚æ¨¡å¼åŒ¹é…è½¬å‘è¯­ä¹‰ç†è§£ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.12991v1">RAGExplorer: A Visual Analytics System for the Comparative Diagnosis of RAG Systems</a></td><td><details><summary>å±•å¼€</summary>The advent of Retrieval-Augmented Generation (RAG) has significantly enhanced the ability of Large Language Models (LLMs) to produce factually accurate and up-to-date responses. However, the performance of a RAG system is not determined by a single component but emerges from a complex interplay of modular choices, such as embedding models and retrieval algorithms. This creates a vast and often opaque configuration space, making it challenging for developers to understand performance trade-offs and identify optimal designs. To address this challenge, we present RAGExplorer, a visual analytics system for the systematic comparison and diagnosis of RAG configurations. RAGExplorer guides users through a seamless macro-to-micro analytical workflow. Initially, it empowers developers to survey the performance landscape across numerous configurations, allowing for a high-level understanding of which design choices are most effective. For a deeper analysis, the system enables users to drill down into individual failure cases, investigate how differences in retrieved information contribute to errors, and interactively test hypotheses by manipulating the provided context to observe the resulting impact on the generated answer. We demonstrate the effectiveness of RAGExplorer through detailed case studies and user studies, validating its ability to empower developers in navigating the complex RAG design space. Our code and user guide are publicly available at https://github.com/Thymezzz/RAGExplorer.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†RAGExplorerï¼Œä¸€ä¸ªç”¨äºç³»ç»Ÿæ¯”è¾ƒå’Œè¯Šæ–­RAGé…ç½®çš„å¯è§†åŒ–åˆ†æç³»ç»Ÿï¼Œæ—¨åœ¨å¸®åŠ©å¼€å‘è€…ç†è§£æ€§èƒ½æƒè¡¡å¹¶ä¼˜åŒ–RAGè®¾è®¡ï¼Œé€šè¿‡å®è§‚åˆ°å¾®è§‚çš„åˆ†æå·¥ä½œæµç¨‹è¯„ä¼°ä¸åŒé…ç½®çš„æ•ˆæœï¼Œå¹¶æ”¯æŒäº¤äº’å¼é”™è¯¯è¯Šæ–­å’Œå‡è®¾æµ‹è¯•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.12974v1">Bridging the Knowledge-Action Gap by Evaluating LLMs in Dynamic Dental Clinical Scenarios</a></td><td><details><summary>å±•å¼€</summary>The transition of Large Language Models (LLMs) from passive knowledge retrievers to autonomous clinical agents demands a shift in evaluation-from static accuracy to dynamic behavioral reliability. To explore this boundary in dentistry, a domain where high-quality AI advice uniquely empowers patient-participatory decision-making, we present the Standardized Clinical Management & Performance Evaluation (SCMPE) benchmark, which comprehensively assesses performance from knowledge-oriented evaluations (static objective tasks) to workflow-based simulations (multi-turn simulated patient interactions). Our analysis reveals that while models demonstrate high proficiency in static objective tasks, their performance precipitates in dynamic clinical dialogues, identifying that the primary bottleneck lies not in knowledge retention, but in the critical challenges of active information gathering and dynamic state tracking. Mapping "Guideline Adherence" versus "Decision Quality" reveals a prevalent "High Efficacy, Low Safety" risk in general models. Furthermore, we quantify the impact of Retrieval-Augmented Generation (RAG). While RAG mitigates hallucinations in static tasks, its efficacy in dynamic workflows is limited and heterogeneous, sometimes causing degradation. This underscores that external knowledge alone cannot bridge the reasoning gap without domain-adaptive pre-training. This study empirically charts the capability boundaries of dental LLMs, providing a roadmap for bridging the gap between standardized knowledge and safe, autonomous clinical practice.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç‰™ç§‘é¢†åŸŸä»é™æ€çŸ¥è¯†æ£€ç´¢å‘åŠ¨æ€ä¸´åºŠä»£ç†çš„è½¬å˜ï¼Œæå‡ºSCMPEåŸºå‡†ä»¥è¯„ä¼°æ¨¡å‹åœ¨é™æ€ä»»åŠ¡å’ŒåŠ¨æ€ä¸´åºŠå¯¹è¯ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹åœ¨é™æ€ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åŠ¨æ€å¯¹è¯ä¸­å­˜åœ¨ä¿¡æ¯æ”¶é›†å’ŒçŠ¶æ€è·Ÿè¸ªçš„ç“¶é¢ˆï¼Œå¹¶æ­ç¤ºäº†é€šç”¨æ¨¡å‹â€œé«˜æ•ˆä½å®‰å…¨â€çš„é£é™©ã€‚æ­¤å¤–ï¼Œè®ºæ–‡é‡åŒ–äº†RAGçš„ä½œç”¨ï¼ŒæŒ‡å‡ºå…¶åœ¨é™æ€ä»»åŠ¡ä¸­å‡è½»å¹»è§‰æ•ˆæœæ˜¾è‘—ï¼Œä½†åœ¨åŠ¨æ€å·¥ä½œæµä¸­æ•ˆæœæœ‰é™ä¸”ä¸ç¨³å®šï¼Œå¼ºè°ƒéœ€ç»“åˆé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒæ‰èƒ½å¼¥è¡¥æ¨ç†å·®è·ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.12945v1">A Component-Based Survey of Interactions between Large Language Models and Multi-Armed Bandits</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) have become powerful and widely used systems for language understanding and generation, while multi-armed bandit (MAB) algorithms provide a principled framework for adaptive decision-making under uncertainty. This survey explores the potential at the intersection of these two fields. As we know, it is the first survey to systematically review the bidirectional interaction between large language models and multi-armed bandits at the component level. We highlight the bidirectional benefits: MAB algorithms address critical LLM challenges, spanning from pre-training to retrieval-augmented generation (RAG) and personalization. Conversely, LLMs enhance MAB systems by redefining core components such as arm definition and environment modeling, thereby improving decision-making in sequential tasks. We analyze existing LLM-enhanced bandit systems and bandit-enhanced LLM systems, providing insights into their design, methodologies, and performance. Key challenges and representative findings are identified to help guide future research. An accompanying GitHub repository that indexes relevant literature is available at https://github.com/bucky1119/Awesome-LLM-Bandit-Interaction.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ˜¯ä¸€ç¯‡ç»¼è¿°æ€§æ–‡ç« ï¼Œç³»ç»Ÿåœ°æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸å¤šè‡‚è€è™æœºï¼ˆMABï¼‰ç®—æ³•ä¹‹é—´çš„åŒå‘äº¤äº’å…³ç³»ã€‚å®ƒæŒ‡å‡ºMABç®—æ³•å¯ä»¥å¸®åŠ©è§£å†³LLMsåœ¨å¤šä¸ªé˜¶æ®µï¼ˆåŒ…æ‹¬é¢„è®­ç»ƒã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œä¸ªæ€§åŒ–ï¼‰æ‰€é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ï¼›åŒæ—¶ï¼ŒLLMsä¹Ÿèƒ½å¤Ÿé€šè¿‡é‡æ–°å®šä¹‰MABç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶æ¥æå‡å…¶å†³ç­–èƒ½åŠ›ã€‚æ–‡ç« åˆ†æäº†ç°æœ‰ç³»ç»Ÿï¼Œå¹¶æŒ‡å‡ºäº†å…³é”®æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.12921v1">Injecting Knowledge from Social Science Journals to Improve Indonesian Cultural Understanding by LLMs</a></td><td><details><summary>å±•å¼€</summary>Recently there have been intensifying efforts to improve the understanding of Indonesian cultures by large language models (LLMs). An attractive source of cultural knowledge that has been largely overlooked is local journals of social science, which likely contain substantial cultural studies from a native perspective. We present a novel text dataset of journal article passages, created from 151 open-source Indonesian social science journals, called IndoSoSci. We demonstrate an effective recipe for injecting Indonesian cultural knowledge therein into LLMs: extracting the facts related to Indonesian culture, and apply retrieval-augmented generation (RAG) with LLM-generated hypothetical documents as queries during retrieval. The proposed recipe yields strong performance gains over several strong baselines on the IndoCulture benchmark. Additionally, by combining IndoSoSci with Indonesian Wikipedia, we set a new state-of-the-art accuracy on the IndoCulture benchmark.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºIndoSoSciçš„å°å°¼ç¤¾ä¼šç§‘å­¦æœŸåˆŠæ–‡æœ¬æ•°æ®é›†ï¼Œå¹¶ä»‹ç»äº†ä¸€ç§åˆ©ç”¨RAGæŠ€æœ¯å°†å°å°¼æ–‡åŒ–çŸ¥è¯†æ³¨å…¥å¤§è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆæ–¹æ³•ï¼Œé€šè¿‡LLMç”Ÿæˆçš„å‡è®¾æ–‡æ¡£ä½œä¸ºæ£€ç´¢æŸ¥è¯¢ï¼Œæ˜¾è‘—æå‡äº†åœ¨IndoCultureåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ï¼Œå¹¶ç»“åˆå°å°¼ç»´åŸºç™¾ç§‘å–å¾—äº†å½“å‰æœ€ä½³å‡†ç¡®ç‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.12904v1">From Prefix Cache to Fusion RAG Cache: Accelerating LLM Inference in Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation enhances Large Language Models by integrating external knowledge, which reduces hallucinations but increases prompt length. This increase leads to higher computational costs and longer Time to First Token (TTFT). To mitigate this issue, existing solutions aim to reuse the preprocessed KV cache of each retrieved chunk to accelerate RAG. However, the lack of cross-chunk contextual information leads to a significant drop in generation quality, leaving the potential benefits of KV cache reuse largely unfulfilled. The challenge lies in how to reuse the precomputed KV cache of chunks while preserving generation quality. We propose FusionRAG, a novel inference framework that optimizes both the preprocessing and reprocessing stages of RAG. In the offline preprocessing stage, we embed information from other related text chunks into each chunk, while in the online reprocessing stage, we recompute the KV cache for tokens that the model focuses on. As a result, we achieve a better trade-off between generation quality and efficiency. According to our experiments, FusionRAG significantly improves generation quality at the same recomputation ratio compared to previous state-of-the-art solutions. By recomputing fewer than 15% of the tokens, FusionRAG achieves up to 70% higher normalized F1 scores than baselines and reduces TTFT by 2.66x-9.39x compared to Full Attention.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºFusionRAGçš„æ–°å‹æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–RAGæŠ€æœ¯åœ¨é¢„å¤„ç†å’Œå†å¤„ç†é˜¶æ®µçš„æ•ˆç‡é—®é¢˜ã€‚é€šè¿‡åœ¨ä¸åŒæ–‡æœ¬å—é—´åµŒå…¥ç›¸å…³ä¿¡æ¯å¹¶é‡æ–°è®¡ç®—æ¨¡å‹å…³æ³¨çš„tokençš„KVç¼“å­˜ï¼ŒFusionRAGåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶æ˜¾è‘—å‡å°‘äº†é¦–æ¬¡ä»¤ç‰Œæ—¶é—´ï¼ˆTTFTï¼‰ï¼Œå¹¶åœ¨å®éªŒä¸­æ˜¾ç¤ºå‡ºæ¯”ç°æœ‰è§£å†³æ–¹æ¡ˆæ›´é«˜çš„ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.12779v1">Open Vocabulary Panoptic Segmentation With Retrieval Augmentation</a></td><td><details><summary>å±•å¼€</summary>Given an input image and set of class names, panoptic segmentation aims to label each pixel in an image with class labels and instance labels. In comparison, Open Vocabulary Panoptic Segmentation aims to facilitate the segmentation of arbitrary classes according to user input. The challenge is that a panoptic segmentation system trained on a particular dataset typically does not generalize well to unseen classes beyond the training data. In this work, we propose RetCLIP, a retrieval-augmented panoptic segmentation method that improves the performance of unseen classes. In particular, we construct a masked segment feature database using paired image-text data. At inference time, we use masked segment features from the input image as query keys to retrieve similar features and associated class labels from the database. Classification scores for the masked segment are assigned based on the similarity between query features and retrieved features. The retrieval-based classification scores are combined with CLIP-based scores to produce the final output. We incorporate our solution with a previous SOTA method (FC-CLIP). When trained on COCO, the proposed method demonstrates 30.9 PQ, 19.3 mAP, 44.0 mIoU on the ADE20k dataset, achieving +4.5 PQ, +2.5 mAP, +10.0 mIoU absolute improvement over the baseline.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRetCLIPçš„æ£€ç´¢å¢å¼ºå…¨æ™¯åˆ†å‰²æ–¹æ³•ï¼Œé€šè¿‡ä»å›¾åƒ-æ–‡æœ¬å¯¹æ„å»ºçš„æ©ç ç‰‡æ®µç‰¹å¾æ•°æ®åº“ä¸­æ£€ç´¢ç›¸ä¼¼ç‰¹å¾å’Œç±»åˆ«æ ‡ç­¾ï¼Œç»“åˆCLIPæ¨¡å‹çš„åˆ†ç±»åˆ†æ•°æ¥æå‡æ¨¡å‹å¯¹æœªè§è¿‡ç±»åˆ«çš„åˆ†å‰²æ€§èƒ½ï¼Œå¹¶åœ¨åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.12658v1">Augmenting Question Answering with A Hybrid RAG Approach</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has emerged as a powerful technique for enhancing the quality of responses in Question-Answering (QA) tasks. However, existing approaches often struggle with retrieving contextually relevant information, leading to incomplete or suboptimal answers. In this paper, we introduce Structured-Semantic RAG (SSRAG), a hybrid architecture that enhances QA quality by integrating query augmentation, agentic routing, and a structured retrieval mechanism combining vector and graph based techniques with context unification. By refining retrieval processes and improving contextual grounding, our approach improves both answer accuracy and informativeness. We conduct extensive evaluations on three popular QA datasets, TruthfulQA, SQuAD and WikiQA, across five Large Language Models (LLMs), demonstrating that our proposed approach consistently improves response quality over standard RAG implementations.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSSRAGçš„æ··åˆæ¶æ„ï¼Œé€šè¿‡ç»“åˆæŸ¥è¯¢å¢å¼ºã€æ™ºèƒ½è·¯ç”±ä»¥åŠå‘é‡ä¸å›¾æ£€ç´¢æŠ€æœ¯çš„ç»“æ„åŒ–æ£€ç´¢æœºåˆ¶ï¼Œä¼˜åŒ–äº†RAGåœ¨é—®ç­”ä»»åŠ¡ä¸­çš„æ£€ç´¢ç›¸å…³æ€§å’Œä¸Šä¸‹æ–‡æ•´åˆèƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªé—®ç­”æ•°æ®é›†å’Œå¤§è¯­è¨€æ¨¡å‹ä¸Šæ˜¾è‘—æå‡äº†å›ç­”å‡†ç¡®æ€§å’Œä¿¡æ¯ä¸°å¯Œåº¦ï¼Œä¼˜äºæ ‡å‡†RAGå®ç°ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.12641v1">STEP-LLM: Generating CAD STEP Models from Natural Language with Large Language Models</a></td><td><details><summary>å±•å¼€</summary>Computer-aided design (CAD) is vital to modern manufacturing, yet model creation remains labor-intensive and expertise-heavy. To enable non-experts to translate intuitive design intent into manufacturable artifacts, recent large language models-based text-to-CAD efforts focus on command sequences or script-based formats like CadQuery. However, these formats are kernel-dependent and lack universality for manufacturing. In contrast, the Standard for the Exchange of Product Data (STEP, ISO 10303) file is a widely adopted, neutral boundary representation (B-rep) format directly compatible with manufacturing, but its graph-structured, cross-referenced nature poses unique challenges for auto-regressive LLMs. To address this, we curate a dataset of ~40K STEP-caption pairs and introduce novel preprocessing tailored for the graph-structured format of STEP, including a depth-first search-based reserialization that linearizes cross-references while preserving locality and chain-of-thought(CoT)-style structural annotations that guide global coherence. We integrate retrieval-augmented generation to ground predictions in relevant examples for supervised fine-tuning, and refine generation quality through reinforcement learning with a specific Chamfer Distance-based geometric reward. Experiments demonstrate consistent gains of our STEP-LLM in geometric fidelity over the Text2CAD baseline, with improvements arising from multiple stages of our framework: the RAG module substantially enhances completeness and renderability, the DFS-based reserialization strengthens overall accuracy, and the RL further reduces geometric discrepancy. Both metrics and visual comparisons confirm that STEP-LLM generates shapes with higher fidelity than Text2CAD. These results show the feasibility of LLM-driven STEP model generation from natural language, showing its potential to democratize CAD design for manufacturing.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–‡æœ¬åˆ°CADæ¨¡å‹ç”Ÿæˆæ–¹æ³•ï¼ˆSTEP-LLMï¼‰ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯æ•´åˆç›¸å…³ç¤ºä¾‹ä»¥æå‡ç”Ÿæˆè´¨é‡ï¼Œå¹¶é‡‡ç”¨æ·±åº¦ä¼˜å…ˆæœç´¢ï¼ˆDFSï¼‰é‡æ–°åºåˆ—åŒ–å’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å‡ ä½•ä¸€è‡´æ€§ï¼Œæ˜¾è‘—æé«˜äº†STEPæ ¼å¼CADæ¨¡å‹ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œå¯åˆ¶é€ æ€§ï¼Œä¸ºéä¸“å®¶ç”¨æˆ·æä¾›äº†æ›´ä¾¿æ·çš„è®¾è®¡å·¥å…·ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.12331v1">Efficient Privacy-Preserving Retrieval Augmented Generation with Distance-Preserving Encryption</a></td><td><details><summary>å±•å¼€</summary>RAG has emerged as a key technique for enhancing response quality of LLMs without high computational cost. In traditional architectures, RAG services are provided by a single entity that hosts the dataset within a trusted local environment. However, individuals or small organizations often lack the resources to maintain data storage servers, leading them to rely on outsourced cloud storage. This dependence on untrusted third-party services introduces privacy risks. Embedding-based retrieval mechanisms, commonly used in RAG systems, are vulnerable to privacy leakage such as vector-to-text reconstruction attacks and structural leakage via vector analysis. Several privacy-preserving RAG techniques have been proposed but most existing approaches rely on partially homomorphic encryption, which incurs substantial computational overhead. To address these challenges, we propose an efficient privacy-preserving RAG framework (ppRAG) tailored for untrusted cloud environments that defends against vector-to-text attack, vector analysis, and query analysis. We propose Conditional Approximate Distance-Comparison-Preserving Symmetric Encryption (CAPRISE) that encrypts embeddings while still allowing the cloud to compute similarity between an encrypted query and the encrypted database embeddings. CAPRISE preserves only the relative distance ordering between the encrypted query and each encrypted database embedding, without exposing inter-database distances, thereby enhancing both privacy and efficiency. To mitigate query analysis, we introduce DP by perturbing the query embedding prior to encryption, preventing the cloud from inferring sensitive patterns. Experimental results show that ppRAG achieves efficient processing throughput, high retrieval accuracy, strong privacy guarantees, making it a practical solution for resource-constrained users seeking secure cloud-augmented LLMs.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹ä¸ä¿¡ä»»äº‘ç¯å¢ƒçš„é«˜æ•ˆéšç§ä¿æŠ¤RAGæ¡†æ¶ï¼ˆppRAGï¼‰ï¼Œé€šè¿‡è®¾è®¡æ–°å‹å¯¹ç§°åŠ å¯†æ–¹æ¡ˆï¼ˆCAPRISEï¼‰å’Œå·®åˆ†éšç§æŠ€æœ¯ï¼Œè§£å†³äº†ä¼ ç»ŸåŸºäºåµŒå…¥çš„RAGç³»ç»Ÿåœ¨äº‘å­˜å‚¨ä¸­çš„éšç§æ³„éœ²é£é™©ï¼ˆå¦‚å‘é‡é‡å»ºæ”»å‡»å’ŒæŸ¥è¯¢åˆ†æï¼‰ï¼ŒåŒæ—¶ä¿æŒæ£€ç´¢æ•ˆç‡ä¸å‡†ç¡®æ€§ï¼Œé€‚ç”¨äºèµ„æºå—é™ç”¨æˆ·çš„å®‰å…¨äº‘å¢å¼ºLLMåœºæ™¯ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.12276v1">Predictive Prototyping: Evaluating Design Concepts with ChatGPT</a></td><td><details><summary>å±•å¼€</summary>The design-build-test cycle is essential for innovation, but physical prototyping is often slow and expensive. Although physics-based simulation and strategic prototyping can reduce cost, meaningful evaluation is frequently constrained until an integrated prototype is built. This paper investigates whether a generative pretrained transformer (GPT) can predict information typically obtained through prototyping, including cost, performance, and perceived usability. We introduce a retrieval-augmented generation (RAG) method to emulate design feedback using OpenAI GPT-4o, grounded in prototyping data scraped from Instructables.com to increase access to relevant precedent. Two studies are reported. First, a controlled experiment compares GPT-RAG and human designers, who receive design sketches and predict cost, performance, and usability; predictions are evaluated against ground-truth results from physical prototypes. Second, we report an applied demonstration in which a physical prototype is produced from GPT-RAG recommendations and compared with a commercial baseline and a topology-optimized design. Results show that GPT-RAG provides more accurate cost and performance estimates than individual or crowd human estimates, while yielding comparable usability insights; the GPT-RAG-informed prototype also outperforms both comparison prototypes. Repeated querying with response averaging significantly improves accuracy, suggesting that LLMs can emulate crowd aggregation effects consistent with the law of large numbers.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ï¼Œç»“åˆGPT-4oå’Œä»Instructables.comè·å–çš„ prototyping æ•°æ®ï¼Œé¢„æµ‹è®¾è®¡åé¦ˆï¼ˆå¦‚æˆæœ¬ã€æ€§èƒ½å’Œå¯ç”¨æ€§ï¼‰ã€‚é€šè¿‡å®éªŒæ¯”è¾ƒ GPT-RAG ä¸äººç±»è®¾è®¡å¸ˆçš„é¢„æµ‹å‡†ç¡®æ€§ï¼Œå¹¶å±•ç¤ºå…¶åœ¨ç‰©ç†åŸå‹è®¾è®¡ä¸­çš„å®é™…åº”ç”¨æ•ˆæœï¼Œç»“æœæ˜¾ç¤º GPT-RAG åœ¨æˆæœ¬å’Œæ€§èƒ½è¯„ä¼°ä¸Šä¼˜äºäººç±»ï¼Œä¸”é€šè¿‡å¤šæ¬¡æŸ¥è¯¢å¹³å‡å“åº”å¯æ˜¾è‘—æå‡å‡†ç¡®æ€§ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2026-01-17
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2601.12331v1">Efficient Privacy-Preserving Retrieval Augmented Generation with Distance-Preserving Encryption</a></td><td><details><summary>å±•å¼€</summary>RAG has emerged as a key technique for enhancing response quality of LLMs without high computational cost. In traditional architectures, RAG services are provided by a single entity that hosts the dataset within a trusted local environment. However, individuals or small organizations often lack the resources to maintain data storage servers, leading them to rely on outsourced cloud storage. This dependence on untrusted third-party services introduces privacy risks. Embedding-based retrieval mechanisms, commonly used in RAG systems, are vulnerable to privacy leakage such as vector-to-text reconstruction attacks and structural leakage via vector analysis. Several privacy-preserving RAG techniques have been proposed but most existing approaches rely on partially homomorphic encryption, which incurs substantial computational overhead. To address these challenges, we propose an efficient privacy-preserving RAG framework (ppRAG) tailored for untrusted cloud environments that defends against vector-to-text attack, vector analysis, and query analysis. We propose Conditional Approximate Distance-Comparison-Preserving Symmetric Encryption (CAPRISE) that encrypts embeddings while still allowing the cloud to compute similarity between an encrypted query and the encrypted database embeddings. CAPRISE preserves only the relative distance ordering between the encrypted query and each encrypted database embedding, without exposing inter-database distances, thereby enhancing both privacy and efficiency. To mitigate query analysis, we introduce DP by perturbing the query embedding prior to encryption, preventing the cloud from inferring sensitive patterns. Experimental results show that ppRAG achieves efficient processing throughput, high retrieval accuracy, strong privacy guarantees, making it a practical solution for resource-constrained users seeking secure cloud-augmented LLMs.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹ä¸å—ä¿¡ä»»äº‘ç¯å¢ƒçš„é«˜æ•ˆéšç§ä¿æŠ¤RAGæ¡†æ¶ï¼ˆppRAGï¼‰ï¼Œé€šè¿‡æ–°å‹åŠ å¯†æ–¹æ¡ˆCAPRISEå’Œå·®åˆ†éšç§æŠ€æœ¯ï¼Œè§£å†³äº†ç°æœ‰RAGç³»ç»Ÿå› ä¾èµ–å¤–åŒ…äº‘å­˜å‚¨å¯¼è‡´çš„éšç§æ³„éœ²é£é™©ï¼ˆå¦‚å‘é‡é‡æ„æ”»å‡»ã€æŸ¥è¯¢åˆ†æç­‰ï¼‰ï¼Œåœ¨ä¿è¯æ£€ç´¢ç²¾åº¦çš„åŒæ—¶æ˜¾è‘—æå‡äº†å®‰å…¨æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.12276v1">Predictive Prototyping: Evaluating Design Concepts with ChatGPT</a></td><td><details><summary>å±•å¼€</summary>The design-build-test cycle is essential for innovation, but physical prototyping is often slow and expensive. Although physics-based simulation and strategic prototyping can reduce cost, meaningful evaluation is frequently constrained until an integrated prototype is built. This paper investigates whether a generative pretrained transformer (GPT) can predict information typically obtained through prototyping, including cost, performance, and perceived usability. We introduce a retrieval-augmented generation (RAG) method to emulate design feedback using OpenAI GPT-4o, grounded in prototyping data scraped from Instructables.com to increase access to relevant precedent. Two studies are reported. First, a controlled experiment compares GPT-RAG and human designers, who receive design sketches and predict cost, performance, and usability; predictions are evaluated against ground-truth results from physical prototypes. Second, we report an applied demonstration in which a physical prototype is produced from GPT-RAG recommendations and compared with a commercial baseline and a topology-optimized design. Results show that GPT-RAG provides more accurate cost and performance estimates than individual or crowd human estimates, while yielding comparable usability insights; the GPT-RAG-informed prototype also outperforms both comparison prototypes. Repeated querying with response averaging significantly improves accuracy, suggesting that LLMs can emulate crowd aggregation effects consistent with the law of large numbers.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ–¹æ³•ï¼Œåˆ©ç”¨GPT-4oç»“åˆä»Instructables.comè·å–çš„ prototyping æ•°æ®æ¥æ¨¡æ‹Ÿè®¾è®¡åé¦ˆï¼Œé¢„æµ‹æˆæœ¬ã€æ€§èƒ½å’Œå¯ç”¨æ€§ã€‚é€šè¿‡å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é¢„æµ‹å‡†ç¡®æ€§å’Œå®é™…åŸå‹æ€§èƒ½ä¸Šä¼˜äºäººç±»è®¾è®¡å¸ˆå’Œå…¶ä»–è®¾è®¡æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.12078v1">Optimizing User Profiles via Contextual Bandits for Retrieval-Augmented LLM Personalization</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) excel at general-purpose tasks, yet adapting their responses to individual users remains challenging. Retrieval augmentation provides a lightweight alternative to fine-tuning by conditioning LLMs on user history records, and existing approaches typically select these records based on semantic relevance. We argue that relevance serves as an unreliable proxy for utility: a record may be semantically similar to a query yet fail to improve generation quality or even degrade it due to redundancy or conflicting information. To bridge this gap, we propose PURPLE, a contextual bandit framework that oPtimizes UseR Profiles for Llm pErsonalization. In contrast to a greedy selection of the most relevant records, PURPLE treats profile construction as a set generation process and utilizes a Plackett-Luce ranking model to capture complex inter-record dependencies. By training with dense feedback provided by the likelihood of the reference response, our method aligns retrieval directly with generation quality. Extensive experiments on nine personalization tasks demonstrate that PURPLE consistently outperforms strong heuristic and retrieval-augmented baselines in both effectiveness and efficiency, establishing a principled and scalable solution for optimizing user profiles.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºPURPLEçš„ä¸Šä¸‹æ–‡åŒªå¾’æ¡†æ¶ï¼Œç”¨äºä¼˜åŒ–LLMä¸ªæ€§åŒ–ä¸­çš„ç”¨æˆ·æ¡£æ¡ˆæ„å»ºï¼Œé€šè¿‡Plackett-Luceæ’åæ¨¡å‹æ•è·è®°å½•é—´å¤æ‚ä¾èµ–å…³ç³»ï¼Œå¹¶åˆ©ç”¨ç”Ÿæˆè´¨é‡çš„å¯†é›†åé¦ˆç›´æ¥å¯¹é½æ£€ç´¢ä¸ç”Ÿæˆï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºäºè¯­ä¹‰ç›¸å…³æ€§çš„æ£€ç´¢å¢å¼ºæ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.12075v1">To Copy or Not to Copy: Copying Is Easier to Induce Than Recall</a></td><td><details><summary>å±•å¼€</summary>Language models used in retrieval-augmented settings must arbitrate between parametric knowledge stored in their weights and contextual information in the prompt. This work presents a mechanistic study of that choice by extracting an \emph{arbitration vector} from model activations on a curated dataset designed to disentangle (i) irrelevant contexts that elicit parametric recall and (ii) relevant but false contexts that elicit copying. The vector is computed as the residual-stream centroid difference between these regimes across 27 relations, and is injected as an additive intervention at selected layers and token spans to steer behavior in two directions: Copy$\rightarrow$Recall (suppressing context use) and Recall$\rightarrow$Copy (inducing the model to copy any token from the context). Experiments on two architectures (decoder-only and encoder/decoder) and two open-domain QA benchmarks show consistent behavior shifts under moderate scaling while monitoring accuracy and fluency. Mechanistic analyses of attention routing, MLP contributions, and layer-wise probability trajectories reveal an asymmetry: inducing copying is an easy ``reactivation'' process that can be triggered at different locations in the input, while restoring recall is a ``suppression'' process that is more fragile and strongly tied to object-token interventions.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­è¯­è¨€æ¨¡å‹åœ¨å‚æ•°åŒ–çŸ¥è¯†ï¼ˆæƒé‡ä¸­å­˜å‚¨ï¼‰å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆæ£€ç´¢åˆ°çš„å¤–éƒ¨å†…å®¹ï¼‰ä¹‹é—´çš„æƒè¡¡æœºåˆ¶ã€‚ä½œè€…é€šè¿‡æå–â€œä»²è£å‘é‡â€åˆ†ææ¨¡å‹è¡Œä¸ºï¼Œè®¾è®¡äº†å¹²é¢„å®éªŒä»¥æ§åˆ¶æ¨¡å‹çš„â€œå¤åˆ¶â€æˆ–â€œå›å¿†â€å€¾å‘ï¼Œå¹¶åœ¨ä¸¤ç§æ¶æ„ï¼ˆä»…è§£ç å™¨å’Œç¼–ç å™¨-è§£ç å™¨ï¼‰ä¸ŠéªŒè¯äº†å¹²é¢„æ•ˆæœï¼Œæ­ç¤ºäº†å¤åˆ¶è¡Œä¸ºçš„æ˜“è§¦å‘æ€§ä¸å›å¿†æ¢å¤çš„è„†å¼±æ€§å·®å¼‚ã€‚ç ”ç©¶èšç„¦äºæ¨¡å‹å¦‚ä½•èåˆæ£€ç´¢å†…å®¹çš„å†…éƒ¨æœºåˆ¶ï¼Œå±äºRAGæŠ€æœ¯çš„åº•å±‚åŸç†æ¢ç´¢ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.11888v1">Agentic-R: Learning to Retrieve for Agentic Search</a></td><td><details><summary>å±•å¼€</summary>Agentic search has recently emerged as a powerful paradigm, where an agent interleaves multi-step reasoning with on-demand retrieval to solve complex questions. Despite its success, how to design a retriever for agentic search remains largely underexplored. Existing search agents typically rely on similarity-based retrievers, while similar passages are not always useful for final answer generation. In this paper, we propose a novel retriever training framework tailored for agentic search. Unlike retrievers designed for single-turn retrieval-augmented generation (RAG) that only rely on local passage utility, we propose to use both local query-passage relevance and global answer correctness to measure passage utility in a multi-turn agentic search. We further introduce an iterative training strategy, where the search agent and the retriever are optimized bidirectionally and iteratively. Different from RAG retrievers that are only trained once with fixed questions, our retriever is continuously improved using evolving and higher-quality queries from the agent. Extensive experiments on seven single-hop and multi-hop QA benchmarks demonstrate that our retriever, termed \ours{}, consistently outperforms strong baselines across different search agents. Our codes are available at: https://github.com/8421BCD/Agentic-R.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸“ä¸ºä»£ç†æœç´¢è®¾è®¡çš„æ–°æ£€ç´¢å™¨è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå±€éƒ¨æŸ¥è¯¢-æ®µè½ç›¸å…³æ€§å’Œå…¨å±€ç­”æ¡ˆæ­£ç¡®æ€§æ¥ä¼˜åŒ–å¤šè½®ä»£ç†æœç´¢ä¸­çš„æ®µè½æ•ˆç”¨ï¼Œå¹¶é‡‡ç”¨è¿­ä»£è®­ç»ƒç­–ç•¥ä½¿æ£€ç´¢å™¨å’Œæœç´¢ä»£ç†ç›¸äº’ä¼˜åŒ–ï¼Œæ˜¾è‘—æå‡äº†å¤šç§å•è·³å’Œå¤šè·³é—®ç­”åŸºå‡†ä¸Šçš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.11863v1">Utilizing Metadata for Better Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation systems depend on retrieving semantically relevant document chunks to support accurate, grounded outputs from large language models. In structured and repetitive corpora such as regulatory filings, chunk similarity alone often fails to distinguish between documents with overlapping language. Practitioners often flatten metadata into input text as a heuristic, but the impact and trade-offs of this practice remain poorly understood. We present a systematic study of metadata-aware retrieval strategies, comparing plain-text baselines with approaches that embed metadata directly. Our evaluation spans metadata-as-text (prefix and suffix), a dual-encoder unified embedding that fuses metadata and content in a single index, dual-encoder late-fusion retrieval, and metadata-aware query reformulation. Across multiple retrieval metrics and question types, we find that prefixing and unified embeddings consistently outperform plain-text baselines, with the unified at times exceeding prefixing while being easier to maintain. Beyond empirical comparisons, we analyze embedding space, showing that metadata integration improves effectiveness by increasing intra-document cohesion, reducing inter-document confusion, and widening the separation between relevant and irrelevant chunks. Field-level ablations show that structural cues provide strong disambiguating signals. Our code, evaluation framework, and the RAGMATE-10K dataset are publicly hosted.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­ï¼Œå¦‚ä½•é€šè¿‡æ•´åˆæ–‡æ¡£å…ƒæ•°æ®æ¥æ”¹è¿›æ£€ç´¢æ•ˆæœã€‚ä½œè€…æ¯”è¾ƒäº†å¤šç§å…ƒæ•°æ®æ„ŸçŸ¥çš„æ£€ç´¢ç­–ç•¥ï¼ˆå¦‚å…ƒæ•°æ®ä½œä¸ºæ–‡æœ¬å‰ç¼€/åç¼€ã€ç»Ÿä¸€åµŒå…¥ã€å»¶è¿Ÿèåˆå’ŒæŸ¥è¯¢é‡æ„ï¼‰ï¼Œå‘ç°å…ƒæ•°æ®é›†æˆèƒ½æå‡æ£€ç´¢å‡†ç¡®æ€§ï¼Œå¹¶é€šè¿‡åˆ†æåµŒå…¥ç©ºé—´æ­ç¤ºäº†å…ƒæ•°æ®å¢å¼ºæ–‡æ¡£å†…èšæ€§å’ŒåŒºåˆ†æ€§çš„æœºåˆ¶ã€‚ç ”ç©¶è¿˜å…¬å¼€äº†ä»£ç ã€è¯„ä¼°æ¡†æ¶å’ŒRAGMATE-10Kæ•°æ®é›†ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2026-01-16
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2601.12078v1">Optimizing User Profiles via Contextual Bandits for Retrieval-Augmented LLM Personalization</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) excel at general-purpose tasks, yet adapting their responses to individual users remains challenging. Retrieval augmentation provides a lightweight alternative to fine-tuning by conditioning LLMs on user history records, and existing approaches typically select these records based on semantic relevance. We argue that relevance serves as an unreliable proxy for utility: a record may be semantically similar to a query yet fail to improve generation quality or even degrade it due to redundancy or conflicting information. To bridge this gap, we propose PURPLE, a contextual bandit framework that oPtimizes UseR Profiles for Llm pErsonalization. In contrast to a greedy selection of the most relevant records, PURPLE treats profile construction as a set generation process and utilizes a Plackett-Luce ranking model to capture complex inter-record dependencies. By training with dense feedback provided by the likelihood of the reference response, our method aligns retrieval directly with generation quality. Extensive experiments on nine personalization tasks demonstrate that PURPLE consistently outperforms strong heuristic and retrieval-augmented baselines in both effectiveness and efficiency, establishing a principled and scalable solution for optimizing user profiles.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºPURPLEçš„ä¸Šä¸‹æ–‡è€è™æœºæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–ç”¨æˆ·å†å²è®°å½•çš„é€‰æ‹©æ¥æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¸ªæ€§åŒ–å“åº”èƒ½åŠ›ã€‚ä¼ ç»Ÿæ–¹æ³•åŸºäºè¯­ä¹‰ç›¸å…³æ€§é€‰æ‹©è®°å½•ï¼Œä½†PURPLEé€šè¿‡è€ƒè™‘è®°å½•é—´çš„å¤æ‚ä¾èµ–å…³ç³»åŠå…¶å¯¹ç”Ÿæˆè´¨é‡çš„ç›´æ¥å½±å“ï¼Œåˆ©ç”¨Plackett-Luceæ’åºæ¨¡å‹å’Œå¯†é›†åé¦ˆè®­ç»ƒï¼Œæ˜¾è‘—æé«˜äº†æ£€ç´¢å¢å¼ºç”Ÿæˆçš„æ•ˆæœå’Œæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPURPLEåœ¨å¤šä»»åŠ¡ä¸ªæ€§åŒ–åœºæ™¯ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.12075v1">To Copy or Not to Copy: Copying Is Easier to Induce Than Recall</a></td><td><details><summary>å±•å¼€</summary>Language models used in retrieval-augmented settings must arbitrate between parametric knowledge stored in their weights and contextual information in the prompt. This work presents a mechanistic study of that choice by extracting an \emph{arbitration vector} from model activations on a curated dataset designed to disentangle (i) irrelevant contexts that elicit parametric recall and (ii) relevant but false contexts that elicit copying. The vector is computed as the residual-stream centroid difference between these regimes across 27 relations, and is injected as an additive intervention at selected layers and token spans to steer behavior in two directions: Copy$\rightarrow$Recall (suppressing context use) and Recall$\rightarrow$Copy (inducing the model to copy any token from the context). Experiments on two architectures (decoder-only and encoder/decoder) and two open-domain QA benchmarks show consistent behavior shifts under moderate scaling while monitoring accuracy and fluency. Mechanistic analyses of attention routing, MLP contributions, and layer-wise probability trajectories reveal an asymmetry: inducing copying is an easy ``reactivation'' process that can be triggered at different locations in the input, while restoring recall is a ``suppression'' process that is more fragile and strongly tied to object-token interventions.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ç ”ç©¶äº†æ£€ç´¢å¢å¼ºåœºæ™¯ä¸­è¯­è¨€æ¨¡å‹åœ¨å‚æ•°åŒ–çŸ¥è¯†ï¼ˆå­˜å‚¨åœ¨æ¨¡å‹æƒé‡ä¸­ï¼‰ä¸ä¸Šä¸‹æ–‡ä¿¡æ¯ä¹‹é—´çš„æƒè¡¡æœºåˆ¶ï¼Œæå‡ºäº†é€šè¿‡æå–â€œä»²è£å‘é‡â€æ¥å¹²é¢„æ¨¡å‹è¡Œä¸ºçš„æ–¹æ³•ï¼Œå¹¶é€šè¿‡å®éªŒåœ¨å¼€æ”¾åŸŸQAä»»åŠ¡ä¸­éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨å¤„ç†æ— å…³/é”™è¯¯ä¸Šä¸‹æ–‡æ—¶çš„éå¯¹ç§°æœºåˆ¶ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.12061v1">Codebook-Injected Dialogue Segmentation for Multi-Utterance Constructs Annotation: LLM-Assisted and Gold-Label-Free Evaluation</a></td><td><details><summary>å±•å¼€</summary>Dialogue Act (DA) annotation typically treats communicative or pedagogical intent as localized to individual utterances or turns. This leads annotators to agree on the underlying action while disagreeing on segment boundaries, reducing apparent reliability. We propose codebook-injected segmentation, which conditions boundary decisions on downstream annotation criteria, and evaluate LLM-based segmenters against standard and retrieval-augmented baselines. To assess these without gold labels, we introduce evaluation metrics for span consistency, distinctiveness, and human-AI distributional agreement. We found DA-awareness produces segments that are internally more consistent than text-only baselines. While LLMs excel at creating construct-consistent spans, coherence-based baselines remain superior at detecting global shifts in dialogue flow. Across two datasets, no single segmenter dominates. Improvements in within-segment coherence frequently trade off against boundary distinctiveness and human-AI distributional agreement. These results highlight segmentation as a consequential design choice that should be optimized for downstream objectives rather than a single performance score.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åŸºäºæ£€ç´¢å¢å¼ºï¼ˆRAGï¼‰å’Œæ ‡å‡†åŸºçº¿çš„LLMå¯¹è¯åˆ†å‰²æ–¹æ³•ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆä¸‹æ¸¸æ ‡æ³¨æ ‡å‡†çš„â€œcodebook-injected segmentationâ€æŠ€æœ¯ï¼Œå¹¶å¼€å‘äº†è¯„ä¼°æŒ‡æ ‡æ¥è¡¡é‡åˆ†å‰²çš„ä¸€è‡´æ€§ã€åŒºåˆ†åº¦åŠäººæœºåˆ†å¸ƒä¸€è‡´æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œä¸åŒåˆ†å‰²æ–¹æ³•åœ¨ä¸åŒæ•°æ®é›†ä¸Šå„æœ‰ä¼˜åŠ£ï¼Œå¼ºè°ƒåˆ†å‰²è®¾è®¡åº”æ ¹æ®å…·ä½“ä¸‹æ¸¸ä»»åŠ¡ç›®æ ‡ä¼˜åŒ–ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.11888v1">Agentic-R: Learning to Retrieve for Agentic Search</a></td><td><details><summary>å±•å¼€</summary>Agentic search has recently emerged as a powerful paradigm, where an agent interleaves multi-step reasoning with on-demand retrieval to solve complex questions. Despite its success, how to design a retriever for agentic search remains largely underexplored. Existing search agents typically rely on similarity-based retrievers, while similar passages are not always useful for final answer generation. In this paper, we propose a novel retriever training framework tailored for agentic search. Unlike retrievers designed for single-turn retrieval-augmented generation (RAG) that only rely on local passage utility, we propose to use both local query-passage relevance and global answer correctness to measure passage utility in a multi-turn agentic search. We further introduce an iterative training strategy, where the search agent and the retriever are optimized bidirectionally and iteratively. Different from RAG retrievers that are only trained once with fixed questions, our retriever is continuously improved using evolving and higher-quality queries from the agent. Extensive experiments on seven single-hop and multi-hop QA benchmarks demonstrate that our retriever, termed \ours{}, consistently outperforms strong baselines across different search agents. Our codes are available at: https://github.com/8421BCD/Agentic-R.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§ä¸“ä¸ºä»£ç†æœç´¢è®¾è®¡çš„æ–°å‹æ£€ç´¢å™¨è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå±€éƒ¨æŸ¥è¯¢-æ®µè½ç›¸å…³æ€§å’Œå…¨å±€ç­”æ¡ˆæ­£ç¡®æ€§æ¥è¡¡é‡å¤šè½®ä»£ç†æœç´¢ä¸­çš„æ®µè½æ•ˆç”¨ï¼Œå¹¶é‡‡ç”¨è¿­ä»£è®­ç»ƒç­–ç•¥ä¼˜åŒ–æœç´¢ä»£ç†å’Œæ£€ç´¢å™¨ã€‚ä¸å•è½®RAGæ£€ç´¢å™¨ä¸åŒï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ä»£ç†ç”Ÿæˆçš„åŠ¨æ€é«˜è´¨é‡æŸ¥è¯¢æŒç»­æ”¹è¿›æ£€ç´¢å™¨ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨å¤šä¸ªé—®ç­”åŸºå‡†ä¸Šä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.11863v1">Utilizing Metadata for Better Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation systems depend on retrieving semantically relevant document chunks to support accurate, grounded outputs from large language models. In structured and repetitive corpora such as regulatory filings, chunk similarity alone often fails to distinguish between documents with overlapping language. Practitioners often flatten metadata into input text as a heuristic, but the impact and trade-offs of this practice remain poorly understood. We present a systematic study of metadata-aware retrieval strategies, comparing plain-text baselines with approaches that embed metadata directly. Our evaluation spans metadata-as-text (prefix and suffix), a dual-encoder unified embedding that fuses metadata and content in a single index, dual-encoder late-fusion retrieval, and metadata-aware query reformulation. Across multiple retrieval metrics and question types, we find that prefixing and unified embeddings consistently outperform plain-text baselines, with the unified at times exceeding prefixing while being easier to maintain. Beyond empirical comparisons, we analyze embedding space, showing that metadata integration improves effectiveness by increasing intra-document cohesion, reducing inter-document confusion, and widening the separation between relevant and irrelevant chunks. Field-level ablations show that structural cues provide strong disambiguating signals. Our code, evaluation framework, and the RAGMATE-10K dataset are publicly hosted.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶RAGç³»ç»Ÿä¸­ç»“æ„åŒ–æ–‡æœ¬ï¼ˆå¦‚ç›‘ç®¡æ–‡ä»¶ï¼‰çš„å…ƒæ•°æ®æ„ŸçŸ¥æ£€ç´¢ç­–ç•¥ï¼Œé€šè¿‡å¯¹æ¯”å¤šç§åµŒå…¥æ–¹æ³•ï¼ˆå¦‚å…ƒæ•°æ®å‰ç¼€/åç¼€ã€ç»Ÿä¸€åµŒå…¥ç­‰ï¼‰ï¼Œå‘ç°èåˆå…ƒæ•°æ®å¯æå‡æ–‡æ¡£å†…èšæ€§å’Œæ£€ç´¢æ•ˆæœï¼Œå¹¶æå‡ºå…¬å¼€æ•°æ®é›†ä¸è¯„ä¼°æ¡†æ¶ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.11825v1">AI Co-Scientist for Knowledge Synthesis in Medical Contexts: A Proof of Concept</a></td><td><details><summary>å±•å¼€</summary>Research waste in biomedical science is driven by redundant studies, incomplete reporting, and the limited scalability of traditional evidence synthesis workflows. We present an AI co-scientist for scalable and transparent knowledge synthesis based on explicit formalization of Population, Intervention, Comparator, Outcome, and Study design (PICOS). The platform integrates relational storage, vector-based semantic retrieval, and a Neo4j knowledge graph. Evaluation was conducted on dementia-sport and non-communicable disease corpora. Automated PICOS compliance and study design classification from titles and abstracts were performed using a Bidirectional Long Short-Term Memory baseline and a transformer-based multi-task classifier fine-tuned from PubMedBERT. Full-text synthesis employed retrieval-augmented generation with hybrid vector and graph retrieval, while BERTopic was used to identify thematic structure, redundancy, and evidence gaps. The transformer model achieved 95.7% accuracy for study design classification with strong agreement against expert annotations, while the Bi-LSTM achieved 87% accuracy for PICOS compliance detection. Retrieval-augmented generation outperformed non-retrieval generation for queries requiring structured constraints, cross-study integration, and graph-based reasoning, whereas non-retrieval approaches remained competitive for high-level summaries. Topic modeling revealed substantial thematic redundancy and identified underexplored research areas. These results demonstrate that PICOS-aware and explainable natural language processing can improve the scalability, transparency, and efficiency of evidence synthesis. The proposed architecture is domain-agnostic and offers a practical framework for reducing research waste across biomedical disciplines.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºPICOSæ¡†æ¶çš„AIååŒç§‘å­¦å®¶å¹³å°ï¼Œç”¨äºç”Ÿç‰©åŒ»å­¦çŸ¥è¯†åˆæˆï¼Œæ•´åˆäº†å…³ç³»å­˜å‚¨ã€å‘é‡è¯­ä¹‰æ£€ç´¢å’ŒçŸ¥è¯†å›¾è°±æŠ€æœ¯ã€‚ç ”ç©¶åœ¨ç—´å‘†ç—‡å’Œéä¼ æŸ“æ€§ç–¾ç—…è¯­æ–™åº“ä¸Šè¯„ä¼°äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ•ˆæœï¼Œæ˜¾ç¤ºå…¶åœ¨ç»“æ„åŒ–çº¦æŸã€è·¨ç ”ç©¶æ•´åˆå’Œå›¾æ¨ç†ä»»åŠ¡ä¸­ä¼˜äºéæ£€ç´¢ç”Ÿæˆæ–¹æ³•ï¼ŒåŒæ—¶åˆ©ç”¨BERTopicè¯†åˆ«ä¸»é¢˜å†—ä½™å’Œè¯æ®ç¼ºå£ï¼Œä»è€Œæå‡è¯æ®åˆæˆçš„å¯æ‰©å±•æ€§å’Œé€æ˜åº¦ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.11808v1">GPU-Resident Inverted File Index for Streaming Vector Databases</a></td><td><details><summary>å±•å¼€</summary>Vector search has emerged as the computational backbone of modern AI infrastructure, powering critical systems ranging from Vector Databases to Retrieval-Augmented Generation (RAG). While the GPU-accelerated Inverted File (IVF) index acts as one of the most widely used techniques for these large-scale workloads due to its memory efficiency, its traditional architecture remains fundamentally static. Existing designs rely on rigid and contiguous memory layouts that lack native support for in-place mutation, creating a severe bottleneck for streaming scenarios. In applications requiring real-time knowledge updates, such as live recommendation engines or dynamic RAG systems, maintaining index freshness necessitates expensive CPU-GPU roundtrips that cause system latency to spike from milliseconds to seconds. In this paper, we propose SIVF (Streaming Inverted File), a new GPU-native architecture designed to empower vector databases with high-velocity data ingestion and deletion capabilities. SIVF replaces the static memory layout with a slab-based allocation system and a validity bitmap, enabling lock-free and in-place mutation directly in VRAM. We further introduce a GPU-resident address translation table (ATT) to resolve the overhead of locating vectors, providing $O(1)$ access to physical storage slots. We evaluate SIVF against the industry-standard GPU IVF implementation on the SIFT1M and GIST1M datasets. Microbenchmarks demonstrate that SIVF reduces deletion latency by up to $13,300\times$ (from 11.8 seconds to 0.89 ms on GIST1M) and improves ingestion throughput by $36\times$ to $105\times$. In end-to-end sliding window scenarios, SIVF eliminates system freezes and achieves a $161\times$ to $266\times$ speedup with single-digit millisecond latency. Notably, this performance incurs negligible storage penalty, maintaining less than 0.8\% memory overhead compared to static indices.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSIVFï¼ˆStreaming Inverted Fileï¼‰çš„æ–°å‹GPUåŸç”Ÿæ¶æ„ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿé™æ€å€’æ’ç´¢å¼•ï¼ˆIVFï¼‰åœ¨å¤„ç†å®æ—¶æ•°æ®æ›´æ–°æ—¶çš„æ€§èƒ½ç“¶é¢ˆã€‚SIVFé€šè¿‡åŸºäºslabçš„å†…å­˜åˆ†é…ç³»ç»Ÿå’Œæœ‰æ•ˆæ€§ä½å›¾ï¼Œå®ç°äº†ç›´æ¥åœ¨VRAMä¸­è¿›è¡Œæ— é”å’ŒåŸåœ°ä¿®æ”¹çš„èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†å‘é‡æ•°æ®åº“åœ¨åŠ¨æ€RAGç³»ç»Ÿå’Œå®æ—¶æ¨èå¼•æ“ç­‰åœºæ™¯ä¸­çš„æ•°æ®å¤„ç†é€Ÿåº¦å’Œæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSIVFåœ¨åˆ é™¤å’Œæ’å…¥æ“ä½œä¸Šçš„æ€§èƒ½è¿œè¶…ä¼ ç»Ÿé™æ€ç´¢å¼•ï¼ŒåŒæ—¶ä¿æŒäº†æä½çš„å†…å­˜å¼€é”€ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.11722v1">RAC: Retrieval-Augmented Clarification for Faithful Conversational Search</a></td><td><details><summary>å±•å¼€</summary>Clarification questions help conversational search systems resolve ambiguous or underspecified user queries. While prior work has focused on fluency and alignment with user intent, especially through facet extraction, much less attention has been paid to grounding clarifications in the underlying corpus. Without such grounding, systems risk asking questions that cannot be answered from the available documents. We introduce RAC (Retrieval-Augmented Clarification), a framework for generating corpus-faithful clarification questions. After comparing several indexing strategies for retrieval, we fine-tune a large language model to make optimal use of research context and to encourage the generation of evidence-based question. We then apply contrastive preference optimization to favor questions supported by retrieved passages over ungrounded alternatives. Evaluated on four benchmarks, RAC demonstrate significant improvements over baselines. In addition to LLM-as-Judge assessments, we introduce novel metrics derived from NLI and data-to-text to assess how well questions are anchored in the context, and we demonstrate that our approach consistently enhances faithfulness.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†RACï¼ˆRetrieval-Augmented Clarificationï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆæ£€ç´¢æŠ€æœ¯å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”ŸæˆåŸºäºæ–‡æ¡£åº“çš„æ¾„æ¸…é—®é¢˜ï¼Œç¡®ä¿é—®é¢˜ä¸æ–‡æ¡£å†…å®¹ä¸€è‡´ï¼Œåˆ©ç”¨å¯¹æ¯”åå¥½ä¼˜åŒ–æå‡é—®é¢˜çš„å¯ä¿¡åº¦ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.11443v1">Predict the Retrieval! Test time adaptation for Retrieval Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has emerged as a powerful approach for enhancing large language models' question-answering capabilities through the integration of external knowledge. However, when adapting RAG systems to specialized domains, challenges arise from distribution shifts, resulting in suboptimal generalization performance. In this work, we propose TTARAG, a test-time adaptation method that dynamically updates the language model's parameters during inference to improve RAG system performance in specialized domains. Our method introduces a simple yet effective approach where the model learns to predict retrieved content, enabling automatic parameter adjustment to the target domain. Through extensive experiments across six specialized domains, we demonstrate that TTARAG achieves substantial performance improvements over baseline RAG systems. Code available at https://github.com/sunxin000/TTARAG.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºTTARAGçš„æµ‹è¯•æ—¶è‡ªé€‚åº”æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€æ›´æ–°è¯­è¨€æ¨¡å‹å‚æ•°æ¥ä¼˜åŒ–RAGç³»ç»Ÿåœ¨ä¸“ä¸šé¢†åŸŸä¸­çš„æ€§èƒ½ï¼Œè§£å†³äº†é¢†åŸŸåˆ†å¸ƒåç§»å¯¼è‡´çš„æ³›åŒ–é—®é¢˜ï¼Œå¹¶åœ¨å…­ä¸ªä¸“ä¸šé¢†åŸŸå®éªŒä¸­éªŒè¯äº†å…¶æ˜¾è‘—ä¼˜äºåŸºçº¿RAGç³»ç»Ÿçš„æ•ˆæœã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.11344v1">How Much Would a Clinician Edit This Draft? Evaluating LLM Alignment for Patient Message Response Drafting</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) show promise in drafting responses to patient portal messages, yet their integration into clinical workflows raises various concerns, including whether they would actually save clinicians time and effort in their portal workload. We investigate LLM alignment with individual clinicians through a comprehensive evaluation of the patient message response drafting task. We develop a novel taxonomy of thematic elements in clinician responses and propose a novel evaluation framework for assessing clinician editing load of LLM-drafted responses at both content and theme levels. We release an expert-annotated dataset and conduct large-scale evaluations of local and commercial LLMs using various adaptation techniques including thematic prompting, retrieval-augmented generation, supervised fine-tuning, and direct preference optimization. Our results reveal substantial epistemic uncertainty in aligning LLM drafts with clinician responses. While LLMs demonstrate capability in drafting certain thematic elements, they struggle with clinician-aligned generation in other themes, particularly question asking to elicit further information from patients. Theme-driven adaptation strategies yield improvements across most themes. Our findings underscore the necessity of adapting LLMs to individual clinician preferences to enable reliable and responsible use in patient-clinician communication workflows.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨èµ·è‰æ‚£è€…é—¨æˆ·æ¶ˆæ¯å›å¤ä¸­çš„åº”ç”¨ï¼Œæ¢è®¨äº†å¦‚ä½•é€šè¿‡åŒ…æ‹¬æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨å†…çš„å¤šç§é€‚é…æŠ€æœ¯ï¼ˆå¦‚ä¸»é¢˜æç¤ºã€ç›‘ç£å¾®è°ƒç­‰ï¼‰æ¥ä¼˜åŒ–LLMç”Ÿæˆçš„å†…å®¹ï¼Œä½¿å…¶æ›´ç¬¦åˆä¸´åºŠåŒ»ç”Ÿçš„åå¥½å’Œéœ€æ±‚ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶æ¥è¡¡é‡ç¼–è¾‘è´Ÿæ‹…ã€‚ç ”ç©¶å‘ç°LLMsåœ¨æŸäº›ä¸»é¢˜å…ƒç´ ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å…¶ä»–æ–¹é¢ï¼ˆå¦‚æé—®æ‚£è€…ï¼‰ä»æœ‰æŒ‘æˆ˜ï¼Œå¼ºè°ƒäº†ä¸ªæ€§åŒ–é€‚é…çš„é‡è¦æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.11342v1">Unlocking the Potentials of Retrieval-Augmented Generation for Diffusion Language Models</a></td><td><details><summary>å±•å¼€</summary>Diffusion Language Models (DLMs) have recently demonstrated remarkable capabilities in natural language processing tasks. However, the potential of Retrieval-Augmented Generation (RAG), which shows great successes for enhancing large language models (LLMs), has not been well explored, due to the fundamental difference between LLM and DLM decoding. To fill this critical gap, we systematically test the performance of DLMs within the RAG framework. Our findings reveal that DLMs coupled with RAG show promising potentials with stronger dependency on contextual information, but suffer from limited generation precision. We identify a key underlying issue: Response Semantic Drift (RSD), where the generated answer progressively deviates from the query's original semantics, leading to low precision content. We trace this problem to the denoising strategies in DLMs, which fail to maintain semantic alignment with the query throughout the iterative denoising process. To address this, we propose Semantic-Preserving REtrieval-Augmented Diffusion (SPREAD), a novel framework that introduces a query-relevance-guided denoising strategy. By actively guiding the denoising trajectory, SPREAD ensures the generation remains anchored to the query's semantics and effectively suppresses drift. Experimental results demonstrate that SPREAD significantly enhances the precision and effectively mitigates RSD of generated answers within the RAG framework.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆDLMsï¼‰åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ä¸­çš„è¡¨ç°ï¼Œå‘ç°DLMsç»“åˆRAGè™½ä¾èµ–ä¸Šä¸‹æ–‡ä¿¡æ¯ä½†å­˜åœ¨ç”Ÿæˆç²¾åº¦ä¸è¶³çš„é—®é¢˜ï¼Œä¸»è¦æºäºâ€œå“åº”è¯­ä¹‰æ¼‚ç§»â€ï¼ˆRSDï¼‰ã€‚ä½œè€…æå‡ºæ–°æ¡†æ¶SPREADï¼Œé€šè¿‡æŸ¥è¯¢ç›¸å…³æ€§å¼•å¯¼çš„å»å™ªç­–ç•¥è§£å†³è¯­ä¹‰æ¼‚ç§»ï¼Œæ˜¾è‘—æå‡äº†RAGæ¡†æ¶ä¸‹ç”Ÿæˆç­”æ¡ˆçš„ç²¾åº¦ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.11286v1">XChoice: Explainable Evaluation of AI-Human Alignment in LLM-based Constrained Choice Decision Making</a></td><td><details><summary>å±•å¼€</summary>We present XChoice, an explainable framework for evaluating AI-human alignment in constrained decision making. Moving beyond outcome agreement such as accuracy and F1 score, XChoice fits a mechanism-based decision model to human data and LLM-generated decisions, recovering interpretable parameters that capture the relative importance of decision factors, constraint sensitivity, and implied trade-offs. Alignment is assessed by comparing these parameter vectors across models, options, and subgroups. We demonstrate XChoice on Americans' daily time allocation using the American Time Use Survey (ATUS) as human ground truth, revealing heterogeneous alignment across models and activities and salient misalignment concentrated in Black and married groups. We further validate robustness of XChoice via an invariance analysis and evaluate targeted mitigation with a retrieval augmented generation (RAG) intervention. Overall, XChoice provides mechanism-based metrics that diagnose misalignment and support informed improvements beyond surface outcome matching.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºXChoiceçš„å¯è§£é‡Šæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å—é™å†³ç­–ä¸­AIä¸äººç±»çš„å¯¹é½æƒ…å†µï¼Œé€šè¿‡æœºåˆ¶é©±åŠ¨çš„å†³ç­–æ¨¡å‹åˆ†æäººç±»ä¸LLMå†³ç­–çš„å·®å¼‚ï¼Œå¹¶åœ¨éªŒè¯é˜¶æ®µé‡‡ç”¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å¹²é¢„æ¥è¯„ä¼°é’ˆå¯¹æ€§ç¼“è§£æªæ–½çš„æ•ˆæœã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.11273v1">Rank4Gen: RAG-Preference-Aligned Document Set Selection and Ranking</a></td><td><details><summary>å±•å¼€</summary>In the RAG paradigm, the information retrieval module provides context for generators by retrieving and ranking multiple documents to support the aggregation of evidence. However, existing ranking models are primarily optimized for query--document relevance, which often misaligns with generators' preferences for evidence selection and citation, limiting their impact on response quality. Moreover, most approaches do not account for preference differences across generators, resulting in unstable cross-generator performance. We propose \textbf{Rank4Gen}, a generator-aware ranker for RAG that targets the goal of \emph{Ranking for Generators}. Rank4Gen introduces two key preference modeling strategies: (1) \textbf{From Ranking Relevance to Response Quality}, which optimizes ranking with respect to downstream response quality rather than query--document relevance; and (2) \textbf{Generator-Specific Preference Modeling}, which conditions a single ranker on different generators to capture their distinct ranking preferences. To enable such modeling, we construct \textbf{PRISM}, a dataset built from multiple open-source corpora and diverse downstream generators. Experiments on five challenging and recent RAG benchmarks demonstrate that RRank4Gen achieves strong and competitive performance for complex evidence composition in RAG.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRank4Gençš„æ–°å‹æ’åºå™¨ï¼Œæ—¨åœ¨è§£å†³RAGç³»ç»Ÿä¸­æ£€ç´¢æ¨¡å—ä¸ç”Ÿæˆæ¨¡å‹åå¥½ä¸åŒ¹é…çš„é—®é¢˜ã€‚è¯¥ç ”ç©¶é€šè¿‡å°†æ’åºç›®æ ‡ä»æŸ¥è¯¢-æ–‡æ¡£ç›¸å…³æ€§è½¬å‘ä¸‹æ¸¸å“åº”è´¨é‡ï¼Œå¹¶å»ºæ¨¡ç”Ÿæˆå™¨ç‰¹å®šåå¥½ï¼Œä»è€Œä¼˜åŒ–è¯æ®é€‰æ‹©å’Œå¼•ç”¨ï¼Œæå‡å¤šåŸºå‡†æµ‹è¯•ä¸­çš„RAGæ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.11255v1">Reasoning in Trees: Improving Retrieval-Augmented Generation for Multi-Hop Question Answering</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has demonstrated significant effectiveness in enhancing large language models (LLMs) for complex multi-hop question answering (QA). For multi-hop QA tasks, current iterative approaches predominantly rely on LLMs to self-guide and plan multi-step exploration paths during retrieval, leading to substantial challenges in maintaining reasoning coherence across steps from inaccurate query decomposition and error propagation. To address these issues, we introduce Reasoning Tree Guided RAG (RT-RAG), a novel hierarchical framework for complex multi-hop QA. RT-RAG systematically decomposes multi-hop questions into explicit reasoning trees, minimizing inaccurate decomposition through structured entity analysis and consensus-based tree selection that clearly separates core queries, known entities, and unknown entities. Subsequently, a bottom-up traversal strategy employs iterative query rewriting and refinement to collect high-quality evidence, thereby mitigating error propagation. Comprehensive experiments show that RT-RAG substantially outperforms state-of-the-art methods by 7.0% F1 and 6.0% EM, demonstrating the effectiveness of RT-RAG in complex multi-hop QA.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRT-RAGçš„æ–°é¢–åˆ†å±‚æ¡†æ¶ï¼Œç”¨äºè§£å†³å¤æ‚å¤šè·³é—®ç­”ä»»åŠ¡ä¸­æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é¢ä¸´çš„æ¨ç†è¿è´¯æ€§é—®é¢˜ã€‚RT-RAGé€šè¿‡å°†å¤šè·³é—®é¢˜åˆ†è§£ä¸ºæ˜¾å¼çš„æ¨ç†æ ‘ï¼Œé‡‡ç”¨ç»“æ„åŒ–å®ä½“åˆ†æå’ŒåŸºäºå…±è¯†çš„æ ‘é€‰æ‹©æ¥å‡å°‘ä¸å‡†ç¡®åˆ†è§£ï¼Œå¹¶é€šè¿‡è‡ªåº•å‘ä¸Šçš„éå†ç­–ç•¥è¿›è¡ŒæŸ¥è¯¢é‡å†™å’Œè¯æ®æ”¶é›†ï¼Œä»è€Œæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨F1å’ŒEMæŒ‡æ ‡ä¸Šåˆ†åˆ«ä¼˜äºç°æœ‰æœ€ä½³æ–¹æ³•7.0%å’Œ6.0%ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.11199v1">SD-RAG: A Prompt-Injection-Resilient Framework for Selective Disclosure in Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has attracted significant attention due to its ability to combine the generative capabilities of Large Language Models (LLMs) with knowledge obtained through efficient retrieval mechanisms over large-scale data collections. Currently, the majority of existing approaches overlook the risks associated with exposing sensitive or access-controlled information directly to the generation model. Only a few approaches propose techniques to instruct the generative model to refrain from disclosing sensitive information; however, recent studies have also demonstrated that LLMs remain vulnerable to prompt injection attacks that can override intended behavioral constraints. For these reasons, we propose a novel approach to Selective Disclosure in Retrieval-Augmented Generation, called SD-RAG, which decouples the enforcement of security and privacy constraints from the generation process itself. Rather than relying on prompt-level safeguards, SD-RAG applies sanitization and disclosure controls during the retrieval phase, prior to augmenting the language model's input. Moreover, we introduce a semantic mechanism to allow the ingestion of human-readable dynamic security and privacy constraints together with an optimized graph-based data model that supports fine-grained, policy-aware retrieval. Our experimental evaluation demonstrates the superiority of SD-RAG over baseline existing approaches, achieving up to a $58\%$ improvement in the privacy score, while also showing a strong resilience to prompt injection attacks targeting the generative model.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSD-RAGçš„æ–°æ–¹æ³•ï¼Œä¸“æ³¨äºåœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­å®ç°é€‰æ‹©æ€§æŠ«éœ²ï¼Œé€šè¿‡åœ¨æ£€ç´¢é˜¶æ®µåº”ç”¨ä¿¡æ¯å‡€åŒ–å’ŒæŠ«éœ²æ§åˆ¶æ¥å¢å¼ºéšç§å’Œå®‰å…¨æ€§ï¼Œè€Œéä¾èµ–ç”Ÿæˆæ¨¡å‹çš„æç¤ºçº§é˜²æŠ¤ã€‚è¯¥æ–¹æ³•è¿˜å¼•å…¥äº†åŠ¨æ€å®‰å…¨ç­–ç•¥çš„è¯­ä¹‰æœºåˆ¶å’ŒåŸºäºå›¾çš„ä¼˜åŒ–æ•°æ®æ¨¡å‹ï¼Œå®éªŒæ˜¾ç¤ºå…¶æ˜¾è‘—æå‡äº†éšç§åˆ†æ•°å¹¶æŠµå¾¡é’ˆå¯¹ç”Ÿæˆæ¨¡å‹çš„æç¤ºæ³¨å…¥æ”»å‡»ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.11144v2">Deep GraphRAG: A Balanced Approach to Hierarchical Retrieval and Adaptive Integration</a></td><td><details><summary>å±•å¼€</summary>Graph-based Retrieval-Augmented Generation (GraphRAG) frameworks face a trade-off between the comprehensiveness of global search and the efficiency of local search. Existing methods are often challenged by navigating large-scale hierarchical graphs, optimizing retrieval paths, and balancing exploration-exploitation dynamics, frequently lacking robust multi-stage re-ranking. To overcome these deficits, we propose Deep GraphRAG, a framework designed for a balanced approach to hierarchical retrieval and adaptive integration. It introduces a hierarchical global-to-local retrieval strategy that integrates macroscopic inter-community and microscopic intra-community contextual relations. This strategy employs a three-stage process: (1) inter-community filtering, which prunes the search space using local context; (2) community-level refinement, which prioritizes relevant subgraphs via entity-interaction analysis; and (3) entity-level fine-grained search within target communities. A beam search-optimized dynamic re-ranking module guides this process, continuously filtering candidates to balance efficiency and global comprehensiveness. Deep GraphRAG also features a Knowledge Integration Module leveraging a compact LLM, trained with Dynamic Weighting Reward GRPO (DW-GRPO). This novel reinforcement learning approach dynamically adjusts reward weights to balance three key objectives: relevance, faithfulness, and conciseness. This training enables compact models (1.5B) to approach the performance of large models (70B) in the integration task. Evaluations on Natural Questions and HotpotQA demonstrate that Deep GraphRAG significantly outperforms baseline graph retrieval methods in both accuracy and efficiency.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDeep GraphRAGçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŸºäºå›¾çš„RAGç³»ç»Ÿåœ¨å…¨å±€æœç´¢å…¨é¢æ€§å’Œå±€éƒ¨æœç´¢æ•ˆç‡ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åˆ†å±‚å…¨å±€åˆ°å±€éƒ¨æ£€ç´¢ç­–ç•¥ï¼Œé€šè¿‡ä¸‰é˜¶æ®µè¿‡ç¨‹ï¼ˆç¤¾åŒºé—´è¿‡æ»¤ã€ç¤¾åŒºçº§ç»†åŒ–å’Œå®ä½“çº§ç»†ç²’åº¦æœç´¢ï¼‰ä¼˜åŒ–æ£€ç´¢è·¯å¾„ï¼Œå¹¶å¼•å…¥åŠ¨æ€é‡æ–°æ’åºæ¨¡å—å’ŒçŸ¥è¯†æ•´åˆæ¨¡å—ï¼Œä»¥æé«˜æ£€ç´¢çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeep GraphRAGåœ¨è‡ªç„¶é—®é¢˜å’ŒHotpotQAæ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.11024v1">PruneRAG: Confidence-Guided Query Decomposition Trees for Efficient Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) has become a powerful framework for enhancing large language models in knowledge-intensive and reasoning tasks. However, as reasoning chains deepen or search trees expand, RAG systems often face two persistent failures: evidence forgetting, where retrieved knowledge is not effectively used, and inefficiency, caused by uncontrolled query expansions and redundant retrieval. These issues reveal a critical gap between retrieval and evidence utilization in current RAG architectures. We propose PruneRAG, a confidence-guided query decomposition framework that builds a structured query decomposition tree to perform stable and efficient reasoning. PruneRAG introduces three key mechanisms: adaptive node expansion that regulates tree width and depth, confidence-guided decisions that accept reliable answers and prune uncertain branches, and fine-grained retrieval that extracts entity-level anchors to improve retrieval precision. Together, these components preserve salient evidence throughout multi-hop reasoning while significantly reducing retrieval overhead. To better analyze evidence misuse, we define the Evidence Forgetting Rate as a metric to quantify cases where golden evidence is retrieved but not correctly used. Extensive experiments across various multi-hop QA benchmarks show that PruneRAG achieves superior accuracy and efficiency over state-of-the-art baselines.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºPruneRAGçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³RAGç³»ç»Ÿåœ¨æ·±åº¦æ¨ç†æˆ–æœç´¢æ ‘æ‰©å±•æ—¶å‡ºç°çš„è¯æ®é—å¿˜å’Œæ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚PruneRAGé€šè¿‡ç»“æ„åŒ–æŸ¥è¯¢åˆ†è§£æ ‘ã€è‡ªé€‚åº”èŠ‚ç‚¹æ‰©å±•ã€ç½®ä¿¡åº¦å¼•å¯¼çš„å†³ç­–å’Œç»†ç²’åº¦æ£€ç´¢ç­‰æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†å¤šè·³æ¨ç†ä¸­çš„è¯æ®åˆ©ç”¨æ•ˆç‡å’Œæ£€ç´¢ç²¾åº¦ï¼Œå¹¶åœ¨å¤šä¸ªå¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.11004v1">NAACL: Noise-AwAre Verbal Confidence Calibration for LLMs in RAG Systems</a></td><td><details><summary>å±•å¼€</summary>Accurately assessing model confidence is essential for deploying large language models (LLMs) in mission-critical factual domains. While retrieval-augmented generation (RAG) is widely adopted to improve grounding, confidence calibration in RAG settings remains poorly understood. We conduct a systematic study across four benchmarks, revealing that LLMs exhibit poor calibration performance due to noisy retrieved contexts. Specifically, contradictory or irrelevant evidence tends to inflate the model's false certainty, leading to severe overconfidence. To address this, we propose NAACL Rules (Noise-AwAre Confidence CaLibration Rules) to provide a principled foundation for resolving overconfidence under noise. We further design NAACL, a noise-aware calibration framework that synthesizes supervision from about 2K HotpotQA examples guided by these rules. By performing supervised fine-tuning (SFT) with this data, NAACL equips models with intrinsic noise awareness without relying on stronger teacher models. Empirical results show that NAACL yields substantial gains, improving ECE scores by 10.9% in-domain and 8.0% out-of-domain. By bridging the gap between retrieval noise and verbal calibration, NAACL paves the way for both accurate and epistemically reliable LLMs.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç½®ä¿¡åº¦æ ¡å‡†é—®é¢˜ï¼Œå‘ç°å™ªå£°æ£€ç´¢ä¸Šä¸‹æ–‡ä¼šå¯¼è‡´æ¨¡å‹è¿‡åº¦è‡ªä¿¡ï¼Œå¹¶æå‡ºäº†ä¸€ç§å™ªå£°æ„ŸçŸ¥æ ¡å‡†æ¡†æ¶NAACLï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒæ˜¾è‘—æ”¹å–„äº†æ¨¡å‹çš„æ ¡å‡†æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.10955v1">Beyond Max Tokens: Stealthy Resource Amplification via Tool Calling Chains in LLM Agents</a></td><td><details><summary>å±•å¼€</summary>The agent-tool communication loop is a critical attack surface in modern Large Language Model (LLM) agents. Existing Denial-of-Service (DoS) attacks, primarily triggered via user prompts or injected retrieval-augmented generation (RAG) context, are ineffective for this new paradigm. They are fundamentally single-turn and often lack a task-oriented approach, making them conspicuous in goal-oriented workflows and unable to exploit the compounding costs of multi-turn agent-tool interactions. We introduce a stealthy, multi-turn economic DoS attack that operates at the tool layer under the guise of a correctly completed task. Our method adjusts text-visible fields and a template-governed return policy in a benign, Model Context Protocol (MCP)-compatible tool server, optimizing these edits with a Monte Carlo Tree Search (MCTS) optimizer. These adjustments leave function signatures unchanged and preserve the final payload, steering the agent into prolonged, verbose tool-calling sequences using text-only notices. This compounds costs across turns, escaping single-turn caps while keeping the final answer correct to evade validation. Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. It drives GPU KV cache occupancy from <1% to 35-74% and cuts co-running throughput by approximately 50%. Because the server remains protocol-compatible and task outcomes are correct, conventional checks fail. These results elevate the agent-tool interface to a first-class security frontier, demanding a paradigm shift from validating final answers to monitoring the economic and computational cost of the entire agentic process.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†é’ˆå¯¹ç°ä»£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†ä¸­å·¥å…·é€šä¿¡å¾ªç¯çš„æ–°å‹ç»æµå‹æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ï¼Œç‰¹åˆ«æåˆ°ç°æœ‰æ”»å‡»é€šè¿‡ç”¨æˆ·æç¤ºæˆ–æ³¨å…¥çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸Šä¸‹æ–‡è§¦å‘ï¼Œä½†æ•ˆæœæœ‰é™ã€‚ä½œè€…æå‡ºäº†ä¸€ç§éšè”½çš„å¤šè½®æ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡è°ƒæ•´å·¥å…·æœåŠ¡å™¨çš„æ–‡æœ¬å¯è§å­—æ®µå’Œè¿”å›ç­–ç•¥ï¼Œè¯±å¯¼ä»£ç†è¿›è¡Œé•¿æ—¶é—´ã€é«˜æˆæœ¬çš„å·¥å…·è°ƒç”¨åºåˆ—ï¼Œæ˜¾è‘—å¢åŠ è®¡ç®—èµ„æºå’Œèƒ½æºæ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒä»»åŠ¡ç»“æœçš„æ­£ç¡®æ€§ä»¥é€ƒé¿æ£€æµ‹ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2026-01-15
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2601.10681v1">Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems</a></td><td><details><summary>å±•å¼€</summary>Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºç»“æ„ä¿¡æ¯å’Œå¤šæ ·æ€§çº¦æŸçš„ä¸Šä¸‹æ–‡æ„å»ºæ¡†æ¶ï¼ˆcontext bubbleï¼‰ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸRAGæ–¹æ³•ä¸­ä¿¡æ¯ç¢ç‰‡åŒ–ã€è¿‡åº¦æ£€ç´¢å’Œå†…å®¹é‡å¤ç­‰é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»„ç»‡å¤šç²’åº¦æ–‡æœ¬ç‰‡æ®µï¼ˆå¦‚ç« èŠ‚å’Œè¡Œï¼‰ã€åˆ©ç”¨ä»»åŠ¡æ¡ä»¶åŒ–çš„ç»“æ„å…ˆéªŒæŒ‡å¯¼æ£€ç´¢ï¼Œå¹¶åœ¨ä¸¥æ ¼tokené¢„ç®—ä¸‹æ„å»ºè¿è´¯ä¸”å¯å¼•ç”¨çš„ä¸Šä¸‹æ–‡é›†åˆã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—å‡å°‘å†—ä½™ä¸Šä¸‹æ–‡ã€æ›´å¥½åœ°è¦†ç›–æ¬¡è¦ä¿¡æ¯ï¼Œå¹¶åœ¨æœ‰é™ä¸Šä¸‹æ–‡çª—å£ä¸­æé«˜å›ç­”è´¨é‡å’Œå¼•ç”¨å‡†ç¡®æ€§ã€‚æ¶ˆèç ”ç©¶éªŒè¯äº†ç»“æ„å…ˆéªŒå’Œå¤šæ ·æ€§çº¦æŸçš„å¿…è¦æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.10644v1">RoutIR: Fast Serving of Retrieval Pipelines for Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval models are key components of Retrieval-Augmented Generation (RAG) systems, which generate search queries, process the documents returned, and generate a response. RAG systems are often dynamic and may involve multiple rounds of retrieval. While many state-of-the-art retrieval methods are available through academic IR platforms, these platforms are typically designed for the Cranfield paradigm in which all queries are known up front and can be batch processed offline. This simplification accelerates research but leaves state-of-the-art retrieval models unable to support downstream applications that require online services, such as arbitrary dynamic RAG pipelines that involve looping, feedback, or even self-organizing agents. In this work, we introduce RoutIR, a Python package that provides a simple and efficient HTTP API that wraps arbitrary retrieval methods, including first stage retrieval, reranking, query expansion, and result fusion. By providing a minimal JSON configuration file specifying the retrieval models to serve, RoutIR can be used to construct and query retrieval pipelines on-the-fly using any permutation of available models (e.g., fusing the results of several first-stage retrieval methods followed by reranking). The API automatically performs asynchronous query batching and caches results by default. While many state-of-the-art retrieval methods are already supported by the package, RoutIR is also easily expandable by implementing the Engine abstract class. The package is open-sourced and publicly available on GitHub: http://github.com/hltcoe/routir.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªåä¸ºRoutIRçš„PythonåŒ…ï¼Œæ—¨åœ¨ä¸ºåŠ¨æ€RAGç³»ç»Ÿä¸­çš„æ£€ç´¢æ¨¡å‹æä¾›é«˜æ•ˆçš„HTTP APIæ”¯æŒï¼ŒåŒ…æ‹¬æŸ¥è¯¢æ‰©å±•ã€é‡æ’åºå’Œç»“æœèåˆç­‰åŠŸèƒ½ï¼Œè§£å†³ç°æœ‰æ£€ç´¢æ¨¡å‹éš¾ä»¥é€‚åº”åœ¨çº¿æœåŠ¡éœ€æ±‚çš„é—®é¢˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.10413v1">LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies</a></td><td><details><summary>å±•å¼€</summary>Privacy policies help inform people about organisations' personal data processing practices, covering different aspects such as data collection, data storage, and sharing of personal data with third parties. Privacy policies are often difficult for people to fully comprehend due to the lengthy and complex legal language used and inconsistent practices across different sectors and organisations. To help conduct automated and large-scale analyses of privacy policies, many researchers have studied applications of machine learning and natural language processing techniques, including large language models (LLMs). While a limited number of prior studies utilised LLMs for extracting personal data flows from privacy policies, our approach builds on this line of work by combining LLMs with retrieval-augmented generation (RAG) and a customised knowledge base derived from existing studies. This paper presents the development of LADFA, an end-to-end computational framework, which can process unstructured text in a given privacy policy, extract personal data flows and construct a personal data flow graph, and conduct analysis of the data flow graph to facilitate insight discovery. The framework consists of a pre-processor, an LLM-based processor, and a data flow post-processor. We demonstrated and validated the effectiveness and accuracy of the proposed approach by conducting a case study that involved examining ten selected privacy policies from the automotive industry. Moreover, it is worth noting that LADFA is designed to be flexible and customisable, making it suitable for a range of text-based analysis tasks beyond privacy policy analysis.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯çš„ç«¯åˆ°ç«¯è®¡ç®—æ¡†æ¶LADFAï¼Œç”¨äºä»éšç§æ”¿ç­–ä¸­æå–ä¸ªäººæ•°æ®æµå¹¶æ„å»ºæ•°æ®æµå›¾ï¼Œé€šè¿‡æ¡ˆä¾‹éªŒè¯äº†å…¶åœ¨æ±½è½¦è¡Œä¸šéšç§æ”¿ç­–åˆ†æä¸­çš„æœ‰æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.10342v1">C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing</a></td><td><details><summary>å±•å¼€</summary>Heart rate variability (HRV) is a pivotal noninvasive marker for autonomic monitoring; however, applying Large Language Models (LLMs) to HRV interpretation is hindered by physiological hallucinations. These include respiratory sinus arrhythmia (RSA) contamination, short-data instability in nonlinear metrics, and the neglect of individualized baselines in favor of population norms. We propose C-GRASP (Clinically-Grounded Reasoning for Affective Signal Processing), a guardrailed RAG-enhanced pipeline that decomposes HRV interpretation into eight traceable reasoning steps. Central to C-GRASP is a Z-score Priority Hierarchy that enforces the weighting of individualized baseline shifts over normative statistics. The system effectively mitigates spectral hallucinations through automated RSA-aware guardrails, preventing contamination of frequency-domain indices. Evaluated on 414 trials from the DREAMER dataset, C-GRASP integrated with high-scale reasoning models (e.g., MedGemma3-thinking) achieved superior performance in 4-class emotion classification (37.3% accuracy) and a Clinical Reasoning Consistency (CRC) score of 69.6%. Ablation studies confirm that the individualized Delta Z-score module serves as the critical logical anchor, preventing the "population bias" common in native LLMs. Ultimately, C-GRASP transitions affective computing from black-box classification to transparent, evidence-based clinical decision support, paving the way for safer AI integration in biomedical engineering.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†C-GRASPï¼ˆClinically-Grounded Reasoning for Affective Signal Processingï¼‰ï¼Œä¸€ç§åŸºäºRAGå¢å¼ºçš„æµç¨‹ï¼Œç”¨äºè§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¿ƒç‡å˜å¼‚æ€§ï¼ˆHRVï¼‰è§£é‡Šä¸­çš„ç”Ÿç†å¹»è§‰é—®é¢˜ã€‚C-GRASPé€šè¿‡åˆ†è§£HRVè§£é‡Šä¸ºå…«ä¸ªå¯è¿½è¸ªçš„æ¨ç†æ­¥éª¤ï¼Œå¹¶å¼•å…¥Z-scoreä¼˜å…ˆçº§å±‚æ¬¡ç»“æ„å’Œè‡ªåŠ¨åŒ–RSAæ„ŸçŸ¥é˜²æŠ¤æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†æƒ…ç»ªåˆ†ç±»çš„å‡†ç¡®æ€§å’Œä¸´åºŠæ¨ç†ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨DREAMERæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶æœ‰æ•ˆé¿å…äº†LLMså¸¸è§çš„â€œç¾¤ä½“åå·®â€ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.10246v1">coTherapist: A Behavior-Aligned Small Language Model to Support Mental Healthcare Experts</a></td><td><details><summary>å±•å¼€</summary>Access to mental healthcare is increasingly strained by workforce shortages and rising demand, motivating the development of intelligent systems that can support mental healthcare experts. We introduce coTherapist, a unified framework utilizing a small language model to emulate core therapeutic competencies through domain-specific fine-tuning, retrieval augmentation, and agentic reasoning. Evaluation on clinical queries demonstrates that coTherapist generates more relevant and clinically grounded responses than contemporary baselines. Using our novel T-BARS rubric and psychometric profiling, we confirm coTherapist exhibits high empathy and therapist-consistent personality traits. Furthermore, human evaluation by domain experts validates that coTherapist delivers accurate, trustworthy, and safe responses. coTherapist was deployed and tested by clinical experts. Collectively, these findings demonstrate that small models can be engineered to exhibit expert-like behavior, offering a scalable pathway for digital mental health tools.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†coTherapistæ¡†æ¶ï¼Œå®ƒé€šè¿‡é¢†åŸŸå¾®è°ƒã€æ£€ç´¢å¢å¼ºï¼ˆRAGï¼‰å’Œæ™ºèƒ½ä½“æ¨ç†ï¼Œåˆ©ç”¨å°å‹è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿæ ¸å¿ƒæ²»ç–—èƒ½åŠ›ï¼Œç”Ÿæˆæ¯”ç°æœ‰åŸºçº¿æ›´ç›¸å…³ä¸”ä¸´åºŠåŸºç¡€æ‰å®çš„å“åº”ï¼Œå¹¶åœ¨å¿ƒç†æµ‹è¯„å’Œä¸“å®¶è¯„ä¼°ä¸­å±•ç°å‡ºé«˜å…±æƒ…ã€ä¸“ä¸šæ€§å’Œå®‰å…¨æ€§ï¼Œä¸ºæ•°å­—å¿ƒç†å¥åº·å·¥å…·æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.10215v1">Topo-RAG: Topology-aware retrieval for hybrid text-table documents</a></td><td><details><summary>å±•å¼€</summary>In enterprise datasets, documents are rarely pure. They are not just text, nor just numbers; they are a complex amalgam of narrative and structure. Current Retrieval-Augmented Generation (RAG) systems have attempted to address this complexity with a blunt tool: linearization. We convert rich, multidimensional tables into simple Markdown-style text strings, hoping that an embedding model will capture the geometry of a spreadsheet in a single vector. But it has already been shown that this is mathematically insufficient.
  This work presents Topo-RAG, a framework that challenges the assumption that "everything is text". We propose a dual architecture that respects the topology of the data: we route fluid narrative through traditional dense retrievers, while tabular structures are processed by a Cell-Aware Late Interaction mechanism, preserving their spatial relationships. Evaluated on SEC-25, a synthetic enterprise corpus that mimics real-world complexity, Topo-RAG demonstrates an 18.4% improvement in nDCG@10 on hybrid queries compared to standard linearization approaches. It's not just about searching better; it's about understanding the shape of information.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†Topo-RAGæ¡†æ¶ï¼Œé’ˆå¯¹ä¼ä¸šæ•°æ®ä¸­æ··åˆæ–‡æœ¬å’Œè¡¨æ ¼çš„å¤æ‚æ–‡æ¡£ï¼Œæ”¹è¿›äº†ä¼ ç»ŸRAGç³»ç»Ÿç®€å•çº¿æ€§åŒ–å¤„ç†è¡¨æ ¼çš„ä¸è¶³ï¼Œé€šè¿‡åŒæ¶æ„åˆ†åˆ«å¤„ç†æ–‡æœ¬å’Œè¡¨æ ¼ç»“æ„ï¼ˆä¿ç•™ç©ºé—´å…³ç³»ï¼‰ï¼Œåœ¨åˆæˆæ•°æ®é›†SEC-25ä¸Šå®ç°äº†18.4%çš„nDCG@10æ€§èƒ½æå‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.10168v1">RAG-3DSG: Enhancing 3D Scene Graphs with Re-Shot Guided Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Open-vocabulary 3D Scene Graph (3DSG) generation can enhance various downstream tasks in robotics, such as manipulation and navigation, by leveraging structured semantic representations. A 3DSG is constructed from multiple images of a scene, where objects are represented as nodes and relationships as edges. However, existing works for open-vocabulary 3DSG generation suffer from both low object-level recognition accuracy and speed, mainly due to constrained viewpoints, occlusions, and redundant surface density. To address these challenges, we propose RAG-3DSG to mitigate aggregation noise through re-shot guided uncertainty estimation and support object-level Retrieval-Augmented Generation (RAG) via reliable low-uncertainty objects. Furthermore, we propose a dynamic downsample-mapping strategy to accelerate cross-image object aggregation with adaptive granularity. Experiments on Replica dataset demonstrate that RAG-3DSG significantly improves node captioning accuracy in 3DSG generation while reducing the mapping time by two-thirds compared to the vanilla version.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRAG-3DSGçš„æ–¹æ³•ï¼Œç”¨äºè§£å†³å¼€æ”¾è¯æ±‡3Dåœºæ™¯å›¾ç”Ÿæˆä¸­å¯¹è±¡è¯†åˆ«ç²¾åº¦ä½å’Œé€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡é‡æ–°æ‹æ‘„å¼•å¯¼çš„ä¸ç¡®å®šæ€§ä¼°è®¡å‡å°‘èšåˆå™ªå£°ï¼Œå¹¶åˆ©ç”¨å¯é çš„ä½ä¸ç¡®å®šæ€§å¯¹è±¡å®ç°å¯¹è±¡çº§çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ï¼ŒåŒæ—¶è¿˜é€šè¿‡åŠ¨æ€ä¸‹é‡‡æ ·æ˜ å°„ç­–ç•¥åŠ å¿«è·¨å›¾åƒå¯¹è±¡èšåˆé€Ÿåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†èŠ‚ç‚¹æ ‡æ³¨çš„å‡†ç¡®æ€§ï¼Œå¹¶å°†æ˜ å°„æ—¶é—´å‡å°‘äº†ä¸‰åˆ†ä¹‹äºŒã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.10131v1">M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints</a></td><td><details><summary>å±•å¼€</summary>Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multi-objective control and numeric reasoning without external structure and feedback. We introduce \textbf{M olGen}, a fragment-level, retrieval-augmented, two-stage framework for molecule generation under multi-property constraints. Stage I : Prototype generation: a multi-agent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II : RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multi-hop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMolGençš„ç‰‡æ®µçº§ã€æ£€ç´¢å¢å¼ºçš„ä¸¤é˜¶æ®µåˆ†å­ç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºåœ¨å¤šé‡ç‰©ç†åŒ–å­¦æ€§è´¨çº¦æŸä¸‹ç”Ÿæˆåˆ†å­ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡å¤šæ™ºèƒ½ä½“æ¨ç†å™¨è¿›è¡ŒåŸºäºæ£€ç´¢çš„ç‰‡æ®µçº§ç¼–è¾‘ï¼Œç”Ÿæˆæ¥è¿‘å¯è¡ŒåŒºåŸŸçš„å€™é€‰åˆ†å­ï¼›ç¬¬äºŒé˜¶æ®µåˆ©ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œç»†ç²’åº¦ä¼˜åŒ–ï¼Œæœ€å°åŒ–ç›®æ ‡æ€§è´¨è¯¯å·®ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è‡ªåŠ¨æ„å»ºçš„æ•°æ®é›†æ”¯æŒå¯æ§çš„å¤šè·³æ¨ç†ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨æ»¡è¶³å¤šé‡æ€§è´¨çº¦æŸæ–¹é¢ä¼˜äºç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œå›¾åŸºç®—æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.10011v1">Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL</a></td><td><details><summary>å±•å¼€</summary>Existing NL2SQL systems face two critical limitations: (1) they rely on in-context learning with only correct examples, overlooking the rich signal in historical error-fix pairs that could guide more robust self-correction; and (2) test-time scaling approaches often decompose questions arbitrarily, producing near-identical SQL candidates across runs and diminishing ensemble gains. Moreover, these methods suffer from a stark accuracy-efficiency trade-off: high performance demands excessive computation, while fast variants compromise quality. We present Memo-SQL, a training-free framework that addresses these issues through two simple ideas: structured decomposition and experience-aware self-correction. Instead of leaving decomposition to chance, we apply three clear strategies, entity-wise, hierarchical, and atomic sequential, to encourage diverse reasoning. For correction, we build a dynamic memory of both successful queries and historical error-fix pairs, and use retrieval-augmented prompting to bring relevant examples into context at inference time, no fine-tuning or external APIs required. On BIRD, Memo-SQL achieves 68.5% execution accuracy, setting a new state of the art among open, zero-fine-tuning methods, while using over 10 times fewer resources than prior TTS approaches.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†Memo-SQLæ¡†æ¶ï¼Œé€šè¿‡ç»“æ„åŒ–åˆ†è§£å’Œç»éªŒæ„ŸçŸ¥çš„è‡ªæ ¡æ­£æŠ€æœ¯æ”¹è¿›NL2SQLç³»ç»Ÿã€‚å…¶æ ¸å¿ƒåˆ›æ–°ç‚¹åŒ…æ‹¬åˆ©ç”¨æ£€ç´¢å¢å¼ºæç¤ºï¼ˆretrieval-augmented promptingï¼‰åŠ¨æ€è°ƒç”¨å†å²æˆåŠŸæŸ¥è¯¢å’Œé”™è¯¯ä¿®æ­£å¯¹ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œæ— éœ€å¾®è°ƒå³å¯æå‡SQLç”Ÿæˆçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œåœ¨BIRDåŸºå‡†ä¸Šä»¥æ›´ä½èµ„æºæ¶ˆè€—è¾¾åˆ°å½“å‰é›¶å¾®è°ƒæ–¹æ³•çš„æœ€é«˜æ°´å¹³ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.09985v1">FaTRQ: Tiered Residual Quantization for LLM Vector Search in Far-Memory-Aware ANNS Systems</a></td><td><details><summary>å±•å¼€</summary>Approximate Nearest-Neighbor Search (ANNS) is a key technique in retrieval-augmented generation (RAG), enabling rapid identification of the most relevant high-dimensional embeddings from massive vector databases. Modern ANNS engines accelerate this process using prebuilt indexes and store compressed vector-quantized representations in fast memory. However, they still rely on a costly second-pass refinement stage that reads full-precision vectors from slower storage like SSDs. For modern text and multimodal embeddings, these reads now dominate the latency of the entire query. We propose FaTRQ, a far-memory-aware refinement system using tiered memory that eliminates the need to fetch full vectors from storage. It introduces a progressive distance estimator that refines coarse scores using compact residuals streamed from far memory. Refinement stops early once a candidate is provably outside the top-k. To support this, we propose tiered residual quantization, which encodes residuals as ternary values stored efficiently in far memory. A custom accelerator is deployed in a CXL Type-2 device to perform low-latency refinement locally. Together, FaTRQ improves the storage efficiency by 2.4$\times$ and improves the throughput by up to 9$ \times$ than SOTA GPU ANNS system.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºFaTRQçš„è¿œå†…å­˜æ„ŸçŸ¥ä¼˜åŒ–ç³»ç»Ÿï¼Œç”¨äºè§£å†³æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ï¼ˆANNSï¼‰çš„ç“¶é¢ˆé—®é¢˜ã€‚é€šè¿‡åˆ†å±‚å†…å­˜å’Œæ¸è¿›å¼è·ç¦»ä¼°è®¡æŠ€æœ¯ï¼ŒFaTRQé¿å…äº†ä»æ…¢é€Ÿå­˜å‚¨ä¸­è¯»å–å…¨ç²¾åº¦å‘é‡çš„éœ€æ±‚ï¼Œæ˜¾è‘—æå‡äº†å­˜å‚¨æ•ˆç‡å’ŒæŸ¥è¯¢ååé‡ï¼Œç›¸æ¯”ç°æœ‰GPU ANNSç³»ç»Ÿå®ç°äº†2.4å€çš„å­˜å‚¨æ•ˆç‡æå‡å’Œæœ€é«˜9å€çš„ååé‡æå‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.09982v1">Context Volume Drives Performance: Tackling Domain Shift in Extremely Low-Resource Translation via RAG</a></td><td><details><summary>å±•å¼€</summary>Neural Machine Translation (NMT) models for low-resource languages suffer significant performance degradation under domain shift. We quantify this challenge using Dhao, an indigenous language of Eastern Indonesia with no digital footprint beyond the New Testament (NT). When applied to the unseen Old Testament (OT), a standard NMT model fine-tuned on the NT drops from an in-domain score of 36.17 chrF++ to 27.11 chrF++. To recover this loss, we introduce a hybrid framework where a fine-tuned NMT model generates an initial draft, which is then refined by a Large Language Model (LLM) using Retrieval-Augmented Generation (RAG). The final system achieves 35.21 chrF++ (+8.10 recovery), effectively matching the original in-domain quality. Our analysis reveals that this performance is driven primarily by the number of retrieved examples rather than the choice of retrieval algorithm. Qualitative analysis confirms the LLM acts as a robust "safety net," repairing severe failures in zero-shot domains.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡é’ˆå¯¹ä½èµ„æºè¯­è¨€ï¼ˆå°å°¼åœŸè‘—è¯­è¨€Dhaoï¼‰åœ¨é¢†åŸŸè¿ç§»æ—¶ç¥ç»æœºå™¨ç¿»è¯‘ï¼ˆNMTï¼‰æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ··åˆæ¡†æ¶ï¼šå…ˆé€šè¿‡å¾®è°ƒçš„NMTæ¨¡å‹ç”Ÿæˆåˆç¨¿ï¼Œå†ä½¿ç”¨åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œä¿®æ­£ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœªè§è¿‡çš„ã€Šæ—§çº¦ã€‹é¢†åŸŸæ˜¾è‘—æå‡äº†ç¿»è¯‘è´¨é‡ï¼ˆchrF++ä»27.11æ¢å¤è‡³35.21ï¼‰ï¼Œä¸”æ€§èƒ½ä¸»è¦ä¾èµ–æ£€ç´¢ç¤ºä¾‹æ•°é‡è€Œéæ£€ç´¢ç®—æ³•é€‰æ‹©ï¼ŒLLMèƒ½æœ‰æ•ˆä¿®å¤é›¶æ ·æœ¬é¢†åŸŸçš„ä¸¥é‡é”™è¯¯ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2026-01-12
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2601.09028v1">OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG</a></td><td><details><summary>å±•å¼€</summary>The development of large language models (LLMs) has achieved superior performance in a range of downstream tasks, including LLM-based retrieval-augmented generation (RAG). The quality of generated content heavily relies on the usefulness of the retrieved information and the capacity of LLMs' internal information processing mechanism to incorporate it in answer generation. It is generally assumed that the retrieved information is relevant to the question. However, the retrieved information may have a variable degree of relevance and usefulness, depending on the question and the document collection. It is important to take into account the relevance of the retrieved information in answer generation. In this paper, we propose OpenDecoder, a new approach that leverages explicit evaluation of the retrieved information as quality indicator features for generation. We aim to build a RAG model that is more robust to varying levels of noisy context. Three types of explicit evaluation information are considered: relevance score, ranking score, and QPP (query performance prediction) score. The experimental results on five benchmark datasets demonstrate the effectiveness and better robustness of OpenDecoder by outperforming various baseline methods. Importantly, this paradigm is flexible to be integrated with the post-training of LLMs for any purposes and incorporated with any type of external indicators.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºOpenDecoderçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡æ˜¾å¼è¯„ä¼°æ£€ç´¢ä¿¡æ¯çš„è´¨é‡ï¼ˆç›¸å…³æ€§è¯„åˆ†ã€æ’åºè¯„åˆ†å’ŒQPPè¯„åˆ†ï¼‰æ¥å¢å¼ºRAGæ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œæ—¨åœ¨æå‡æ¨¡å‹å¯¹å™ªå£°ä¸Šä¸‹æ–‡çš„é²æ£’æ€§ï¼Œå¹¶åœ¨å®éªŒä¸­éªŒè¯äº†å…¶ä¼˜äºåŸºçº¿æ–¹æ³•çš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.08773v1">Reliable Graph-RAG for Codebases: AST-Derived Graphs vs LLM-Extracted Knowledge Graphs</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation for software engineering often relies on vector similarity search, which captures topical similarity but can fail on multi-hop architectural reasoning such as controller to service to repository chains, interface-driven wiring, and inheritance. This paper benchmarks three retrieval pipelines on Java codebases (Shopizer, with additional runs on ThingsBoard and OpenMRS Core): (A) vector-only No-Graph RAG, (B) an LLM-generated knowledge graph RAG (LLM-KB), and (C) a deterministic AST-derived knowledge graph RAG (DKB) built with Tree-sitter and bidirectional traversal.
  Using 15 architecture and code-tracing queries per repository, we measure indexing time, query latency, corpus coverage, cost, and answer correctness. DKB builds its graph in seconds, while LLM-KB requires much longer graph generation. LLM-KB also shows indexing incompleteness: on Shopizer, 377 files are skipped or missed, reducing embedded chunk coverage and graph size compared to DKB. End-to-end cost is modest for DKB relative to the vector-only baseline but much higher for LLM-KB, especially as repository scale increases. Query latency is similar for No-Graph and DKB, while LLM-KB is slower and more variable. On the Shopizer question suite, DKB achieves the highest correctness, LLM-KB is close behind, and the vector-only baseline performs worst on upstream architectural queries and has the highest hallucination risk. Overall, deterministic AST-derived graphs provide more reliable coverage and multi-hop grounding than LLM-extracted graphs at substantially lower indexing cost.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åœ¨è½¯ä»¶å·¥ç¨‹é¢†åŸŸä¸­æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯çš„åº”ç”¨ï¼Œæ¯”è¾ƒäº†ä¸‰ç§ä¸åŒçš„æ£€ç´¢æµç¨‹ï¼ˆåŸºäºå‘é‡çš„æ— å›¾RAGã€åŸºäºLLMç”Ÿæˆçš„çŸ¥è¯†å›¾è°±RAGå’ŒåŸºäºç¡®å®šæ€§ASTæ´¾ç”Ÿçš„çŸ¥è¯†å›¾è°±RAGï¼‰åœ¨Javaä»£ç åº“ä¸Šçš„æ€§èƒ½è¡¨ç°ï¼ŒåŒ…æ‹¬ç´¢å¼•æ—¶é—´ã€æŸ¥è¯¢å»¶è¿Ÿã€è¦†ç›–èŒƒå›´å’Œç­”æ¡ˆæ­£ç¡®æ€§ç­‰æŒ‡æ ‡ï¼Œæœ€ç»ˆå‘ç°åŸºäºASTçš„ç¡®å®šæ€§çŸ¥è¯†å›¾è°±RAGåœ¨è¦†ç›–èŒƒå›´å’Œå¤šè·³æ¨ç†æ–¹é¢è¡¨ç°æœ€ä½³ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.08747v2">To Retrieve or To Think? An Agentic Approach for Context Evolution</a></td><td><details><summary>å±•å¼€</summary>Current context augmentation methods, such as retrieval-augmented generation, are essential for solving knowledge-intensive reasoning tasks. However, they typically adhere to a rigid, brute-force strategy that executes retrieval at every step. This indiscriminate approach not only incurs unnecessary computational costs but also degrades performance by saturating the context with irrelevant noise. To address these limitations, we introduce Agentic Context Evolution (ACE), a framework inspired by human metacognition that dynamically determines whether to seek new evidence or reason with existing knowledge. ACE employs a central orchestrator agent to make decisions strategically via majority voting. It aims to alternate between activating a retriever agent for external retrieval and a reasoner agent for internal analysis and refinement. By eliminating redundant retrieval steps, ACE maintains a concise and evolved context. Extensive experiments on challenging multi-hop QA benchmarks demonstrate that ACE significantly outperforms competitive baselines in accuracy while achieving efficient token consumption. Our work provides valuable insights into advancing context-evolved generation for complex, knowledge-intensive tasks.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºAgentic Context Evolution (ACE)çš„æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹è¿›ç°æœ‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ã€‚ACEé€šè¿‡åŠ¨æ€å†³ç­–æœºåˆ¶ï¼ˆå¦‚åè°ƒå™¨ä»£ç†çš„å¤šæ•°æŠ•ç¥¨ï¼‰é€‰æ‹©æ€§è§¦å‘æ£€ç´¢æˆ–æ¨ç†ï¼Œå‡å°‘å†—ä½™æ£€ç´¢é€ æˆçš„è®¡ç®—å¼€é”€å’Œå™ªå£°å¹²æ‰°ï¼Œåœ¨å¤æ‚å¤šè·³é—®ç­”ä»»åŠ¡ä¸­å®ç°äº†æ›´é«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.08741v1">From Rows to Reasoning: A Retrieval-Augmented Multimodal Framework for Spreadsheet Understanding</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) struggle to reason over large-scale enterprise spreadsheets containing thousands of numeric rows, multiple linked sheets, and embedded visual content such as charts and receipts. Prior state-of-the-art spreadsheet reasoning approaches typically rely on single-sheet compression or full-context encoding, which limits scalability and fails to reflect how real users interact with complex, multimodal workbooks. We introduce FRTR-Bench, the first large-scale benchmark for multimodal spreadsheet reasoning, comprising 30 enterprise-grade Excel workbooks spanning nearly four million cells and more than 50 embedded images. To address these challenges, we present From Rows to Reasoning (FRTR), an advanced, multimodal retrieval-augmented generation framework that decomposes Excel workbooks into granular row, column, and block embeddings, employs hybrid lexical-dense retrieval with Reciprocal Rank Fusion (RRF), and integrates multimodal embeddings to reason over both numerical and visual information. We tested FRTR on six LLMs, achieving 74% answer accuracy on FRTR-Bench with Claude Sonnet 4.5, a substantial improvement over prior state-of-the-art approaches that reached only 24%. On the SpreadsheetLLM benchmark, FRTR achieved 87% accuracy with GPT-5 while reducing token usage by roughly 50% compared to context-compression methods.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†FRTRï¼ˆFrom Rows to Reasoningï¼‰æ¡†æ¶ï¼Œä¸€ç§é’ˆå¯¹å¤šæ¨¡æ€ç”µå­è¡¨æ ¼ï¼ˆå¦‚Excelå·¥ä½œç°¿ï¼‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ï¼Œé€šè¿‡ç»†ç²’åº¦åµŒå…¥ã€æ··åˆæ£€ç´¢æŠ€æœ¯å’Œå¤šæ¨¡æ€é›†æˆè§£å†³å¤§è¯­è¨€æ¨¡å‹å¤„ç†å¤§è§„æ¨¡ä¼ä¸šçº§è¡¨æ ¼çš„éš¾é¢˜ï¼Œå¹¶åœ¨åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.08739v1">PrivGemo: Privacy-Preserving Dual-Tower Graph Retrieval for Empowering LLM Reasoning with Memory Augmentation</a></td><td><details><summary>å±•å¼€</summary>Knowledge graphs (KGs) provide structured evidence that can ground large language model (LLM) reasoning for knowledge-intensive question answering. However, many practical KGs are private, and sending retrieved triples or exploration traces to closed-source LLM APIs introduces leakage risk. Existing privacy treatments focus on masking entity names, but they still face four limitations: structural leakage under semantic masking, uncontrollable remote interaction, fragile multi-hop and multi-entity reasoning, and limited experience reuse for stability and efficiency. To address these issues, we propose PrivGemo, a privacy-preserving retrieval-augmented framework for KG-grounded reasoning with memory-guided exposure control. PrivGemo uses a dual-tower design to keep raw KG knowledge local while enabling remote reasoning over an anonymized view that goes beyond name masking to limit both semantic and structural exposure. PrivGemo supports multi-hop, multi-entity reasoning by retrieving anonymized long-hop paths that connect all topic entities, while keeping grounding and verification on the local KG. A hierarchical controller and a privacy-aware experience memory further reduce unnecessary exploration and remote interactions. Comprehensive experiments on six benchmarks show that PrivGemo achieves overall state-of-the-art results, outperforming the strongest baseline by up to 17.1%. Furthermore, PrivGemo enables smaller models (e.g., Qwen3-4B) to achieve reasoning performance comparable to that of GPT-4-Turbo.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºPrivGemoçš„éšç§ä¿æŠ¤æ£€ç´¢å¢å¼ºæ¡†æ¶ï¼Œç”¨äºåŸºäºçŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰çš„æ¨ç†ã€‚è¯¥æ¡†æ¶é€šè¿‡åŒå¡”è®¾è®¡ä¿æŒåŸå§‹KGçŸ¥è¯†æœ¬åœ°åŒ–ï¼ŒåŒæ—¶æ”¯æŒåœ¨åŒ¿åè§†å›¾ä¸Šè¿›è¡Œè¿œç¨‹æ¨ç†ï¼Œä»¥é™åˆ¶è¯­ä¹‰å’Œç»“æ„æš´éœ²ã€‚PrivGemoé€šè¿‡æ£€ç´¢è¿æ¥æ‰€æœ‰ä¸»é¢˜å®ä½“çš„åŒ¿åé•¿è·³è·¯å¾„æ¥æ”¯æŒå¤šè·³ã€å¤šå®ä½“æ¨ç†ï¼Œå¹¶åœ¨æœ¬åœ°KGä¸Šè¿›è¡ŒéªŒè¯ã€‚å®éªŒè¡¨æ˜ï¼ŒPrivGemoåœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç”šè‡³ä½¿è¾ƒå°æ¨¡å‹è¾¾åˆ°ä¸GPT-4-Turboç›¸å½“çš„æ¨ç†æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.08699v1">RAGShaper: Eliciting Sophisticated Agentic RAG Skills via Automated Data Synthesis</a></td><td><details><summary>å±•å¼€</summary>Agentic Retrieval-Augmented Generation (RAG) empowers large language models to autonomously plan and retrieve information for complex problem-solving. However, the development of robust agents is hindered by the scarcity of high-quality training data that reflects the noise and complexity of real-world retrieval environments. Conventional manual annotation is unscalable and often fails to capture the dynamic reasoning strategies required to handle retrieval failures. To bridge this gap, we introduce RAGShaper, a novel data synthesis framework designed to automate the construction of RAG tasks and robust agent trajectories. RAGShaper incorporates an InfoCurator to build dense information trees enriched with adversarial distractors spanning Perception and Cognition levels. Furthermore, we propose a constrained navigation strategy that forces a teacher agent to confront these distractors, thereby eliciting trajectories that explicitly demonstrate error correction and noise rejection. Comprehensive experiments confirm that models trained on our synthesized corpus significantly outperform existing baselines, exhibiting superior robustness in noise-intensive and complex retrieval tasks.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºRAGShaperçš„æ–°å‹æ•°æ®åˆæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ™ºèƒ½RAGä»£ç†è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºåŒ…å«å¯¹æŠ—æ€§å¹²æ‰°ä¿¡æ¯çš„å¯†é›†ä¿¡æ¯æ ‘ï¼Œå¹¶åˆ©ç”¨çº¦æŸå¯¼èˆªç­–ç•¥ç”Ÿæˆèƒ½å¤Ÿåº”å¯¹æ£€ç´¢å¤±è´¥å’Œå™ªå£°çš„é²æ£’ä»£ç†è½¨è¿¹ï¼Œå®éªŒè¡¨æ˜åŸºäºæ­¤æ¡†æ¶è®­ç»ƒçš„æ¨¡å‹åœ¨å¤æ‚æ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.08676v2">Advancing ESG Intelligence: An Expert-level Agent and Comprehensive Benchmark for Sustainable Finance</a></td><td><details><summary>å±•å¼€</summary>Environmental, social, and governance (ESG) criteria are essential for evaluating corporate sustainability and ethical performance. However, professional ESG analysis is hindered by data fragmentation across unstructured sources, and existing large language models (LLMs) often struggle with the complex, multi-step workflows required for rigorous auditing. To address these limitations, we introduce ESGAgent, a hierarchical multi-agent system empowered by a specialized toolset, including retrieval augmentation, web search and domain-specific functions, to generate in-depth ESG analysis. Complementing this agentic system, we present a comprehensive three-level benchmark derived from 310 corporate sustainability reports, designed to evaluate capabilities ranging from atomic common-sense questions to the generation of integrated, in-depth analysis. Empirical evaluations demonstrate that ESGAgent outperforms state-of-the-art closed-source LLMs with an average accuracy of 84.15% on atomic question-answering tasks, and excels in professional report generation by integrating rich charts and verifiable references. These findings confirm the diagnostic value of our benchmark, establishing it as a vital testbed for assessing general and advanced agentic capabilities in high-stakes vertical domains.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ä»‹ç»äº†ESGAgentï¼Œä¸€ä¸ªåˆ©ç”¨æ£€ç´¢å¢å¼ºï¼ˆRetrieval Augmentationï¼‰ç­‰å¤šå·¥å…·ååŒçš„åˆ†å±‚å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œç”¨äºè§£å†³ESGåˆ†æä¸­çš„æ•°æ®ç¢ç‰‡åŒ–å’Œå¤æ‚å·¥ä½œæµé—®é¢˜ï¼Œå¹¶é€šè¿‡æ„å»ºä¸‰çº§åŸºå‡†æµ‹è¯•éªŒè¯å…¶åœ¨åŸå­é—®ç­”å’Œä¸“ä¸šæŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§èƒ½ï¼ˆ84.15%å‡†ç¡®ç‡ï¼‰ï¼Œå‡¸æ˜¾äº†æ£€ç´¢å¢å¼ºæŠ€æœ¯åœ¨å‚ç›´é¢†åŸŸçš„å…³é”®ä½œç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.08670v1">Parallel Context-of-Experts Decoding for Retrieval Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval Augmented Generation faces a trade-off: concatenating documents in a long prompt enables multi-document reasoning but creates prefill bottlenecks, while encoding document KV caches separately offers speed but breaks cross-document interaction. We propose Parallel Context-of-Experts Decoding (Pced), a training-free framework that shifts evidence aggregation from the attention mechanism to the decoding. Pced treats retrieved documents as isolated "experts", synchronizing their predictions via a novel retrieval-aware contrastive decoding rule that weighs expert logits against the model prior. This approach recovers cross-document reasoning capabilities without constructing a shared attention across documents.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºParallel Context-of-Experts Decoding (Pced)çš„è®­ç»ƒæ— å…³æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³RAGä¸­å¤šæ–‡æ¡£æ£€ç´¢ä¸æ¨ç†çš„æ•ˆç‡é—®é¢˜ã€‚Pcedé€šè¿‡å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£è§†ä¸ºç‹¬ç«‹çš„"ä¸“å®¶"ï¼Œå¹¶åˆ©ç”¨ä¸€ç§æ–°é¢–çš„æ£€ç´¢æ„ŸçŸ¥å¯¹æ¯”è§£ç è§„åˆ™åŒæ­¥å®ƒä»¬çš„é¢„æµ‹ï¼Œä»è€Œåœ¨ä¸æ„å»ºè·¨æ–‡æ¡£å…±äº«æ³¨æ„åŠ›çš„æƒ…å†µä¸‹æ¢å¤äº†è·¨æ–‡æ¡£çš„æ¨ç†èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.08620v1">ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://hf.co/vidore.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ä»‹ç»äº†ViDoRe v3ï¼Œä¸€ä¸ªé’ˆå¯¹å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„ç»¼åˆè¯„ä¼°åŸºå‡†ã€‚è¯¥åŸºå‡†æ—¨åœ¨è§£å†³å¤æ‚æŒ‘æˆ˜ï¼Œå¦‚è§£æè§†è§‰å…ƒç´ ã€è·¨æ–‡æ¡£ä¿¡æ¯æ•´åˆå’Œç²¾ç¡®æº¯æºã€‚å®ƒåŒ…å«æ¥è‡ªå¤šä¸ªä¸“ä¸šé¢†åŸŸçš„çº¦26,000ä»½æ–‡æ¡£å’Œ3,099æ¡äººå·¥éªŒè¯æŸ¥è¯¢ï¼Œæ”¯æŒ6ç§è¯­è¨€ã€‚é€šè¿‡å¯¹å…ˆè¿›RAGç®¡é“çš„è¯„ä¼°ï¼Œç ”ç©¶å‘ç°è§†è§‰æ£€ç´¢å™¨ä¼˜äºæ–‡æœ¬æ£€ç´¢å™¨ï¼Œå¹¶æŒ‡å‡ºå½“å‰æ¨¡å‹åœ¨å¤„ç†éæ–‡æœ¬å…ƒç´ ç­‰æ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.08559v1">WaterCopilot: An AI-Driven Virtual Assistant for Water Management</a></td><td><details><summary>å±•å¼€</summary>Sustainable water resource management in transboundary river basins is challenged by fragmented data, limited real-time access, and the complexity of integrating diverse information sources. This paper presents WaterCopilot-an AI-driven virtual assistant developed through collaboration between the International Water Management Institute (IWMI) and Microsoft Research for the Limpopo River Basin (LRB) to bridge these gaps through a unified, interactive platform. Built on Retrieval-Augmented Generation (RAG) and tool-calling architectures, WaterCopilot integrates static policy documents and real-time hydrological data via two custom plugins: the iwmi-doc-plugin, which enables semantic search over indexed documents using Azure AI Search, and the iwmi-api-plugin, which queries live databases to deliver dynamic insights such as environmental-flow alerts, rainfall trends, reservoir levels, water accounting, and irrigation data. The system features guided multilingual interactions (English, Portuguese, French), transparent source referencing, automated calculations, and visualization capabilities. Evaluated using the RAGAS framework, WaterCopilot achieves an overall score of 0.8043, with high answer relevancy (0.8571) and context precision (0.8009). Key innovations include automated threshold-based alerts, integration with the LRB Digital Twin, and a scalable deployment pipeline hosted on AWS. While limitations in processing non-English technical documents and API latency remain, WaterCopilot establishes a replicable AI-augmented framework for enhancing water governance in data-scarce, transboundary contexts. The study demonstrates the potential of this AI assistant to support informed, timely decision-making and strengthen water security in complex river basins.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ä»‹ç»äº†WaterCopilotâ€”â€”ä¸€ä¸ªåŸºäºRAGå’Œå·¥å…·è°ƒç”¨æ¶æ„çš„AIè™šæ‹ŸåŠ©æ‰‹ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆé™æ€æ”¿ç­–æ–‡æ¡£å’Œå®æ—¶æ°´æ–‡æ•°æ®ï¼ˆå¦‚ç¯å¢ƒæµé‡è­¦æŠ¥ã€é™é›¨è¶‹åŠ¿ç­‰ï¼‰ï¼Œä¸ºè·¨å›½ç•Œæ²³æµæµåŸŸï¼ˆå¦‚æ—æ³¢æ³¢æ²³ç›†åœ°ï¼‰æä¾›ç»Ÿä¸€äº¤äº’å¹³å°ï¼Œæ”¯æŒå¤šè¯­è¨€äº¤äº’ã€é€æ˜æ•°æ®å¼•ç”¨åŠè‡ªåŠ¨åŒ–è®¡ç®—ï¼Œå¹¶é€šè¿‡RAGASæ¡†æ¶è¯„ä¼°æ˜¾ç¤ºå…¶é«˜æ•ˆæ€§ï¼ˆæ€»åˆ†0.8043ï¼‰ï¼Œæœ€ç»ˆæå‡ºå¯æ‰©å±•çš„AIå¢å¼ºæ°´æ²»ç†æ¡†æ¶ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.08545v2">Learner-Tailored Program Repair: A Solution Generator with Iterative Edit-Driven Retrieval Enhancement</a></td><td><details><summary>å±•å¼€</summary>With the development of large language models (LLMs) in the field of programming, intelligent programming coaching systems have gained widespread attention. However, most research focuses on repairing the buggy code of programming learners without providing the underlying causes of the bugs. To address this gap, we introduce a novel task, namely LRP (Learner-Tailored Program Repair). We then propose a novel and effective framework, LSGEN (Learner-Tailored Solution Generator), to enhance program repair while offering the bug descriptions for the buggy code. In the first stage, we utilize a repair solution retrieval framework to construct a solution retrieval database and then employ an edit-driven code retrieval approach to retrieve valuable solutions, guiding LLMs in identifying and fixing the bugs in buggy code. In the second stage, we propose a solution-guided program repair method, which fixes the code and provides explanations under the guidance of retrieval solutions. Moreover, we propose an Iterative Retrieval Enhancement method that utilizes evaluation results of the generated code to iteratively optimize the retrieval direction and explore more suitable repair strategies, improving performance in practical programming coaching scenarios. The experimental results show that our approach outperforms a set of baselines by a large margin, validating the effectiveness of our framework for the newly proposed LPR task.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºLSGENçš„æ¡†æ¶ï¼Œç”¨äºç¼–ç¨‹è¾…å¯¼ç³»ç»Ÿä¸­çš„å­¦ä¹ è€…å®šåˆ¶ç¨‹åºä¿®å¤ï¼ˆLRPï¼‰ä»»åŠ¡ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Œé¦–å…ˆé€šè¿‡ä¿®å¤è§£å†³æ–¹æ¡ˆæ£€ç´¢æ¡†æ¶æ„å»ºè§£å†³æ–¹æ¡ˆæ•°æ®åº“ï¼Œå¹¶é‡‡ç”¨ç¼–è¾‘é©±åŠ¨çš„ä»£ç æ£€ç´¢æ–¹æ³•æ£€ç´¢æœ‰ä»·å€¼çš„è§£å†³æ–¹æ¡ˆï¼Œä»¥æŒ‡å¯¼å¤§è¯­è¨€æ¨¡å‹è¯†åˆ«å’Œä¿®å¤ä»£ç é”™è¯¯ï¼›éšåé€šè¿‡è§£å†³æ–¹æ¡ˆå¼•å¯¼çš„ç¨‹åºä¿®å¤æ–¹æ³•ç”Ÿæˆä¿®å¤åçš„ä»£ç åŠè§£é‡Šã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§è¿­ä»£æ£€ç´¢å¢å¼ºæ–¹æ³•ï¼Œåˆ©ç”¨ç”Ÿæˆä»£ç çš„è¯„ä¼°ç»“æœä¼˜åŒ–æ£€ç´¢æ–¹å‘ï¼Œæå‡å®é™…ç¼–ç¨‹è¾…å¯¼åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.08311v1">Enhancing Image Quality Assessment Ability of LMMs via Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Large Multimodal Models (LMMs) have recently shown remarkable promise in low-level visual perception tasks, particularly in Image Quality Assessment (IQA), demonstrating strong zero-shot capability. However, achieving state-of-the-art performance often requires computationally expensive fine-tuning methods, which aim to align the distribution of quality-related token in output with image quality levels. Inspired by recent training-free works for LMM, we introduce IQARAG, a novel, training-free framework that enhances LMMs' IQA ability. IQARAG leverages Retrieval-Augmented Generation (RAG) to retrieve some semantically similar but quality-variant reference images with corresponding Mean Opinion Scores (MOSs) for input image. These retrieved images and input image are integrated into a specific prompt. Retrieved images provide the LMM with a visual perception anchor for IQA task. IQARAG contains three key phases: Retrieval Feature Extraction, Image Retrieval, and Integration & Quality Score Generation. Extensive experiments across multiple diverse IQA datasets, including KADID, KonIQ, LIVE Challenge, and SPAQ, demonstrate that the proposed IQARAG effectively boosts the IQA performance of LMMs, offering a resource-efficient alternative to fine-tuning for quality assessment.</details></td><td><details><summary>å±•å¼€</summary></details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.08288v1">OpenMic: A Multi-Agent-Based Stand-Up Comedy Generation System</a></td><td><details><summary>å±•å¼€</summary>Chinese stand-up comedy generation goes beyond plain text generation, requiring culturally grounded humor, precise timing, stage-performance cues, and implicit multi-step reasoning. Moreover, commonly used Chinese humor datasets are often better suited for humor understanding and evaluation than for long-form stand-up generation, making direct supervision misaligned with the target task. To address these challenges, we present OpenMic, an end-to-end multi-agent system built on AutoGen that transforms a user-provided life topic into a 3-5 minute Chinese stand-up performance and further produces a narrated comedy video. OpenMic orchestrates multiple specialized agents in a multi-round iterative loop-planning to jointly optimize humor, timing, and performability. To mitigate the dataset-task mismatch, we augment generation with retrieval-augmented generation (RAG) for material grounding and idea expansion, and we fine-tune a dedicated JokeWriter to better internalize stand-up-specific setup-punchline structures and long-range callbacks.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†OpenMicç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºAutoGençš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨å°†ç”¨æˆ·è¾“å…¥çš„ç”Ÿæ´»ä¸»é¢˜è½¬åŒ–ä¸º3-5åˆ†é’Ÿçš„ä¸­æ–‡è„±å£ç§€è¡¨æ¼”è§†é¢‘ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å¤šè½®è¿­ä»£åä½œä¼˜åŒ–å¹½é»˜æ€§ã€èŠ‚å¥æ„Ÿå’Œè¡¨æ¼”æ€§ï¼›é’ˆå¯¹æ•°æ®é›†ä¸ä»»åŠ¡ä¸åŒ¹é…çš„é—®é¢˜ï¼Œç‰¹åˆ«é‡‡ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯è¿›è¡Œç´ æå…³è”å’Œåˆ›æ„æ‰©å±•ï¼Œå¹¶å¾®è°ƒä¸“é—¨çš„JokeWriteræ¨¡å‹ä»¥æ›´å¥½æŒæ¡è„±å£ç§€ç‰¹æœ‰çš„åŒ…è¢±ç»“æ„å’Œé•¿ç¨‹callbackæŠ€å·§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.08282v1">D$^2$Plan: Dual-Agent Dynamic Global Planning for Complex Retrieval-Augmented Reasoning</a></td><td><details><summary>å±•å¼€</summary>Recent search-augmented LLMs trained with reinforcement learning (RL) can interleave searching and reasoning for multi-hop reasoning tasks. However, they face two critical failure modes as the accumulating context becomes flooded with both crucial evidence and irrelevant information: (1) ineffective search chain construction that produces incorrect queries or omits retrieval of critical information, and (2) reasoning hijacking by peripheral evidence that causes models to misidentify distractors as valid evidence. To address these challenges, we propose **D$^2$Plan**, a **D**ual-agent **D**ynamic global **Plan**ning paradigm for complex retrieval-augmented reasoning. **D$^2$Plan** operates through the collaboration of a *Reasoner* and a *Purifier*: the *Reasoner* constructs explicit global plans during reasoning and dynamically adapts them based on retrieval feedback; the *Purifier* assesses retrieval relevance and condenses key information for the *Reasoner*. We further introduce a two-stage training framework consisting of supervised fine-tuning (SFT) cold-start on synthesized trajectories and RL with plan-oriented rewards to teach LLMs to master the **D$^2$Plan** paradigm. Extensive experiments demonstrate that **D$^2$Plan** enables more coherent multi-step reasoning and stronger resilience to irrelevant information, thereby achieving superior performance on challenging QA benchmarks.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸º**D$^2$Plan**çš„åŒä»£ç†åŠ¨æ€å…¨å±€è§„åˆ’èŒƒå¼ï¼Œç”¨äºè§£å†³æ£€ç´¢å¢å¼ºæ¨ç†ä¸­çš„æœç´¢é“¾æ„å»ºæ— æ•ˆå’Œæ¨ç†è¢«å¹²æ‰°çš„é—®é¢˜ã€‚é€šè¿‡åˆä½œçš„ä¸¤ä¸ªä»£ç†ï¼ˆ*Reasoner*å’Œ*Purifier*ï¼‰ï¼Œè¯¥æ–¹æ³•å®ç°äº†å…¨å±€è§„åˆ’çš„åŠ¨æ€è°ƒæ•´å’Œæ£€ç´¢ä¿¡æ¯çš„è¿‡æ»¤ä¼˜åŒ–ï¼Œå¹¶ç»“åˆç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†å¤šæ­¥æ¨ç†çš„è¿è´¯æ€§å’Œå¯¹æ— å…³ä¿¡æ¯çš„é²æ£’æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.08226v1">Knowledge-based learning in Text-RAG and Image-RAG</a></td><td><details><summary>å±•å¼€</summary>This research analyzed and compared the multi-modal approach in the Vision Transformer(EVA-ViT) based image encoder with the LlaMA or ChatGPT LLM to reduce the hallucination problem and detect diseases in chest x-ray images. In this research, we utilized the NIH Chest X-ray image to train the model and compared it in image-based RAG, text-based RAG, and baseline. [3] [5] In a result, the text-based RAG[2] e!ectively reduces the hallucination problem by using external knowledge information, and the image-based RAG improved the prediction con"dence and calibration by using the KNN methods. [4] Moreover, the GPT LLM showed better performance, a low hallucination rate, and better Expected Calibration Error(ECE) than Llama Llama-based model. This research shows the challenge of data imbalance, a complex multi-stage structure, but suggests a large experience environment and a balanced example of use.</details></td><td><details><summary>å±•å¼€</summary>è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºVision Transformerï¼ˆEVA-ViTï¼‰å›¾åƒç¼–ç å™¨ä¸LLMï¼ˆå¦‚LlaMAæˆ–ChatGPTï¼‰çš„å¤šæ¨¡æ€æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆå›¾åƒå’Œæ–‡æœ¬çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯æ¥å‡å°‘å¹»è§‰é—®é¢˜å¹¶æå‡èƒ¸éƒ¨Xå…‰å›¾åƒçš„ç–¾ç—…æ£€æµ‹æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œæ–‡æœ¬RAGåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†æœ‰æ•ˆé™ä½å¹»è§‰ï¼Œå›¾åƒRAGé€šè¿‡KNNæ–¹æ³•æé«˜é¢„æµ‹ç½®ä¿¡åº¦ï¼Œä¸”GPTæ¨¡å‹åœ¨æ€§èƒ½å’Œæ ¡å‡†è¯¯å·®ä¸Šä¼˜äºLlaMAï¼ŒåŒæ—¶æ­ç¤ºäº†æ•°æ®ä¸å¹³è¡¡å’Œå¤šé˜¶æ®µç»“æ„çš„æŒ‘æˆ˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.08209v1">Generation-Augmented Generation: A Plug-and-Play Framework for Private Knowledge Injection in Large Language Models</a></td><td><details><summary>å±•å¼€</summary>In domains such as biomedicine, materials, and finance, high-stakes deployment of large language models (LLMs) requires injecting private, domain-specific knowledge that is proprietary, fast-evolving, and under-represented in public pretraining. However, the two dominant paradigms for private knowledge injection each have pronounced drawbacks: fine-tuning is expensive to iterate, and continual updates risk catastrophic forgetting and general-capability regression; retrieval-augmented generation (RAG) keeps the base model intact but is brittle in specialized private corpora due to chunk-induced evidence fragmentation, retrieval drift, and long-context pressure that yields query-dependent prompt inflation. Inspired by how multimodal LLMs align heterogeneous modalities into a shared semantic space, we propose Generation-Augmented Generation (GAG), which treats private expertise as an additional expert modality and injects it via a compact, representation-level interface aligned to the frozen base model, avoiding prompt-time evidence serialization while enabling plug-and-play specialization and scalable multi-domain composition with reliable selective activation. Across two private scientific QA benchmarks (immunology adjuvant and catalytic materials) and mixed-domain evaluations, GAG improves specialist performance over strong RAG baselines by 15.34% and 14.86% on the two benchmarks, respectively, while maintaining performance on six open general benchmarks and enabling near-oracle selective activation for scalable multi-domain deployment.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºGeneration-Augmented Generation (GAG)çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³åœ¨ç§æœ‰é¢†åŸŸçŸ¥è¯†æ³¨å…¥ä¸­RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰çš„å±€é™æ€§ï¼Œå¦‚è¯æ®ç¢ç‰‡åŒ–ã€æ£€ç´¢æ¼‚ç§»å’Œé•¿ä¸Šä¸‹æ–‡å‹åŠ›ç­‰é—®é¢˜ã€‚GAGé€šè¿‡å°†ç§æœ‰ä¸“ä¸šçŸ¥è¯†è§†ä¸ºé¢å¤–çš„ä¸“å®¶æ¨¡æ€ï¼Œå¹¶ä¸å†»ç»“çš„åŸºç¡€æ¨¡å‹å¯¹é½ï¼Œå®ç°äº†æ— éœ€æç¤ºæ—¶è¯æ®åºåˆ—åŒ–çš„å³æ’å³ç”¨ä¸“ä¸šåŒ–ï¼Œå¹¶åœ¨ä¸¤ä¸ªç§æœ‰ç§‘å­¦QAåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºRAGåŸºçº¿ï¼ŒåŒæ—¶ä¿æŒäº†åœ¨å¼€æ”¾é€šç”¨åŸºå‡†ä¸Šçš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.08105v1">Query Suggestion for Retrieval-Augmented Generation via Dynamic In-Context Learning</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation with tool-calling agents (agentic RAG) has become increasingly powerful in understanding, processing, and responding to user queries. However, the scope of the grounding knowledge is limited and asking questions that exceed this scope may lead to issues like hallucination. While guardrail frameworks aim to block out-of-scope questions (Rodriguez et al., 2024), no research has investigated the question of suggesting answerable queries in order to complete the user interaction.
  In this paper, we initiate the study of query suggestion for agentic RAG. We consider the setting where user questions are not answerable, and the suggested queries should be similar to aid the user interaction. Such scenarios are frequent for tool-calling LLMs as communicating the restrictions of the tools or the underlying datasets to the user is difficult, and adding query suggestions enhances the interaction with the RAG agent. As opposed to traditional settings for query recommendations such as in search engines, ensuring that the suggested queries are answerable is a major challenge due to the RAG's multi-step workflow that demands a nuanced understanding of the RAG as a whole, which the executing LLM lacks. As such, we introduce robust dynamic few-shot learning which retrieves examples from relevant workflows. We show that our system can be self-learned, for instance on prior user queries, and is therefore easily applicable in practice. We evaluate our approach on three benchmark datasets based on two unlabeled question datasets collected from real-world user queries. Experiments on real-world datasets confirm that our method produces more relevant and answerable suggestions, outperforming few-shot and retrieval-only baselines, and thus enable safer, more effective user interaction with agentic RAG.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†ä»£ç†å¼RAGï¼ˆagentic RAGï¼‰ä¸­çš„æŸ¥è¯¢å»ºè®®é—®é¢˜ï¼Œé’ˆå¯¹ç”¨æˆ·æå‡ºçš„ä¸å¯å›ç­”é—®é¢˜æå‡ºç›¸ä¼¼ä¸”å¯å›ç­”çš„æ›¿ä»£æŸ¥è¯¢ï¼Œä»¥å¢å¼ºäº¤äº’ä½“éªŒã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŠ¨æ€å°æ ·æœ¬å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æ£€ç´¢ç›¸å…³å·¥ä½œæµç¤ºä¾‹æ¥ç”Ÿæˆæ›´ç›¸å…³ã€å¯å›ç­”çš„æŸ¥è¯¢å»ºè®®ï¼Œå¹¶åœ¨çœŸå®æ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.07790v1">Benchmarking Small Language Models and Small Reasoning Language Models on System Log Severity Classification</a></td><td><details><summary>å±•å¼€</summary>System logs are crucial for monitoring and diagnosing modern computing infrastructure, but their scale and complexity require reliable and efficient automated interpretation. Since severity levels are predefined metadata in system log messages, having a model merely classify them offers limited standalone practical value, revealing little about its underlying ability to interpret system logs. We argue that severity classification is more informative when treated as a benchmark for probing runtime log comprehension rather than as an end task. Using real-world journalctl data from Linux production servers, we evaluate nine small language models (SLMs) and small reasoning language models (SRLMs) under zero-shot, few-shot, and retrieval-augmented generation (RAG) prompting. The results reveal strong stratification. Qwen3-4B achieves the highest accuracy at 95.64% with RAG, while Gemma3-1B improves from 20.25% under few-shot prompting to 85.28% with RAG. Notably, the tiny Qwen3-0.6B reaches 88.12% accuracy despite weak performance without retrieval. In contrast, several SRLMs, including Qwen3-1.7B and DeepSeek-R1-Distill-Qwen-1.5B, degrade substantially when paired with RAG. Efficiency measurements further separate models: most Gemma and Llama variants complete inference in under 1.2 seconds per log, whereas Phi-4-Mini-Reasoning exceeds 228 seconds per log while achieving <10% accuracy. These findings suggest that (1) architectural design, (2) training objectives, and (3) the ability to integrate retrieved context under strict output constraints jointly determine performance. By emphasizing small, deployable models, this benchmark aligns with real-time requirements of digital twin (DT) systems and shows that severity classification serves as a lens for evaluating model competence and real-time deployability, with implications for root cause analysis (RCA) and broader DT integration.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æ¢è®¨äº†ç³»ç»Ÿæ—¥å¿—ä¸¥é‡æ€§åˆ†ç±»ä½œä¸ºè¯„ä¼°æ—¥å¿—ç†è§£èƒ½åŠ›çš„åŸºå‡†ï¼Œé€šè¿‡å®éªŒæ¯”è¾ƒäº†å¤šç§å°å‹è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬åŠRAGæç¤ºä¸‹çš„è¡¨ç°ï¼Œå‘ç°RAGèƒ½æ˜¾è‘—æå‡éƒ¨åˆ†æ¨¡å‹æ€§èƒ½ï¼ˆå¦‚Gemma3-1Bä»20.25%æå‡è‡³85.28%ï¼‰ï¼Œå¹¶åˆ†æäº†æ¨¡å‹æ¶æ„ã€è®­ç»ƒç›®æ ‡ä¸æ£€ç´¢ä¸Šä¸‹æ–‡æ•´åˆèƒ½åŠ›å¯¹ç»“æœçš„å½±å“ï¼Œå¼ºè°ƒäº†æ¨¡å‹åœ¨æ•°å­—å­ªç”Ÿç³»ç»Ÿä¸­çš„å®æ—¶éƒ¨ç½²ä»·å€¼ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.07711v1">Is Agentic RAG worth it? An experimental comparison of RAG approaches</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) systems are usually defined by the combination of a generator and a retrieval component that extracts textual context from a knowledge base to answer user queries. However, such basic implementations exhibit several limitations, including noisy or suboptimal retrieval, misuse of retrieval for out-of-scope queries, weak query-document matching, and variability or cost associated with the generator. These shortcomings have motivated the development of "Enhanced" RAG, where dedicated modules are introduced to address specific weaknesses in the workflow. More recently, the growing self-reflective capabilities of Large Language Models (LLMs) have enabled a new paradigm, which we refer to as "Agentic" RAG. In this approach, the LLM orchestrates the entire process-deciding which actions to perform, when to perform them, and whether to iterate-thereby reducing reliance on fixed, manually engineered modules. Despite the rapid adoption of both paradigms, it remains unclear which approach is preferable under which conditions. In this work, we conduct an extensive, empirically driven evaluation of Enhanced and Agentic RAG across multiple scenarios and dimensions. Our results provide practical insights into the trade-offs between the two paradigms, offering guidance on selecting the most effective RAG design for real-world applications, considering both costs and performance.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡è®¨è®ºäº†å¢å¼ºå‹ï¼ˆEnhancedï¼‰å’Œè‡ªä¸»å‹ï¼ˆAgenticï¼‰RAGç³»ç»Ÿçš„æ¼”è¿›ï¼Œå‰è€…é€šè¿‡ä¸“ç”¨æ¨¡å—è§£å†³ä¼ ç»ŸRAGçš„ç¼ºé™·ï¼ˆå¦‚å™ªå£°æ£€ç´¢ã€æŸ¥è¯¢èŒƒå›´é—®é¢˜ç­‰ï¼‰ï¼Œåè€…åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹çš„è‡ªåæ€èƒ½åŠ›åŠ¨æ€åè°ƒæµç¨‹ã€‚æ–‡ç« é€šè¿‡å¤šåœºæ™¯å®è¯å¯¹æ¯”ä¸¤ç§èŒƒå¼ï¼Œä¸ºå®é™…åº”ç”¨ä¸­çš„æˆæœ¬ä¸æ€§èƒ½æƒè¡¡æä¾›æŒ‡å¯¼ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.07528v1">From RAG to Agentic RAG for Faithful Islamic Question Answering</a></td><td><details><summary>å±•å¼€</summary>LLMs are increasingly used for Islamic question answering, where ungrounded responses may carry serious religious consequences. Yet standard MCQ/MRC-style evaluations do not capture key real-world failure modes, notably free-form hallucinations and whether models appropriately abstain when evidence is lacking. To shed a light on this aspect we introduce ISLAMICFAITHQA, a 3,810-item bilingual (Arabic/English) generative benchmark with atomic single-gold answers, which enables direct measurement of hallucination and abstention. We additionally developed an end-to-end grounded Islamic modelling suite consisting of (i) 25K Arabic text-grounded SFT reasoning pairs, (ii) 5K bilingual preference samples for reward-guided alignment, and (iii) a verse-level Qur'an retrieval corpus of $\sim$6k atomic verses (ayat). Building on these resources, we develop an agentic Quran-grounding framework (agentic RAG) that uses structured tool calls for iterative evidence seeking and answer revision. Experiments across Arabic-centric and multilingual LLMs show that retrieval improves correctness and that agentic RAG yields the largest gains beyond standard RAG, achieving state-of-the-art performance and stronger Arabic-English robustness even with a small model (i.e., Qwen3 4B). We will make the experimental resources and datasets publicly available for the community.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ISLAMICFAITHQAï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°ä¼Šæ–¯å…°é—®ç­”ç³»ç»Ÿæ€§èƒ½çš„åŒè¯­ç”ŸæˆåŸºå‡†ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„åœ°é¢ä¼Šæ–¯å…°å»ºæ¨¡å¥—ä»¶ï¼ŒåŒ…æ‹¬åŸºäºæ–‡æœ¬çš„ç›‘ç£å¾®è°ƒå¯¹ã€åŒè¯­åå¥½æ ·æœ¬å’Œå¤å…°ç»è¯—å¥çº§æ£€ç´¢è¯­æ–™åº“ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜å¼€å‘äº†ä¸€ä¸ªåŸºäºä»£ç†çš„Quran-groundingæ¡†æ¶ï¼ˆagentic RAGï¼‰ï¼Œé€šè¿‡ç»“æ„åŒ–å·¥å…·è°ƒç”¨è¿›è¡Œè¿­ä»£è¯æ®æœç´¢å’Œç­”æ¡ˆä¿®è®¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ£€ç´¢æé«˜äº†æ­£ç¡®æ€§ï¼Œä¸”ä»£ç†RAGç›¸æ¯”æ ‡å‡†RAGæœ‰æ›´å¤§æå‡ï¼Œåœ¨é˜¿æ‹‰ä¼¯è¯­-è‹±è¯­ç¨³å¥æ€§ä¸Šå–å¾—äº†æœ€å…ˆè¿›æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.07504v1">FROAV: A Framework for RAG Observation and Agent Verification -- Lowering the Barrier to LLM Agent Research</a></td><td><details><summary>å±•å¼€</summary>The rapid advancement of Large Language Models (LLMs) and their integration into autonomous agent systems has created unprecedented opportunities for document analysis, decision support, and knowledge retrieval. However, the complexity of developing, evaluating, and iterating on LLM-based agent workflows presents significant barriers to researchers, particularly those without extensive software engineering expertise. We present FROAV (Framework for RAG Observation and Agent Verification), an open-source research platform that democratizes LLM agent research by providing a plug-and-play architecture combining visual workflow orchestration, a comprehensive evaluation framework, and extensible Python integration. FROAV implements a multi-stage Retrieval-Augmented Generation (RAG) pipeline coupled with a rigorous "LLM-as-a-Judge" evaluation system, all accessible through intuitive graphical interfaces. Our framework integrates n8n for no-code workflow design, PostgreSQL for granular data management, FastAPI for flexible backend logic, and Streamlit for human-in-the-loop interaction. Through this integrated ecosystem, researchers can rapidly prototype RAG strategies, conduct prompt engineering experiments, validate agent performance against human judgments, and collect structured feedback-all without writing infrastructure code. We demonstrate the framework's utility through its application to financial document analysis, while emphasizing its material-agnostic architecture that adapts to any domain requiring semantic analysis. FROAV represents a significant step toward making LLM agent research accessible to a broader scientific community, enabling researchers to focus on hypothesis testing and algorithmic innovation rather than system integration challenges.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªåä¸ºFROAVçš„å¼€æºç ”ç©¶å¹³å°ï¼Œæ—¨åœ¨é€šè¿‡å¯è§†åŒ–å·¥ä½œæµç¨‹ç¼–æ’ã€ç»¼åˆè¯„ä¼°æ¡†æ¶å’Œå¯æ‰©å±•çš„Pythoné›†æˆï¼Œé™ä½åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“ç ”ç©¶é—¨æ§›ã€‚FROAVåŒ…å«ä¸€ä¸ªå¤šé˜¶æ®µçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æµç¨‹å’Œä¸¥æ ¼çš„â€œLLM-as-a-Judgeâ€è¯„ä¼°ç³»ç»Ÿï¼Œå¹¶é€šè¿‡å›¾å½¢ç•Œé¢ç®€åŒ–æ“ä½œï¼Œæ”¯æŒé‡‘èæ–‡æ¡£åˆ†æç­‰é¢†åŸŸçš„ç ”ç©¶ã€‚å…¶ç›®æ ‡æ˜¯è®©ç ”ç©¶äººå‘˜æ— éœ€ç¼–å†™åº•å±‚ä»£ç å³å¯å¿«é€Ÿå®éªŒRAGç­–ç•¥å’ŒéªŒè¯æ™ºèƒ½ä½“æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.07329v1">BayesRAG: Probabilistic Mutual Evidence Corroboration for Multimodal Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has become a pivotal paradigm for Large Language Models (LLMs), yet current approaches struggle with visually rich documents by treating text and images as isolated retrieval targets. Existing methods relying solely on cosine similarity often fail to capture the semantic reinforcement provided by cross-modal alignment and layout-induced coherence. To address these limitations, we propose BayesRAG, a novel multimodal retrieval framework grounded in Bayesian inference and Dempster-Shafer evidence theory. Unlike traditional approaches that rank candidates strictly by similarity, BayesRAG models the intrinsic consistency of retrieved candidates across modalities as probabilistic evidence to refine retrieval confidence. Specifically, our method computes the posterior association probability for combinations of multimodal retrieval results, prioritizing text-image pairs that mutually corroborate each other in terms of both semantics and layout. Extensive experiments demonstrate that BayesRAG significantly outperforms state-of-the-art (SOTA) methods on challenging multimodal benchmarks. This study establishes a new paradigm for multimodal retrieval fusion that effectively resolves the isolation of heterogeneous modalities through an evidence fusion mechanism and enhances the robustness of retrieval outcomes. Our code is available at https://github.com/TioeAre/BayesRAG.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºBayesRAGçš„æ–°å‹å¤šæ¨¡æ€æ£€ç´¢æ¡†æ¶ï¼ŒåŸºäºè´å¶æ–¯æ¨ç†å’ŒDempster-Shaferè¯æ®ç†è®ºï¼Œæ—¨åœ¨è§£å†³ç°æœ‰RAGæ–¹æ³•åœ¨å¤„ç†å¯Œè§†è§‰æ–‡æ¡£æ—¶å› æ–‡æœ¬å’Œå›¾åƒå­¤ç«‹æ£€ç´¢è€Œå¯¼è‡´çš„è¯­ä¹‰å’Œå¸ƒå±€è¿è´¯æ€§ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡è®¡ç®—å¤šæ¨¡æ€æ£€ç´¢ç»“æœçš„åéªŒå…³è”æ¦‚ç‡ï¼Œä¼˜å…ˆé€‰æ‹©è¯­ä¹‰å’Œå¸ƒå±€ç›¸äº’ä½è¯çš„æ–‡æœ¬-å›¾åƒå¯¹ï¼Œä»è€Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•çš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.07260v1">ActiShade: Activating Overshadowed Knowledge to Guide Multi-Hop Reasoning in Large Language Models</a></td><td><details><summary>å±•å¼€</summary>In multi-hop reasoning, multi-round retrieval-augmented generation (RAG) methods typically rely on LLM-generated content as the retrieval query. However, these approaches are inherently vulnerable to knowledge overshadowing - a phenomenon where critical information is overshadowed during generation. As a result, the LLM-generated content may be incomplete or inaccurate, leading to irrelevant retrieval and causing error accumulation during the iteration process. To address this challenge, we propose ActiShade, which detects and activates overshadowed knowledge to guide large language models (LLMs) in multi-hop reasoning. Specifically, ActiShade iteratively detects the overshadowed keyphrase in the given query, retrieves documents relevant to both the query and the overshadowed keyphrase, and generates a new query based on the retrieved documents to guide the next-round iteration. By supplementing the overshadowed knowledge during the formulation of next-round queries while minimizing the introduction of irrelevant noise, ActiShade reduces the error accumulation caused by knowledge overshadowing. Extensive experiments show that ActiShade outperforms existing methods across multiple datasets and LLMs.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºActiShadeçš„æ–¹æ³•ï¼Œç”¨äºè§£å†³å¤šè½®æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­å› çŸ¥è¯†é®è”½ï¼ˆcritical information overshadowedï¼‰å¯¼è‡´çš„é”™è¯¯ç´¯ç§¯é—®é¢˜ã€‚ActiShadeé€šè¿‡è¿­ä»£æ£€æµ‹æŸ¥è¯¢ä¸­è¢«é®è”½çš„å…³é”®è¯ï¼Œæ£€ç´¢ä¸æŸ¥è¯¢åŠé®è”½å…³é”®è¯ç›¸å…³çš„æ–‡æ¡£ï¼Œå¹¶åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆä¸‹ä¸€è½®æŸ¥è¯¢ï¼Œä»è€Œè¡¥å……é®è”½çŸ¥è¯†å¹¶å‡å°‘æ— å…³å™ªå£°ï¼Œæœ€ç»ˆæå‡å¤šè·³æ¨ç†çš„å‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.07226v1">Lost in the Noise: How Reasoning Models Fail with Contextual Distractors</a></td><td><details><summary>å±•å¼€</summary>Recent advances in reasoning models and agentic AI systems have led to an increased reliance on diverse external information. However, this shift introduces input contexts that are inherently noisy, a reality that current sanitized benchmarks fail to capture. We introduce NoisyBench, a comprehensive benchmark that systematically evaluates model robustness across 11 datasets in RAG, reasoning, alignment, and tool-use tasks against diverse noise types, including random documents, irrelevant chat histories, and hard negative distractors. Our evaluation reveals a catastrophic performance drop of up to 80% in state-of-the-art models when faced with contextual distractors. Crucially, we find that agentic workflows often amplify these errors by over-trusting noisy tool outputs, and distractors can trigger emergent misalignment even without adversarial intent. We find that prompting, context engineering, SFT, and outcome-reward only RL fail to ensure robustness; in contrast, our proposed Rationale-Aware Reward (RARE) significantly strengthens resilience by incentivizing the identification of helpful information within noise. Finally, we uncover an inverse scaling trend where increased test-time computation leads to worse performance in noisy settings and demonstrate via attention visualization that models disproportionately focus on distractor tokens, providing vital insights for building the next generation of robust, reasoning-capable agents.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ä»‹ç»äº†NoisyBenchï¼Œä¸€ä¸ªè¯„ä¼°æ¨¡å‹åœ¨RAGã€æ¨ç†ã€å¯¹é½å’Œå·¥å…·ä½¿ç”¨ç­‰ä»»åŠ¡ä¸­å¯¹å¤šç§å™ªå£°ç±»å‹é²æ£’æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†å…ˆè¿›æ¨¡å‹åœ¨å™ªå£°ç¯å¢ƒä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™çš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºRAREçš„ Rationale-Aware Reward æ–¹æ³•æ¥å¢å¼ºæ¨¡å‹æŠ—å™ªèƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.07192v1">Relink: Constructing Query-Driven Evidence Graph On-the-Fly for GraphRAG</a></td><td><details><summary>å±•å¼€</summary>Graph-based Retrieval-Augmented Generation (GraphRAG) mitigates hallucinations in Large Language Models (LLMs) by grounding them in structured knowledge. However, current GraphRAG methods are constrained by a prevailing \textit{build-then-reason} paradigm, which relies on a static, pre-constructed Knowledge Graph (KG). This paradigm faces two critical challenges. First, the KG's inherent incompleteness often breaks reasoning paths. Second, the graph's low signal-to-noise ratio introduces distractor facts, presenting query-relevant but misleading knowledge that disrupts the reasoning process.
  To address these challenges, we argue for a \textit{reason-and-construct} paradigm and propose Relink, a framework that dynamically builds a query-specific evidence graph. To tackle incompleteness, \textbf{Relink} instantiates required facts from a latent relation pool derived from the original text corpus, repairing broken paths on the fly. To handle misleading or distractor facts, Relink employs a unified, query-aware evaluation strategy that jointly considers candidates from both the KG and latent relations, selecting those most useful for answering the query rather than relying on their pre-existence. This empowers Relink to actively discard distractor facts and construct the most faithful and precise evidence path for each query.
  Extensive experiments on five Open-Domain Question Answering benchmarks show that Relink achieves significant average improvements of 5.4\% in EM and 5.2\% in F1 over leading GraphRAG baselines, demonstrating the superiority of our proposed framework.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†åä¸ºRelinkçš„åŠ¨æ€å›¾ç»“æ„æ¡†æ¶ï¼Œé’ˆå¯¹ç°æœ‰GraphRAGæ–¹æ³•ä¾èµ–é™æ€çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰å¯¼è‡´çš„è·¯å¾„ä¸å®Œæ•´å’Œå™ªå£°å¹²æ‰°é—®é¢˜ï¼Œåˆ›æ–°æ€§åœ°é‡‡ç”¨"è¾¹æ¨ç†è¾¹æ„å»º"èŒƒå¼ã€‚é€šè¿‡ä»åŸå§‹è¯­æ–™ä¸­æå–æ½œåœ¨å…³ç³»æ± å®æ—¶ä¿®å¤ç¼ºå¤±è·¯å¾„ï¼Œå¹¶ç»“åˆæŸ¥è¯¢æ„ŸçŸ¥çš„è”åˆè¯„ä¼°ç­–ç•¥ç­›é€‰æœ€ä¼˜è¯æ®ï¼Œæ˜¾è‘—æå‡äº†å¼€æ”¾åŸŸé—®ç­”ä»»åŠ¡ä¸­çš„å‡†ç¡®ç‡ï¼ˆEMæå‡5.4%ï¼ŒF1æå‡5.2%ï¼‰ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2026-01-11
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2601.07790v1">Benchmarking Small Language Models and Small Reasoning Language Models on System Log Severity Classification</a></td><td><details><summary>å±•å¼€</summary>System logs are crucial for monitoring and diagnosing modern computing infrastructure, but their scale and complexity require reliable and efficient automated interpretation. Since severity levels are predefined metadata in system log messages, having a model merely classify them offers limited standalone practical value, revealing little about its underlying ability to interpret system logs. We argue that severity classification is more informative when treated as a benchmark for probing runtime log comprehension rather than as an end task. Using real-world journalctl data from Linux production servers, we evaluate nine small language models (SLMs) and small reasoning language models (SRLMs) under zero-shot, few-shot, and retrieval-augmented generation (RAG) prompting. The results reveal strong stratification. Qwen3-4B achieves the highest accuracy at 95.64% with RAG, while Gemma3-1B improves from 20.25% under few-shot prompting to 85.28% with RAG. Notably, the tiny Qwen3-0.6B reaches 88.12% accuracy despite weak performance without retrieval. In contrast, several SRLMs, including Qwen3-1.7B and DeepSeek-R1-Distill-Qwen-1.5B, degrade substantially when paired with RAG. Efficiency measurements further separate models: most Gemma and Llama variants complete inference in under 1.2 seconds per log, whereas Phi-4-Mini-Reasoning exceeds 228 seconds per log while achieving <10% accuracy. These findings suggest that (1) architectural design, (2) training objectives, and (3) the ability to integrate retrieved context under strict output constraints jointly determine performance. By emphasizing small, deployable models, this benchmark aligns with real-time requirements of digital twin (DT) systems and shows that severity classification serves as a lens for evaluating model competence and real-time deployability, with implications for root cause analysis (RCA) and broader DT integration.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ç³»ç»Ÿæ—¥å¿—ä¸¥é‡æ€§åˆ†ç±»ä½œä¸ºè¯„ä¼°æ¨¡å‹è¿è¡Œæ—¶æ—¥å¿—ç†è§£èƒ½åŠ›çš„åŸºå‡†ï¼Œè€Œéç‹¬ç«‹ä»»åŠ¡ã€‚ç ”ç©¶é€šè¿‡çœŸå®LinuxæœåŠ¡å™¨æ—¥å¿—æ•°æ®ï¼Œæ¯”è¾ƒäº†9ç§å°å‹è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’ŒRAGæç¤ºä¸‹çš„è¡¨ç°ï¼Œå‘ç°RAGæ˜¾è‘—æå‡éƒ¨åˆ†æ¨¡å‹æ€§èƒ½ï¼ˆå¦‚Gemma3-1Bä»20.25%å‡è‡³85.28%ï¼‰ï¼Œä½†ä¹Ÿæ­ç¤ºæŸäº›æ¨¡å‹ä¸RAGç»“åˆæ—¶æ€§èƒ½ä¸‹é™ã€‚ç ”ç©¶å¼ºè°ƒæ¨¡å‹æ¶æ„ã€è®­ç»ƒç›®æ ‡åŠæ£€ç´¢ä¸Šä¸‹æ–‡æ•´åˆèƒ½åŠ›å¯¹æ€§èƒ½çš„å½±å“ï¼Œå¹¶æŒ‡å‡ºè¯¥åŸºå‡†å¯¹å®æ—¶æ•°å­—å­ªç”Ÿç³»ç»Ÿéƒ¨ç½²çš„é€‚ç”¨æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.07711v1">Is Agentic RAG worth it? An experimental comparison of RAG approaches</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) systems are usually defined by the combination of a generator and a retrieval component that extracts textual context from a knowledge base to answer user queries. However, such basic implementations exhibit several limitations, including noisy or suboptimal retrieval, misuse of retrieval for out-of-scope queries, weak query-document matching, and variability or cost associated with the generator. These shortcomings have motivated the development of "Enhanced" RAG, where dedicated modules are introduced to address specific weaknesses in the workflow. More recently, the growing self-reflective capabilities of Large Language Models (LLMs) have enabled a new paradigm, which we refer to as "Agentic" RAG. In this approach, the LLM orchestrates the entire process-deciding which actions to perform, when to perform them, and whether to iterate-thereby reducing reliance on fixed, manually engineered modules. Despite the rapid adoption of both paradigms, it remains unclear which approach is preferable under which conditions. In this work, we conduct an extensive, empirically driven evaluation of Enhanced and Agentic RAG across multiple scenarios and dimensions. Our results provide practical insights into the trade-offs between the two paradigms, offering guidance on selecting the most effective RAG design for real-world applications, considering both costs and performance.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿçš„ä¸¤ç§è¿›é˜¶èŒƒå¼â€”â€”"Enhanced RAG"ï¼ˆé€šè¿‡ä¸“ç”¨æ¨¡å—è§£å†³åŸºç¡€RAGçš„å±€é™æ€§ï¼‰å’Œ"Agentic RAG"ï¼ˆåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹çš„è‡ªä¸»å†³ç­–èƒ½åŠ›åŠ¨æ€åè°ƒæµç¨‹ï¼‰ï¼Œå¹¶é€šè¿‡å¤šåœºæ™¯å®è¯è¯„ä¼°å¯¹æ¯”ä¸¤è€…çš„æ€§èƒ½ä¸æˆæœ¬å·®å¼‚ï¼Œä¸ºå®é™…åº”ç”¨ä¸­çš„èŒƒå¼é€‰æ‹©æä¾›æŒ‡å¯¼ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.07528v1">From RAG to Agentic RAG for Faithful Islamic Question Answering</a></td><td><details><summary>å±•å¼€</summary>LLMs are increasingly used for Islamic question answering, where ungrounded responses may carry serious religious consequences. Yet standard MCQ/MRC-style evaluations do not capture key real-world failure modes, notably free-form hallucinations and whether models appropriately abstain when evidence is lacking. To shed a light on this aspect we introduce ISLAMICFAITHQA, a 3,810-item bilingual (Arabic/English) generative benchmark with atomic single-gold answers, which enables direct measurement of hallucination and abstention. We additionally developed an end-to-end grounded Islamic modelling suite consisting of (i) 25K Arabic text-grounded SFT reasoning pairs, (ii) 5K bilingual preference samples for reward-guided alignment, and (iii) a verse-level Qur'an retrieval corpus of $\sim$6k atomic verses (ayat). Building on these resources, we develop an agentic Quran-grounding framework (agentic RAG) that uses structured tool calls for iterative evidence seeking and answer revision. Experiments across Arabic-centric and multilingual LLMs show that retrieval improves correctness and that agentic RAG yields the largest gains beyond standard RAG, achieving state-of-the-art performance and stronger Arabic-English robustness even with a small model (i.e., Qwen3 4B). We will make the experimental resources and datasets publicly available for the community.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ä»‹ç»äº†ISLAMICFAITHQAï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°ä¼Šæ–¯å…°é—®ç­”ä¸­æ¨¡å‹å¹»è§‰å’Œå¼ƒç­”çš„ç”Ÿæˆå¼åŒè¯­åŸºå‡†ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„åŸºäºå¤å…°ç»çš„å»ºæ¨¡å¥—ä»¶ã€‚å…¶æ ¸å¿ƒè´¡çŒ®æ˜¯å¼€å‘äº†ä¸€ä¸ªä»£ç†äººå¼æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆagentic RAGï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç»“æ„åŒ–å·¥å…·è°ƒç”¨è¿›è¡Œè¿­ä»£å¼è¯æ®æ£€ç´¢å’Œç­”æ¡ˆä¿®æ­£ï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨é˜¿æ‹‰ä¼¯è¯­å’Œå¤šè¯­è¨€å¤§æ¨¡å‹ä¸­æ˜¾è‘—æå‡äº†å›ç­”çš„æ­£ç¡®æ€§å’Œé²æ£’æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.07504v1">FROAV: A Framework for RAG Observation and Agent Verification -- Lowering the Barrier to LLM Agent Research</a></td><td><details><summary>å±•å¼€</summary>The rapid advancement of Large Language Models (LLMs) and their integration into autonomous agent systems has created unprecedented opportunities for document analysis, decision support, and knowledge retrieval. However, the complexity of developing, evaluating, and iterating on LLM-based agent workflows presents significant barriers to researchers, particularly those without extensive software engineering expertise. We present FROAV (Framework for RAG Observation and Agent Verification), an open-source research platform that democratizes LLM agent research by providing a plug-and-play architecture combining visual workflow orchestration, a comprehensive evaluation framework, and extensible Python integration. FROAV implements a multi-stage Retrieval-Augmented Generation (RAG) pipeline coupled with a rigorous "LLM-as-a-Judge" evaluation system, all accessible through intuitive graphical interfaces. Our framework integrates n8n for no-code workflow design, PostgreSQL for granular data management, FastAPI for flexible backend logic, and Streamlit for human-in-the-loop interaction. Through this integrated ecosystem, researchers can rapidly prototype RAG strategies, conduct prompt engineering experiments, validate agent performance against human judgments, and collect structured feedback-all without writing infrastructure code. We demonstrate the framework's utility through its application to financial document analysis, while emphasizing its material-agnostic architecture that adapts to any domain requiring semantic analysis. FROAV represents a significant step toward making LLM agent research accessible to a broader scientific community, enabling researchers to focus on hypothesis testing and algorithmic innovation rather than system integration challenges.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†FROAVæ¡†æ¶ï¼Œä¸€ä¸ªå¼€æºç ”ç©¶å¹³å°ï¼Œæ—¨åœ¨ç®€åŒ–åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªä¸»ä»£ç†ç³»ç»Ÿçš„å¼€å‘ã€è¯„ä¼°å’Œè¿­ä»£ã€‚FROAVé€šè¿‡ç»“åˆå¯è§†åŒ–å·¥ä½œæµç¼–æ’ã€å…¨é¢è¯„ä¼°æ¡†æ¶å’Œå¯æ‰©å±•çš„Pythoné›†æˆï¼Œå®ç°äº†å¤šé˜¶æ®µæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æµç¨‹ï¼Œå¹¶é‡‡ç”¨ä¸¥æ ¼çš„â€œLLM-as-a-Judgeâ€è¯„ä¼°ç³»ç»Ÿã€‚è¯¥æ¡†æ¶æ”¯æŒæ— ä»£ç å·¥ä½œæµè®¾è®¡ã€ç»†ç²’åº¦æ•°æ®ç®¡ç†å’Œäººæœºäº¤äº’ï¼Œé€‚ç”¨äºé‡‘èæ–‡æ¡£åˆ†æç­‰éœ€è¦è¯­ä¹‰åˆ†æçš„é¢†åŸŸï¼Œé™ä½äº†LLMä»£ç†ç ”ç©¶çš„é—¨æ§›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.07329v1">BayesRAG: Probabilistic Mutual Evidence Corroboration for Multimodal Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has become a pivotal paradigm for Large Language Models (LLMs), yet current approaches struggle with visually rich documents by treating text and images as isolated retrieval targets. Existing methods relying solely on cosine similarity often fail to capture the semantic reinforcement provided by cross-modal alignment and layout-induced coherence. To address these limitations, we propose BayesRAG, a novel multimodal retrieval framework grounded in Bayesian inference and Dempster-Shafer evidence theory. Unlike traditional approaches that rank candidates strictly by similarity, BayesRAG models the intrinsic consistency of retrieved candidates across modalities as probabilistic evidence to refine retrieval confidence. Specifically, our method computes the posterior association probability for combinations of multimodal retrieval results, prioritizing text-image pairs that mutually corroborate each other in terms of both semantics and layout. Extensive experiments demonstrate that BayesRAG significantly outperforms state-of-the-art (SOTA) methods on challenging multimodal benchmarks. This study establishes a new paradigm for multimodal retrieval fusion that effectively resolves the isolation of heterogeneous modalities through an evidence fusion mechanism and enhances the robustness of retrieval outcomes. Our code is available at https://github.com/TioeAre/BayesRAG.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºBayesRAGçš„æ–°å‹å¤šæ¨¡æ€æ£€ç´¢æ¡†æ¶ï¼Œé€šè¿‡è´å¶æ–¯æ¨ç†å’ŒDempster-Shaferè¯æ®ç†è®ºè§£å†³ä¼ ç»ŸRAGæ–¹æ³•åœ¨å¤„ç†è§†è§‰ä¸°å¯Œæ–‡æ¡£æ—¶æ–‡æœ¬ä¸å›¾åƒæ¨¡æ€å­¤ç«‹æ£€ç´¢çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è·¨æ¨¡æ€è¯­ä¹‰å’Œå¸ƒå±€ä¸€è‡´æ€§ä½œä¸ºæ¦‚ç‡è¯æ®ä¼˜åŒ–æ£€ç´¢ç½®ä¿¡åº¦ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•çš„æ€§èƒ½ï¼Œä¸ºå¼‚æ„æ¨¡æ€èåˆå»ºç«‹äº†æ–°èŒƒå¼ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.07260v1">ActiShade: Activating Overshadowed Knowledge to Guide Multi-Hop Reasoning in Large Language Models</a></td><td><details><summary>å±•å¼€</summary>In multi-hop reasoning, multi-round retrieval-augmented generation (RAG) methods typically rely on LLM-generated content as the retrieval query. However, these approaches are inherently vulnerable to knowledge overshadowing - a phenomenon where critical information is overshadowed during generation. As a result, the LLM-generated content may be incomplete or inaccurate, leading to irrelevant retrieval and causing error accumulation during the iteration process. To address this challenge, we propose ActiShade, which detects and activates overshadowed knowledge to guide large language models (LLMs) in multi-hop reasoning. Specifically, ActiShade iteratively detects the overshadowed keyphrase in the given query, retrieves documents relevant to both the query and the overshadowed keyphrase, and generates a new query based on the retrieved documents to guide the next-round iteration. By supplementing the overshadowed knowledge during the formulation of next-round queries while minimizing the introduction of irrelevant noise, ActiShade reduces the error accumulation caused by knowledge overshadowing. Extensive experiments show that ActiShade outperforms existing methods across multiple datasets and LLMs.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºActiShadeçš„æ–¹æ³•ï¼Œç”¨äºè§£å†³å¤šè½®æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­çŸ¥è¯†é®è”½ï¼ˆknowledge overshadowingï¼‰å¯¼è‡´çš„ä¿¡æ¯ä¸å®Œæ•´æˆ–é”™è¯¯çš„é—®é¢˜ã€‚ActiShadeé€šè¿‡æ£€æµ‹å¹¶æ¿€æ´»è¢«é®è”½çš„å…³é”®çŸ­è¯­ï¼Œç»“åˆæ£€ç´¢åˆ°çš„æ–‡æ¡£ç”Ÿæˆæ–°çš„æŸ¥è¯¢ï¼Œä»¥å‡å°‘è¿­ä»£è¿‡ç¨‹ä¸­çš„é”™è¯¯ç´¯ç§¯ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†å’Œå¤§è¯­è¨€æ¨¡å‹ä¸Šè¡¨ç°å‡ºä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.07226v1">Lost in the Noise: How Reasoning Models Fail with Contextual Distractors</a></td><td><details><summary>å±•å¼€</summary>Recent advances in reasoning models and agentic AI systems have led to an increased reliance on diverse external information. However, this shift introduces input contexts that are inherently noisy, a reality that current sanitized benchmarks fail to capture. We introduce NoisyBench, a comprehensive benchmark that systematically evaluates model robustness across 11 datasets in RAG, reasoning, alignment, and tool-use tasks against diverse noise types, including random documents, irrelevant chat histories, and hard negative distractors. Our evaluation reveals a catastrophic performance drop of up to 80% in state-of-the-art models when faced with contextual distractors. Crucially, we find that agentic workflows often amplify these errors by over-trusting noisy tool outputs, and distractors can trigger emergent misalignment even without adversarial intent. We find that prompting, context engineering, SFT, and outcome-reward only RL fail to ensure robustness; in contrast, our proposed Rationale-Aware Reward (RARE) significantly strengthens resilience by incentivizing the identification of helpful information within noise. Finally, we uncover an inverse scaling trend where increased test-time computation leads to worse performance in noisy settings and demonstrate via attention visualization that models disproportionately focus on distractor tokens, providing vital insights for building the next generation of robust, reasoning-capable agents.</details></td><td><details><summary>å±•å¼€</summary></details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.07192v1">Relink: Constructing Query-Driven Evidence Graph On-the-Fly for GraphRAG</a></td><td><details><summary>å±•å¼€</summary>Graph-based Retrieval-Augmented Generation (GraphRAG) mitigates hallucinations in Large Language Models (LLMs) by grounding them in structured knowledge. However, current GraphRAG methods are constrained by a prevailing \textit{build-then-reason} paradigm, which relies on a static, pre-constructed Knowledge Graph (KG). This paradigm faces two critical challenges. First, the KG's inherent incompleteness often breaks reasoning paths. Second, the graph's low signal-to-noise ratio introduces distractor facts, presenting query-relevant but misleading knowledge that disrupts the reasoning process.
  To address these challenges, we argue for a \textit{reason-and-construct} paradigm and propose Relink, a framework that dynamically builds a query-specific evidence graph. To tackle incompleteness, \textbf{Relink} instantiates required facts from a latent relation pool derived from the original text corpus, repairing broken paths on the fly. To handle misleading or distractor facts, Relink employs a unified, query-aware evaluation strategy that jointly considers candidates from both the KG and latent relations, selecting those most useful for answering the query rather than relying on their pre-existence. This empowers Relink to actively discard distractor facts and construct the most faithful and precise evidence path for each query.
  Extensive experiments on five Open-Domain Question Answering benchmarks show that Relink achieves significant average improvements of 5.4\% in EM and 5.2\% in F1 over leading GraphRAG baselines, demonstrating the superiority of our proposed framework.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRelinkçš„åŠ¨æ€å›¾æ„å»ºæ¡†æ¶ï¼ˆGraphRAGæ”¹è¿›æ–¹æ³•ï¼‰ï¼Œé€šè¿‡"reason-and-construct"èŒƒå¼è§£å†³ä¼ ç»Ÿé™æ€çŸ¥è¯†å›¾è°±çš„ç¼ºé™·ã€‚å®ƒä»åŸå§‹æ–‡æœ¬ä¸­åŠ¨æ€æå–æ½œåœ¨å…³ç³»ä¿®å¤æ¨ç†è·¯å¾„ï¼Œå¹¶åŸºäºæŸ¥è¯¢è”åˆè¯„ä¼°çŸ¥è¯†å›¾è°±ä¸æ½œåœ¨å…³ç³»ï¼Œç­›é€‰æœ€ä¼˜è¯æ®è·¯å¾„ã€‚å®éªŒè¡¨æ˜Relinkåœ¨äº”é¡¹å¼€æ”¾åŸŸé—®ç­”åŸºå‡†ä¸Šå¹³å‡æå‡5.4% EMå’Œ5.2% F1ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰GraphRAGæ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.07072v1">Overcoming the Retrieval Barrier: Indirect Prompt Injection in the Wild for LLM Systems</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) increasingly rely on retrieving information from external corpora. This creates a new attack surface: indirect prompt injection (IPI), where hidden instructions are planted in the corpora and hijack model behavior once retrieved. Previous studies have highlighted this risk but often avoid the hardest step: ensuring that malicious content is actually retrieved. In practice, unoptimized IPI is rarely retrieved under natural queries, which leaves its real-world impact unclear.
  We address this challenge by decomposing the malicious content into a trigger fragment that guarantees retrieval and an attack fragment that encodes arbitrary attack objectives. Based on this idea, we design an efficient and effective black-box attack algorithm that constructs a compact trigger fragment to guarantee retrieval for any attack fragment. Our attack requires only API access to embedding models, is cost-efficient (as little as $0.21 per target user query on OpenAI's embedding models), and achieves near-100% retrieval across 11 benchmarks and 8 embedding models (including both open-source models and proprietary services).
  Based on this attack, we present the first end-to-end IPI exploits under natural queries and realistic external corpora, spanning both RAG and agentic systems with diverse attack objectives. These results establish IPI as a practical and severe threat: when a user issued a natural query to summarize emails on frequently asked topics, a single poisoned email was sufficient to coerce GPT-4o into exfiltrating SSH keys with over 80% success in a multi-agent workflow. We further evaluate several defenses and find that they are insufficient to prevent the retrieval of malicious text, highlighting retrieval as a critical open vulnerability.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†é—´æ¥æç¤ºæ³¨å…¥ï¼ˆIPIï¼‰æ”»å‡»åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„å®é™…å¨èƒï¼Œç‰¹åˆ«æ˜¯åœ¨ä¾èµ–å¤–éƒ¨çŸ¥è¯†åº“æ£€ç´¢çš„RAGå’Œä»£ç†ç³»ç»Ÿä¸­ã€‚ä½œè€…æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„é»‘ç›’æ”»å‡»ç®—æ³•ï¼Œå°†æ¶æ„å†…å®¹åˆ†è§£ä¸ºè§¦å‘ç‰‡æ®µï¼ˆç¡®ä¿æ£€ç´¢ï¼‰å’Œæ”»å‡»ç‰‡æ®µï¼ˆå®ç°æ”»å‡»ç›®æ ‡ï¼‰ï¼Œå¹¶åœ¨11ä¸ªåŸºå‡†æµ‹è¯•å’Œ8ç§åµŒå…¥æ¨¡å‹ä¸­éªŒè¯äº†å…¶è¿‘100%çš„æ£€ç´¢æˆåŠŸç‡ã€‚ç ”ç©¶é€šè¿‡çœŸå®åœºæ™¯ï¼ˆå¦‚å¤šä»£ç†å·¥ä½œæµä¸­åˆ©ç”¨GPT-4oæ³„éœ²SSHå¯†é’¥ï¼‰è¯æ˜äº†IPIçš„ä¸¥é‡æ€§ï¼Œå¹¶æŒ‡å‡ºç°æœ‰é˜²å¾¡æªæ–½æ— æ³•æœ‰æ•ˆé˜»æ­¢æ¶æ„æ–‡æœ¬æ£€ç´¢ï¼Œæ­ç¤ºäº†æ£€ç´¢ç¯èŠ‚çš„å…³é”®æ¼æ´ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.07054v1">Fine-Tuning vs. RAG for Multi-Hop Question Answering with Novel Knowledge</a></td><td><details><summary>å±•å¼€</summary>Multi-hop question answering is widely used to evaluate the reasoning capabilities of large language models (LLMs), as it requires integrating multiple pieces of supporting knowledge to arrive at a correct answer. While prior work has explored different mechanisms for providing knowledge to LLMs, such as finetuning and retrieval-augmented generation (RAG), their relative effectiveness for multi-hop question answering remains insufficiently understood, particularly when the required knowledge is temporally novel.
  In this paper, we systematically compare parametric and non-parametric knowledge injection methods for open-domain multi-hop question answering. We evaluate unsupervised fine-tuning (continual pretraining), supervised fine-tuning, and retrieval-augmented generation across three 7B-parameter open-source LLMs. Experiments are conducted on two benchmarks: QASC, a standard multi-hop science question answering dataset, and a newly constructed dataset of over 10,000 multi-hop questions derived from Wikipedia events in 2024, designed to test knowledge beyond the models' pretraining cutoff.
  Our results show that unsupervised fine-tuning provides only limited gains over base models, suggesting that continual pretraining alone is insufficient for improving multi-hop reasoning accuracy. In contrast, retrieval-augmented generation yields substantial and consistent improvements, particularly when answering questions that rely on temporally novel information. Supervised fine-tuning achieves the highest overall accuracy across models and datasets. These findings highlight fundamental differences in how knowledge injection mechanisms support multi-hop question answering and underscore the importance of retrieval-based methods when external or compositional knowledge is required.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç³»ç»Ÿåœ°æ¯”è¾ƒäº†å‚æ•°åŒ–ï¼ˆå¦‚å¾®è°ƒï¼‰å’Œéå‚æ•°åŒ–ï¼ˆå¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ŒRAGï¼‰çŸ¥è¯†æ³¨å…¥æ–¹æ³•åœ¨å¼€æ”¾åŸŸå¤šè·³é—®ç­”ä»»åŠ¡ä¸­çš„æ•ˆæœï¼Œç‰¹åˆ«å…³æ³¨äº†æ—¶é—´æ–°é¢–æ€§çŸ¥è¯†çš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRAGåœ¨ä¾èµ–æ—¶æ•ˆæ€§ä¿¡æ¯çš„é—®ç­”ä¸­è¡¨ç°æ˜¾è‘—ä¼˜äºæ— ç›‘ç£å¾®è°ƒï¼Œè€Œç›‘ç£å¾®è°ƒæ•´ä½“å‡†ç¡®ç‡æœ€é«˜ï¼Œä½†RAGåœ¨å¤–éƒ¨æˆ–ç»„åˆçŸ¥è¯†éœ€æ±‚åœºæ™¯ä¸‹å…·æœ‰ç‹¬ç‰¹ä¼˜åŠ¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.06979v1">MedTutor: A Retrieval-Augmented LLM System for Case-Based Medical Education</a></td><td><details><summary>å±•å¼€</summary>The learning process for medical residents presents significant challenges, demanding both the ability to interpret complex case reports and the rapid acquisition of accurate medical knowledge from reliable sources. Residents typically study case reports and engage in discussions with peers and mentors, but finding relevant educational materials and evidence to support their learning from these cases is often time-consuming and challenging. To address this, we introduce MedTutor, a novel system designed to augment resident training by automatically generating evidence-based educational content and multiple-choice questions from clinical case reports. MedTutor leverages a Retrieval-Augmented Generation (RAG) pipeline that takes clinical case reports as input and produces targeted educational materials. The system's architecture features a hybrid retrieval mechanism that synergistically queries a local knowledge base of medical textbooks and academic literature (using PubMed, Semantic Scholar APIs) for the latest related research, ensuring the generated content is both foundationally sound and current. The retrieved evidence is filtered and ordered using a state-of-the-art reranking model and then an LLM generates the final long-form output describing the main educational content regarding the case-report. We conduct a rigorous evaluation of the system. First, three radiologists assessed the quality of outputs, finding them to be of high clinical and educational value. Second, we perform a large scale evaluation using an LLM-as-a Judge to understand if LLMs can be used to evaluate the output of the system. Our analysis using correlation between LLMs outputs and human expert judgments reveals a moderate alignment and highlights the continued necessity of expert oversight.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ä»‹ç»äº†MedTutorç³»ç»Ÿï¼Œå®ƒåˆ©ç”¨RAGæŠ€æœ¯ä»ä¸´åºŠç—…ä¾‹æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆå¾ªè¯æ•™è‚²å†…å®¹å’Œé€‰æ‹©é¢˜ï¼Œé€šè¿‡æ··åˆæ£€ç´¢æœºåˆ¶æ•´åˆåŒ»å­¦æ•™ç§‘ä¹¦å’Œå­¦æœ¯æ–‡çŒ®ï¼ˆå¦‚PubMedï¼‰çš„æœ€æ–°ç ”ç©¶ï¼Œç»é‡æ’åºå’ŒLLMå¤„ç†åè¾“å‡ºé«˜è´¨é‡æ•™è‚²ææ–™ï¼Œå¹¶é€šè¿‡æ”¾å°„ç§‘åŒ»ç”Ÿå’ŒLLMè¯„ä¼°éªŒè¯å…¶ä¸´åºŠä»·å€¼ä¸å±€é™æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.06922v1">TreePS-RAG: Tree-based Process Supervision for Reinforcement Learning in Agentic RAG</a></td><td><details><summary>å±•å¼€</summary>Agentic retrieval-augmented generation (RAG) formulates question answering as a multi-step interaction between reasoning and information retrieval, and has recently been advanced by reinforcement learning (RL) with outcome-based supervision. While effective, relying solely on sparse final rewards limits step-wise credit assignment and provides weak guidance for intermediate reasoning and actions. Recent efforts explore process-level supervision, but typically depend on offline constructed training data, which risks distribution shift, or require costly intermediate annotations. We present TreePS-RAG, an online, tree-based RL framework for agentic RAG that enables step-wise credit assignment while retaining standard outcome-only rewards. Our key insight is to model agentic RAG reasoning as a rollout tree, where each reasoning step naturally maps to a node. This tree structure allows step utility to be estimated via Monte Carlo estimation over its descendant outcomes, yielding fine-grained process advantages without requiring intermediate labels. To make this paradigm practical, we introduce an efficient online tree construction strategy that preserves exploration diversity under a constrained computational budget. With a rollout cost comparable to strong baselines like Search-R1, experiments on seven multi-hop and general QA benchmarks across multiple model scales show that TreePS-RAG consistently and significantly outperforms both outcome-supervised and leading process-supervised RL methods.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTreePS-RAGçš„åœ¨çº¿ã€åŸºäºæ ‘çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè§£å†³æ™ºèƒ½ä½“é©±åŠ¨çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†å¤šæ­¥æ¨ç†è¿‡ç¨‹å»ºæ¨¡ä¸ºæ ‘å½¢ç»“æ„ï¼Œåˆ©ç”¨è’™ç‰¹å¡æ´›ä¼°è®¡ä¸ºä¸­é—´æ­¥éª¤åˆ†é…ç»†ç²’åº¦çš„ä¿¡ç”¨ï¼Œä»è€Œåœ¨ä»…ä½¿ç”¨ç»“æœå¥–åŠ±çš„æƒ…å†µä¸‹å®ç°è¿‡ç¨‹çº§ç›‘ç£ï¼Œæœ‰æ•ˆæå‡äº†å¤šæ­¥é—®ç­”ä»»åŠ¡çš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.06842v1">Seeing through the Conflict: Transparent Knowledge Conflict Handling in Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) equipped with retrieval--the Retrieval-Augmented Generation (RAG) paradigm--should combine their parametric knowledge with external evidence, yet in practice they often hallucinate, over-trust noisy snippets, or ignore vital context. We introduce TCR (Transparent Conflict Resolution), a plug-and-play framework that makes this decision process observable and controllable. TCR (i) disentangles semantic match and factual consistency via dual contrastive encoders, (ii) estimates self-answerability to gauge confidence in internal memory, and (iii) feeds the three scalar signals to the generator through a lightweight soft-prompt with SNR-based weighting. Across seven benchmarks TCR improves conflict detection (+5-18 F1), raises knowledge-gap recovery by +21.4 pp and cuts misleading-context overrides by -29.3 pp, while adding only 0.3% parameters. The signals align with human judgements and expose temporal decision patterns.</details></td><td><details><summary>å±•å¼€</summary></details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.06799v1">CIRAG: Construction-Integration Retrieval and Adaptive Generation for Multi-hop Question Answering</a></td><td><details><summary>å±•å¼€</summary>Triple-based Iterative Retrieval-Augmented Generation (iRAG) mitigates document-level noise for multi-hop question answering. However, existing methods still face limitations: (i) greedy single-path expansion, which propagates early errors and fails to capture parallel evidence from different reasoning branches, and (ii) granularity-demand mismatch, where a single evidence representation struggles to balance noise control with contextual sufficiency. In this paper, we propose the Construction-Integration Retrieval and Adaptive Generation model, CIRAG. It introduces an Iterative Construction-Integration module that constructs candidate triples and history-conditionally integrates them to distill core triples and generate the next-hop query. This module mitigates the greedy trap by preserving multiple plausible evidence chains. Besides, we propose an Adaptive Cascaded Multi-Granularity Generation module that progressively expands contextual evidence based on the problem requirements, from triples to supporting sentences and full passages. Moreover, we introduce Trajectory Distillation, which distills the teacher model's integration policy into a lightweight student, enabling efficient and reliable long-horizon reasoning. Extensive experiments demonstrate that CIRAG achieves superior performance compared to existing iRAG methods.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºCIRAGçš„æ”¹è¿›å‹RAGæ–¹æ³•ï¼Œé’ˆå¯¹å¤šè·³é—®ç­”ä»»åŠ¡ä¸­çš„æ–‡æ¡£çº§å™ªå£°å’Œæ¨ç†è·¯å¾„å•ä¸€é—®é¢˜ï¼Œè®¾è®¡äº†è¿­ä»£æ„å»º-æ•´åˆæ¨¡å—å’Œè‡ªé€‚åº”å¤šç²’åº¦ç”Ÿæˆæ¨¡å—ï¼Œé€šè¿‡ä¿ç•™å¤šè¯æ®é“¾å’ŒåŠ¨æ€è°ƒæ•´ä¸Šä¸‹æ–‡ç²’åº¦æå‡æ€§èƒ½ï¼Œå¹¶é‡‡ç”¨è½¨è¿¹è’¸é¦æŠ€æœ¯å®ç°é«˜æ•ˆé•¿ç¨‹æ¨ç†ã€‚å®éªŒè¡¨æ˜å…¶ä¼˜äºç°æœ‰iRAGæ–¹æ³•ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2026-01-10
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2601.07072v1">Overcoming the Retrieval Barrier: Indirect Prompt Injection in the Wild for LLM Systems</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) increasingly rely on retrieving information from external corpora. This creates a new attack surface: indirect prompt injection (IPI), where hidden instructions are planted in the corpora and hijack model behavior once retrieved. Previous studies have highlighted this risk but often avoid the hardest step: ensuring that malicious content is actually retrieved. In practice, unoptimized IPI is rarely retrieved under natural queries, which leaves its real-world impact unclear.
  We address this challenge by decomposing the malicious content into a trigger fragment that guarantees retrieval and an attack fragment that encodes arbitrary attack objectives. Based on this idea, we design an efficient and effective black-box attack algorithm that constructs a compact trigger fragment to guarantee retrieval for any attack fragment. Our attack requires only API access to embedding models, is cost-efficient (as little as $0.21 per target user query on OpenAI's embedding models), and achieves near-100% retrieval across 11 benchmarks and 8 embedding models (including both open-source models and proprietary services).
  Based on this attack, we present the first end-to-end IPI exploits under natural queries and realistic external corpora, spanning both RAG and agentic systems with diverse attack objectives. These results establish IPI as a practical and severe threat: when a user issued a natural query to summarize emails on frequently asked topics, a single poisoned email was sufficient to coerce GPT-4o into exfiltrating SSH keys with over 80% success in a multi-agent workflow. We further evaluate several defenses and find that they are insufficient to prevent the retrieval of malicious text, highlighting retrieval as a critical open vulnerability.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­çš„é—´æ¥æç¤ºæ³¨å…¥ï¼ˆIPIï¼‰æ”»å‡»é£é™©ï¼Œæå‡ºäº†ä¸€ç§å°†æ¶æ„å†…å®¹åˆ†è§£ä¸ºè§¦å‘ç‰‡æ®µå’Œæ”»å‡»ç‰‡æ®µçš„æ–¹æ³•ï¼Œç¡®ä¿æ¶æ„å†…å®¹è¢«æ£€ç´¢å¹¶å½±å“æ¨¡å‹è¡Œä¸ºã€‚ç ”ç©¶é€šè¿‡å®éªŒå±•ç¤ºäº†IPIæ”»å‡»åœ¨è‡ªç„¶æŸ¥è¯¢å’Œå®é™…å¤–éƒ¨è¯­æ–™åº“ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æŒ‡å‡ºç°æœ‰é˜²å¾¡æªæ–½çš„ä¸è¶³ï¼Œå¼ºè°ƒäº†æ£€ç´¢ç¯èŠ‚çš„å®‰å…¨æ¼æ´ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.07054v1">Fine-Tuning vs. RAG for Multi-Hop Question Answering with Novel Knowledge</a></td><td><details><summary>å±•å¼€</summary>Multi-hop question answering is widely used to evaluate the reasoning capabilities of large language models (LLMs), as it requires integrating multiple pieces of supporting knowledge to arrive at a correct answer. While prior work has explored different mechanisms for providing knowledge to LLMs, such as finetuning and retrieval-augmented generation (RAG), their relative effectiveness for multi-hop question answering remains insufficiently understood, particularly when the required knowledge is temporally novel.
  In this paper, we systematically compare parametric and non-parametric knowledge injection methods for open-domain multi-hop question answering. We evaluate unsupervised fine-tuning (continual pretraining), supervised fine-tuning, and retrieval-augmented generation across three 7B-parameter open-source LLMs. Experiments are conducted on two benchmarks: QASC, a standard multi-hop science question answering dataset, and a newly constructed dataset of over 10,000 multi-hop questions derived from Wikipedia events in 2024, designed to test knowledge beyond the models' pretraining cutoff.
  Our results show that unsupervised fine-tuning provides only limited gains over base models, suggesting that continual pretraining alone is insufficient for improving multi-hop reasoning accuracy. In contrast, retrieval-augmented generation yields substantial and consistent improvements, particularly when answering questions that rely on temporally novel information. Supervised fine-tuning achieves the highest overall accuracy across models and datasets. These findings highlight fundamental differences in how knowledge injection mechanisms support multi-hop question answering and underscore the importance of retrieval-based methods when external or compositional knowledge is required.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç³»ç»Ÿåœ°æ¯”è¾ƒäº†å‚æ•°åŒ–ï¼ˆå¦‚æ— ç›‘ç£å’Œæœ‰ç›‘ç£å¾®è°ƒï¼‰ä¸éå‚æ•°åŒ–ï¼ˆå³æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ŒRAGï¼‰çš„çŸ¥è¯†æ³¨å…¥æ–¹æ³•åœ¨å¼€æ”¾é¢†åŸŸå¤šè·³é—®ç­”ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é€šè¿‡åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒï¼Œç ”ç©¶å‘ç°RAGæ–¹æ³•åœ¨å¤„ç†éœ€è¦æ—¶é—´æ–°çŸ¥è¯†çš„å¤æ‚é—®é¢˜æ—¶èƒ½å¸¦æ¥æ˜¾è‘—ä¸”ç¨³å®šçš„æ€§èƒ½æå‡ï¼Œå°¤å…¶ä¼˜äºæ— ç›‘ç£å¾®è°ƒï¼›è€Œç›‘ç£å¾®è°ƒè™½ç„¶æ•´ä½“å‡†ç¡®ç‡æœ€é«˜ï¼Œä½†RAGåœ¨åº”å¯¹æ¨¡å‹é¢„è®­ç»ƒæˆªæ­¢æ—¥æœŸåçš„æ–°ä¿¡æ¯æ—¶å±•ç°å‡ºç‹¬ç‰¹ä¼˜åŠ¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.06979v1">MedTutor: A Retrieval-Augmented LLM System for Case-Based Medical Education</a></td><td><details><summary>å±•å¼€</summary>The learning process for medical residents presents significant challenges, demanding both the ability to interpret complex case reports and the rapid acquisition of accurate medical knowledge from reliable sources. Residents typically study case reports and engage in discussions with peers and mentors, but finding relevant educational materials and evidence to support their learning from these cases is often time-consuming and challenging. To address this, we introduce MedTutor, a novel system designed to augment resident training by automatically generating evidence-based educational content and multiple-choice questions from clinical case reports. MedTutor leverages a Retrieval-Augmented Generation (RAG) pipeline that takes clinical case reports as input and produces targeted educational materials. The system's architecture features a hybrid retrieval mechanism that synergistically queries a local knowledge base of medical textbooks and academic literature (using PubMed, Semantic Scholar APIs) for the latest related research, ensuring the generated content is both foundationally sound and current. The retrieved evidence is filtered and ordered using a state-of-the-art reranking model and then an LLM generates the final long-form output describing the main educational content regarding the case-report. We conduct a rigorous evaluation of the system. First, three radiologists assessed the quality of outputs, finding them to be of high clinical and educational value. Second, we perform a large scale evaluation using an LLM-as-a Judge to understand if LLMs can be used to evaluate the output of the system. Our analysis using correlation between LLMs outputs and human expert judgments reveals a moderate alignment and highlights the continued necessity of expert oversight.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ä»‹ç»äº†MedTutorç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé€šè¿‡RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æŠ€æœ¯ï¼Œå°†ä¸´åºŠç—…ä¾‹æŠ¥å‘Šè½¬åŒ–ä¸ºå®šåˆ¶åŒ–çš„åŒ»å­¦æ•™è‚²èµ„æºã€‚ç³»ç»Ÿé‡‡ç”¨æ··åˆæ£€ç´¢æœºåˆ¶ä»åŒ»å­¦æ•™ç§‘ä¹¦å’Œå­¦æœ¯æ–‡çŒ®ä¸­è·å–æœ€æ–°è¯æ®ï¼Œç»é‡æ’åºååˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„æ•™å­¦å†…å®¹å’Œå¤šé€‰é¢˜ï¼Œæ—¨åœ¨æå‡ä½é™¢åŒ»å¸ˆçš„åŸ¹è®­æ•ˆç‡ã€‚ç ”ç©¶é€šè¿‡æ”¾å°„ç§‘åŒ»å¸ˆè¯„ä¼°å’ŒLLM-as-a-Judgeå®éªŒéªŒè¯äº†ç”Ÿæˆå†…å®¹çš„è´¨é‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.06922v1">TreePS-RAG: Tree-based Process Supervision for Reinforcement Learning in Agentic RAG</a></td><td><details><summary>å±•å¼€</summary>Agentic retrieval-augmented generation (RAG) formulates question answering as a multi-step interaction between reasoning and information retrieval, and has recently been advanced by reinforcement learning (RL) with outcome-based supervision. While effective, relying solely on sparse final rewards limits step-wise credit assignment and provides weak guidance for intermediate reasoning and actions. Recent efforts explore process-level supervision, but typically depend on offline constructed training data, which risks distribution shift, or require costly intermediate annotations. We present TreePS-RAG, an online, tree-based RL framework for agentic RAG that enables step-wise credit assignment while retaining standard outcome-only rewards. Our key insight is to model agentic RAG reasoning as a rollout tree, where each reasoning step naturally maps to a node. This tree structure allows step utility to be estimated via Monte Carlo estimation over its descendant outcomes, yielding fine-grained process advantages without requiring intermediate labels. To make this paradigm practical, we introduce an efficient online tree construction strategy that preserves exploration diversity under a constrained computational budget. With a rollout cost comparable to strong baselines like Search-R1, experiments on seven multi-hop and general QA benchmarks across multiple model scales show that TreePS-RAG consistently and significantly outperforms both outcome-supervised and leading process-supervised RL methods.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºTreePS-RAGçš„åœ¨çº¿æ ‘å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæ”¹è¿›åŸºäºä»£ç†çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿã€‚ä¼ ç»ŸRAGä¾èµ–ç¨€ç–çš„æœ€ç»ˆå¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œè€ŒTreePS-RAGé€šè¿‡å°†æ¨ç†è¿‡ç¨‹å»ºæ¨¡ä¸ºæ ‘ç»“æ„ï¼Œåˆ©ç”¨è’™ç‰¹å¡æ´›ä¼°è®¡å®ç°ç»†ç²’åº¦çš„è¿‡ç¨‹çº§ä¿¡ç”¨åˆ†é…ï¼Œæ— éœ€ä¸­é—´æ ‡æ³¨ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä»…ä¾èµ–ç»“æœç›‘ç£æˆ–è¿‡ç¨‹ç›‘ç£çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.06842v1">Seeing through the Conflict: Transparent Knowledge Conflict Handling in Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) equipped with retrieval--the Retrieval-Augmented Generation (RAG) paradigm--should combine their parametric knowledge with external evidence, yet in practice they often hallucinate, over-trust noisy snippets, or ignore vital context. We introduce TCR (Transparent Conflict Resolution), a plug-and-play framework that makes this decision process observable and controllable. TCR (i) disentangles semantic match and factual consistency via dual contrastive encoders, (ii) estimates self-answerability to gauge confidence in internal memory, and (iii) feeds the three scalar signals to the generator through a lightweight soft-prompt with SNR-based weighting. Across seven benchmarks TCR improves conflict detection (+5-18 F1), raises knowledge-gap recovery by +21.4 pp and cuts misleading-context overrides by -29.3 pp, while adding only 0.3% parameters. The signals align with human judgements and expose temporal decision patterns.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºTCRï¼ˆTransparent Conflict Resolutionï¼‰çš„å³æ’å³ç”¨æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­å­˜åœ¨çš„å¹»è§‰ã€è¿‡åº¦ä¿¡ä»»å™ªå£°ç‰‡æ®µæˆ–å¿½ç•¥å…³é”®ä¸Šä¸‹æ–‡ç­‰é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åŒå¯¹æ¯”ç¼–ç å™¨åˆ†ç¦»è¯­ä¹‰åŒ¹é…ä¸äº‹å®ä¸€è‡´æ€§ã€è¯„ä¼°è‡ªæˆ‘å¯ç­”æ€§ä»¥è¡¡é‡å¯¹å†…éƒ¨çŸ¥è¯†çš„ç½®ä¿¡åº¦ï¼Œå¹¶åˆ©ç”¨åŸºäºä¿¡å™ªæ¯”åŠ æƒçš„è½»é‡çº§è½¯æç¤ºå°†ä¸‰ä¸ªæ ‡é‡ä¿¡å·ä¼ é€’ç»™ç”Ÿæˆå™¨ï¼Œä»è€Œæå‡å†²çªæ£€æµ‹èƒ½åŠ›ã€çŸ¥è¯†ç¼ºå£æ¢å¤ç‡ï¼Œå¹¶å‡å°‘è¯¯å¯¼æ€§ä¸Šä¸‹æ–‡çš„å¹²æ‰°ï¼ŒåŒæ—¶ä»…å¢åŠ å°‘é‡å‚æ•°ã€‚è¯¥æ¡†æ¶ä½¿RAGçš„å†³ç­–è¿‡ç¨‹å˜å¾—å¯è§‚æµ‹å’Œå¯æ§ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…¶ä¿¡å·ä¸äººç±»åˆ¤æ–­ä¸€è‡´å¹¶èƒ½æ­ç¤ºæ—¶åºå†³ç­–æ¨¡å¼ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.06799v1">CIRAG: Construction-Integration Retrieval and Adaptive Generation for Multi-hop Question Answering</a></td><td><details><summary>å±•å¼€</summary>Triple-based Iterative Retrieval-Augmented Generation (iRAG) mitigates document-level noise for multi-hop question answering. However, existing methods still face limitations: (i) greedy single-path expansion, which propagates early errors and fails to capture parallel evidence from different reasoning branches, and (ii) granularity-demand mismatch, where a single evidence representation struggles to balance noise control with contextual sufficiency. In this paper, we propose the Construction-Integration Retrieval and Adaptive Generation model, CIRAG. It introduces an Iterative Construction-Integration module that constructs candidate triples and history-conditionally integrates them to distill core triples and generate the next-hop query. This module mitigates the greedy trap by preserving multiple plausible evidence chains. Besides, we propose an Adaptive Cascaded Multi-Granularity Generation module that progressively expands contextual evidence based on the problem requirements, from triples to supporting sentences and full passages. Moreover, we introduce Trajectory Distillation, which distills the teacher model's integration policy into a lightweight student, enabling efficient and reliable long-horizon reasoning. Extensive experiments demonstrate that CIRAG achieves superior performance compared to existing iRAG methods.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºCIRAGçš„æ¨¡å‹ï¼Œé’ˆå¯¹å¤šè·³é—®ç­”ä»»åŠ¡ä¸­ç°æœ‰è¿­ä»£æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆiRAGï¼‰æ–¹æ³•çš„å±€é™æ€§è¿›è¡Œæ”¹è¿›ã€‚CIRAGé€šè¿‡è¿­ä»£æ„å»º-é›†æˆæ¨¡å—ä¿ç•™å¤šè·¯å¾„è¯æ®é“¾ä»¥é¿å…è´ªå©ªå•è·¯å¾„æ‰©å±•çš„è¯¯å·®ä¼ æ’­ï¼Œå¹¶é‡‡ç”¨è‡ªé€‚åº”çº§è”å¤šç²’åº¦ç”Ÿæˆæ¨¡å—åŠ¨æ€è°ƒæ•´è¯æ®ç²’åº¦ï¼ˆä»ä¸‰å…ƒç»„åˆ°å®Œæ•´æ®µè½ï¼‰ã€‚æ­¤å¤–ï¼Œé€šè¿‡è½¨è¿¹è’¸é¦æŠ€æœ¯å°†æ•™å¸ˆæ¨¡å‹çš„é›†æˆç­–ç•¥è¿ç§»è‡³è½»é‡çº§å­¦ç”Ÿæ¨¡å‹ï¼Œæå‡é•¿ç¨‹æ¨ç†æ•ˆç‡ã€‚å®éªŒè¡¨æ˜CIRAGåœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰iRAGæ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.06607v1">Pragya: An AI-Based Semantic Recommendation System for Sanskrit Subhasitas</a></td><td><details><summary>å±•å¼€</summary>Sanskrit Subhasitas encapsulate centuries of cultural and philosophical wisdom, yet remain underutilized in the digital age due to linguistic and contextual barriers. In this work, we present Pragya, a retrieval-augmented generation (RAG) framework for semantic recommendation of Subhasitas. We curate a dataset of 200 verses annotated with thematic tags such as motivation, friendship, and compassion. Using sentence embeddings (IndicBERT), the system retrieves top-k verses relevant to user queries. The retrieved results are then passed to a generative model (Mistral LLM) to produce transliterations, translations, and contextual explanations. Experimental evaluation demonstrates that semantic retrieval significantly outperforms keyword matching in precision and relevance, while user studies highlight improved accessibility through generated summaries. To our knowledge, this is the first attempt at integrating retrieval and generation for Sanskrit Subhasitas, bridging cultural heritage with modern applied AI.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºPragyaæ¡†æ¶ï¼Œåˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯æ¨èæ¢µæ–‡Subhasitasï¼ˆè°šè¯­ï¼‰ï¼Œé€šè¿‡IndicBERTåµŒå…¥æ£€ç´¢ä¸ç”¨æˆ·æŸ¥è¯¢ç›¸å…³çš„è°šè¯­ï¼Œå†ç»“åˆMistralå¤§æ¨¡å‹ç”Ÿæˆè¯‘æ–‡å’Œè§£é‡Šï¼Œå®éªŒè¯æ˜å…¶è¯­ä¹‰æ£€ç´¢ä¼˜äºå…³é”®è¯åŒ¹é…ï¼Œå¹¶é¦–æ¬¡å°†RAGåº”ç”¨äºæ­¤æ–‡åŒ–é—äº§é¢†åŸŸã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.06603v1">N2N-GQA: Noise-to-Narrative for Graph-Based Table-Text Question Answering Using LLMs</a></td><td><details><summary>å±•å¼€</summary>Multi-hop question answering over hybrid table-text data requires retrieving and reasoning across multiple evidence pieces from large corpora, but standard Retrieval-Augmented Generation (RAG) pipelines process documents as flat ranked lists, causing retrieval noise to obscure reasoning chains. We introduce N2N-GQA. To our knowledge, it is the first zeroshot framework for open-domain hybrid table-text QA that constructs dynamic evidence graphs from noisy retrieval outputs. Our key insight is that multi-hop reasoning requires understanding relationships between evidence pieces: by modeling documents as graph nodes with semantic relationships as edges, we identify bridge documents connecting reasoning steps, a capability absent in list-based retrieval. On OTT-QA, graph-based evidence curation provides a 19.9-point EM improvement over strong baselines, demonstrating that organizing retrieval results as structured graphs is critical for multihop reasoning. N2N-GQA achieves 48.80 EM, matching finetuned retrieval models (CORE: 49.0 EM) and approaching heavily optimized systems (COS: 56.9 EM) without any task specific training. This establishes graph-structured evidence organization as essential for scalable, zero-shot multi-hop QA systems and demonstrates that simple, interpretable graph construction can rival sophisticated fine-tuned approaches.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†N2N-GQAï¼Œä¸€ç§é’ˆå¯¹æ··åˆè¡¨æ–‡æœ¬æ•°æ®çš„å¤šè·³é—®ç­”é›¶æ ·æœ¬æ¡†æ¶ï¼Œé€šè¿‡å°†æ£€ç´¢ç»“æœå»ºæ¨¡ä¸ºåŠ¨æ€è¯æ®å›¾ï¼ˆèŠ‚ç‚¹ä¸ºæ–‡æ¡£ï¼Œè¾¹ä¸ºè¯­ä¹‰å…³ç³»ï¼‰æ¥æå‡å¤šè·³æ¨ç†èƒ½åŠ›ã€‚ç›¸æ¯”ä¼ ç»ŸRAGçš„æ‰å¹³åŒ–åˆ—è¡¨æ£€ç´¢ï¼Œè¯¥æ–¹æ³•å‡å°‘äº†å™ªå£°å¹¶è¯†åˆ«æ¡¥æ¥æ–‡æ¡£ï¼Œåœ¨OTT-QAæ•°æ®é›†ä¸Šå®ç°19.9åˆ†EMæå‡ï¼Œé›¶æ ·æœ¬æ€§èƒ½æ¥è¿‘å¾®è°ƒæ¨¡å‹ï¼Œè¯æ˜äº†å›¾ç»“æ„æ£€ç´¢å¯¹å¤šè·³QAçš„é—œéµä½œç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.06564v1">CSR-RAG: An Efficient Retrieval System for Text-to-SQL on the Enterprise Scale</a></td><td><details><summary>å±•å¼€</summary>Natural language to SQL translation (Text-to-SQL) is one of the long-standing problems that has recently benefited from advances in Large Language Models (LLMs). While most academic Text-to-SQL benchmarks request schema description as a part of natural language input, enterprise-scale applications often require table retrieval before SQL query generation. To address this need, we propose a novel hybrid Retrieval Augmented Generation (RAG) system consisting of contextual, structural, and relational retrieval (CSR-RAG) to achieve computationally efficient yet sufficiently accurate retrieval for enterprise-scale databases. Through extensive enterprise benchmarks, we demonstrate that CSR-RAG achieves up to 40% precision and over 80% recall while incurring a negligible average query generation latency of only 30ms on commodity data center hardware, which makes it appropriate for modern LLM-based enterprise-scale systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ··åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆCSR-RAGï¼‰ç³»ç»Ÿï¼Œç»“åˆä¸Šä¸‹æ–‡ã€ç»“æ„å’Œå…³ç³»æ£€ç´¢ï¼Œç”¨äºä¼ä¸šçº§æ•°æ®åº“çš„è‡ªç„¶è¯­è¨€åˆ°SQLè½¬æ¢ï¼ˆText-to-SQLï¼‰ï¼Œåœ¨ä¿æŒé«˜ç²¾åº¦å’Œå¬å›ç‡çš„åŒæ—¶å®ç°äº†æä½çš„å»¶è¿Ÿã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.06551v1">L-RAG: Balancing Context and Retrieval with Entropy-Based Lazy Loading</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has emerged as the predominant paradigm for grounding Large Language Model outputs in factual knowledge, effectively mitigating hallucinations. However, conventional RAG systems operate under a "retrieve-always" assumption, querying vector databases for every input regardless of query complexity. This static approach incurs substantial computational overhead and inference latency, particularly problematic for high-throughput production deployments. We introduce L-RAG (Lazy Retrieval-Augmented Generation), an adaptive framework that implements hierarchical context management through entropy-based gating. L-RAG employs a two-tier architecture: queries are first processed with a compact document summary, and expensive chunk retrieval is triggered only when the model's predictive entropy exceeds a calibrated threshold, signaling genuine uncertainty. Through experiments on SQuAD 2.0 (N=500) using the Phi-2 model, we demonstrate that L-RAG provides a tunable accuracy-efficiency trade-off: at a conservative threshold (tau=0.5), L-RAG achieves 78.2% accuracy, matching Standard RAG (77.8%), with 8% retrieval reduction; at a balanced threshold (tau=1.0), retrieval reduction increases to 26% with modest accuracy trade-off (76.0%). Latency analysis shows that L-RAG saves 80-210ms per query when retrieval latency exceeds 500ms. Analysis of entropy distributions reveals statistically significant separation (p < 0.001) between correct predictions (H=1.72) and errors (H=2.20), validating entropy as a reliable uncertainty signal. L-RAG offers a practical, training-free approach toward more efficient RAG deployment, providing system architects with a configurable knob to balance accuracy and throughput requirements.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºL-RAGï¼ˆLazy Retrieval-Augmented Generationï¼‰çš„è‡ªé€‚åº”æ¡†æ¶ï¼Œé€šè¿‡åŸºäºç†µçš„é—¨æ§æœºåˆ¶å®ç°åˆ†å±‚ä¸Šä¸‹æ–‡ç®¡ç†ï¼Œä»¥å‡å°‘ä¼ ç»ŸRAGç³»ç»Ÿä¸­ä¸å¿…è¦çš„æ£€ç´¢å¼€é”€ï¼›å®éªŒæ˜¾ç¤ºL-RAGå¯åœ¨ä¿æŒä¸æ ‡å‡†RAGç›¸è¿‘çš„å‡†ç¡®ç‡ï¼ˆå¦‚78.2% vs 77.8%ï¼‰çš„åŒæ—¶æ˜¾è‘—é™ä½æ£€ç´¢é¢‘ç‡ï¼ˆ8%-26%ï¼‰å’Œå»¶è¿Ÿï¼ˆèŠ‚çœ80-210msï¼‰ï¼Œå…¶æ ¸å¿ƒæ˜¯åˆ©ç”¨æ¨¡å‹é¢„æµ‹ç†µåŠ¨æ€è§¦å‘æ£€ç´¢ï¼ŒéªŒè¯äº†ç†µä½œä¸ºä¸ç¡®å®šæ€§ä¿¡å·çš„å¯é æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.06519v1">MedRAGChecker: Claim-Level Verification for Biomedical Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Biomedical retrieval-augmented generation (RAG) can ground LLM answers in medical literature, yet long-form outputs often contain isolated unsupported or contradictory claims with safety implications.
  We introduce MedRAGChecker, a claim-level verification and diagnostic framework for biomedical RAG.
  Given a question, retrieved evidence, and a generated answer, MedRAGChecker decomposes the answer into atomic claims and estimates claim support by combining evidence-grounded natural language inference (NLI) with biomedical knowledge-graph (KG) consistency signals.
  Aggregating claim decisions yields answer-level diagnostics that help disentangle retrieval and generation failures, including faithfulness, under-evidence, contradiction, and safety-critical error rates.
  To enable scalable evaluation, we distill the pipeline into compact biomedical models and use an ensemble verifier with class-specific reliability weighting.
  Experiments on four biomedical QA benchmarks show that MedRAGChecker reliably flags unsupported and contradicted claims and reveals distinct risk profiles across generators, particularly on safety-critical biomedical relations.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡é’ˆå¯¹ç”Ÿç‰©åŒ»å­¦é¢†åŸŸRAGæŠ€æœ¯ç”Ÿæˆé•¿æ–‡æœ¬æ—¶å¯èƒ½å­˜åœ¨çš„æ— æ”¯æŒæˆ–çŸ›ç›¾å£°æ˜é—®é¢˜ï¼Œæå‡ºäº†MedRAGCheckeræ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†è§£ç”Ÿæˆç­”æ¡ˆä¸ºåŸå­å£°æ˜ï¼Œç»“åˆè‡ªç„¶è¯­è¨€æ¨ç†å’ŒçŸ¥è¯†å›¾è°±ä¸€è‡´æ€§éªŒè¯å£°æ˜å¯é æ€§ï¼Œå¹¶æä¾›ç­”æ¡ˆçº§è¯Šæ–­ä»¥åŒºåˆ†æ£€ç´¢ä¸ç”Ÿæˆé”™è¯¯ï¼Œæœ€ç»ˆåœ¨å››ä¸ªç”Ÿç‰©åŒ»å­¦QAåŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.06426v1">NC-Bench: An LLM Benchmark for Evaluating Conversational Competence</a></td><td><details><summary>å±•å¼€</summary>The Natural Conversation Benchmark (NC-Bench) introduce a new approach to evaluating the general conversational competence of large language models (LLMs). Unlike prior benchmarks that focus on the content of model behavior, NC-Bench focuses on the form and structure of natural conversation. Grounded in the IBM Natural Conversation Framework (NCF), NC-Bench comprises three distinct sets. The Basic Conversation Competence set evaluates fundamental sequence management practices, such as answering inquiries, repairing responses, and closing conversational pairs. The RAG set applies the same sequence management patterns as the first set but incorporates retrieval-augmented generation (RAG). The Complex Request set extends the evaluation to complex requests involving more intricate sequence management patterns. Each benchmark tests a model's ability to produce contextually appropriate conversational actions in response to characteristic interaction patterns. Initial evaluations across 6 open-source models and 14 interaction patterns show that models perform well on basic answering tasks, struggle more with repair tasks (especially repeat), have mixed performance on closing sequences, and find complex multi-turn requests most challenging, with Qwen models excelling on the Basic set and Granite models on the RAG set and the Complex Request set. By operationalizing fundamental principles of human conversation, NC-Bench provides a lightweight, extensible, and theory-grounded framework for assessing and improving the conversational abilities of LLMs beyond topical or task-specific benchmarks.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†NC-Benchï¼Œä¸€ä¸ªåŸºäºIBMè‡ªç„¶å¯¹è¯æ¡†æ¶ï¼ˆNCFï¼‰çš„è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šç”¨å¯¹è¯èƒ½åŠ›çš„åŸºå‡†ã€‚å…¶ä¸­ä¸“é—¨è®¾ç½®äº†RAGè¯„æµ‹é›†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ç»“åˆæ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯æ—¶çš„å¯¹è¯åºåˆ—ç®¡ç†èƒ½åŠ›ï¼ˆå¦‚å›ç­”ã€ä¿®æ­£ã€ç»“æŸå¯¹è¯ç­‰ï¼‰ï¼Œå¹¶å‘ç°Graniteæ¨¡å‹åœ¨è¯¥éƒ¨åˆ†è¡¨ç°æœ€ä¼˜ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜åˆ†æäº†æ¨¡å‹åœ¨åŸºç¡€å¯¹è¯ã€å¤æ‚å¤šè½®è¯·æ±‚ç­‰ä»»åŠ¡ä¸­çš„è¡¨ç°å·®å¼‚ï¼Œæå‡ºäº†ä¸€ä¸ªç†è®ºé©±åŠ¨çš„è½»é‡çº§è¯„ä¼°æ¡†æ¶ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.07861v1">EmbeddingRWKV: State-Centric Retrieval with Reusable States</a></td><td><details><summary>å±•å¼€</summary>Current Retrieval-Augmented Generation (RAG) systems typically employ a traditional two-stage pipeline: an embedding model for initial retrieval followed by a reranker for refinement. However, this paradigm suffers from significant inefficiency due to the lack of shared information between stages, leading to substantial redundant computation. To address this limitation, we propose \textbf{State-Centric Retrieval}, a unified retrieval paradigm that utilizes "states" as a bridge to connect embedding models and rerankers. First, we perform state representation learning by fine-tuning an RWKV-based LLM, transforming it into \textbf{EmbeddingRWKV}, a unified model that serves as both an embedding model and a state backbone for extracting compact, reusable states. Building upon these reusable states, we further design a state-based reranker to fully leverage precomputed information. During reranking, the model processes only query tokens, decoupling inference cost from document length and yielding a 5.4$\times$--44.8$\times$ speedup. Furthermore, we observe that retaining all intermediate layer states is unnecessary; with a uniform layer selection strategy, our model maintains 98.62\% of full-model performance using only 25\% of the layers. Extensive experiments demonstrate that State-Centric Retrieval achieves high-quality retrieval and reranking results while significantly enhancing overall system efficiency. Code is available at \href{https://github.com/howard-hou/EmbeddingRWKV}{our GitHub repository}.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸º"çŠ¶æ€ä¸­å¿ƒæ£€ç´¢"ï¼ˆState-Centric Retrievalï¼‰çš„æ–°é¢–æ£€ç´¢èŒƒå¼ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸRAGä¸¤é˜¶æ®µï¼ˆåµŒå…¥æ¨¡å‹åˆç­›+é‡æ’ï¼‰å­˜åœ¨çš„æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚é€šè¿‡å°†RWKV-based LLMå¾®è°ƒä¸ºç»Ÿä¸€æ¨¡å‹EmbeddingRWKVï¼Œæ—¢èƒ½ä½œä¸ºåµŒå…¥æ¨¡å‹åˆèƒ½ç”Ÿæˆå¯å¤ç”¨çš„ç´§å‡‘çŠ¶æ€è¡¨å¾ï¼Œå¹¶è®¾è®¡åŸºäºçŠ¶æ€çš„é‡æ’å™¨å¤§å¹…é™ä½è®¡ç®—å†—ä½™ï¼ˆæé€Ÿ5.4-44.8å€ï¼‰ï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨ä¿è¯98.62%æ€§èƒ½çš„åŒæ—¶ä»…éœ€25%è®¡ç®—é‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.06411v1">Structured Episodic Event Memory</a></td><td><details><summary>å±•å¼€</summary>Current approaches to memory in Large Language Models (LLMs) predominantly rely on static Retrieval-Augmented Generation (RAG), which often results in scattered retrieval and fails to capture the structural dependencies required for complex reasoning. For autonomous agents, these passive and flat architectures lack the cognitive organization necessary to model the dynamic and associative nature of long-term interaction. To address this, we propose Structured Episodic Event Memory (SEEM), a hierarchical framework that synergizes a graph memory layer for relational facts with a dynamic episodic memory layer for narrative progression. Grounded in cognitive frame theory, SEEM transforms interaction streams into structured Episodic Event Frames (EEFs) anchored by precise provenance pointers. Furthermore, we introduce an agentic associative fusion and Reverse Provenance Expansion (RPE) mechanism to reconstruct coherent narrative contexts from fragmented evidence. Experimental results on the LoCoMo and LongMemEval benchmarks demonstrate that SEEM significantly outperforms baselines, enabling agents to maintain superior narrative coherence and logical consistency.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡é’ˆå¯¹å½“å‰å¤§è¯­è¨€æ¨¡å‹ä¸­é™æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„å±€é™æ€§ï¼ˆå¦‚æ£€ç´¢åˆ†æ•£ã€ç¼ºä¹ç»“æ„åŒ–ä¾èµ–ï¼‰ï¼Œæå‡ºäº†ä¸€ç§åä¸ºSEEMçš„åˆ†å±‚è®°å¿†æ¡†æ¶ã€‚SEEMç»“åˆå›¾è®°å¿†å±‚å’ŒåŠ¨æ€æƒ…æ™¯è®°å¿†å±‚ï¼Œé€šè¿‡è®¤çŸ¥æ¡†æ¶ç†è®ºå°†äº¤äº’æµè½¬åŒ–ä¸ºç»“æ„åŒ–äº‹ä»¶å¸§ï¼Œå¹¶å¼•å…¥å…³è”èåˆä¸é€†å‘æº¯æºæ‰©å±•æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†ä»£ç†åœ¨å™äº‹è¿è´¯æ€§å’Œé€»è¾‘ä¸€è‡´æ€§ä¸Šçš„è¡¨ç°ã€‚å®éªŒè¯æ˜å…¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2026-01-09
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2601.06607v1">Pragya: An AI-Based Semantic Recommendation System for Sanskrit Subhasitas</a></td><td><details><summary>å±•å¼€</summary>Sanskrit Subhasitas encapsulate centuries of cultural and philosophical wisdom, yet remain underutilized in the digital age due to linguistic and contextual barriers. In this work, we present Pragya, a retrieval-augmented generation (RAG) framework for semantic recommendation of Subhasitas. We curate a dataset of 200 verses annotated with thematic tags such as motivation, friendship, and compassion. Using sentence embeddings (IndicBERT), the system retrieves top-k verses relevant to user queries. The retrieved results are then passed to a generative model (Mistral LLM) to produce transliterations, translations, and contextual explanations. Experimental evaluation demonstrates that semantic retrieval significantly outperforms keyword matching in precision and relevance, while user studies highlight improved accessibility through generated summaries. To our knowledge, this is the first attempt at integrating retrieval and generation for Sanskrit Subhasitas, bridging cultural heritage with modern applied AI.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºPragyaçš„RAGæ¡†æ¶ï¼Œç”¨äºæ¢µæ–‡Subhasitasï¼ˆè°šè¯­ï¼‰çš„è¯­ä¹‰æ¨èï¼Œé€šè¿‡æ£€ç´¢ï¼ˆIndicBERTåµŒå…¥ï¼‰ä¸ç”Ÿæˆï¼ˆMistralå¤§æ¨¡å‹ï¼‰ç»“åˆï¼Œå®ç°è°šè¯­çš„ç¿»è¯‘ã€è§£é‡ŠåŠæƒ…å¢ƒåŒ–è¾“å‡ºï¼Œå®éªŒè¡¨æ˜å…¶è¯­ä¹‰æ£€ç´¢æ•ˆæœä¼˜äºå…³é”®è¯åŒ¹é…ï¼Œå¹¶æå‡äº†æ–‡åŒ–å†…å®¹çš„å¯è®¿é—®æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.06603v1">N2N-GQA: Noise-to-Narrative for Graph-Based Table-Text Question Answering Using LLMs</a></td><td><details><summary>å±•å¼€</summary>Multi-hop question answering over hybrid table-text data requires retrieving and reasoning across multiple evidence pieces from large corpora, but standard Retrieval-Augmented Generation (RAG) pipelines process documents as flat ranked lists, causing retrieval noise to obscure reasoning chains. We introduce N2N-GQA. To our knowledge, it is the first zeroshot framework for open-domain hybrid table-text QA that constructs dynamic evidence graphs from noisy retrieval outputs. Our key insight is that multi-hop reasoning requires understanding relationships between evidence pieces: by modeling documents as graph nodes with semantic relationships as edges, we identify bridge documents connecting reasoning steps, a capability absent in list-based retrieval. On OTT-QA, graph-based evidence curation provides a 19.9-point EM improvement over strong baselines, demonstrating that organizing retrieval results as structured graphs is critical for multihop reasoning. N2N-GQA achieves 48.80 EM, matching finetuned retrieval models (CORE: 49.0 EM) and approaching heavily optimized systems (COS: 56.9 EM) without any task specific training. This establishes graph-structured evidence organization as essential for scalable, zero-shot multi-hop QA systems and demonstrates that simple, interpretable graph construction can rival sophisticated fine-tuned approaches.</details></td><td><details><summary>å±•å¼€</summary></details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.06564v1">CSR-RAG: An Efficient Retrieval System for Text-to-SQL on the Enterprise Scale</a></td><td><details><summary>å±•å¼€</summary>Natural language to SQL translation (Text-to-SQL) is one of the long-standing problems that has recently benefited from advances in Large Language Models (LLMs). While most academic Text-to-SQL benchmarks request schema description as a part of natural language input, enterprise-scale applications often require table retrieval before SQL query generation. To address this need, we propose a novel hybrid Retrieval Augmented Generation (RAG) system consisting of contextual, structural, and relational retrieval (CSR-RAG) to achieve computationally efficient yet sufficiently accurate retrieval for enterprise-scale databases. Through extensive enterprise benchmarks, we demonstrate that CSR-RAG achieves up to 40% precision and over 80% recall while incurring a negligible average query generation latency of only 30ms on commodity data center hardware, which makes it appropriate for modern LLM-based enterprise-scale systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ··åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆCSR-RAGï¼‰ç³»ç»Ÿï¼Œç”¨äºä¼ä¸šçº§æ•°æ®åº“çš„è‡ªç„¶è¯­è¨€åˆ°SQLè½¬æ¢ï¼ˆText-to-SQLï¼‰ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†ä¸Šä¸‹æ–‡ã€ç»“æ„å’Œå…³ç³»æ£€ç´¢ï¼Œä»¥æé«˜æ£€ç´¢çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå¹¶åœ¨ä¼ä¸šçº§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå®ç°äº†é«˜ç²¾åº¦å’Œä½å»¶è¿Ÿã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.06551v1">L-RAG: Balancing Context and Retrieval with Entropy-Based Lazy Loading</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has emerged as the predominant paradigm for grounding Large Language Model outputs in factual knowledge, effectively mitigating hallucinations. However, conventional RAG systems operate under a "retrieve-always" assumption, querying vector databases for every input regardless of query complexity. This static approach incurs substantial computational overhead and inference latency, particularly problematic for high-throughput production deployments. We introduce L-RAG (Lazy Retrieval-Augmented Generation), an adaptive framework that implements hierarchical context management through entropy-based gating. L-RAG employs a two-tier architecture: queries are first processed with a compact document summary, and expensive chunk retrieval is triggered only when the model's predictive entropy exceeds a calibrated threshold, signaling genuine uncertainty. Through experiments on SQuAD 2.0 (N=500) using the Phi-2 model, we demonstrate that L-RAG provides a tunable accuracy-efficiency trade-off: at a conservative threshold (tau=0.5), L-RAG achieves 78.2% accuracy, matching Standard RAG (77.8%), with 8% retrieval reduction; at a balanced threshold (tau=1.0), retrieval reduction increases to 26% with modest accuracy trade-off (76.0%). Latency analysis shows that L-RAG saves 80-210ms per query when retrieval latency exceeds 500ms. Analysis of entropy distributions reveals statistically significant separation (p < 0.001) between correct predictions (H=1.72) and errors (H=2.20), validating entropy as a reliable uncertainty signal. L-RAG offers a practical, training-free approach toward more efficient RAG deployment, providing system architects with a configurable knob to balance accuracy and throughput requirements.</details></td><td><details><summary>å±•å¼€</summary></details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.06519v1">MedRAGChecker: Claim-Level Verification for Biomedical Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Biomedical retrieval-augmented generation (RAG) can ground LLM answers in medical literature, yet long-form outputs often contain isolated unsupported or contradictory claims with safety implications.
  We introduce MedRAGChecker, a claim-level verification and diagnostic framework for biomedical RAG.
  Given a question, retrieved evidence, and a generated answer, MedRAGChecker decomposes the answer into atomic claims and estimates claim support by combining evidence-grounded natural language inference (NLI) with biomedical knowledge-graph (KG) consistency signals.
  Aggregating claim decisions yields answer-level diagnostics that help disentangle retrieval and generation failures, including faithfulness, under-evidence, contradiction, and safety-critical error rates.
  To enable scalable evaluation, we distill the pipeline into compact biomedical models and use an ensemble verifier with class-specific reliability weighting.
  Experiments on four biomedical QA benchmarks show that MedRAGChecker reliably flags unsupported and contradicted claims and reveals distinct risk profiles across generators, particularly on safety-critical biomedical relations.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMedRAGCheckerçš„æ¡†æ¶ï¼Œç”¨äºéªŒè¯å’Œè¯Šæ–­ç”Ÿç‰©åŒ»å­¦é¢†åŸŸRAGç”Ÿæˆçš„ç­”æ¡ˆï¼Œé€šè¿‡åˆ†è§£ç­”æ¡ˆä¸ºåŸå­åŒ–ä¸»å¼ å¹¶ç»“åˆè‡ªç„¶è¯­è¨€æ¨ç†ä¸çŸ¥è¯†å›¾è°±ä¸€è‡´æ€§ä¿¡å·æ¥è¯„ä¼°ä¸»å¼ çš„å¯ä¿¡åº¦ï¼Œä»è€Œè¯†åˆ«æœªæ”¯æŒæˆ–çŸ›ç›¾çš„ä¸»å¼ ï¼Œå¹¶åŒºåˆ†æ£€ç´¢ä¸ç”Ÿæˆé”™è¯¯ï¼Œæœ€ç»ˆåœ¨å››ä¸ªç”Ÿç‰©åŒ»å­¦QAåŸºå‡†ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.06426v1">NC-Bench: An LLM Benchmark for Evaluating Conversational Competence</a></td><td><details><summary>å±•å¼€</summary>The Natural Conversation Benchmark (NC-Bench) introduce a new approach to evaluating the general conversational competence of large language models (LLMs). Unlike prior benchmarks that focus on the content of model behavior, NC-Bench focuses on the form and structure of natural conversation. Grounded in the IBM Natural Conversation Framework (NCF), NC-Bench comprises three distinct sets. The Basic Conversation Competence set evaluates fundamental sequence management practices, such as answering inquiries, repairing responses, and closing conversational pairs. The RAG set applies the same sequence management patterns as the first set but incorporates retrieval-augmented generation (RAG). The Complex Request set extends the evaluation to complex requests involving more intricate sequence management patterns. Each benchmark tests a model's ability to produce contextually appropriate conversational actions in response to characteristic interaction patterns. Initial evaluations across 6 open-source models and 14 interaction patterns show that models perform well on basic answering tasks, struggle more with repair tasks (especially repeat), have mixed performance on closing sequences, and find complex multi-turn requests most challenging, with Qwen models excelling on the Basic set and Granite models on the RAG set and the Complex Request set. By operationalizing fundamental principles of human conversation, NC-Bench provides a lightweight, extensible, and theory-grounded framework for assessing and improving the conversational abilities of LLMs beyond topical or task-specific benchmarks.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†Natural Conversation Benchmark (NC-Bench)ï¼Œä¸€ä¸ªåŸºäºIBMè‡ªç„¶å¯¹è¯æ¡†æ¶(NCF)çš„è¯„ä¼°åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„é€šç”¨å¯¹è¯èƒ½åŠ›ã€‚NC-BenchåŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼šåŸºç¡€å¯¹è¯èƒ½åŠ›æµ‹è¯•ã€æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æµ‹è¯•å’Œå¤æ‚è¯·æ±‚æµ‹è¯•ï¼Œé‡ç‚¹å…³æ³¨å¯¹è¯çš„å½¢å¼å’Œç»“æ„è€Œéå†…å®¹ã€‚åˆæ­¥è¯„ä¼°æ˜¾ç¤ºï¼Œä¸åŒæ¨¡å‹åœ¨åŸºç¡€é—®ç­”ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ä¿®å¤ä»»åŠ¡å’Œå¤æ‚å¤šè½®è¯·æ±‚ä¸­è¡¨ç°ä¸ä¸€ï¼Œå…¶ä¸­Qwenæ¨¡å‹åœ¨åŸºç¡€æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè€ŒGraniteæ¨¡å‹åœ¨RAGå’Œå¤æ‚è¯·æ±‚æµ‹è¯•ä¸­é¢†å…ˆã€‚è¯¥åŸºå‡†ä¸ºè¯„ä¼°å’Œæ”¹è¿›LLMsçš„å¯¹è¯èƒ½åŠ›æä¾›äº†ä¸€ä¸ªè½»é‡çº§ã€å¯æ‰©å±•ä¸”ç†è®ºåŸºç¡€çš„æ¡†æ¶ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.07861v1">EmbeddingRWKV: State-Centric Retrieval with Reusable States</a></td><td><details><summary>å±•å¼€</summary>Current Retrieval-Augmented Generation (RAG) systems typically employ a traditional two-stage pipeline: an embedding model for initial retrieval followed by a reranker for refinement. However, this paradigm suffers from significant inefficiency due to the lack of shared information between stages, leading to substantial redundant computation. To address this limitation, we propose \textbf{State-Centric Retrieval}, a unified retrieval paradigm that utilizes "states" as a bridge to connect embedding models and rerankers. First, we perform state representation learning by fine-tuning an RWKV-based LLM, transforming it into \textbf{EmbeddingRWKV}, a unified model that serves as both an embedding model and a state backbone for extracting compact, reusable states. Building upon these reusable states, we further design a state-based reranker to fully leverage precomputed information. During reranking, the model processes only query tokens, decoupling inference cost from document length and yielding a 5.4$\times$--44.8$\times$ speedup. Furthermore, we observe that retaining all intermediate layer states is unnecessary; with a uniform layer selection strategy, our model maintains 98.62\% of full-model performance using only 25\% of the layers. Extensive experiments demonstrate that State-Centric Retrieval achieves high-quality retrieval and reranking results while significantly enhancing overall system efficiency. Code is available at \href{https://github.com/howard-hou/EmbeddingRWKV}{our GitHub repository}.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸º"State-Centric Retrieval"çš„æ–°å‹æ£€ç´¢èŒƒå¼ï¼Œé€šè¿‡å°†åµŒå…¥æ¨¡å‹å’Œé‡æ’åºå™¨ç»Ÿä¸€ä¸ºä¸€ä¸ªåŸºäºçŠ¶æ€å…±äº«çš„RWKVæ¶æ„æ¨¡å‹ï¼ˆEmbeddingRWKVï¼‰ï¼Œè§£å†³äº†ä¼ ç»ŸRAGä¸¤é˜¶æ®µæµç¨‹ä¸­ä¿¡æ¯ä¸å…±äº«å¯¼è‡´çš„å†—ä½™è®¡ç®—é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¯å¤ç”¨çš„ä¸­é—´çŠ¶æ€è¡¨ç¤ºï¼Œä½¿é‡æ’åºé˜¶æ®µä»…éœ€å¤„ç†æŸ¥è¯¢ä»¤ç‰Œï¼Œå®ç°5.4-44.8å€åŠ é€Ÿï¼Œå¹¶é€šè¿‡åˆ†å±‚é€‰æ‹©ç­–ç•¥åœ¨ä¿æŒ98.62%æ€§èƒ½çš„åŒæ—¶å‡å°‘75%è®¡ç®—é‡ï¼Œæ˜¾è‘—æå‡äº†RAGç³»ç»Ÿçš„æ•ˆç‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.06411v1">Structured Episodic Event Memory</a></td><td><details><summary>å±•å¼€</summary>Current approaches to memory in Large Language Models (LLMs) predominantly rely on static Retrieval-Augmented Generation (RAG), which often results in scattered retrieval and fails to capture the structural dependencies required for complex reasoning. For autonomous agents, these passive and flat architectures lack the cognitive organization necessary to model the dynamic and associative nature of long-term interaction. To address this, we propose Structured Episodic Event Memory (SEEM), a hierarchical framework that synergizes a graph memory layer for relational facts with a dynamic episodic memory layer for narrative progression. Grounded in cognitive frame theory, SEEM transforms interaction streams into structured Episodic Event Frames (EEFs) anchored by precise provenance pointers. Furthermore, we introduce an agentic associative fusion and Reverse Provenance Expansion (RPE) mechanism to reconstruct coherent narrative contexts from fragmented evidence. Experimental results on the LoCoMo and LongMemEval benchmarks demonstrate that SEEM significantly outperforms baselines, enabling agents to maintain superior narrative coherence and logical consistency.</details></td><td><details><summary>å±•å¼€</summary></details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.06282v1">Amory: Building Coherent Narrative-Driven Agent Memory through Agentic Reasoning</a></td><td><details><summary>å±•å¼€</summary>Long-term conversational agents face a fundamental scalability challenge as interactions extend over time: repeatedly processing entire conversation histories becomes computationally prohibitive. Current approaches attempt to solve this through memory frameworks that predominantly fragment conversations into isolated embeddings or graph representations and retrieve relevant ones in a RAG style. While computationally efficient, these methods often treat memory formation minimally and fail to capture the subtlety and coherence of human memory. We introduce Amory, a working memory framework that actively constructs structured memory representations through enhancing agentic reasoning during offline time. Amory organizes conversational fragments into episodic narratives, consolidates memories with momentum, and semanticizes peripheral facts into semantic memory. At retrieval time, the system employs coherence-driven reasoning over narrative structures. Evaluated on the LOCOMO benchmark for long-term reasoning, Amory achieves considerable improvements over previous state-of-the-art, with performance comparable to full context reasoning while reducing response time by 50%. Analysis shows that momentum-aware consolidation significantly enhances response quality, while coherence-driven retrieval provides superior memory coverage compared to embedding-based approaches.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAmoryçš„å·¥ä½œè®°å¿†æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é•¿æœŸå¯¹è¯ä»£ç†å› å¤„ç†å®Œæ•´å¯¹è¯å†å²å¸¦æ¥çš„è®¡ç®—æ•ˆç‡é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ç¦»çº¿å¢å¼ºä»£ç†æ¨ç†ï¼Œä¸»åŠ¨æ„å»ºç»“æ„åŒ–è®°å¿†ï¼ˆå¦‚å°†å¯¹è¯ç‰‡æ®µç»„ç»‡ä¸ºæƒ…æ™¯å™äº‹ã€è¿›è¡ŒåŠ¨é‡æ•´åˆè®°å¿†ã€å°†è¾¹ç¼˜äº‹å®è¯­ä¹‰åŒ–ï¼‰ï¼Œå¹¶åœ¨æ£€ç´¢æ—¶é‡‡ç”¨åŸºäºå™äº‹ç»“æ„çš„è¿è´¯æ€§é©±åŠ¨æ¨ç†ã€‚åœ¨LOCOMOåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAmoryåœ¨ä¿æŒä¸å…¨ä¸Šä¸‹æ–‡æ¨ç†ç›¸è¿‘æ€§èƒ½çš„åŒæ—¶å¤§å¹…æå‡æ•ˆç‡ï¼Œå…¶åŸºäºè¿è´¯æ€§çš„æ£€ç´¢æ¯”åµŒå…¥æ–¹æ³•å…·æœ‰æ›´ä¼˜çš„è®°å¿†è¦†ç›–èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.05866v2">FACTUM: Mechanistic Detection of Citation Hallucination in Long-Form RAG</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) models are critically undermined by citation hallucinations, a deceptive failure where a model cites a source that fails to support its claim. While existing work attributes hallucination to a simple over-reliance on parametric knowledge, we reframe this failure as an evolving, scale-dependent coordination failure between the Attention (reading) and Feed-Forward Network (recalling) pathways. We introduce FACTUM (Framework for Attesting Citation Trustworthiness via Underlying Mechanisms), a framework of four mechanistic scores: Contextual Alignment (CAS), Attention Sink Usage (BAS), Parametric Force (PFS), and Pathway Alignment (PAS). Our analysis reveals that correct citations are consistently marked by higher parametric force (PFS) and greater use of the attention sink (BAS) for information synthesis. Crucially, we find that "one-size-fits-all" theories are insufficient as the signature of correctness evolves with scale: while the 3B model relies on high pathway alignment (PAS), our best-performing 8B detector identifies a shift toward a specialized strategy where pathways provide distinct, orthogonal information. By capturing this complex interplay, FACTUM outperforms state-of-the-art baselines by up to 37.5% in AUC. Our results demonstrate that high parametric force is constructive when successfully coordinated with the Attention pathway, paving the way for more nuanced and reliable RAG systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†RAGæ¨¡å‹ä¸­çš„â€œå¼•ç”¨å¹»è§‰â€é—®é¢˜ï¼Œå³æ¨¡å‹é”™è¯¯å¼•ç”¨ä¸æ”¯æŒçš„æ¥æºï¼Œå¹¶æå‡ºFACTUMæ¡†æ¶ï¼Œé€šè¿‡å››ä¸ªæœºåˆ¶æ€§è¯„åˆ†ï¼ˆCASã€BASã€PFSã€PASï¼‰åˆ†ææ³¨æ„åŠ›ä¸è®°å¿†è·¯å¾„çš„åè°ƒæ€§ï¼Œå‘ç°æ­£ç¡®å¼•ç”¨çš„ç‰¹å¾éšæ¨¡å‹è§„æ¨¡å˜åŒ–ï¼Œæœ€ç»ˆFACTUMåœ¨æ£€æµ‹æ€§èƒ½ä¸Šè¶…è¶Šç°æœ‰åŸºçº¿37.5%ï¼Œä¸ºæ›´å¯é çš„RAGç³»ç»Ÿæä¾›äº†æ–°æ€è·¯ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.06235v1">An Intelligent AI glasses System with Multi-Agent Architecture for Real-Time Voice Processing and Task Execution</a></td><td><details><summary>å±•å¼€</summary>This paper presents an AI glasses system that integrates real-time voice processing, artificial intelligence(AI) agents, and cross-network streaming capabilities. The system employs dual-agent architecture where Agent 01 handles Automatic Speech Recognition (ASR) and Agent 02 manages AI processing through local Large Language Models (LLMs), Model Context Protocol (MCP) tools, and Retrieval-Augmented Generation (RAG). The system supports real-time RTSP streaming for voice and video data transmission, eye tracking data collection, and remote task execution through RabbitMQ messaging. Implementation demonstrates successful voice command processing with multilingual support and cross-platform task execution capabilities.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§é›†æˆå®æ—¶è¯­éŸ³å¤„ç†ã€AIä»£ç†å’Œè·¨ç½‘ç»œæµåª’ä½“åŠŸèƒ½çš„æ™ºèƒ½çœ¼é•œç³»ç»Ÿï¼Œé‡‡ç”¨åŒä»£ç†æ¶æ„ï¼Œå…¶ä¸­Agent 02åˆ©ç”¨æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰å·¥å…·å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯è¿›è¡ŒAIå¤„ç†ï¼Œæ”¯æŒå¤šè¯­è¨€è¯­éŸ³å‘½ä»¤å’Œè·¨å¹³å°ä»»åŠ¡æ‰§è¡Œã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.05648v1">Open World Knowledge Aided Single-Cell Foundation Model with Robust Cross-Modal Cell-Language Pre-training</a></td><td><details><summary>å±•å¼€</summary>Recent advancements in single-cell multi-omics, particularly RNA-seq, have provided profound insights into cellular heterogeneity and gene regulation. While pre-trained language model (PLM) paradigm based single-cell foundation models have shown promise, they remain constrained by insufficient integration of in-depth individual profiles and neglecting the influence of noise within multi-modal data. To address both issues, we propose an Open-world Language Knowledge-Aided Robust Single-Cell Foundation Model (OKR-CELL). It is built based on a cross-modal Cell-Language pre-training framework, which comprises two key innovations: (1) leveraging Large Language Models (LLMs) based workflow with retrieval-augmented generation (RAG) enriches cell textual descriptions using open-world knowledge; (2) devising a Cross-modal Robust Alignment (CRA) objective that incorporates sample reliability assessment, curriculum learning, and coupled momentum contrastive learning to strengthen the model's resistance to noisy data. After pretraining on 32M cell-text pairs, OKR-CELL obtains cutting-edge results across 6 evaluation tasks. Beyond standard benchmarks such as cell clustering, cell-type annotation, batch-effect correction, and few-shot annotation, the model also demonstrates superior performance in broader multi-modal applications, including zero-shot cell-type annotation and bidirectional cell-text retrieval.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºOKR-CELLçš„å¼€æ”¾ä¸–ç•Œè¯­è¨€çŸ¥è¯†è¾…åŠ©é²æ£’å•ç»†èƒåŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡è·¨æ¨¡æ€ç»†èƒ-è¯­è¨€é¢„è®­ç»ƒæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯æ¥ä¸°å¯Œç»†èƒæ–‡æœ¬æè¿°ï¼Œå¹¶ç»“åˆè·¨æ¨¡æ€é²æ£’å¯¹é½ï¼ˆCRAï¼‰ç›®æ ‡æ¥æé«˜æ¨¡å‹å¯¹å™ªå£°æ•°æ®çš„æŠµæŠ—åŠ›ã€‚è¯¥æ¨¡å‹åœ¨å¤šä¸ªè¯„ä¼°ä»»åŠ¡ä¸­å–å¾—äº†å…ˆè¿›çš„ç»“æœï¼ŒåŒ…æ‹¬ç»†èƒèšç±»ã€ç»†èƒç±»å‹æ³¨é‡Šã€æ‰¹æ¬¡æ•ˆåº”æ ¡æ­£ç­‰ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.05549v1">Efficient Temporal-aware Matryoshka Adaptation for Temporal Information Retrieval</a></td><td><details><summary>å±•å¼€</summary>Retrievers are a key bottleneck in Temporal Retrieval-Augmented Generation (RAG) systems: failing to retrieve temporally relevant context can degrade downstream generation, regardless of LLM reasoning. We propose Temporal-aware Matryoshka Representation Learning (TMRL), an efficient method that equips retrievers with temporal-aware Matryoshka embeddings. TMRL leverages the nested structure of Matryoshka embeddings to introduce a temporal subspace, enhancing temporal encoding while preserving general semantic representations. Experiments show that TMRL efficiently adapts diverse text embedding models, achieving competitive temporal retrieval and temporal RAG performance compared to prior Matryoshka-based non-temporal methods and prior temporal methods, while enabling flexible accuracy-efficiency trade-offs.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTMRLï¼ˆTemporal-aware Matryoshka Representation Learningï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ—¶é—´æ•æ„Ÿçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆTemporal RAGï¼‰ç³»ç»Ÿä¸­æ£€ç´¢å™¨çš„å…³é”®ç“¶é¢ˆé—®é¢˜ã€‚é€šè¿‡å¼•å…¥æ—¶é—´æ„ŸçŸ¥çš„MatryoshkaåµŒå…¥ï¼ŒTMRLåœ¨ä¿æŒé€šç”¨è¯­ä¹‰è¡¨å¾çš„åŒæ—¶å¢å¼ºäº†å¯¹æ—¶é—´ä¿¡æ¯çš„ç¼–ç ï¼Œä»è€Œæå‡äº†æ—¶é—´ç›¸å…³æ£€ç´¢å’Œç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ï¼Œå¹¶åœ¨å®éªŒä¸­å±•ç°äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„çµæ´»æ€§ä¸æ•ˆç‡æƒè¡¡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.05495v1">MMViR: A Multi-Modal and Multi-Granularity Representation for Long-range Video Understanding</a></td><td><details><summary>å±•å¼€</summary>Long videos, ranging from minutes to hours, present significant challenges for current Multi-modal Large Language Models (MLLMs) due to their complex events, diverse scenes, and long-range dependencies. Direct encoding of such videos is computationally too expensive, while simple video-to-text conversion often results in redundant or fragmented content. To address these limitations, we introduce MMViR, a novel multi-modal, multi-grained structured representation for long video understanding. MMViR identifies key turning points to segment the video and constructs a three-level description that couples global narratives with fine-grained visual details. This design supports efficient query-based retrieval and generalizes well across various scenarios. Extensive evaluations across three tasks, including QA, summarization, and retrieval, show that MMViR outperforms the prior strongest method, achieving a 19.67% improvement in hour-long video understanding while reducing processing latency to 45.4% of the original.</details></td><td><details><summary>å±•å¼€</summary></details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.05465v1">PRISMA: Reinforcement Learning Guided Two-Stage Policy Optimization in Multi-Agent Architecture for Open-Domain Multi-Hop Question Answering</a></td><td><details><summary>å±•å¼€</summary>Answering real-world open-domain multi-hop questions over massive corpora is a critical challenge in Retrieval-Augmented Generation (RAG) systems. Recent research employs reinforcement learning (RL) to end-to-end optimize the retrieval-augmented reasoning process, directly enhancing its capacity to resolve complex queries. However, reliable deployment is hindered by two obstacles. 1) Retrieval Collapse: iterative retrieval over large corpora fails to locate intermediate evidence containing bridge answers without reasoning-guided planning, causing downstream reasoning to collapse. 2) Learning Instability: end-to-end trajectory training suffers from weak credit assignment across reasoning chains and poor error localization across modules, causing overfitting to benchmark-specific heuristics that limit transferability and stability. To address these problems, we propose PRISMA, a decoupled RL-guided framework featuring a Plan-Retrieve-Inspect-Solve-Memoize architecture. PRISMA's strength lies in reasoning-guided collaboration: the Inspector provides reasoning-based feedback to refine the Planner's decomposition and fine-grained retrieval, while enforcing evidence-grounded reasoning in the Solver. We optimize individual agent capabilities via Two-Stage Group Relative Policy Optimization (GRPO). Stage I calibrates the Planner and Solver as specialized experts in planning and reasoning, while Stage II utilizes Observation-Aware Residual Policy Optimization (OARPO) to enhance the Inspector's ability to verify context and trigger targeted recovery. Experiments show that PRISMA achieves state-of-the-art performance on ten benchmarks and can be deployed efficiently in real-world scenarios.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºPRISMAçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³RAGç³»ç»Ÿä¸­å¤šè·³é—®ç­”ä»»åŠ¡ä¸­çš„æ£€ç´¢å´©æºƒå’Œå­¦ä¹ ä¸ç¨³å®šé—®é¢˜ã€‚PRISMAé‡‡ç”¨è§£è€¦çš„å¼ºåŒ–å­¦ä¹ å¼•å¯¼æ¶æ„ï¼ˆPlan-Retrieve-Inspect-Solve-Memoizeï¼‰ï¼Œé€šè¿‡æ¨ç†å¼•å¯¼çš„åä½œæœºåˆ¶ä¼˜åŒ–æ£€ç´¢å’Œæ¨ç†è¿‡ç¨‹ï¼ŒåŒ…æ‹¬è§„åˆ’å™¨åˆ†è§£ã€ç»†ç²’åº¦æ£€ç´¢å’Œè¯æ®é©±åŠ¨çš„æ±‚è§£å™¨æ¨ç†ï¼Œå¹¶é€šè¿‡ä¸¤é˜¶æ®µç­–ç•¥ä¼˜åŒ–æå‡å„æ¨¡å—èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜å…¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€ä¼˜æ€§èƒ½ï¼Œå¹¶å…·å¤‡å®é™…éƒ¨ç½²æ•ˆç‡ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2026-01-08
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2601.05038v1">ArcAligner: Adaptive Recursive Aligner for Compressed Context Embeddings in RAG</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) helps LLMs stay accurate, but feeding long documents into a prompt makes the model slow and expensive. This has motivated context compression, ranging from token pruning and summarization to embedding-based compression. While researchers have tried ''compressing'' these documents into smaller summaries or mathematical embeddings, there is a catch: the more you compress the data, the more the LLM struggles to understand it. To address this challenge, we propose ArcAligner (Adaptive recursive context *Aligner*), a lightweight module integrated into the language model layers to help the model better utilize highly compressed context representations for downstream generation. It uses an adaptive ''gating'' system that only adds extra processing power when the information is complex, keeping the system fast. Across knowledge-intensive QA benchmarks, ArcAligner consistently beats compression baselines at comparable compression rates, especially on multi-hop and long-tail settings. The source code is publicly available.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºArcAlignerçš„è½»é‡çº§æ¨¡å—ï¼Œæ—¨åœ¨è§£å†³RAGä¸­å› é«˜åº¦å‹ç¼©ä¸Šä¸‹æ–‡ä¿¡æ¯å¯¼è‡´å¤§è¯­è¨€æ¨¡å‹ç†è§£èƒ½åŠ›ä¸‹é™çš„é—®é¢˜ã€‚è¯¥æ¨¡å—é€šè¿‡è‡ªé€‚åº”é—¨æ§ç³»ç»ŸåŠ¨æ€è°ƒæ•´å¤„ç†å¤æ‚åº¦ï¼Œåœ¨ä¿æŒé«˜æ•ˆæ€§çš„åŒæ—¶æå‡æ¨¡å‹å¯¹å‹ç¼©åä¸Šä¸‹æ–‡çš„åˆ©ç”¨ç‡ï¼Œå¹¶åœ¨çŸ¥è¯†å¯†é›†å‹é—®ç­”ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰å‹ç¼©æ–¹æ³•ï¼Œå°¤å…¶åœ¨å¤šè·³å’Œé•¿å°¾åœºæ™¯ä¸‹è¡¨ç°çªå‡ºã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.05027v1">OptiSet: Unified Optimizing Set Selection and Ranking for Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) improves generation quality by incorporating evidence retrieved from large external corpora. However, most existing methods rely on statically selecting top-k passages based on individual relevance, which fails to exploit combinatorial gains among passages and often introduces substantial redundancy. To address this limitation, we propose OptiSet, a set-centric framework that unifies set selection and set-level ranking for RAG. OptiSet adopts an "Expand-then-Refine" paradigm: it first expands a query into multiple perspectives to enable a diverse candidate pool and then refines the candidate pool via re-selection to form a compact evidence set. We then devise a self-synthesis strategy without strong LLM supervision to derive preference labels from the set conditional utility changes of the generator, thereby identifying complementary and redundant evidence. Finally, we introduce a set-list wise training strategy that jointly optimizes set selection and set-level ranking, enabling the model to favor compact, high-gain evidence sets. Extensive experiments demonstrate that OptiSet improves performance on complex combinatorial problems and makes generation more efficient. The source code is publicly available.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºOptiSetæ¡†æ¶ï¼Œé€šè¿‡â€œæ‰©å±•-ç²¾ç‚¼â€ç­–ç•¥åŠ¨æ€ä¼˜åŒ–RAGä¸­çš„è¯æ®é›†é€‰æ‹©ï¼Œåˆ©ç”¨å¤šè§†è§’æŸ¥è¯¢æ‰©å±•å’Œè‡ªåˆæˆåå¥½æ ‡ç­¾å‡å°‘å†—ä½™ï¼Œå¹¶è”åˆè®­ç»ƒé›†åˆé€‰æ‹©ä¸æ’åºæ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆæ•ˆç‡ä¸ç»„åˆæ€§é—®é¢˜å¤„ç†èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.04945v1">T-Retriever: Tree-based Hierarchical Retrieval Augmented Generation for Textual Graphs</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has significantly enhanced Large Language Models' ability to access external knowledge, yet current graph-based RAG approaches face two critical limitations in managing hierarchical information: they impose rigid layer-specific compression quotas that damage local graph structures, and they prioritize topological structure while neglecting semantic content. We introduce T-Retriever, a novel framework that reformulates attributed graph retrieval as tree-based retrieval using a semantic and structure-guided encoding tree. Our approach features two key innovations: (1) Adaptive Compression Encoding, which replaces artificial compression quotas with a global optimization strategy that preserves the graph's natural hierarchical organization, and (2) Semantic-Structural Entropy ($S^2$-Entropy), which jointly optimizes for both structural cohesion and semantic consistency when creating hierarchical partitions. Experiments across diverse graph reasoning benchmarks demonstrate that T-Retriever significantly outperforms state-of-the-art RAG methods, providing more coherent and contextually relevant responses to complex queries.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºT-Retrieverçš„æ–°å‹RAGæ¡†æ¶ï¼Œé€šè¿‡æ ‘å½¢æ£€ç´¢å’Œè¯­ä¹‰ç»“æ„ä¼˜åŒ–è§£å†³äº†å½“å‰åŸºäºå›¾çš„RAGæ–¹æ³•åœ¨åˆ†å±‚ä¿¡æ¯å¤„ç†ä¸­çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬è‡ªé€‚åº”å‹ç¼©ç¼–ç å’Œè¯­ä¹‰ç»“æ„ç†µä¼˜åŒ–ï¼Œæ˜¾è‘—æå‡äº†å¤æ‚æŸ¥è¯¢çš„å“åº”è´¨é‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.04859v1">A Navigational Approach for Comprehensive RAG via Traversal over Proposition Graphs</a></td><td><details><summary>å±•å¼€</summary>Standard RAG pipelines based on chunking excel at simple factual retrieval but fail on complex multi-hop queries due to a lack of structural connectivity. Conversely, initial strategies that interleave retrieval with reasoning often lack global corpus awareness, while Knowledge Graph (KG)-based RAG performs strongly on complex multi-hop tasks but suffers on fact-oriented single-hop queries. To bridge this gap, we propose a novel RAG framework: ToPG (Traversal over Proposition Graphs). ToPG models its knowledge base as a heterogeneous graph of propositions, entities, and passages, effectively combining the granular fact density of propositions with graph connectivity. We leverage this structure using iterative Suggestion-Selection cycles, where the Suggestion phase enables a query-aware traversal of the graph, and the Selection phase provides LLM feedback to prune irrelevant propositions and seed the next iteration. Evaluated on three distinct QA tasks (Simple, Complex, and Abstract QA), ToPG demonstrates strong performance across both accuracy- and quality-based metrics. Overall, ToPG shows that query-aware graph traversal combined with factual granularity is a critical component for efficient structured RAG systems. ToPG is available at https://github.com/idiap/ToPG.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºToPGçš„æ–°å‹RAGæ¡†æ¶ï¼Œé€šè¿‡å°†çŸ¥è¯†åº“å»ºæ¨¡ä¸ºå‘½é¢˜ã€å®ä½“å’Œæ®µè½çš„å¼‚æ„å›¾ï¼Œç»“åˆå‘½é¢˜çš„ç»†ç²’åº¦äº‹å®å¯†åº¦å’Œå›¾è¿é€šæ€§ã€‚è¯¥æ¡†æ¶é‡‡ç”¨è¿­ä»£çš„å»ºè®®-é€‰æ‹©å¾ªç¯è¿›è¡Œæ£€ç´¢å’Œæ¨ç†ï¼Œåœ¨ç®€å•ã€å¤æ‚å’ŒæŠ½è±¡é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†æŸ¥è¯¢æ„ŸçŸ¥å›¾éå†ä¸äº‹å®ç²’åº¦ç»“åˆå¯¹é«˜æ•ˆç»“æ„åŒ–RAGç³»ç»Ÿçš„é‡è¦æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.04853v1">RAAR: Retrieval Augmented Agentic Reasoning for Cross-Domain Misinformation Detection</a></td><td><details><summary>å±•å¼€</summary>Cross-domain misinformation detection is challenging, as misinformation arises across domains with substantial differences in knowledge and discourse. Existing methods often rely on single-perspective cues and struggle to generalize to challenging or underrepresented domains, while reasoning large language models (LLMs), though effective on complex tasks, are limited to same-distribution data. To address these gaps, we introduce RAAR, the first retrieval-augmented agentic reasoning framework for cross-domain misinformation detection. To enable cross-domain transfer beyond same-distribution assumptions, RAAR retrieves multi-perspective source-domain evidence aligned with each target sample's semantics, sentiment, and writing style. To overcome single-perspective modeling and missing systematic reasoning, RAAR constructs verifiable multi-step reasoning paths through specialized multi-agent collaboration, where perspective-specific agents produce complementary analyses and a summary agent integrates them under verifier guidance. RAAR further applies supervised fine-tuning and reinforcement learning to train a single multi-task verifier to enhance verification and reasoning capabilities. Based on RAAR, we trained the RAAR-8b and RAAR-14b models. Evaluation on three cross-domain misinformation detection tasks shows that RAAR substantially enhances the capabilities of the base models and outperforms other cross-domain methods, advanced LLMs, and LLM-based adaptation approaches. The project will be released at https://github.com/lzw108/RAAR.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†RAARï¼Œä¸€ä¸ªåŸºäºæ£€ç´¢å¢å¼ºçš„ä»£ç†æ¨ç†æ¡†æ¶ï¼Œç”¨äºè·¨é¢†åŸŸé”™è¯¯ä¿¡æ¯æ£€æµ‹ã€‚RAARé€šè¿‡æ£€ç´¢ä¸ç›®æ ‡æ ·æœ¬è¯­ä¹‰ã€æƒ…æ„Ÿå’Œå†™ä½œé£æ ¼ä¸€è‡´çš„å¤šè§†è§’æºé¢†åŸŸè¯æ®ï¼Œå¹¶ç»“åˆå¤šä»£ç†åä½œæ„å»ºå¯éªŒè¯çš„å¤šæ­¥æ¨ç†è·¯å¾„ï¼Œä»¥å…‹æœå•è§†è§’å»ºæ¨¡å’Œç³»ç»Ÿæ€§æ¨ç†ä¸è¶³çš„é—®é¢˜ã€‚æ­¤å¤–ï¼ŒRAARåˆ©ç”¨ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤šä»»åŠ¡éªŒè¯å™¨ï¼Œæœ€ç»ˆåœ¨è·¨é¢†åŸŸé”™è¯¯ä¿¡æ¯æ£€æµ‹ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•å’Œå…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.04764v1">Orion-RAG: Path-Aligned Hybrid Retrieval for Graphless Data</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has proven effective for knowledge synthesis, yet it encounters significant challenges in practical scenarios where data is inherently discrete and fragmented. In most environments, information is distributed across isolated files like reports and logs that lack explicit links. Standard search engines process files independently, ignoring the connections between them. Furthermore, manually building Knowledge Graphs is impractical for such vast data. To bridge this gap, we present Orion-RAG. Our core insight is simple yet effective: we do not need heavy algorithms to organize this data. Instead, we use a low-complexity strategy to extract lightweight paths that naturally link related concepts. We demonstrate that this streamlined approach suffices to transform fragmented documents into semi-structured data, enabling the system to link information across different files effectively. Extensive experiments demonstrate that Orion-RAG consistently outperforms mainstream frameworks across diverse domains, supporting real-time updates and explicit Human-in-the-Loop verification with high cost-efficiency. Experiments on FinanceBench demonstrate superior precision with a 25.2% relative improvement over strong baselines.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†Orion-RAGæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸRAGåœ¨å¤„ç†ç¦»æ•£ã€ç¢ç‰‡åŒ–æ•°æ®æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚é€šè¿‡ä½å¤æ‚åº¦ç­–ç•¥æå–è½»é‡çº§è·¯å¾„å…³è”ç›¸å…³æ¦‚å¿µï¼Œå°†ç¢ç‰‡æ–‡æ¡£è½¬åŒ–ä¸ºåŠç»“æ„åŒ–æ•°æ®ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨å¤šä¸ªé¢†åŸŸä¼˜äºä¸»æµæ¡†æ¶ï¼Œå¹¶æ”¯æŒå®æ—¶æ›´æ–°å’Œäººå·¥éªŒè¯ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.04745v1">KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions</a></td><td><details><summary>å±•å¼€</summary>Existing long-horizon memory benchmarks mostly use multi-turn dialogues or synthetic user histories, which makes retrieval performance an imperfect proxy for person understanding. We present \BenchName, a publicly releasable benchmark built from long-form autobiographical narratives, where actions, context, and inner thoughts provide dense evidence for inferring stable motivations and decision principles. \BenchName~reconstructs each narrative into a flashback-aware, time-anchored stream and evaluates models with evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning. Across diverse narrative sources, retrieval-augmented systems mainly improve factual accuracy, while errors persist on temporally grounded explanations and higher-level inferences, highlighting the need for memory mechanisms beyond retrieval. Our data is in \href{KnowMeBench}{https://github.com/QuantaAlpha/KnowMeBench}.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†KnowMeBenchï¼Œä¸€ä¸ªåŸºäºè‡ªä¼ ä½“å™äº‹æ„å»ºçš„é•¿æœŸè®°å¿†åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿè¯„ä¼°æ¨¡å‹åœ¨äº‹å®å›å¿†ã€ä¸»è§‚çŠ¶æ€å½’å› å’ŒåŸåˆ™çº§æ¨ç†ç­‰æ–¹é¢çš„æ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶RAGç³»ç»Ÿæé«˜äº†äº‹å®å‡†ç¡®æ€§ï¼Œä½†åœ¨æ—¶é—´é”šå®šçš„è§£é‡Šå’Œé«˜çº§æ¨ç†æ–¹é¢ä»å­˜åœ¨ä¸è¶³ï¼Œå¼ºè°ƒäº†éœ€è¦è¶…è¶Šæ£€ç´¢çš„è®°å¿†æœºåˆ¶ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.04742v1">Tool-MAD: A Multi-Agent Debate Framework for Fact Verification with Diverse Tool Augmentation and Adaptive Retrieval</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) suffer from hallucinations and factual inaccuracies, especially in complex reasoning and fact verification tasks. Multi-Agent Debate (MAD) systems aim to improve answer accuracy by enabling multiple LLM agents to engage in dialogue, promoting diverse reasoning and mutual verification. However, existing MAD frameworks primarily rely on internal knowledge or static documents, making them vulnerable to hallucinations. While MADKE introduces external evidence to mitigate this, its one-time retrieval mechanism limits adaptability to new arguments or emerging information during the debate. To address these limitations, We propose Tool-MAD, a multi-agent debate framework that enhances factual verification by assigning each agent a distinct external tool, such as a search API or RAG module. Tool-MAD introduces three key innovations: (1) a multi-agent debate framework where agents leverage heterogeneous external tools, encouraging diverse perspectives, (2) an adaptive query formulation mechanism that iteratively refines evidence retrieval based on the flow of the debate, and (3) the integration of Faithfulness and Answer Relevance scores into the final decision process, allowing the Judge agent to quantitatively assess the coherence and question alignment of each response and effectively detect hallucinations. Experimental results on four fact verification benchmarks demonstrate that Tool-MAD consistently outperforms state-of-the-art MAD frameworks, achieving up to 5.5% accuracy improvement. Furthermore, in medically specialized domains, Tool-MAD exhibits strong robustness and adaptability across various tool configurations and domain conditions, confirming its potential for broader real-world fact-checking applications.</details></td><td><details><summary>å±•å¼€</summary>  </details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.04709v1">Bridging Temporal and Textual Modalities: A Multimodal Framework for Automated Cloud Failure Root Cause Analysis</a></td><td><details><summary>å±•å¼€</summary>Root cause analysis in modern cloud infrastructure demands sophisticated understanding of heterogeneous data sources, particularly time-series performance metrics that involve core failure signatures. While large language models demonstrate remarkable capabilities in textual reasoning, their discrete token-based architecture creates fundamental incompatibilities with continuous numerical sequences exhibiting temporal dependencies. Current methodologies inadequately address this modality mismatch, constraining the potential of language model-driven automation in incident management workflows. This paper presents a multimodal diagnostic framework that harmonizes time-series representations with pretrained language model embedding spaces. Our approach contributes three technical advances: (1) a semantic compression technique that distills temporal segments into single-token abstractions while preserving pattern semantics, (2) an alignment encoder utilizing gated cross-attention to project time-series features into language model latent space, and (3) a retrieval-augmented diagnostic pipeline that synthesizes aligned embeddings with historical incident knowledge for expert-level failure attribution. Comprehensive evaluation across six cloud system benchmarks demonstrates that our framework achieves leading performance, reaching 48.75% diagnostic accuracy with notable improvements on scenarios involving compound failure modes. The results validate embedding-space alignment as an effective strategy for enabling language models to reason over multimodal telemetry data in production incident response contexts.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€è¯Šæ–­æ¡†æ¶ï¼Œé€šè¿‡è¯­ä¹‰å‹ç¼©æŠ€æœ¯ã€å¯¹é½ç¼–ç å™¨å’Œæ£€ç´¢å¢å¼ºçš„è¯Šæ–­æµç¨‹ï¼Œå°†æ—¶é—´åºåˆ—æ•°æ®ä¸é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„åµŒå…¥ç©ºé—´å¯¹é½ï¼Œå¹¶ç»“åˆå†å²äº‹ä»¶çŸ¥è¯†è¿›è¡Œæ•…éšœå½’å› ï¼Œæ˜¾è‘—æå‡äº†äº‘ç³»ç»Ÿæ•…éšœè¯Šæ–­çš„å‡†ç¡®æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.04653v1">Vibe Coding an LLM-powered Theorem Prover</a></td><td><details><summary>å±•å¼€</summary>We present Isabellm, an LLM-powered theorem prover for Isabelle/HOL that performs fully automatic proof synthesis. Isabellm works with any local LLM on Ollama and APIs such as Gemini CLI, and it is designed to run on consumer grade computers. The system combines a stepwise prover, which uses large language models to propose proof commands validated by Isabelle in a bounded search loop, with a higher-level proof planner that generates structured Isar outlines and attempts to fill and repair remaining gaps. The framework includes beam search for tactics, tactics reranker ML and RL models, premise selection with small transformer models, micro-RAG for Isar proofs built from AFP, and counter-example guided proof repair. All the code is implemented by GPT 4.1 - 5.2, Gemini 3 Pro, and Claude 4.5. Empirically, Isabellm can prove certain lemmas that defeat Isabelle's standard automation, including Sledgehammer, demonstrating the practical value of LLM-guided proof search. At the same time, we find that even state-of-the-art LLMs, such as GPT 5.2 Extended Thinking and Gemini 3 Pro struggle to reliably implement the intended fill-and-repair mechanisms with complex algorithmic designs, highlighting fundamental challenges in LLM code generation and reasoning. The code of Isabellm is available at https://github.com/zhehou/llm-isabelle</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ä»‹ç»äº†Isabellmï¼Œä¸€ä¸ªåŸºäºLLMçš„Isabelle/HOLå®šç†è¯æ˜å™¨ï¼Œé€šè¿‡ç»“åˆåˆ†æ­¥è¯æ˜å™¨å’Œé«˜å±‚æ¬¡è¯æ˜è§„åˆ’å™¨å®ç°è‡ªåŠ¨è¯æ˜åˆæˆï¼Œå¹¶é‡‡ç”¨å¾®RAGæŠ€æœ¯ä»AFPæ„å»ºIsarè¯æ˜ï¼ŒåŒæ—¶æ•´åˆäº†å¤šç§æ£€ç´¢ä¸é‡æ’æœºåˆ¶ï¼Œæå‡LLMåœ¨å½¢å¼åŒ–è¯æ˜ä¸­çš„è¡¨ç°ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.04651v1">Adversarial Yet Cooperative: Multi-Perspective Reasoning in Retrieved-Augmented Language Models</a></td><td><details><summary>å±•å¼€</summary>Recent advances in synergizing large reasoning models (LRMs) with retrieval-augmented generation (RAG) have shown promising results, yet two critical challenges remain: (1) reasoning models typically operate from a single, unchallenged perspective, limiting their ability to conduct deep, self-correcting reasoning over external documents, and (2) existing training paradigms rely excessively on outcome-oriented rewards, which provide insufficient signal for shaping the complex, multi-step reasoning process. To address these issues, we propose an Reasoner-Verifier framework named Adversarial Reasoning RAG (ARR). The Reasoner and Verifier engage in reasoning on retrieved evidence and critiquing each other's logic while being guided by process-aware advantage that requires no external scoring model. This reward combines explicit observational signals with internal model uncertainty to jointly optimize reasoning fidelity and verification rigor. Experiments on multiple benchmarks demonstrate the effectiveness of our method.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºAdversarial Reasoning RAG (ARR)çš„Reasoner-Verifieræ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰RAGç³»ç»Ÿä¸­æ¨ç†æ¨¡å‹è§†è§’å•ä¸€å’Œè®­ç»ƒèŒƒå¼è¿‡åº¦ä¾èµ–ç»“æœå¯¼å‘å¥–åŠ±çš„é—®é¢˜ã€‚é€šè¿‡è®©Reasonerå’ŒVerifieråœ¨æ£€ç´¢è¯æ®ä¸Šè¿›è¡Œç›¸äº’æ‰¹åˆ¤æ€§æ¨ç†ï¼Œå¹¶åˆ©ç”¨è¿‡ç¨‹æ„ŸçŸ¥ä¼˜åŠ¿è¿›è¡Œä¼˜åŒ–ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.04568v1">Neurosymbolic Retrievers for Retrieval-augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval Augmented Generation (RAG) has made significant strides in overcoming key limitations of large language models, such as hallucination, lack of contextual grounding, and issues with transparency. However, traditional RAG systems consist of three interconnected neural components - the retriever, re-ranker, and generator - whose internal reasoning processes remain opaque. This lack of transparency complicates interpretability, hinders debugging efforts, and erodes trust, especially in high-stakes domains where clear decision-making is essential. To address these challenges, we introduce the concept of Neurosymbolic RAG, which integrates symbolic reasoning using a knowledge graph with neural retrieval techniques. This new framework aims to answer two primary questions: (a) Can retrievers provide a clear and interpretable basis for document selection? (b) Can symbolic knowledge enhance the clarity of the retrieval process? We propose three methods to improve this integration. First is MAR (Knowledge Modulation Aligned Retrieval) that employs modulation networks to refine query embeddings using interpretable symbolic features, thereby making document matching more explicit. Second, KG-Path RAG enhances queries by traversing knowledge graphs to improve overall retrieval quality and interpretability. Lastly, Process Knowledge-infused RAG utilizes domain-specific tools to reorder retrieved content based on validated workflows. Preliminary results from mental health risk assessment tasks indicate that this neurosymbolic approach enhances both transparency and overall performance</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸º"Neurosymbolic RAG"çš„æ–°æ¡†æ¶ï¼Œé€šè¿‡å°†çŸ¥è¯†å›¾è°±çš„ç¬¦å·æ¨ç†ä¸ç¥ç»æ£€ç´¢æŠ€æœ¯ç›¸ç»“åˆï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸRAGç³»ç»Ÿåœ¨å¯è§£é‡Šæ€§ã€é€æ˜åº¦å’Œå†³ç­–è¿‡ç¨‹æ¸…æ™°åº¦æ–¹é¢çš„ä¸è¶³ã€‚è®ºæ–‡ä»‹ç»äº†ä¸‰ç§æ”¹è¿›æ–¹æ³•ï¼ˆMARã€KG-Path RAGå’ŒProcess Knowledge-infused RAGï¼‰ï¼Œå¹¶åœ¨å¿ƒç†å¥åº·é£é™©è¯„ä¼°ä»»åŠ¡ä¸­éªŒè¯äº†è¯¥æ¡†æ¶åœ¨æå‡é€æ˜åº¦å’Œæ€§èƒ½æ–¹é¢çš„æ•ˆæœã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.04531v1">Self-MedRAG: a Self-Reflective Hybrid Retrieval-Augmented Generation Framework for Reliable Medical Question Answering</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) have demonstrated significant potential in medical Question Answering (QA), yet they remain prone to hallucinations and ungrounded reasoning, limiting their reliability in high-stakes clinical scenarios. While Retrieval-Augmented Generation (RAG) mitigates these issues by incorporating external knowledge, conventional single-shot retrieval often fails to resolve complex biomedical queries requiring multi-step inference. To address this, we propose Self-MedRAG, a self-reflective hybrid framework designed to mimic the iterative hypothesis-verification process of clinical reasoning. Self-MedRAG integrates a hybrid retrieval strategy, combining sparse (BM25) and dense (Contriever) retrievers via Reciprocal Rank Fusion (RRF) to maximize evidence coverage. It employs a generator to produce answers with supporting rationales, which are then assessed by a lightweight self-reflection module using Natural Language Inference (NLI) or LLM-based verification. If the rationale lacks sufficient evidentiary support, the system autonomously reformulates the query and iterates to refine the context. We evaluated Self-MedRAG on the MedQA and PubMedQA benchmarks. The results demonstrate that our hybrid retrieval approach significantly outperforms single-retriever baselines. Furthermore, the inclusion of the self-reflective loop yielded substantial gains, increasing accuracy on MedQA from 80.00% to 83.33% and on PubMedQA from 69.10% to 79.82%. These findings confirm that integrating hybrid retrieval with iterative, evidence-based self-reflection effectively reduces unsupported claims and enhances the clinical reliability of LLM-based systems.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSelf-MedRAGçš„è‡ªæˆ‘åæ€æ··åˆæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç»“åˆç¨€ç–ï¼ˆBM25ï¼‰å’Œå¯†é›†ï¼ˆContrieverï¼‰æ£€ç´¢å™¨çš„æ··åˆæ£€ç´¢ç­–ç•¥ï¼Œä»¥åŠè¿­ä»£çš„è‡ªåæ€æ¨¡å—ï¼Œæ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»å­¦é—®ç­”ï¼ˆQAï¼‰ä¸­çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚è¯¥æ–¹æ³•åœ¨MedQAå’ŒPubMedQAåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—å‡å°‘äº†æœªç»éªŒè¯çš„ä¸»å¼ ï¼Œå¹¶æé«˜äº†ä¸´åºŠå¯é æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.04525v1">GRACE: Reinforcement Learning for Grounded Response and Abstention under Contextual Evidence</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) integrates external knowledge to enhance Large Language Models (LLMs), yet systems remain susceptible to two critical flaws: providing correct answers without explicit grounded evidence and producing fabricated responses when the retrieved context is insufficient. While prior research has addressed these issues independently, a unified framework that integrates evidence-based grounding and reliable abstention is currently lacking. In this paper, we propose GRACE, a reinforcement-learning framework that simultaneously mitigates both types of flaws. GRACE employs a data construction method that utilizes heterogeneous retrievers to generate diverse training samples without manual annotation. A multi-stage gated reward function is then employed to train the model to assess evidence sufficiency, extract key supporting evidence, and provide answers or explicitly abstain. Experimental results on two benchmarks demonstrate that GRACE achieves state-of-the-art overall accuracy and strikes a favorable balance between accurate response and rejection, while requiring only 10% of the annotation costs of prior methods. Our code is available at https://github.com/YiboZhao624/Grace..</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†GRACEæ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è§£å†³RAGç³»ç»Ÿä¸­å­˜åœ¨çš„ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼ˆæ— è¯æ®çš„æ­£ç¡®å›ç­”å’Œæ£€ç´¢ä¸è¶³æ—¶çš„è™šæ„å›ç­”ï¼‰ï¼Œæ•´åˆäº†åŸºäºè¯æ®çš„ grounding å’Œå¯é  abstention åŠŸèƒ½ï¼Œå¹¶åˆ©ç”¨å¼‚æ„æ£€ç´¢å™¨ç”Ÿæˆå¤šæ ·åŒ–è®­ç»ƒæ ·æœ¬ä»¥é™ä½æ ‡æ³¨æˆæœ¬ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨å‡†ç¡®æ€§å’Œæ‹’ç»å›ç­”çš„å¹³è¡¡ä¸Šè¾¾åˆ°æœ€ä¼˜æ€§èƒ½ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2026-01-07
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2601.03981v1">RADAR: Retrieval-Augmented Detector with Adversarial Refinement for Robust Fake News Detection</a></td><td><details><summary>å±•å¼€</summary>To efficiently combat the spread of LLM-generated misinformation, we present RADAR, a retrieval-augmented detector with adversarial refinement for robust fake news detection. Our approach employs a generator that rewrites real articles with factual perturbations, paired with a lightweight detector that verifies claims using dense passage retrieval. To enable effective co-evolution, we introduce verbal adversarial feedback (VAF). Rather than relying on scalar rewards, VAF issues structured natural-language critiques; these guide the generator toward more sophisticated evasion attempts, compelling the detector to adapt and improve. On a fake news detection benchmark, RADAR achieves 86.98% ROC-AUC, significantly outperforming general-purpose LLMs with retrieval. Ablation studies confirm that detector-side retrieval yields the largest gains, while VAF and few-shot demonstrations provide critical signals for robust training.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†RADARï¼Œä¸€ç§ç»“åˆæ£€ç´¢å¢å¼ºå’Œå¯¹æŠ—æ€§ä¼˜åŒ–çš„è™šå‡æ–°é—»æ£€æµ‹æ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆå™¨å¯¹çœŸå®æ–‡ç« è¿›è¡Œäº‹å®æ‰°åŠ¨æ”¹å†™ï¼Œå¹¶åˆ©ç”¨è½»é‡çº§æ£€æµ‹å™¨åŸºäºå¯†é›†æ®µè½æ£€ç´¢éªŒè¯å£°æ˜ï¼›åˆ›æ–°æ€§å¼•å…¥è‡ªç„¶è¯­è¨€å½¢å¼çš„å¯¹æŠ—åé¦ˆï¼ˆVAFï¼‰ä¿ƒè¿›ç”Ÿæˆä¸æ£€æµ‹çš„ååŒè¿›åŒ–ï¼Œå®éªŒæ˜¾ç¤ºå…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºé€šç”¨æ£€ç´¢å¢å¼ºLLMï¼Œå…³é”®æå‡æ¥è‡ªæ£€æµ‹ç«¯çš„æ£€ç´¢æœºåˆ¶å’ŒVAFæä¾›çš„é²æ£’è®­ç»ƒä¿¡å·ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.03979v1">SoK: Privacy Risks and Mitigations in Retrieval-Augmented Generation Systems</a></td><td><details><summary>å±•å¼€</summary>The continued promise of Large Language Models (LLMs), particularly in their natural language understanding and generation capabilities, has driven a rapidly increasing interest in identifying and developing LLM use cases. In an effort to complement the ingrained "knowledge" of LLMs, Retrieval-Augmented Generation (RAG) techniques have become widely popular. At its core, RAG involves the coupling of LLMs with domain-specific knowledge bases, whereby the generation of a response to a user question is augmented with contextual and up-to-date information. The proliferation of RAG has sparked concerns about data privacy, particularly with the inherent risks that arise when leveraging databases with potentially sensitive information. Numerous recent works have explored various aspects of privacy risks in RAG systems, from adversarial attacks to proposed mitigations. With the goal of surveying and unifying these works, we ask one simple question: What are the privacy risks in RAG, and how can they be measured and mitigated? To answer this question, we conduct a systematic literature review of RAG works addressing privacy, and we systematize our findings into a comprehensive set of privacy risks, mitigation techniques, and evaluation strategies. We supplement these findings with two primary artifacts: a Taxonomy of RAG Privacy Risks and a RAG Privacy Process Diagram. Our work contributes to the study of privacy in RAG not only by conducting the first systematization of risks and mitigations, but also by uncovering important considerations when mitigating privacy risks in RAG systems and assessing the current maturity of proposed mitigations.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ä¸­çš„æ•°æ®éšç§é£é™©ï¼Œé€šè¿‡ç³»ç»Ÿæ–‡çŒ®ç»¼è¿°æ¢³ç†äº†RAGç³»ç»Ÿä¸­çš„éšç§å¨èƒç±»å‹ã€ç¼“è§£æªæ–½åŠè¯„ä¼°ç­–ç•¥ï¼Œå¹¶æå‡ºäº†éšç§é£é™©åˆ†ç±»æ³•å’Œæµç¨‹æ¡†æ¶ï¼Œæ—¨åœ¨å¡«è¡¥è¯¥é¢†åŸŸç ”ç©¶çš„ç©ºç™½å¹¶æ¨åŠ¨éšç§ä¿æŠ¤çš„å®è·µå‘å±•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.03948v1">Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification</a></td><td><details><summary>å±•å¼€</summary>Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to achieve remarkable reasoning in domains like mathematics and coding, where verifiable rewards provide clear signals. However, extending this paradigm to financial decision is challenged by the market's stochastic nature: rewards are verifiable but inherently noisy, causing standard RL to degenerate into reward hacking. To address this, we propose Trade-R1, a model training framework that bridges verifiable rewards to stochastic environments via process-level reasoning verification. Our key innovation is a verification method that transforms the problem of evaluating reasoning over lengthy financial documents into a structured Retrieval-Augmented Generation (RAG) task. We construct a triangular consistency metric, assessing pairwise alignment between retrieved evidence, reasoning chains, and decisions to serve as a validity filter for noisy market returns. We explore two reward integration strategies: Fixed-effect Semantic Reward (FSR) for stable alignment signals, and Dynamic-effect Semantic Reward (DSR) for coupled magnitude optimization. Experiments on different country asset selection demonstrate that our paradigm reduces reward hacking, with DSR achieving superior cross-market generalization while maintaining the highest reasoning consistency.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†Trade-R1æ¡†æ¶ï¼Œé€šè¿‡è¿‡ç¨‹çº§æ¨ç†éªŒè¯å°†å¯éªŒè¯å¥–åŠ±ä¸éšæœºç¯å¢ƒï¼ˆå¦‚é‡‘èå¸‚åœºï¼‰ç»“åˆï¼Œå…¶æ ¸å¿ƒåˆ›æ–°æ˜¯å°†é‡‘èæ–‡æ¡£çš„æ¨ç†è¯„ä¼°è½¬åŒ–ä¸ºç»“æ„åŒ–RAGä»»åŠ¡ï¼Œåˆ©ç”¨æ£€ç´¢è¯æ®ã€æ¨ç†é“¾å’Œå†³ç­–çš„ä¸‰å…ƒä¸€è‡´æ€§ä½œä¸ºå™ªå£°å¸‚åœºå›æŠ¥çš„éªŒè¯è¿‡æ»¤å™¨ï¼Œå¹¶è®¾è®¡äº†ä¸¤ç§å¥–åŠ±æ•´åˆç­–ç•¥ï¼ˆFSRå’ŒDSRï¼‰æ¥ä¼˜åŒ–æ¨¡å‹è¡¨ç°ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.03908v1">Decide Then Retrieve: A Training-Free Framework with Uncertainty-Guided Triggering and Dual-Path Retrieval</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external knowledge, but existing approaches indiscriminately trigger retrieval and rely on single-path evidence construction, often introducing noise and limiting performance gains. In this work, we propose Decide Then Retrieve (DTR), a training-free framework that adaptively determines when retrieval is necessary and how external information should be selected. DTR leverages generation uncertainty to guide retrieval triggering and introduces a dual-path retrieval mechanism with adaptive information selection to better handle sparse and ambiguous queries. Extensive experiments across five open-domain QA benchmarks, multiple model scales, and different retrievers demonstrate that DTR consistently improves EM and F1 over standard RAG and strong retrieval-enhanced baselines, while reducing unnecessary retrievals. The code and data used in this paper are available at https://github.com/ChenWangHKU/DTR.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDecide Then Retrieve (DTR)çš„è®­ç»ƒå…è´¹æ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”åœ°å†³å®šä½•æ—¶è¿›è¡Œæ£€ç´¢ä»¥åŠå¦‚ä½•é€‰æ‹©å¤–éƒ¨ä¿¡æ¯ï¼Œè§£å†³äº†ç°æœ‰RAGæ–¹æ³•ä¸­ç›²ç›®è§¦å‘æ£€ç´¢å’Œå•ä¸€è·¯å¾„è¯æ®æ„å»ºçš„é—®é¢˜ã€‚DTRåˆ©ç”¨ç”Ÿæˆä¸ç¡®å®šæ€§æŒ‡å¯¼æ£€ç´¢è§¦å‘ï¼Œå¹¶å¼•å…¥åŒè·¯å¾„æ£€ç´¢æœºåˆ¶å’Œè‡ªé€‚åº”ä¿¡æ¯é€‰æ‹©ï¼Œä»¥æ›´å¥½åœ°å¤„ç†ç¨€ç–å’Œæ¨¡ç³ŠæŸ¥è¯¢ã€‚å®éªŒè¡¨æ˜ï¼ŒDTRåœ¨å¤šä¸ªå¼€æ”¾åŸŸQAåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºæ ‡å‡†RAGå’Œå…¶ä»–æ£€ç´¢å¢å¼ºåŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶å‡å°‘äº†ä¸å¿…è¦çš„æ£€ç´¢ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.03903v1">Unleashing the Potential of Neighbors: Diffusion-based Latent Neighbor Generation for Session-based Recommendation</a></td><td><details><summary>å±•å¼€</summary>Session-based recommendation aims to predict the next item that anonymous users may be interested in, based on their current session interactions. Recent studies have demonstrated that retrieving neighbor sessions to augment the current session can effectively alleviate the data sparsity issue and improve recommendation performance. However, existing methods typically rely on explicitly observed session data, neglecting latent neighbors - not directly observed but potentially relevant within the interest space - thereby failing to fully exploit the potential of neighbor sessions in recommendation. To address the above limitation, we propose a novel model of diffusion-based latent neighbor generation for session-based recommendation, named DiffSBR. Specifically, DiffSBR leverages two diffusion modules, including retrieval-augmented diffusion and self-augmented diffusion, to generate high-quality latent neighbors. In the retrieval-augmented diffusion module, we leverage retrieved neighbors as guiding signals to constrain and reconstruct the distribution of latent neighbors. Meanwhile, we adopt a training strategy that enables the retriever to learn from the feedback provided by the generator. In the self-augmented diffusion module, we explicitly guide the generation of latent neighbors by injecting the current session's multi-modal signals through contrastive learning. After obtaining the generated latent neighbors, we utilize them to enhance session representations for improving session-based recommendation. Extensive experiments on four public datasets show that DiffSBR generates effective latent neighbors and improves recommendation performance against state-of-the-art baselines.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºDiffSBRçš„æ–°å‹æ¨¡å‹ï¼Œé€šè¿‡åŸºäºæ‰©æ•£çš„æ½œåœ¨é‚»å±…ç”Ÿæˆä¸ºåŸºäºä¼šè¯çš„æ¨èç³»ç»Ÿæä¾›å¢å¼ºã€‚æ¨¡å‹åˆ©ç”¨æ£€ç´¢å¢å¼ºæ‰©æ•£å’Œè‡ªå¢å¼ºæ‰©æ•£ä¸¤ä¸ªæ¨¡å—ç”Ÿæˆé«˜è´¨é‡çš„æ½œåœ¨é‚»å±…ï¼Œç¼“è§£æ•°æ®ç¨€ç–æ€§é—®é¢˜ï¼Œå¹¶ç»“åˆæ£€ç´¢å™¨å’Œç”Ÿæˆå™¨çš„åä½œå­¦ä¹ ä¼˜åŒ–æ¨èæ€§èƒ½ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨ç”Ÿæˆæ½œåœ¨é‚»å±…å’Œæå‡æ¨èæ•ˆæœä¸Šä¼˜äºç°æœ‰åŸºçº¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.03792v1">VietMed-MCQ: A Consistency-Filtered Data Synthesis Framework for Vietnamese Traditional Medicine Evaluation</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) have demonstrated remarkable proficiency in general medical domains. However, their performance significantly degrades in specialized, culturally specific domains such as Vietnamese Traditional Medicine (VTM), primarily due to the scarcity of high-quality, structured benchmarks. In this paper, we introduce VietMed-MCQ, a novel multiple-choice question dataset generated via a Retrieval-Augmented Generation (RAG) pipeline with an automated consistency check mechanism. Unlike previous synthetic datasets, our framework incorporates a dual-model validation approach to ensure reasoning consistency through independent answer verification, though the substring-based evidence checking has known limitations. The complete dataset of 3,190 questions spans three difficulty levels and underwent validation by one medical expert and four students, achieving 94.2 percent approval with substantial inter-rater agreement (Fleiss' kappa = 0.82). We benchmark seven open-source models on VietMed-MCQ. Results reveal that general-purpose models with strong Chinese priors outperform Vietnamese-centric models, highlighting cross-lingual conceptual transfer, while all models still struggle with complex diagnostic reasoning. Our code and dataset are publicly available to foster research in low-resource medical domains.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†VietMed-MCQï¼Œä¸€ä¸ªé’ˆå¯¹è¶Šå—ä¼ ç»ŸåŒ»å­¦ï¼ˆVTMï¼‰çš„å¤šé€‰é¢˜æ•°æ®é›†ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æµç¨‹å’Œè‡ªåŠ¨ä¸€è‡´æ€§æ£€æŸ¥æœºåˆ¶ç”Ÿæˆã€‚ç ”ç©¶é‡‡ç”¨åŒæ¨¡å‹éªŒè¯ç¡®ä¿ç­”æ¡ˆä¸€è‡´æ€§ï¼Œå¹¶è¯„ä¼°äº†ä¸ƒä¸ªå¼€æºæ¨¡å‹çš„æ€§èƒ½ï¼Œå‘ç°å…·æœ‰ä¸­æ–‡å…ˆéªŒçš„é€šç”¨æ¨¡å‹è¡¨ç°ä¼˜äºè¶Šå—è¯­ä¸“ç”¨æ¨¡å‹ï¼ŒåŒæ—¶æ­ç¤ºäº†æ¨¡å‹åœ¨å¤æ‚è¯Šæ–­æ¨ç†ä¸Šçš„ä¸è¶³ã€‚æ•°æ®é›†å’Œä»£ç å·²å…¬å¼€ä»¥ä¿ƒè¿›ä½èµ„æºåŒ»å­¦é¢†åŸŸç ”ç©¶ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.03748v1">Bridging OLAP and RAG: A Multidimensional Approach to the Design of Corpus Partitioning</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) systems are increasingly deployed on large-scale document collections, often comprising millions of documents and tens of millions of text chunks. In industrial-scale retrieval platforms, scalability is typically addressed through horizontal sharding and a combination of Approximate Nearest-Neighbor search, hybrid indexing, and optimized metadata filtering. Although effective from an efficiency perspective, these mechanisms rely on bottom-up, similarity-driven organization and lack a conceptual rationale for corpus partitioning. In this paper, we claim that the design of large-scale RAG systems may benefit from the combination of two orthogonal strategies: semantic clustering, which optimizes locality in embedding space, and multidimensional partitioning, which governs where retrieval should occur based on conceptual dimensions such as time and organizational context. Although such dimensions are already implicitly present in current systems, they are used in an ad hoc and poorly structured manner. We propose the Dimensional Fact Model (DFM) as a conceptual framework to guide the design of multidimensional partitions for RAG corpora. The DFM provides a principled way to reason about facts, dimensions, hierarchies, and granularity in retrieval-oriented settings. This framework naturally supports hierarchical routing and controlled fallback strategies, ensuring that retrieval remains robust even in the presence of incomplete metadata, while transforming the search process from a 'black-box' similarity matching into a governable and deterministic workflow. This work is intended as a position paper; its goal is to bridge the gap between OLAP-style multidimensional modeling and modern RAG architectures, and to stimulate further research on principled, explainable, and governable retrieval strategies at scale.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§è§„æ¨¡RAGç³»ç»Ÿçš„è®¾è®¡ä¼˜åŒ–ï¼Œæå‡ºç»“åˆè¯­ä¹‰èšç±»å’Œå¤šç»´åˆ†åŒºçš„ç­–ç•¥ï¼Œå¹¶å¼•å…¥Dimensional Fact Modelï¼ˆDFMï¼‰ä½œä¸ºæ¦‚å¿µæ¡†æ¶ï¼Œä»¥æå‡æ£€ç´¢çš„åˆç†æ€§å’Œå¯è§£é‡Šæ€§ï¼Œæ—¨åœ¨å¼¥åˆOLAPå¼å¤šç»´å»ºæ¨¡ä¸ç°ä»£RAGæ¶æ„ä¹‹é—´çš„å·®è·ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.03746v1">Whose Facts Win? LLM Source Preferences under Knowledge Conflicts</a></td><td><details><summary>å±•å¼€</summary>As large language models (LLMs) are more frequently used in retrieval-augmented generation pipelines, it is increasingly relevant to study their behavior under knowledge conflicts. Thus far, the role of the source of the retrieved information has gone unexamined. We address this gap with a novel framework to investigate how source preferences affect LLM resolution of inter-context knowledge conflicts in English, motivated by interdisciplinary research on credibility. With a comprehensive, tightly-controlled evaluation of 13 open-weight LLMs, we find that LLMs prefer institutionally-corroborated information (e.g., government or newspaper sources) over information from people and social media. However, these source preferences can be reversed by simply repeating information from less credible sources. To mitigate repetition effects and maintain consistent preferences, we propose a novel method that reduces repetition bias by up to 99.8%, while also maintaining at least 88.8% of original preferences. We release all data and code to encourage future work on credibility and source preferences in knowledge-intensive NLP.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æµç¨‹ä¸­ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ä½•å¤„ç†çŸ¥è¯†å†²çªï¼Œç‰¹åˆ«æ˜¯ä¿¡æ¯æ¥æºçš„å¯ä¿¡åº¦å¯¹æ¨¡å‹å†³ç­–çš„å½±å“ã€‚ä½œè€…é€šè¿‡å®éªŒå‘ç°ï¼ŒLLMså€¾å‘äºé€‰æ‹©æœºæ„éªŒè¯çš„ä¿¡æ¯ï¼ˆå¦‚æ”¿åºœæˆ–æ–°é—»æ¥æºï¼‰ï¼Œè€Œéç¤¾äº¤åª’ä½“æˆ–ä¸ªäººæä¾›çš„ä¿¡æ¯ï¼Œä½†è¿™ç§åå¥½å¯èƒ½å› é‡å¤ä½å¯ä¿¡åº¦ä¿¡æ¯è€Œè¢«é€†è½¬ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œæœ‰æ•ˆå‡å°‘é‡å¤åè§å¹¶ä¿æŒæ¨¡å‹å¯¹å¯ä¿¡æ¥æºçš„åå¥½ï¼ŒåŒæ—¶å…¬å¼€äº†ç›¸å…³æ•°æ®å’Œä»£ç ä»¥ä¿ƒè¿›æœªæ¥ç ”ç©¶ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2026-01-06
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2601.03052v1">Detecting Hallucinations in Retrieval-Augmented Generation via Semantic-level Internal Reasoning Graph</a></td><td><details><summary>å±•å¼€</summary>The Retrieval-augmented generation (RAG) system based on Large language model (LLM) has made significant progress. It can effectively reduce factuality hallucinations, but faithfulness hallucinations still exist. Previous methods for detecting faithfulness hallucinations either neglect to capture the models' internal reasoning processes or handle those features coarsely, making it difficult for discriminators to learn. This paper proposes a semantic-level internal reasoning graph-based method for detecting faithfulness hallucination. Specifically, we first extend the layer-wise relevance propagation algorithm from the token level to the semantic level, constructing an internal reasoning graph based on attribution vectors. This provides a more faithful semantic-level representation of dependency. Furthermore, we design a general framework based on a small pre-trained language model to utilize the dependencies in LLM's reasoning for training and hallucination detection, which can dynamically adjust the pass rate of correct samples through a threshold. Experimental results demonstrate that our method achieves better overall performance compared to state-of-the-art baselines on RAGTruth and Dolly-15k.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¯­ä¹‰çº§å†…éƒ¨æ¨ç†å›¾çš„æ–¹æ³•ï¼Œç”¨äºæ£€æµ‹RAGç³»ç»Ÿä¸­å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿ å®æ€§å¹»è§‰é—®é¢˜ã€‚é€šè¿‡æ”¹è¿›å±‚æ¬¡ç›¸å…³æ€§ä¼ æ’­ç®—æ³•å¹¶æ„å»ºå†…éƒ¨æ¨ç†å›¾ï¼Œç»“åˆé¢„è®­ç»ƒå°æ¨¡å‹æ¡†æ¶åŠ¨æ€è°ƒæ•´é˜ˆå€¼ï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨RAGTruthå’ŒDolly-15kæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰åŸºçº¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.03014v1">SentGraph: Hierarchical Sentence Graph for Multi-hop Retrieval-Augmented Question Answering</a></td><td><details><summary>å±•å¼€</summary>Traditional Retrieval-Augmented Generation (RAG) effectively supports single-hop question answering with large language models but faces significant limitations in multi-hop question answering tasks, which require combining evidence from multiple documents. Existing chunk-based retrieval often provides irrelevant and logically incoherent context, leading to incomplete evidence chains and incorrect reasoning during answer generation. To address these challenges, we propose SentGraph, a sentence-level graph-based RAG framework that explicitly models fine-grained logical relationships between sentences for multi-hop question answering. Specifically, we construct a hierarchical sentence graph offline by first adapting Rhetorical Structure Theory to distinguish nucleus and satellite sentences, and then organizing them into topic-level subgraphs with cross-document entity bridges. During online retrieval, SentGraph performs graph-guided evidence selection and path expansion to retrieve fine-grained sentence-level evidence. Extensive experiments on four multi-hop question answering benchmarks demonstrate the effectiveness of SentGraph, validating the importance of explicitly modeling sentence-level logical dependencies for multi-hop reasoning.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSentGraphçš„åŸºäºå¥å­çº§åˆ«å›¾ç»“æ„çš„RAGæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸRAGåœ¨å¤šè·³é—®ç­”ä»»åŠ¡ä¸­å› æ£€ç´¢ä¸ç›¸å…³æˆ–é€»è¾‘ä¸è¿è´¯çš„ä¸Šä¸‹æ–‡è€Œå¯¼è‡´çš„è¯æ®é“¾ä¸å®Œæ•´å’Œæ¨ç†é”™è¯¯çš„é—®é¢˜ã€‚SentGraphé€šè¿‡æ„å»ºå±‚æ¬¡åŒ–çš„å¥å­å›¾ï¼Œåˆ©ç”¨ä¿®è¾ç»“æ„ç†è®ºåŒºåˆ†æ ¸å¿ƒå¥å’Œå«æ˜Ÿå¥ï¼Œå¹¶é€šè¿‡è·¨æ–‡æ¡£å®ä½“æ¡¥æ¥ç»„ç»‡ä¸»é¢˜çº§å­å›¾ï¼Œä»è€Œåœ¨åœ¨çº¿æ£€ç´¢æ—¶å®ç°å›¾å¼•å¯¼çš„è¯æ®é€‰æ‹©å’Œè·¯å¾„æ‰©å±•ï¼Œæ˜¾è‘—æå‡äº†å¤šè·³é—®ç­”çš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.02993v1">Stable-RAG: Mitigating Retrieval-Permutation-Induced Hallucinations in Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has become a key paradigm for reducing factual hallucinations in large language models (LLMs), yet little is known about how the order of retrieved documents affects model behavior. We empirically show that under Top-5 retrieval with the gold document included, LLM answers vary substantially across permutations of the retrieved set, even when the gold document is fixed in the first position. This reveals a previously underexplored sensitivity to retrieval permutations. Although robust RAG methods primarily focus on enhancing LLM robustness to low-quality retrieval and mitigating positional bias to distribute attention fairly over long contexts, neither approach directly addresses permutation sensitivity. In this paper, we propose Stable-RAG, which exploits permutation sensitivity estimation to mitigate permutation-induced hallucinations. Stable-RAG runs the generator under multiple retrieval orders, clusters hidden states, and decodes from a cluster-center representation that captures the dominant reasoning pattern. It then uses these reasoning results to align hallucinated outputs toward the correct answer, encouraging the model to produce consistent and accurate predictions across document permutations. Experiments on three QA datasets show that Stable-RAG significantly improves answer accuracy, reasoning consistency and robust generalization across datasets, retrievers, and input lengths compared with baselines.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ç ”ç©¶äº†RAGä¸­æ£€ç´¢æ–‡æ¡£é¡ºåºå¯¹ç”Ÿæˆç»“æœçš„å½±å“ï¼Œæå‡ºæ–°æ–¹æ³•Stable-RAGï¼Œé€šè¿‡å¤šé¡ºåºç”Ÿæˆã€èšç±»éšè—çŠ¶æ€åŠå¯¹é½çº é”™æ¥æå‡æ¨¡å‹å¯¹æ–‡æ¡£æ’åˆ—çš„é²æ£’æ€§ï¼Œå®éªŒè¡¨æ˜å…¶æ˜¾è‘—æ”¹å–„äº†ç­”æ¡ˆå‡†ç¡®æ€§ã€æ¨ç†ä¸€è‡´æ€§å’Œè·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.02957v1">LLM-Augmented Changepoint Detection: A Framework for Ensemble Detection and Automated Explanation</a></td><td><details><summary>å±•å¼€</summary>This paper introduces a novel changepoint detection framework that combines ensemble statistical methods with Large Language Models (LLMs) to enhance both detection accuracy and the interpretability of regime changes in time series data. Two critical limitations in the field are addressed. First, individual detection methods exhibit complementary strengths and weaknesses depending on data characteristics, making method selection non-trivial and prone to suboptimal results. Second, automated, contextual explanations for detected changes are largely absent. The proposed ensemble method aggregates results from ten distinct changepoint detection algorithms, achieving superior performance and robustness compared to individual methods. Additionally, an LLM-powered explanation pipeline automatically generates contextual narratives, linking detected changepoints to potential real-world historical events. For private or domain-specific data, a Retrieval-Augmented Generation (RAG) solution enables explanations grounded in user-provided documents. The open source Python framework demonstrates practical utility in diverse domains, including finance, political science, and environmental science, transforming raw statistical output into actionable insights for analysts and decision-makers.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆé›†æˆç»Ÿè®¡æ–¹æ³•ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ–°é¢–å˜ç‚¹æ£€æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨æå‡æ—¶é—´åºåˆ—æ•°æ®ä¸­ regime å˜åŒ–çš„æ£€æµ‹å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚é€šè¿‡èšåˆåç§ä¸åŒç®—æ³•çš„ç»“æœï¼Œå…¶é›†æˆæ–¹æ³•ä¼˜äºå•ä¸€æ–¹æ³•ï¼Œå¹¶åˆ©ç”¨LLMè‡ªåŠ¨ç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³çš„è§£é‡Šï¼ˆå¦‚å…³è”å†å²äº‹ä»¶ï¼‰ã€‚é’ˆå¯¹ç§æœ‰æˆ–é¢†åŸŸç‰¹å®šæ•°æ®ï¼Œé‡‡ç”¨RAGæŠ€æœ¯åŸºäºç”¨æˆ·æä¾›çš„æ–‡æ¡£ç”Ÿæˆè§£é‡Šï¼Œæœ€ç»ˆé€šè¿‡å¼€æºå·¥å…·å®ç°å¤šé¢†åŸŸåº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.02956v1">Enhancing Multilingual RAG Systems with Debiased Language Preference-Guided Query Fusion</a></td><td><details><summary>å±•å¼€</summary>Multilingual Retrieval-Augmented Generation (mRAG) systems often exhibit a perceived preference for high-resource languages, particularly English, resulting in the widespread adoption of English pivoting. While prior studies attribute this advantage to the superior English-centric capabilities of Large Language Models (LLMs), we find that such measurements are significantly distorted by structural priors inherent in evaluation benchmarks. Specifically, we identify exposure bias and a gold availability prior-both driven by the disproportionate concentration of resources in English-as well as cultural priors rooted in topic locality, as factors that hinder accurate assessment of genuine language preference. To address these biases, we propose DeLP (Debiased Language Preference), a calibrated metric designed to explicitly factor out these structural confounds. Our analysis using DeLP reveals that the previously reported English preference is largely a byproduct of evidence distribution rather than an inherent model bias. Instead, we find that retrievers fundamentally favor monolingual alignment between the query and the document language. Building on this insight, we introduce DELTA (DEbiased Language preference-guided Text Augmentation), a lightweight and efficient mRAG framework that strategically leverages monolingual alignment to optimize cross-lingual retrieval and generation. Experimental results demonstrate that DELTA consistently outperforms English pivoting and mRAG baselines across diverse languages.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤šè¯­è¨€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆmRAGï¼‰ç³»ç»Ÿä¸­å¯¹é«˜èµ„æºè¯­è¨€ï¼ˆå¦‚è‹±è¯­ï¼‰çš„åå¥½é—®é¢˜ï¼ŒæŒ‡å‡ºç°æœ‰è¯„ä¼°åŸºå‡†ä¸­çš„ç»“æ„æ€§åå·®ï¼ˆå¦‚æ›å…‰åå·®ã€èµ„æºåˆ†å¸ƒä¸å‡å’Œæ–‡åŒ–ä¸»é¢˜å±€éƒ¨æ€§ï¼‰å¯¼è‡´äº†å¯¹è¯­è¨€åå¥½è¯„ä¼°çš„å¤±çœŸã€‚ä½œè€…æå‡ºäº†å»åè¯­è¨€åå¥½æŒ‡æ ‡ï¼ˆDeLPï¼‰ä»¥æ¶ˆé™¤è¿™äº›åå·®ï¼Œå¹¶å‘ç°æ£€ç´¢å™¨æœ¬è´¨ä¸Šå€¾å‘äºæŸ¥è¯¢ä¸æ–‡æ¡£è¯­è¨€çš„å•è¯­å¯¹é½ã€‚åŸºäºæ­¤ï¼Œä»–ä»¬æå‡ºäº†DELTAæ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–è·¨è¯­è¨€æ£€ç´¢å’Œç”Ÿæˆæ¥æå‡æ€§èƒ½ï¼Œå®éªŒè¡¨æ˜DELTAåœ¨å¤šè¯­è¨€ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.02917v1">RAL2M: Retrieval Augmented Learning-To-Match Against Hallucination in Compliance-Guaranteed Service Systems</a></td><td><details><summary>å±•å¼€</summary>Hallucination is a major concern in LLM-driven service systems, necessitating explicit knowledge grounding for compliance-guaranteed responses. In this paper, we introduce Retrieval-Augmented Learning-to-Match (RAL2M), a novel framework that eliminates generation hallucination by repositioning LLMs as query-response matching judges within a retrieval-based system, providing a robust alternative to purely generative approaches. To further mitigate judgment hallucination, we propose a query-adaptive latent ensemble strategy that explicitly models heterogeneous model competence and interdependencies among LLMs, deriving a calibrated consensus decision. Extensive experiments on large-scale benchmarks demonstrate that the proposed method effectively leverages the "wisdom of the crowd" and significantly outperforms strong baselines. Finally, we discuss best practices and promising directions for further exploiting latent representations in future work.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†Retrieval-Augmented Learning-to-Match (RAL2M)æ¡†æ¶ï¼Œé€šè¿‡å°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é‡æ–°å®šä½ä¸ºåŸºäºæ£€ç´¢ç³»ç»Ÿä¸­çš„æŸ¥è¯¢-å“åº”åŒ¹é…åˆ¤æ–­å™¨ï¼Œä»¥æ¶ˆé™¤ç”Ÿæˆå¹»è§‰ï¼Œå¹¶é‡‡ç”¨æŸ¥è¯¢è‡ªé€‚åº”çš„æ½œåœ¨é›†æˆç­–ç•¥æ¥è¿›ä¸€æ­¥å‡å°‘åˆ¤æ–­å¹»è§‰ï¼Œä»è€Œåœ¨ä¿è¯åˆè§„æ€§å“åº”çš„åŒæ—¶æ˜¾è‘—æå‡æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.02814v1">Causal-Enhanced AI Agents for Medical Research Screening</a></td><td><details><summary>å±•å¼€</summary>Systematic reviews are essential for evidence-based medicine, but reviewing 1.5 million+ annual publications manually is infeasible. Current AI approaches suffer from hallucinations in systematic review tasks, with studies reporting rates ranging from 28--40% for earlier models to 2--15% for modern implementations which is unacceptable when errors impact patient care.
  We present a causal graph-enhanced retrieval-augmented generation system integrating explicit causal reasoning with dual-level knowledge graphs. Our approach enforces evidence-first protocols where every causal claim traces to retrieved literature and automatically generates directed acyclic graphs visualizing intervention-outcome pathways.
  Evaluation on 234 dementia exercise abstracts shows CausalAgent achieves 95% accuracy, 100% retrieval success, and zero hallucinations versus 34% accuracy and 10% hallucinations for baseline AI. Automatic causal graphs enable explicit mechanism modeling, visual synthesis, and enhanced interpretability. While this proof-of-concept evaluation used ten questions focused on dementia exercise research, the architectural approach demonstrates transferable principles for trustworthy medical AI and causal reasoning's potential for high-stakes healthcare.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆå› æœæ¨ç†å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„ç³»ç»Ÿï¼Œé€šè¿‡åŒå±‚æ¬¡çŸ¥è¯†å›¾è°±å’Œè¯æ®ä¼˜å…ˆåè®®ï¼Œæ˜¾è‘—é™ä½äº†ç³»ç»Ÿç»¼è¿°ä¸­çš„å¹»è§‰é—®é¢˜ã€‚åœ¨é’ˆå¯¹234ç¯‡ç—´å‘†ç—‡è¿åŠ¨ç›¸å…³æ‘˜è¦çš„è¯„ä¼°ä¸­ï¼Œè¯¥ç³»ç»Ÿå®ç°äº†95%çš„å‡†ç¡®ç‡å’Œé›¶å¹»è§‰ï¼Œä¼˜äºåŸºçº¿AIæ¨¡å‹ï¼Œå¹¶è‡ªåŠ¨ç”Ÿæˆå¹²é¢„-ç»“æœè·¯å¾„çš„æœ‰å‘æ— ç¯å›¾ï¼Œæå‡äº†åŒ»å­¦AIçš„å¯è§£é‡Šæ€§å’Œå¯é æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.02744v1">SYNAPSE: Empowering LLM Agents with Episodic-Semantic Memory via Spreading Activation</a></td><td><details><summary>å±•å¼€</summary>While Large Language Models (LLMs) excel at generalized reasoning, standard retrieval-augmented approaches fail to address the disconnected nature of long-term agentic memory. To bridge this gap, we introduce Synapse (Synergistic Associative Processing Semantic Encoding), a unified memory architecture that transcends static vector similarity. Drawing from cognitive science, Synapse models memory as a dynamic graph where relevance emerges from spreading activation rather than pre-computed links. By integrating lateral inhibition and temporal decay, the system dynamically highlights relevant sub-graphs while filtering interference. We implement a Triple Hybrid Retrieval strategy that fuses geometric embeddings with activation-based graph traversal. Comprehensive evaluations on the LoCoMo benchmark show that Synapse significantly outperforms state-of-the-art methods in complex temporal and multi-hop reasoning tasks, offering a robust solution to the "Contextual Tunneling" problem. Our code and data will be made publicly available upon acceptance.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSynapseçš„æ–°å‹ç»Ÿä¸€è®°å¿†æ¶æ„ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ£€ç´¢å¢å¼ºæ–¹æ³•åœ¨é•¿æœŸä»£ç†è®°å¿†ä¸­çš„å±€é™æ€§ã€‚Synapseå€Ÿé‰´è®¤çŸ¥ç§‘å­¦åŸç†ï¼Œå°†è®°å¿†å»ºæ¨¡ä¸ºåŠ¨æ€å›¾ï¼Œé€šè¿‡æ¿€æ´»æ‰©æ•£è€Œéé™æ€å‘é‡ç›¸ä¼¼æ€§æ¥ç¡®å®šå…³è”æ€§ï¼Œå¹¶é‡‡ç”¨ä¸‰é‡æ··åˆæ£€ç´¢ç­–ç•¥ç»“åˆå‡ ä½•åµŒå…¥å’ŒåŸºäºæ¿€æ´»çš„å›¾éå†ã€‚å®éªŒè¡¨æ˜ï¼ŒSynapseåœ¨å¤æ‚æ—¶åºå’Œå¤šè·³æ¨ç†ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæœ‰æ•ˆç¼“è§£äº†"ä¸Šä¸‹æ–‡éš§é“"é—®é¢˜ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2026-01-05
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2601.03052v1">Detecting Hallucinations in Retrieval-Augmented Generation via Semantic-level Internal Reasoning Graph</a></td><td><details><summary>å±•å¼€</summary>The Retrieval-augmented generation (RAG) system based on Large language model (LLM) has made significant progress. It can effectively reduce factuality hallucinations, but faithfulness hallucinations still exist. Previous methods for detecting faithfulness hallucinations either neglect to capture the models' internal reasoning processes or handle those features coarsely, making it difficult for discriminators to learn. This paper proposes a semantic-level internal reasoning graph-based method for detecting faithfulness hallucination. Specifically, we first extend the layer-wise relevance propagation algorithm from the token level to the semantic level, constructing an internal reasoning graph based on attribution vectors. This provides a more faithful semantic-level representation of dependency. Furthermore, we design a general framework based on a small pre-trained language model to utilize the dependencies in LLM's reasoning for training and hallucination detection, which can dynamically adjust the pass rate of correct samples through a threshold. Experimental results demonstrate that our method achieves better overall performance compared to state-of-the-art baselines on RAGTruth and Dolly-15k.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè¯­ä¹‰çº§å†…éƒ¨æ¨ç†å›¾çš„RAGç³»ç»Ÿå¿ å®æ€§å¹»è§‰æ£€æµ‹æ–¹æ³•ï¼Œé€šè¿‡æ”¹è¿›å±‚é—´ç›¸å…³æ€§ä¼ æ’­ç®—æ³•æ„å»ºä¾èµ–å…³ç³»å›¾ï¼Œå¹¶è®¾è®¡é€šç”¨æ¡†æ¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†ä¾èµ–è¿›è¡Œè®­ç»ƒå’Œæ£€æµ‹ï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†ä¸Šä¼˜äºç°æœ‰åŸºçº¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.03014v1">SentGraph: Hierarchical Sentence Graph for Multi-hop Retrieval-Augmented Question Answering</a></td><td><details><summary>å±•å¼€</summary>Traditional Retrieval-Augmented Generation (RAG) effectively supports single-hop question answering with large language models but faces significant limitations in multi-hop question answering tasks, which require combining evidence from multiple documents. Existing chunk-based retrieval often provides irrelevant and logically incoherent context, leading to incomplete evidence chains and incorrect reasoning during answer generation. To address these challenges, we propose SentGraph, a sentence-level graph-based RAG framework that explicitly models fine-grained logical relationships between sentences for multi-hop question answering. Specifically, we construct a hierarchical sentence graph offline by first adapting Rhetorical Structure Theory to distinguish nucleus and satellite sentences, and then organizing them into topic-level subgraphs with cross-document entity bridges. During online retrieval, SentGraph performs graph-guided evidence selection and path expansion to retrieve fine-grained sentence-level evidence. Extensive experiments on four multi-hop question answering benchmarks demonstrate the effectiveness of SentGraph, validating the importance of explicitly modeling sentence-level logical dependencies for multi-hop reasoning.</details></td><td><details><summary>å±•å¼€</summary></details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.02993v1">Stable-RAG: Mitigating Retrieval-Permutation-Induced Hallucinations in Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has become a key paradigm for reducing factual hallucinations in large language models (LLMs), yet little is known about how the order of retrieved documents affects model behavior. We empirically show that under Top-5 retrieval with the gold document included, LLM answers vary substantially across permutations of the retrieved set, even when the gold document is fixed in the first position. This reveals a previously underexplored sensitivity to retrieval permutations. Although robust RAG methods primarily focus on enhancing LLM robustness to low-quality retrieval and mitigating positional bias to distribute attention fairly over long contexts, neither approach directly addresses permutation sensitivity. In this paper, we propose Stable-RAG, which exploits permutation sensitivity estimation to mitigate permutation-induced hallucinations. Stable-RAG runs the generator under multiple retrieval orders, clusters hidden states, and decodes from a cluster-center representation that captures the dominant reasoning pattern. It then uses these reasoning results to align hallucinated outputs toward the correct answer, encouraging the model to produce consistent and accurate predictions across document permutations. Experiments on three QA datasets show that Stable-RAG significantly improves answer accuracy, reasoning consistency and robust generalization across datasets, retrievers, and input lengths compared with baselines.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æŒ‡å‡ºç°æœ‰RAGç³»ç»Ÿå¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£é¡ºåºå…·æœ‰é«˜åº¦æ•æ„Ÿæ€§ï¼Œå¯¼è‡´æ¨¡å‹è¾“å‡ºä¸ç¨³å®šã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæ–‡ç« æå‡ºStable-RAGæ–¹æ³•ï¼Œé€šè¿‡è¯„ä¼°ä¸åŒæ–‡æ¡£æ’åˆ—ä¸‹çš„æ¨¡å‹è¾“å‡ºå·®å¼‚ï¼Œåˆ©ç”¨èšç±»å’Œä¸­å¿ƒè§£ç æŠ€æœ¯æ¥æ•æ‰ä¸»å¯¼æ¨ç†æ¨¡å¼ï¼Œä»è€Œå‡å°‘æ’åˆ—å¼•å‘çš„å¹»è§‰ï¼Œæé«˜ç­”æ¡ˆå‡†ç¡®æ€§ã€ä¸€è‡´æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.02957v1">LLM-Augmented Changepoint Detection: A Framework for Ensemble Detection and Automated Explanation</a></td><td><details><summary>å±•å¼€</summary>This paper introduces a novel changepoint detection framework that combines ensemble statistical methods with Large Language Models (LLMs) to enhance both detection accuracy and the interpretability of regime changes in time series data. Two critical limitations in the field are addressed. First, individual detection methods exhibit complementary strengths and weaknesses depending on data characteristics, making method selection non-trivial and prone to suboptimal results. Second, automated, contextual explanations for detected changes are largely absent. The proposed ensemble method aggregates results from ten distinct changepoint detection algorithms, achieving superior performance and robustness compared to individual methods. Additionally, an LLM-powered explanation pipeline automatically generates contextual narratives, linking detected changepoints to potential real-world historical events. For private or domain-specific data, a Retrieval-Augmented Generation (RAG) solution enables explanations grounded in user-provided documents. The open source Python framework demonstrates practical utility in diverse domains, including finance, political science, and environmental science, transforming raw statistical output into actionable insights for analysts and decision-makers.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆé›†æˆç»Ÿè®¡æ–¹æ³•ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ–°é¢–å˜ç‚¹æ£€æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨æå‡æ—¶é—´åºåˆ—æ•°æ®ä¸­å˜ç‚¹æ£€æµ‹çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚é€šè¿‡é›†æˆåç§ä¸åŒç®—æ³•çš„ç»“æœï¼Œè¯¥æ–¹æ³•ä¼˜äºå•ä¸€æ–¹æ³•ï¼Œå¹¶åˆ©ç”¨LLMç”Ÿæˆä¸Šä¸‹æ–‡è§£é‡Šï¼Œå°†æ£€æµ‹åˆ°çš„å˜ç‚¹ä¸çœŸå®å†å²äº‹ä»¶å…³è”ã€‚é’ˆå¯¹ç§æœ‰æˆ–ç‰¹å®šé¢†åŸŸæ•°æ®ï¼Œé‡‡ç”¨RAGæŠ€æœ¯åŸºäºç”¨æˆ·æä¾›çš„æ–‡æ¡£ç”Ÿæˆè§£é‡Šï¼Œæœ€ç»ˆé€šè¿‡å¼€æºæ¡†æ¶å®ç°è·¨é¢†åŸŸçš„å®é™…åº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.02956v1">Enhancing Multilingual RAG Systems with Debiased Language Preference-Guided Query Fusion</a></td><td><details><summary>å±•å¼€</summary>Multilingual Retrieval-Augmented Generation (mRAG) systems often exhibit a perceived preference for high-resource languages, particularly English, resulting in the widespread adoption of English pivoting. While prior studies attribute this advantage to the superior English-centric capabilities of Large Language Models (LLMs), we find that such measurements are significantly distorted by structural priors inherent in evaluation benchmarks. Specifically, we identify exposure bias and a gold availability prior-both driven by the disproportionate concentration of resources in English-as well as cultural priors rooted in topic locality, as factors that hinder accurate assessment of genuine language preference. To address these biases, we propose DeLP (Debiased Language Preference), a calibrated metric designed to explicitly factor out these structural confounds. Our analysis using DeLP reveals that the previously reported English preference is largely a byproduct of evidence distribution rather than an inherent model bias. Instead, we find that retrievers fundamentally favor monolingual alignment between the query and the document language. Building on this insight, we introduce DELTA (DEbiased Language preference-guided Text Augmentation), a lightweight and efficient mRAG framework that strategically leverages monolingual alignment to optimize cross-lingual retrieval and generation. Experimental results demonstrate that DELTA consistently outperforms English pivoting and mRAG baselines across diverse languages.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤šè¯­è¨€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆmRAGï¼‰ç³»ç»Ÿä¸­å¯¹é«˜èµ„æºè¯­è¨€ï¼ˆå¦‚è‹±è¯­ï¼‰çš„åå¥½é—®é¢˜ï¼ŒæŒ‡å‡ºç°æœ‰è¯„ä¼°åŸºå‡†ä¸­çš„ç»“æ„æ€§åå·®ï¼ˆå¦‚æ›å…‰åå·®ã€èµ„æºåˆ†å¸ƒä¸å‡å’Œæ–‡åŒ–ä¸»é¢˜å±€éƒ¨æ€§ï¼‰å¯¼è‡´äº†å¯¹è¯­è¨€åå¥½è¯„ä¼°çš„å¤±çœŸã€‚ä½œè€…æå‡ºäº†å»åè¯­è¨€åå¥½æŒ‡æ ‡ï¼ˆDeLPï¼‰ä»¥æ¶ˆé™¤è¿™äº›åå·®ï¼Œå¹¶å‘ç°æ£€ç´¢å™¨æœ¬è´¨ä¸Šæ›´å€¾å‘äºæŸ¥è¯¢ä¸æ–‡æ¡£è¯­è¨€çš„å•è¯­å¯¹é½ã€‚åŸºäºæ­¤ï¼Œä»–ä»¬æå‡ºäº†DELTAæ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨å•è¯­å¯¹é½ä¼˜åŒ–è·¨è¯­è¨€æ£€ç´¢å’Œç”Ÿæˆï¼Œå®éªŒè¡¨æ˜DELTAåœ¨å¤šè¯­è¨€ä»»åŠ¡ä¸­ä¼˜äºä¼ ç»Ÿçš„è‹±è¯­æ¢çº½æ–¹æ³•å’Œç°æœ‰mRAGåŸºçº¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.02917v1">RAL2M: Retrieval Augmented Learning-To-Match Against Hallucination in Compliance-Guaranteed Service Systems</a></td><td><details><summary>å±•å¼€</summary>Hallucination is a major concern in LLM-driven service systems, necessitating explicit knowledge grounding for compliance-guaranteed responses. In this paper, we introduce Retrieval-Augmented Learning-to-Match (RAL2M), a novel framework that eliminates generation hallucination by repositioning LLMs as query-response matching judges within a retrieval-based system, providing a robust alternative to purely generative approaches. To further mitigate judgment hallucination, we propose a query-adaptive latent ensemble strategy that explicitly models heterogeneous model competence and interdependencies among LLMs, deriving a calibrated consensus decision. Extensive experiments on large-scale benchmarks demonstrate that the proposed method effectively leverages the "wisdom of the crowd" and significantly outperforms strong baselines. Finally, we discuss best practices and promising directions for further exploiting latent representations in future work.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºRAL2Mï¼ˆRetrieval-Augmented Learning-to-Matchï¼‰çš„æ–°æ¡†æ¶ï¼Œé€šè¿‡å°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é‡æ–°å®šä½ä¸ºåŸºäºæ£€ç´¢ç³»ç»Ÿä¸­çš„æŸ¥è¯¢-å“åº”åŒ¹é…åˆ¤æ–­å™¨ï¼Œæ¶ˆé™¤ç”Ÿæˆå¹»è§‰ï¼Œå¹¶å¼•å…¥æŸ¥è¯¢è‡ªé€‚åº”çš„æ½œåœ¨é›†æˆç­–ç•¥æ¥è¿›ä¸€æ­¥å‡å°‘åˆ¤æ–­å¹»è§‰ï¼Œä»è€Œæ˜¾è‘—æå‡å“åº”å‡†ç¡®æ€§å’Œåˆè§„æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.02814v1">Causal-Enhanced AI Agents for Medical Research Screening</a></td><td><details><summary>å±•å¼€</summary>Systematic reviews are essential for evidence-based medicine, but reviewing 1.5 million+ annual publications manually is infeasible. Current AI approaches suffer from hallucinations in systematic review tasks, with studies reporting rates ranging from 28--40% for earlier models to 2--15% for modern implementations which is unacceptable when errors impact patient care.
  We present a causal graph-enhanced retrieval-augmented generation system integrating explicit causal reasoning with dual-level knowledge graphs. Our approach enforces evidence-first protocols where every causal claim traces to retrieved literature and automatically generates directed acyclic graphs visualizing intervention-outcome pathways.
  Evaluation on 234 dementia exercise abstracts shows CausalAgent achieves 95% accuracy, 100% retrieval success, and zero hallucinations versus 34% accuracy and 10% hallucinations for baseline AI. Automatic causal graphs enable explicit mechanism modeling, visual synthesis, and enhanced interpretability. While this proof-of-concept evaluation used ten questions focused on dementia exercise research, the architectural approach demonstrates transferable principles for trustworthy medical AI and causal reasoning's potential for high-stakes healthcare.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆå› æœå›¾å¢å¼ºçš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿï¼ˆCausalAgentï¼‰ï¼Œé€šè¿‡æ•´åˆæ˜¾å¼å› æœæ¨ç†ä¸åŒå±‚çŸ¥è¯†å›¾è°±ï¼Œåœ¨åŒ»å­¦ç³»ç»Ÿç»¼è¿°ä»»åŠ¡ä¸­å®ç°é«˜ç²¾åº¦ï¼ˆ95%ï¼‰å’Œé›¶å¹»è§‰ã€‚è¯¥ç³»ç»Ÿå¼ºåˆ¶è¦æ±‚æ¯ä¸ªå› æœä¸»å¼ å‡è¿½æº¯è‡³æ£€ç´¢æ–‡çŒ®ï¼Œå¹¶è‡ªåŠ¨ç”Ÿæˆå¹²é¢„-ç»“æœè·¯å¾„çš„æœ‰å‘æ— ç¯å›¾ï¼Œåœ¨ç—´å‘†ç—‡è¿åŠ¨ç ”ç©¶çš„å®éªŒä¸­æ˜¾è‘—ä¼˜äºåŸºçº¿AIï¼ˆ34%å‡†ç¡®ç‡ï¼‰ã€‚ç ”ç©¶è¯æ˜äº†å› æœæ¨ç†åœ¨é«˜é£é™©åŒ»ç–—AIä¸­çš„å¯ä¿¡åº”ç”¨æ½œåŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.02744v1">SYNAPSE: Empowering LLM Agents with Episodic-Semantic Memory via Spreading Activation</a></td><td><details><summary>å±•å¼€</summary>While Large Language Models (LLMs) excel at generalized reasoning, standard retrieval-augmented approaches fail to address the disconnected nature of long-term agentic memory. To bridge this gap, we introduce Synapse (Synergistic Associative Processing Semantic Encoding), a unified memory architecture that transcends static vector similarity. Drawing from cognitive science, Synapse models memory as a dynamic graph where relevance emerges from spreading activation rather than pre-computed links. By integrating lateral inhibition and temporal decay, the system dynamically highlights relevant sub-graphs while filtering interference. We implement a Triple Hybrid Retrieval strategy that fuses geometric embeddings with activation-based graph traversal. Comprehensive evaluations on the LoCoMo benchmark show that Synapse significantly outperforms state-of-the-art methods in complex temporal and multi-hop reasoning tasks, offering a robust solution to the "Contextual Tunneling" problem. Our code and data will be made publicly available upon acceptance.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºSynapseçš„æ–°å‹ç»Ÿä¸€è®°å¿†æ¶æ„ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ£€ç´¢å¢å¼ºæ–¹æ³•åœ¨é•¿æœŸæ™ºèƒ½ä½“è®°å¿†æ–¹é¢çš„å±€é™æ€§ã€‚å®ƒå€Ÿé‰´è®¤çŸ¥ç§‘å­¦åŸç†ï¼Œå°†è®°å¿†å»ºæ¨¡ä¸ºåŸºäºæ¿€æ´»æ‰©æ•£çš„åŠ¨æ€å›¾ï¼ˆè€Œéé™æ€å‘é‡ï¼‰ï¼Œå¹¶èåˆäº†å‡ ä½•åµŒå…¥ä¸åŸºäºå›¾çš„éå†çš„æ··åˆæ£€ç´¢ç­–ç•¥ï¼Œåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.02504v1">Enhancing Debugging Skills with AI-Powered Assistance: A Real-Time Tool for Debugging Support</a></td><td><details><summary>å±•å¼€</summary>Debugging is a crucial skill in programming education and software development, yet it is often overlooked in CS curricula. To address this, we introduce an AI-powered debugging assistant integrated into an IDE. It offers real-time support by analyzing code, suggesting breakpoints, and providing contextual hints. Using RAG with LLMs, program slicing, and custom heuristics, it enhances efficiency by minimizing LLM calls and improving accuracy. A three-level evaluation - technical analysis, UX study, and classroom tests - highlights its potential for teaching debugging.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§é›†æˆåˆ°IDEä¸­çš„AIè°ƒè¯•åŠ©æ‰‹ï¼Œåˆ©ç”¨RAGæŠ€æœ¯ç»“åˆLLMsã€ç¨‹åºåˆ‡ç‰‡å’Œè‡ªå®šä¹‰å¯å‘å¼æ–¹æ³•ï¼Œå®æ—¶åˆ†æä»£ç ã€å»ºè®®æ–­ç‚¹å¹¶æä¾›ä¸Šä¸‹æ–‡æç¤ºï¼Œæ—¨åœ¨æé«˜è°ƒè¯•æ•ˆç‡å¹¶è¾…åŠ©ç¼–ç¨‹æ•™å­¦ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.02144v1">Routing by Analogy: kNN-Augmented Expert Assignment for Mixture-of-Experts</a></td><td><details><summary>å±•å¼€</summary>Mixture-of-Experts (MoE) architectures scale large language models efficiently by employing a parametric "router" to dispatch tokens to a sparse subset of experts. Typically, this router is trained once and then frozen, rendering routing decisions brittle under distribution shifts. We address this limitation by introducing kNN-MoE, a retrieval-augmented routing framework that reuses optimal expert assignments from a memory of similar past cases. This memory is constructed offline by directly optimizing token-wise routing logits to maximize the likelihood on a reference set. Crucially, we use the aggregate similarity of retrieved neighbors as a confidence-driven mixing coefficient, thus allowing the method to fall back to the frozen router when no relevant cases are found. Experiments show kNN-MoE outperforms zero-shot baselines and rivals computationally expensive supervised fine-tuning.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†kNN-MoEï¼Œä¸€ç§æ£€ç´¢å¢å¼ºè·¯ç”±æ¡†æ¶ã€‚å®ƒé€šè¿‡æ£€ç´¢è¿‡å»ç±»ä¼¼æ ·æœ¬çš„æœ€ä¼˜ä¸“å®¶åˆ†é…ç»“æœæ¥æ”¹å–„ç¨€ç–æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰ä¸­è·¯ç”±å™¨åœ¨æ•°æ®åˆ†å¸ƒå˜åŒ–æ—¶æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œå¹¶åœ¨æ²¡æœ‰ç›¸å…³æ¡ˆä¾‹æ—¶å›é€€åˆ°åŸå§‹çš„è·¯ç”±å™¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.02065v1">Cost-Efficient Cross-Lingual Retrieval-Augmented Generation for Low-Resource Languages: A Case Study in Bengali Agricultural Advisory</a></td><td><details><summary>å±•å¼€</summary>Access to reliable agricultural advisory remains limited in many developing regions due to a persistent language barrier: authoritative agricultural manuals are predominantly written in English, while farmers primarily communicate in low-resource local languages such as Bengali. Although recent advances in Large Language Models (LLMs) enable natural language interaction, direct generation in low-resource languages often exhibits poor fluency and factual inconsistency, while cloud-based solutions remain cost-prohibitive. This paper presents a cost-efficient, cross-lingual Retrieval-Augmented Generation (RAG) framework for Bengali agricultural advisory that emphasizes factual grounding and practical deployability. The proposed system adopts a translation-centric architecture in which Bengali user queries are translated into English, enriched through domain-specific keyword injection to align colloquial farmer terminology with scientific nomenclature, and answered via dense vector retrieval over a curated corpus of English agricultural manuals (FAO, IRRI). The generated English response is subsequently translated back into Bengali to ensure accessibility. The system is implemented entirely using open-source models and operates on consumer-grade hardware without reliance on paid APIs. Experimental evaluation demonstrates reliable source-grounded responses, robust rejection of out-of-domain queries, and an average end-to-end latency below 20 seconds. The results indicate that cross-lingual retrieval combined with controlled translation offers a practical and scalable solution for agricultural knowledge access in low-resource language settings</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å­ŸåŠ æ‹‰è¯­å†œä¸šå’¨è¯¢çš„ç»æµé«˜æ•ˆã€è·¨è¯­è¨€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç¿»è¯‘ç”¨æˆ·æŸ¥è¯¢ã€æ³¨å…¥é¢†åŸŸå…³é”®è¯å¢å¼ºæ£€ç´¢å‡†ç¡®æ€§ï¼Œå¹¶ç»“åˆæƒå¨å†œä¸šæ‰‹å†Œçš„å¯†é›†å‘é‡æ£€ç´¢ç”Ÿæˆè‹±è¯­å›ç­”åç¿»è¯‘å›å­ŸåŠ æ‹‰è¯­ï¼Œå®ç°äº†å¼€æºæ¨¡å‹é©±åŠ¨çš„ä½æˆæœ¬éƒ¨ç½²ï¼Œå®éªŒéªŒè¯äº†å…¶åœ¨äº‹å®æ€§ã€é¢†åŸŸé²æ£’æ€§å’Œä½å»¶è¿Ÿï¼ˆ<20ç§’ï¼‰æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.01896v2">Tackling the Inherent Difficulty of Noise Filtering in RAG</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has become a widely adopted approach to enhance Large Language Models (LLMs) by incorporating external knowledge and reducing hallucinations. However, noisy or irrelevant documents are often introduced during RAG, potentially degrading performance and even causing hallucinated outputs. While various methods have been proposed to filter out such noise, we argue that identifying irrelevant information from retrieved content is inherently difficult and limited number of transformer layers can hardly solve this. Consequently, retrievers fail to filter out irrelevant documents entirely. Therefore, LLMs must be robust against such noise, but we demonstrate that standard fine-tuning approaches are often ineffective in enabling the model to selectively utilize relevant information while ignoring irrelevant content due to the structural constraints of attention patterns. To address this, we propose a novel fine-tuning method designed to enhance the model's ability to distinguish between relevant and irrelevant information within retrieved documents. Extensive experiments across multiple benchmarks show that our approach significantly improves the robustness and performance of LLMs.</details></td><td><details><summary>å±•å¼€</summary></details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.01844v1">Clinical Knowledge Graph Construction and Evaluation with Multi-LLMs via Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) offer new opportunities for constructing knowledge graphs (KGs) from unstructured clinical narratives. However, existing approaches often rely on structured inputs and lack robust validation of factual accuracy and semantic consistency, limitations that are especially problematic in oncology. We introduce an end-to-end framework for clinical KG construction and evaluation directly from free text using multi-agent prompting and a schema-constrained Retrieval-Augmented Generation (KG-RAG) strategy. Our pipeline integrates (1) prompt-driven entity, attribute, and relation extraction; (2) entropy-based uncertainty scoring; (3) ontology-aligned RDF/OWL schema generation; and (4) multi-LLM consensus validation for hallucination detection and semantic refinement. Beyond static graph construction, the framework supports continuous refinement and self-supervised evaluation, enabling iterative improvement of graph quality. Applied to two oncology cohorts (PDAC and BRCA), our method produces interpretable, SPARQL-compatible, and clinically grounded knowledge graphs without relying on gold-standard annotations. Experimental results demonstrate consistent gains in precision, relevance, and ontology compliance over baseline methods.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“æç¤ºå’Œæ¨¡å¼çº¦æŸçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆKG-RAGï¼‰ç­–ç•¥çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œç”¨äºç›´æ¥ä»è‡ªç”±æ–‡æœ¬ä¸­æ„å»ºå’Œè¯„ä¼°ä¸´åºŠçŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ã€‚è¯¥æ¡†æ¶æ•´åˆäº†å®ä½“ã€å±æ€§å’Œå…³ç³»æå–ã€åŸºäºç†µçš„ä¸ç¡®å®šæ€§è¯„åˆ†ã€æœ¬ä½“å¯¹é½çš„RDF/OWLæ¨¡å¼ç”Ÿæˆä»¥åŠå¤šLLMå…±è¯†éªŒè¯ï¼Œæ”¯æŒæŒç»­æ”¹è¿›å’Œè‡ªç›‘ç£è¯„ä¼°ï¼Œå¹¶åœ¨ä¸¤ä¸ªè‚¿ç˜¤å­¦é˜Ÿåˆ—ä¸­éªŒè¯äº†å…¶ä¼˜äºåŸºçº¿æ–¹æ³•çš„ç²¾ç¡®æ€§ã€ç›¸å…³æ€§å’Œæœ¬ä½“åˆè§„æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.01785v1">SRAS: A Lightweight Reinforcement Learning-based Document Selector for Edge-Native RAG Pipelines</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) systems often rely on fixed top-k document selection mechanisms that ignore downstream generation quality and impose computational overheads. We propose SRAS (Sparse Reward-Aware Selector), a lightweight document selector trained via reinforcement learning (RL) for edge-native RAG deployment. Unlike prior RL-based retrievers that assume large memory and latency budgets, SRAS learns a compact (~0.76MB) policy using Proximal Policy Optimization (PPO), guided by a hybrid reward signal combining Relaxed F1 and BERTScore. Our method operates under tight token and compute constraints, maintaining <1s latency on CPU. SRAS outperforms supervised and random selectors on a synthetic QA benchmark, and generalizes to real-world data, achieving BERTScore F1 of 0.8546 on SQuAD v2 without domain-specific tuning. This work is the first to demonstrate that RL-based document selection can be made ultra-lightweight, latency-aware, and effective for on-device RAG pipelines.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSRASï¼ˆSparse Reward-Aware Selectorï¼‰çš„è½»é‡çº§æ–‡æ¡£é€‰æ‹©å™¨ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒï¼Œä¸“ä¸ºè¾¹ç¼˜è®¾å¤‡ä¸Šçš„RAGç³»ç»Ÿè®¾è®¡ã€‚SRASé‡‡ç”¨ç´§å‡‘çš„ç­–ç•¥ï¼ˆçº¦0.76MBï¼‰ï¼Œç»“åˆRelaxed F1å’ŒBERTScoreçš„æ··åˆå¥–åŠ±ä¿¡å·ï¼Œåœ¨ä¸¥æ ¼çš„èµ„æºå’Œå»¶è¿Ÿé™åˆ¶ä¸‹ï¼ˆCPUä¸Šå»¶è¿Ÿ<1ç§’ï¼‰ä¼˜åŒ–æ–‡æ¡£é€‰æ‹©ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆè´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒSRASåœ¨åˆæˆQAåŸºå‡†å’ŒçœŸå®æ•°æ®é›†ï¼ˆå¦‚SQuAD v2ï¼‰ä¸Šå‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œé¦–æ¬¡éªŒè¯äº†RLé©±åŠ¨çš„è½»é‡åŒ–æ–‡æ¡£é€‰æ‹©åœ¨ç«¯ä¾§RAGä¸­çš„å¯è¡Œæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.01718v1">Yuan3.0 Flash: An Open Multimodal Large Language Model for Enterprise Applications</a></td><td><details><summary>å±•å¼€</summary>We introduce Yuan3.0 Flash, an open-source Mixture-of-Experts (MoE) MultiModal Large Language Model featuring 3.7B activated parameters and 40B total parameters, specifically designed to enhance performance on enterprise-oriented tasks while maintaining competitive capabilities on general-purpose tasks. To address the overthinking phenomenon commonly observed in Large Reasoning Models (LRMs), we propose Reflection-aware Adaptive Policy Optimization (RAPO), a novel RL training algorithm that effectively regulates overthinking behaviors. In enterprise-oriented tasks such as retrieval-augmented generation (RAG), complex table understanding, and summarization, Yuan3.0 Flash consistently achieves superior performance. Moreover, it also demonstrates strong reasoning capabilities in domains such as mathematics, science, etc., attaining accuracy comparable to frontier model while requiring only approximately 1/4 to 1/2 of the average tokens. Yuan3.0 Flash has been fully open-sourced to facilitate further research and real-world deployment: https://github.com/Yuan-lab-LLM/Yuan3.0.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†Yuan3.0 Flashï¼Œä¸€ä¸ªå¼€æºçš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œä¸“æ³¨äºæå‡ä¼ä¸šçº§ä»»åŠ¡ï¼ˆå¦‚æ£€ç´¢å¢å¼ºç”ŸæˆRAGã€å¤æ‚è¡¨æ ¼ç†è§£å’Œæ‘˜è¦ï¼‰çš„æ€§èƒ½ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°å‹RLè®­ç»ƒç®—æ³•RAPOä»¥è§£å†³å¤§æ¨ç†æ¨¡å‹ä¸­çš„è¿‡åº¦æ€è€ƒé—®é¢˜ã€‚æ¨¡å‹åœ¨ä¿æŒé€šç”¨ä»»åŠ¡ç«äº‰åŠ›çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†RAGç­‰åœºæ™¯çš„è¡¨ç°ï¼Œå¹¶å·²å¼€æºä¾›ç ”ç©¶å’Œéƒ¨ç½²ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2026-01-04
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2601.02504v1">Enhancing Debugging Skills with AI-Powered Assistance: A Real-Time Tool for Debugging Support</a></td><td><details><summary>å±•å¼€</summary>Debugging is a crucial skill in programming education and software development, yet it is often overlooked in CS curricula. To address this, we introduce an AI-powered debugging assistant integrated into an IDE. It offers real-time support by analyzing code, suggesting breakpoints, and providing contextual hints. Using RAG with LLMs, program slicing, and custom heuristics, it enhances efficiency by minimizing LLM calls and improving accuracy. A three-level evaluation - technical analysis, UX study, and classroom tests - highlights its potential for teaching debugging.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ä»‹ç»äº†ä¸€ç§é›†æˆåˆ°IDEä¸­çš„AIè°ƒè¯•åŠ©æ‰‹ï¼Œåˆ©ç”¨RAGæŠ€æœ¯ç»“åˆLLMsã€ç¨‹åºåˆ‡ç‰‡å’Œè‡ªå®šä¹‰å¯å‘å¼æ–¹æ³•ï¼Œé€šè¿‡å®æ—¶åˆ†æä»£ç ã€å»ºè®®æ–­ç‚¹å’Œæä¾›ä¸Šä¸‹æ–‡æç¤ºæ¥æå‡è°ƒè¯•æ•ˆç‡ï¼Œå¹¶åœ¨æŠ€æœ¯åˆ†æã€ç”¨æˆ·ä½“éªŒç ”ç©¶å’Œè¯¾å ‚æµ‹è¯•ä¸­éªŒè¯äº†å…¶æ•™å­¦æ½œåŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.02144v1">Routing by Analogy: kNN-Augmented Expert Assignment for Mixture-of-Experts</a></td><td><details><summary>å±•å¼€</summary>Mixture-of-Experts (MoE) architectures scale large language models efficiently by employing a parametric "router" to dispatch tokens to a sparse subset of experts. Typically, this router is trained once and then frozen, rendering routing decisions brittle under distribution shifts. We address this limitation by introducing kNN-MoE, a retrieval-augmented routing framework that reuses optimal expert assignments from a memory of similar past cases. This memory is constructed offline by directly optimizing token-wise routing logits to maximize the likelihood on a reference set. Crucially, we use the aggregate similarity of retrieved neighbors as a confidence-driven mixing coefficient, thus allowing the method to fall back to the frozen router when no relevant cases are found. Experiments show kNN-MoE outperforms zero-shot baselines and rivals computationally expensive supervised fine-tuning.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºkNN-MoEçš„æ£€ç´¢å¢å¼ºè·¯ç”±æ¡†æ¶ï¼Œé€šè¿‡ä»å†å²æ¡ˆä¾‹ä¸­æ£€ç´¢ç›¸ä¼¼çš„æœ€ä¼˜ä¸“å®¶åˆ†é…æ¥æ”¹è¿›æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰çš„è·¯ç”±å†³ç­–ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ£€ç´¢åˆ°çš„é‚»å±…çš„èšåˆç›¸ä¼¼åº¦ä½œä¸ºç½®ä¿¡åº¦é©±åŠ¨çš„æ··åˆç³»æ•°ï¼Œåœ¨æœªæ‰¾åˆ°ç›¸å…³æ¡ˆä¾‹æ—¶å›é€€åˆ°å†»ç»“çš„è·¯ç”±å™¨ï¼Œå®éªŒè¡¨æ˜å…¶æ€§èƒ½ä¼˜äºé›¶æ ·æœ¬åŸºçº¿å¹¶æ¥è¿‘è®¡ç®—æˆæœ¬é«˜çš„ç›‘ç£å¾®è°ƒã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.02065v1">Cost-Efficient Cross-Lingual Retrieval-Augmented Generation for Low-Resource Languages: A Case Study in Bengali Agricultural Advisory</a></td><td><details><summary>å±•å¼€</summary>Access to reliable agricultural advisory remains limited in many developing regions due to a persistent language barrier: authoritative agricultural manuals are predominantly written in English, while farmers primarily communicate in low-resource local languages such as Bengali. Although recent advances in Large Language Models (LLMs) enable natural language interaction, direct generation in low-resource languages often exhibits poor fluency and factual inconsistency, while cloud-based solutions remain cost-prohibitive. This paper presents a cost-efficient, cross-lingual Retrieval-Augmented Generation (RAG) framework for Bengali agricultural advisory that emphasizes factual grounding and practical deployability. The proposed system adopts a translation-centric architecture in which Bengali user queries are translated into English, enriched through domain-specific keyword injection to align colloquial farmer terminology with scientific nomenclature, and answered via dense vector retrieval over a curated corpus of English agricultural manuals (FAO, IRRI). The generated English response is subsequently translated back into Bengali to ensure accessibility. The system is implemented entirely using open-source models and operates on consumer-grade hardware without reliance on paid APIs. Experimental evaluation demonstrates reliable source-grounded responses, robust rejection of out-of-domain queries, and an average end-to-end latency below 20 seconds. The results indicate that cross-lingual retrieval combined with controlled translation offers a practical and scalable solution for agricultural knowledge access in low-resource language settings</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§é¢å‘å­ŸåŠ æ‹‰è¯­å†œä¸šå’¨è¯¢çš„è·¨è¯­è¨€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç¿»è¯‘ç”¨æˆ·æŸ¥è¯¢ã€æ³¨å…¥é¢†åŸŸå…³é”®è¯å¢å¼ºæ£€ç´¢å‡†ç¡®æ€§ï¼Œå¹¶åŸºäºè‹±æ–‡å†œä¸šæ‰‹å†Œç”Ÿæˆå›ç­”åå›è¯‘ï¼Œå®ç°äº†ä½æˆæœ¬ã€é«˜äº‹å®ä¸€è‡´æ€§çš„æœ¬åœ°åŒ–éƒ¨ç½²æ–¹æ¡ˆã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.02023v1">Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business.</details></td><td><details><summary>å±•å¼€</summary></details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.01896v2">Tackling the Inherent Difficulty of Noise Filtering in RAG</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has become a widely adopted approach to enhance Large Language Models (LLMs) by incorporating external knowledge and reducing hallucinations. However, noisy or irrelevant documents are often introduced during RAG, potentially degrading performance and even causing hallucinated outputs. While various methods have been proposed to filter out such noise, we argue that identifying irrelevant information from retrieved content is inherently difficult and limited number of transformer layers can hardly solve this. Consequently, retrievers fail to filter out irrelevant documents entirely. Therefore, LLMs must be robust against such noise, but we demonstrate that standard fine-tuning approaches are often ineffective in enabling the model to selectively utilize relevant information while ignoring irrelevant content due to the structural constraints of attention patterns. To address this, we propose a novel fine-tuning method designed to enhance the model's ability to distinguish between relevant and irrelevant information within retrieved documents. Extensive experiments across multiple benchmarks show that our approach significantly improves the robustness and performance of LLMs.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­å› æ£€ç´¢åˆ°æ— å…³æˆ–å™ªå£°æ–‡æ¡£å¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¾®è°ƒæ–¹æ³•ä»¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åŒºåˆ†å¹¶åˆ©ç”¨ç›¸å…³æ–‡æ¡£çš„èƒ½åŠ›ï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹çš„é²æ£’æ€§å’Œè¡¨ç°ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.01844v1">Clinical Knowledge Graph Construction and Evaluation with Multi-LLMs via Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) offer new opportunities for constructing knowledge graphs (KGs) from unstructured clinical narratives. However, existing approaches often rely on structured inputs and lack robust validation of factual accuracy and semantic consistency, limitations that are especially problematic in oncology. We introduce an end-to-end framework for clinical KG construction and evaluation directly from free text using multi-agent prompting and a schema-constrained Retrieval-Augmented Generation (KG-RAG) strategy. Our pipeline integrates (1) prompt-driven entity, attribute, and relation extraction; (2) entropy-based uncertainty scoring; (3) ontology-aligned RDF/OWL schema generation; and (4) multi-LLM consensus validation for hallucination detection and semantic refinement. Beyond static graph construction, the framework supports continuous refinement and self-supervised evaluation, enabling iterative improvement of graph quality. Applied to two oncology cohorts (PDAC and BRCA), our method produces interpretable, SPARQL-compatible, and clinically grounded knowledge graphs without relying on gold-standard annotations. Experimental results demonstrate consistent gains in precision, relevance, and ontology compliance over baseline methods.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“æç¤ºå’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆKG-RAGï¼‰ç­–ç•¥çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œç”¨äºç›´æ¥ä»ä¸´åºŠè‡ªç”±æ–‡æœ¬ä¸­æ„å»ºå’Œè¯„ä¼°çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ã€‚æ¡†æ¶æ•´åˆäº†å®ä½“ã€å±æ€§å’Œå…³ç³»æå–ã€ä¸ç¡®å®šæ€§è¯„åˆ†ã€æœ¬ä½“å¯¹é½çš„RDF/OWLæ¨¡å¼ç”Ÿæˆä»¥åŠå¤šLLMå…±è¯†éªŒè¯ï¼Œæ”¯æŒæŒç»­ä¼˜åŒ–å’Œè‡ªæˆ‘ç›‘ç£è¯„ä¼°ï¼Œå¹¶åœ¨è‚¿ç˜¤å­¦æ•°æ®ä¸­éªŒè¯äº†å…¶ä¼˜äºåŸºçº¿æ–¹æ³•çš„ç²¾ç¡®æ€§ã€ç›¸å…³æ€§å’Œæœ¬ä½“åˆè§„æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.01785v1">SRAS: A Lightweight Reinforcement Learning-based Document Selector for Edge-Native RAG Pipelines</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) systems often rely on fixed top-k document selection mechanisms that ignore downstream generation quality and impose computational overheads. We propose SRAS (Sparse Reward-Aware Selector), a lightweight document selector trained via reinforcement learning (RL) for edge-native RAG deployment. Unlike prior RL-based retrievers that assume large memory and latency budgets, SRAS learns a compact (~0.76MB) policy using Proximal Policy Optimization (PPO), guided by a hybrid reward signal combining Relaxed F1 and BERTScore. Our method operates under tight token and compute constraints, maintaining <1s latency on CPU. SRAS outperforms supervised and random selectors on a synthetic QA benchmark, and generalizes to real-world data, achieving BERTScore F1 of 0.8546 on SQuAD v2 without domain-specific tuning. This work is the first to demonstrate that RL-based document selection can be made ultra-lightweight, latency-aware, and effective for on-device RAG pipelines.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSRASçš„è½»é‡çº§æ–‡æ¡£é€‰æ‹©å™¨ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–RAGç³»ç»Ÿä¸­çš„æ–‡æ¡£æ£€ç´¢ç¯èŠ‚ï¼Œä½¿å…¶åœ¨è®¡ç®—èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šé«˜æ•ˆè¿è¡Œï¼ŒåŒæ—¶ä¿æŒé«˜å‡†ç¡®æ€§å’Œä½å»¶è¿Ÿã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.01718v1">Yuan3.0 Flash: An Open Multimodal Large Language Model for Enterprise Applications</a></td><td><details><summary>å±•å¼€</summary>We introduce Yuan3.0 Flash, an open-source Mixture-of-Experts (MoE) MultiModal Large Language Model featuring 3.7B activated parameters and 40B total parameters, specifically designed to enhance performance on enterprise-oriented tasks while maintaining competitive capabilities on general-purpose tasks. To address the overthinking phenomenon commonly observed in Large Reasoning Models (LRMs), we propose Reflection-aware Adaptive Policy Optimization (RAPO), a novel RL training algorithm that effectively regulates overthinking behaviors. In enterprise-oriented tasks such as retrieval-augmented generation (RAG), complex table understanding, and summarization, Yuan3.0 Flash consistently achieves superior performance. Moreover, it also demonstrates strong reasoning capabilities in domains such as mathematics, science, etc., attaining accuracy comparable to frontier model while requiring only approximately 1/4 to 1/2 of the average tokens. Yuan3.0 Flash has been fully open-sourced to facilitate further research and real-world deployment: https://github.com/Yuan-lab-LLM/Yuan3.0.</details></td><td><details><summary>å±•å¼€</summary></details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.02428v1">A Dynamic Retrieval-Augmented Generation System with Selective Memory and Remembrance</a></td><td><details><summary>å±•å¼€</summary>We introduce \emph{Adaptive RAG Memory} (ARM), a retrieval-augmented generation (RAG) framework that replaces a static vector index with a \emph{dynamic} memory substrate governed by selective remembrance and decay. Frequently retrieved items are consolidated and protected from forgetting, while rarely used items gradually decay, inspired by cognitive consolidation and forgetting principles. On a lightweight retrieval benchmark, ARM reaches near state-of-the-art performance (e.g., NDCG@5 $\approx$ 0.940, Recall@5 $=1.000$) with only $\sim$22M parameters in the embedding layer, achieving the best efficiency among ultra-efficient models ($<$25M parameters). In addition, we compare static vs. dynamic RAG combinations across Llama 3.1 and GPT-4o. Llama 3.1 with static RAG achieves the highest key-term coverage (67.2\%) at moderate latency, while GPT-4o with a dynamic selective retrieval policy attains the fastest responses (8.2s on average) with competitive coverage (58.7\%). We further present an engineering optimization of the DynamicRAG implementation, making embedding weights configurable, adjustable at runtime, and robust to invalid settings.
  ARM yields competitive accuracy, self-regularizing memory growth, and interpretable retention dynamics without retraining the generator\color{black} and provides practical trade-off between quality, latency and memory efficiency for production and research RAG system.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºARMï¼ˆAdaptive RAG Memoryï¼‰çš„åŠ¨æ€æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡é€‰æ‹©æ€§è®°å¿†å’Œé—å¿˜æœºåˆ¶ä¼˜åŒ–ä¼ ç»Ÿé™æ€å‘é‡ç´¢å¼•ï¼Œå®ç°é«˜æ•ˆæ£€ç´¢ä¸å†…å­˜ç®¡ç†ã€‚ARMåœ¨è½»é‡çº§æ£€ç´¢ä»»åŠ¡ä¸­æ¥è¿‘æœ€ä¼˜æ€§èƒ½ï¼ˆå¦‚NDCG@5â‰ˆ0.940ï¼‰ï¼Œå‚æ•°é‡ä»…22Mï¼Œå¹¶å¯¹æ¯”äº†é™æ€ä¸åŠ¨æ€RAGåœ¨Llama 3.1å’ŒGPT-4oä¸Šçš„è¡¨ç°ï¼Œå±•ç¤ºäº†è´¨é‡ã€å»¶è¿Ÿä¸å†…å­˜æ•ˆç‡çš„å¹³è¡¡ä¼˜åŒ–ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.01513v1">FastV-RAG: Towards Fast and Fine-Grained Video QA with Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Vision-Language Models (VLMs) excel at visual reasoning but still struggle with integrating external knowledge. Retrieval-Augmented Generation (RAG) is a promising solution, but current methods remain inefficient and often fail to maintain high answer quality. To address these challenges, we propose VideoSpeculateRAG, an efficient VLM-based RAG framework built on two key ideas. First, we introduce a speculative decoding pipeline: a lightweight draft model quickly generates multiple answer candidates, which are then verified and refined by a more accurate heavyweight model, substantially reducing inference latency without sacrificing correctness. Second, we identify a major source of error - incorrect entity recognition in retrieved knowledge - and mitigate it with a simple yet effective similarity-based filtering strategy that improves entity alignment and boosts overall answer accuracy. Experiments demonstrate that VideoSpeculateRAG achieves comparable or higher accuracy than standard RAG approaches while accelerating inference by approximately 2x. Our framework highlights the potential of combining speculative decoding with retrieval-augmented reasoning to enhance efficiency and reliability in complex, knowledge-intensive multimodal tasks.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºVideoSpeculateRAGçš„é«˜æ•ˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆæ¨æµ‹è§£ç ï¼ˆè½»é‡çº§è‰ç¨¿æ¨¡å‹ç”Ÿæˆå€™é€‰ç­”æ¡ˆï¼Œå†ç”±é«˜ç²¾åº¦æ¨¡å‹éªŒè¯ä¼˜åŒ–ï¼‰å’ŒåŸºäºç›¸ä¼¼æ€§çš„å®ä½“è¿‡æ»¤ç­–ç•¥ï¼Œè§£å†³äº†ç°æœ‰RAGæ–¹æ³•æ•ˆç‡ä½å’Œå®ä½“è¯†åˆ«é”™è¯¯çš„é—®é¢˜ï¼Œåœ¨ä¿æŒæˆ–æå‡ç­”æ¡ˆå‡†ç¡®æ€§çš„åŒæ—¶å°†æ¨ç†é€Ÿåº¦æå‡çº¦2å€ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.01341v1">Reasoning Over Recall: Evaluating the Efficacy of Generalist Architectures vs. Specialized Fine-Tunes in RAG-Based Mental Health Dialogue Systems</a></td><td><details><summary>å±•å¼€</summary>The deployment of Large Language Models (LLMs) in mental health counseling faces the dual challenges of hallucinations and lack of empathy. While the former may be mitigated by RAG (retrieval-augmented generation) by anchoring answers in trusted clinical sources, there remains an open question as to whether the most effective model under this paradigm would be one that is fine-tuned on mental health data, or a more general and powerful model that succeeds purely on the basis of reasoning. In this paper, we perform a direct comparison by running four open-source models through the same RAG pipeline using ChromaDB: two generalist reasoners (Qwen2.5-3B and Phi-3-Mini) and two domain-specific fine-tunes (MentalHealthBot-7B and TherapyBot-7B). We use an LLM-as-a-Judge framework to automate evaluation over 50 turns. We find a clear trend: the generalist models outperform the domain-specific ones in empathy (3.72 vs. 3.26, $p < 0.001$) in spite of being much smaller (3B vs. 7B), and all models perform well in terms of safety, but the generalist models show better contextual understanding and are less prone to overfitting as we observe in the domain-specific models. Overall, our results indicate that for RAG-based therapy systems, strong reasoning is more important than training on mental health-specific vocabulary; i.e. a well-reasoned general model would provide more empathetic and balanced support than a larger narrowly fine-tuned model, so long as the answer is already grounded in clinical evidence.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æ¢è®¨äº†åœ¨å¿ƒç†å¥åº·å’¨è¯¢ä¸­ä½¿ç”¨RAGæŠ€æœ¯ç¼“è§£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¹»è§‰é—®é¢˜çš„æ•ˆæœï¼Œé€šè¿‡å¯¹æ¯”é€šç”¨æ¨¡å‹ä¸é¢†åŸŸå¾®è°ƒæ¨¡å‹åœ¨RAGæ¡†æ¶ä¸‹çš„è¡¨ç°ï¼Œå‘ç°é€šç”¨æ¨¡å‹ï¼ˆå°½ç®¡å‚æ•°é‡æ›´å°ï¼‰åœ¨å…±æƒ…èƒ½åŠ›å’Œä¸Šä¸‹æ–‡ç†è§£ä¸Šä¼˜äºä¸“ä¸šå¾®è°ƒæ¨¡å‹ï¼Œå¼ºè°ƒåŸºäºä¸´åºŠè¯æ®æ—¶ï¼Œå¼ºæ¨ç†èƒ½åŠ›æ¯”é¢†åŸŸç‰¹å®šè®­ç»ƒæ›´é‡è¦ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2026-01-03
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2601.02428v1">A Dynamic Retrieval-Augmented Generation System with Selective Memory and Remembrance</a></td><td><details><summary>å±•å¼€</summary>We introduce \emph{Adaptive RAG Memory} (ARM), a retrieval-augmented generation (RAG) framework that replaces a static vector index with a \emph{dynamic} memory substrate governed by selective remembrance and decay. Frequently retrieved items are consolidated and protected from forgetting, while rarely used items gradually decay, inspired by cognitive consolidation and forgetting principles. On a lightweight retrieval benchmark, ARM reaches near state-of-the-art performance (e.g., NDCG@5 $\approx$ 0.940, Recall@5 $=1.000$) with only $\sim$22M parameters in the embedding layer, achieving the best efficiency among ultra-efficient models ($<$25M parameters). In addition, we compare static vs. dynamic RAG combinations across Llama 3.1 and GPT-4o. Llama 3.1 with static RAG achieves the highest key-term coverage (67.2\%) at moderate latency, while GPT-4o with a dynamic selective retrieval policy attains the fastest responses (8.2s on average) with competitive coverage (58.7\%). We further present an engineering optimization of the DynamicRAG implementation, making embedding weights configurable, adjustable at runtime, and robust to invalid settings.
  ARM yields competitive accuracy, self-regularizing memory growth, and interpretable retention dynamics without retraining the generator\color{black} and provides practical trade-off between quality, latency and memory efficiency for production and research RAG system.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºARMï¼ˆAdaptive RAG Memoryï¼‰çš„åŠ¨æ€æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡é€‰æ‹©æ€§è®°å¿†å’Œé—å¿˜æœºåˆ¶ä¼˜åŒ–RAGç³»ç»Ÿçš„è®°å¿†å­˜å‚¨ä¸æ£€ç´¢æ•ˆç‡ï¼Œåœ¨è½»é‡çº§åŸºå‡†æµ‹è¯•ä¸­å®ç°é«˜æ•ˆæ€§èƒ½ï¼ˆå¦‚NDCG@5â‰ˆ0.940ï¼‰ï¼Œå¹¶å¯¹æ¯”äº†é™æ€ä¸åŠ¨æ€RAGç»„åˆåœ¨ä¸åŒæ¨¡å‹ï¼ˆLlama 3.1ã€GPT-4oï¼‰ä¸­çš„è¡¨ç°ï¼ŒåŒæ—¶æå‡ºå·¥ç¨‹ä¼˜åŒ–ä»¥å¹³è¡¡è´¨é‡ã€å»¶è¿Ÿå’Œå†…å­˜æ•ˆç‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.01513v1">FastV-RAG: Towards Fast and Fine-Grained Video QA with Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Vision-Language Models (VLMs) excel at visual reasoning but still struggle with integrating external knowledge. Retrieval-Augmented Generation (RAG) is a promising solution, but current methods remain inefficient and often fail to maintain high answer quality. To address these challenges, we propose VideoSpeculateRAG, an efficient VLM-based RAG framework built on two key ideas. First, we introduce a speculative decoding pipeline: a lightweight draft model quickly generates multiple answer candidates, which are then verified and refined by a more accurate heavyweight model, substantially reducing inference latency without sacrificing correctness. Second, we identify a major source of error - incorrect entity recognition in retrieved knowledge - and mitigate it with a simple yet effective similarity-based filtering strategy that improves entity alignment and boosts overall answer accuracy. Experiments demonstrate that VideoSpeculateRAG achieves comparable or higher accuracy than standard RAG approaches while accelerating inference by approximately 2x. Our framework highlights the potential of combining speculative decoding with retrieval-augmented reasoning to enhance efficiency and reliability in complex, knowledge-intensive multimodal tasks.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºVideoSpeculateRAGçš„é«˜æ•ˆVLM-based RAGæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆæ¨æµ‹è§£ç ï¼ˆè½»é‡çº§è‰æ¡ˆç”Ÿæˆå€™é€‰ç­”æ¡ˆ+é‡é‡çº§æ¨¡å‹éªŒè¯ï¼‰å’ŒåŸºäºç›¸ä¼¼æ€§çš„å®ä½“è¿‡æ»¤ç­–ç•¥ï¼Œè§£å†³äº†ç°æœ‰RAGæ–¹æ³•æ•ˆç‡ä½å’Œå®ä½“è¯†åˆ«é”™è¯¯çš„é—®é¢˜ï¼Œåœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶åŠ é€Ÿæ¨ç†çº¦2å€ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.01341v1">Reasoning Over Recall: Evaluating the Efficacy of Generalist Architectures vs. Specialized Fine-Tunes in RAG-Based Mental Health Dialogue Systems</a></td><td><details><summary>å±•å¼€</summary>The deployment of Large Language Models (LLMs) in mental health counseling faces the dual challenges of hallucinations and lack of empathy. While the former may be mitigated by RAG (retrieval-augmented generation) by anchoring answers in trusted clinical sources, there remains an open question as to whether the most effective model under this paradigm would be one that is fine-tuned on mental health data, or a more general and powerful model that succeeds purely on the basis of reasoning. In this paper, we perform a direct comparison by running four open-source models through the same RAG pipeline using ChromaDB: two generalist reasoners (Qwen2.5-3B and Phi-3-Mini) and two domain-specific fine-tunes (MentalHealthBot-7B and TherapyBot-7B). We use an LLM-as-a-Judge framework to automate evaluation over 50 turns. We find a clear trend: the generalist models outperform the domain-specific ones in empathy (3.72 vs. 3.26, $p < 0.001$) in spite of being much smaller (3B vs. 7B), and all models perform well in terms of safety, but the generalist models show better contextual understanding and are less prone to overfitting as we observe in the domain-specific models. Overall, our results indicate that for RAG-based therapy systems, strong reasoning is more important than training on mental health-specific vocabulary; i.e. a well-reasoned general model would provide more empathetic and balanced support than a larger narrowly fine-tuned model, so long as the answer is already grounded in clinical evidence.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åœ¨å¿ƒç†å¥åº·å’¨è¯¢ä¸­ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ—¶é¢ä¸´çš„å¹»è§‰å’Œç¼ºä¹å…±æƒ…é—®é¢˜ï¼Œé€šè¿‡RAGæŠ€æœ¯å°†ç­”æ¡ˆé”šå®šåœ¨å¯ä¿¡çš„ä¸´åºŠèµ„æ–™ä¸­æ¥ç¼“è§£å¹»è§‰ã€‚ç ”ç©¶æ¯”è¾ƒäº†å››ç§å¼€æºæ¨¡å‹ï¼ˆä¸¤ç§é€šç”¨æ¨ç†æ¨¡å‹å’Œä¸¤ç§é’ˆå¯¹å¿ƒç†å¥åº·é¢†åŸŸå¾®è°ƒçš„æ¨¡å‹ï¼‰åœ¨ç›¸åŒRAGæµç¨‹ä¸­çš„è¡¨ç°ï¼Œå‘ç°é€šç”¨æ¨¡å‹åœ¨å…±æƒ…èƒ½åŠ›ä¸Šä¼˜äºé¢†åŸŸä¸“ç”¨æ¨¡å‹ï¼Œä¸”æ¨ç†èƒ½åŠ›æ¯”é¢†åŸŸç‰¹å®šè¯æ±‡è®­ç»ƒæ›´é‡è¦ï¼Œåªè¦ç­”æ¡ˆåŸºäºä¸´åºŠè¯æ®ï¼Œé€šç”¨æ¨¡å‹èƒ½æä¾›æ›´ empathetic å’Œå¹³è¡¡çš„æ”¯æŒã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.01118v1">ScienceDB AI: An LLM-Driven Agentic Recommender System for Large-Scale Scientific Data Sharing Services</a></td><td><details><summary>å±•å¼€</summary>The rapid growth of AI for Science (AI4S) has underscored the significance of scientific datasets, leading to the establishment of numerous national scientific data centers and sharing platforms. Despite this progress, efficiently promoting dataset sharing and utilization for scientific research remains challenging. Scientific datasets contain intricate domain-specific knowledge and contexts, rendering traditional collaborative filtering-based recommenders inadequate. Recent advances in Large Language Models (LLMs) offer unprecedented opportunities to build conversational agents capable of deep semantic understanding and personalized recommendations. In response, we present ScienceDB AI, a novel LLM-driven agentic recommender system developed on Science Data Bank (ScienceDB), one of the largest global scientific data-sharing platforms. ScienceDB AI leverages natural language conversations and deep reasoning to accurately recommend datasets aligned with researchers' scientific intents and evolving requirements. The system introduces several innovations: a Scientific Intention Perceptor to extract structured experimental elements from complicated queries, a Structured Memory Compressor to manage multi-turn dialogues effectively, and a Trustworthy Retrieval-Augmented Generation (Trustworthy RAG) framework. The Trustworthy RAG employs a two-stage retrieval mechanism and provides citable dataset references via Citable Scientific Task Record (CSTR) identifiers, enhancing recommendation trustworthiness and reproducibility. Through extensive offline and online experiments using over 10 million real-world datasets, ScienceDB AI has demonstrated significant effectiveness. To our knowledge, ScienceDB AI is the first LLM-driven conversational recommender tailored explicitly for large-scale scientific dataset sharing services. The platform is publicly accessible at: https://ai.scidb.cn/en.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ScienceDB AIï¼Œä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™ºèƒ½æ¨èç³»ç»Ÿï¼Œä¸“ä¸ºç§‘å­¦æ•°æ®å…±äº«å¹³å°è®¾è®¡ã€‚è¯¥ç³»ç»Ÿé€šè¿‡è‡ªç„¶è¯­è¨€å¯¹è¯å’Œæ·±åº¦æ¨ç†æ¨èç¬¦åˆç ”ç©¶è€…éœ€æ±‚çš„ç§‘å­¦æ•°æ®é›†ï¼Œå¹¶åˆ›æ–°æ€§åœ°æå‡ºäº†â€œå¯ä¿¡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆTrustworthy RAGï¼‰â€æ¡†æ¶ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µæ£€ç´¢æœºåˆ¶å’Œå¯å¼•ç”¨çš„æ•°æ®é›†æ ‡è¯†ï¼ˆCSTRï¼‰ä»¥æé«˜æ¨èçš„å¯é æ€§å’Œå¯å¤ç°æ€§ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2026-01-02
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2601.01118v1">ScienceDB AI: An LLM-Driven Agentic Recommender System for Large-Scale Scientific Data Sharing Services</a></td><td><details><summary>å±•å¼€</summary>The rapid growth of AI for Science (AI4S) has underscored the significance of scientific datasets, leading to the establishment of numerous national scientific data centers and sharing platforms. Despite this progress, efficiently promoting dataset sharing and utilization for scientific research remains challenging. Scientific datasets contain intricate domain-specific knowledge and contexts, rendering traditional collaborative filtering-based recommenders inadequate. Recent advances in Large Language Models (LLMs) offer unprecedented opportunities to build conversational agents capable of deep semantic understanding and personalized recommendations. In response, we present ScienceDB AI, a novel LLM-driven agentic recommender system developed on Science Data Bank (ScienceDB), one of the largest global scientific data-sharing platforms. ScienceDB AI leverages natural language conversations and deep reasoning to accurately recommend datasets aligned with researchers' scientific intents and evolving requirements. The system introduces several innovations: a Scientific Intention Perceptor to extract structured experimental elements from complicated queries, a Structured Memory Compressor to manage multi-turn dialogues effectively, and a Trustworthy Retrieval-Augmented Generation (Trustworthy RAG) framework. The Trustworthy RAG employs a two-stage retrieval mechanism and provides citable dataset references via Citable Scientific Task Record (CSTR) identifiers, enhancing recommendation trustworthiness and reproducibility. Through extensive offline and online experiments using over 10 million real-world datasets, ScienceDB AI has demonstrated significant effectiveness. To our knowledge, ScienceDB AI is the first LLM-driven conversational recommender tailored explicitly for large-scale scientific dataset sharing services. The platform is publicly accessible at: https://ai.scidb.cn/en.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºScienceDB AIçš„æ–°å‹LLMé©±åŠ¨çš„æ¨èç³»ç»Ÿï¼Œè¯¥ç³»ç»ŸåŸºäºå…¨çƒæœ€å¤§çš„ç§‘å­¦æ•°æ®å…±äº«å¹³å°ä¹‹ä¸€Science Data Bankï¼ˆScienceDBï¼‰ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨è‡ªç„¶è¯­è¨€å¯¹è¯å’Œæ·±åº¦æ¨ç†æ¥æ¨èç¬¦åˆç ”ç©¶äººå‘˜ç§‘å­¦æ„å›¾å’Œéœ€æ±‚çš„æ•°æ®é›†ã€‚æ–‡ç« é‡ç‚¹æå‡ºäº†ä¸€ä¸ªâ€œå¯ä¿¡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆTrustworthy RAGï¼‰â€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µæ£€ç´¢æœºåˆ¶ï¼Œå¹¶é€šè¿‡å¯å¼•ç”¨çš„ç§‘å­¦ä»»åŠ¡è®°å½•ï¼ˆCSTRï¼‰æ ‡è¯†ç¬¦æä¾›å¯ä¿¡çš„å‚è€ƒæ–‡çŒ®ï¼Œå¢å¼ºäº†æ¨èçš„å¯é æ€§å’Œå¯é‡å¤æ€§ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿè¿˜åŒ…å«ç§‘å­¦æ„å›¾æ„ŸçŸ¥å™¨å’Œç»“æ„åŒ–è®°å¿†å‹ç¼©å™¨ç­‰åˆ›æ–°ç»„ä»¶ã€‚å®éªŒè¡¨æ˜ï¼ŒScienceDB AIåœ¨å¤„ç†å¤§è§„æ¨¡ç§‘å­¦æ•°æ®é›†æ¨èä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.00536v1">Retrieval--Reasoning Processes for Multi-hop Question Answering: A Four-Axis Design Framework and Empirical Trends</a></td><td><details><summary>å±•å¼€</summary>Multi-hop question answering (QA) requires systems to iteratively retrieve evidence and reason across multiple hops. While recent RAG and agentic methods report strong results, the underlying retrieval--reasoning \emph{process} is often left implicit, making procedural choices hard to compare across model families. This survey takes the execution procedure as the unit of analysis and introduces a four-axis framework covering (A) overall execution plan, (B) index structure, (C) next-step control (strategies and triggers), and (D) stop/continue criteria. Using this schema, we map representative multi-hop QA systems and synthesize reported ablations and tendencies on standard benchmarks (e.g., HotpotQA, 2WikiMultiHopQA, MuSiQue), highlighting recurring trade-offs among effectiveness, efficiency, and evidence faithfulness. We conclude with open challenges for retrieval--reasoning agents, including structure-aware planning, transferable control policies, and robust stopping under distribution shift.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤šè·³é—®ç­”ï¼ˆQAï¼‰ç³»ç»Ÿä¸­æ£€ç´¢ä¸æ¨ç†çš„æ˜¾å¼æ‰§è¡Œè¿‡ç¨‹ï¼Œæå‡ºäº†ä¸€ä¸ªå››è½´æ¡†æ¶ï¼ˆæ‰§è¡Œè®¡åˆ’ã€ç´¢å¼•ç»“æ„ã€ä¸‹ä¸€æ­¥æ§åˆ¶ã€åœæ­¢/ç»§ç»­æ ‡å‡†ï¼‰æ¥åˆ†æä¸åŒæ¨¡å‹çš„æ£€ç´¢-æ¨ç†æµç¨‹ï¼Œå¹¶å¯¹æ¯”äº†RAGå’ŒåŸºäºæ™ºèƒ½ä½“çš„æ–¹æ³•åœ¨æ•ˆç‡ã€æ•ˆæœå’Œè¯æ®å¯é æ€§ä¸Šçš„æƒè¡¡ï¼Œæœ€åæŒ‡å‡ºäº†è¯¥é¢†åŸŸé¢ä¸´çš„å¼€æ”¾æŒ‘æˆ˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.00526v1">Federated Customization of Large Models: Approaches, Experiments, and Insights</a></td><td><details><summary>å±•å¼€</summary>In this article, we explore federated customization of large models and highlight the key challenges it poses within the federated learning framework. We review several popular large model customization techniques, including full fine-tuning, efficient fine-tuning, prompt engineering, prefix-tuning, knowledge distillation, and retrieval-augmented generation. Then, we discuss how these techniques can be implemented within the federated learning framework. Moreover, we conduct experiments on federated prefix-tuning, which, to the best of our knowledge, is the first trial to apply prefix-tuning in the federated learning setting. The conducted experiments validate its feasibility with performance close to centralized approaches. Further comparison with three other federated customization methods demonstrated its competitive performance, satisfactory efficiency, and consistent robustness.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åœ¨è”é‚¦å­¦ä¹ æ¡†æ¶ä¸‹å¯¹å¤§æ¨¡å‹è¿›è¡Œå®šåˆ¶åŒ–çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­‰å¤šç§æŠ€æœ¯ï¼Œå¹¶é‡ç‚¹ç ”ç©¶äº†è”é‚¦å‰ç¼€è°ƒä¼˜çš„å¯è¡Œæ€§ä¸æ€§èƒ½ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨è”é‚¦å­¦ä¹ ç¯å¢ƒä¸­å…·æœ‰æ¥è¿‘é›†ä¸­å¼æ–¹æ³•çš„æ€§èƒ½å’Œç«äº‰åŠ›ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2026-01-01
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2601.00536v1">Retrieval--Reasoning Processes for Multi-hop Question Answering: A Four-Axis Design Framework and Empirical Trends</a></td><td><details><summary>å±•å¼€</summary>Multi-hop question answering (QA) requires systems to iteratively retrieve evidence and reason across multiple hops. While recent RAG and agentic methods report strong results, the underlying retrieval--reasoning \emph{process} is often left implicit, making procedural choices hard to compare across model families. This survey takes the execution procedure as the unit of analysis and introduces a four-axis framework covering (A) overall execution plan, (B) index structure, (C) next-step control (strategies and triggers), and (D) stop/continue criteria. Using this schema, we map representative multi-hop QA systems and synthesize reported ablations and tendencies on standard benchmarks (e.g., HotpotQA, 2WikiMultiHopQA, MuSiQue), highlighting recurring trade-offs among effectiveness, efficiency, and evidence faithfulness. We conclude with open challenges for retrieval--reasoning agents, including structure-aware planning, transferable control policies, and robust stopping under distribution shift.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ˜¯ä¸€ç¯‡å…³äºå¤šè·³é—®ç­”ç³»ç»Ÿçš„ç»¼è¿°æ€§æ–‡ç« ï¼Œå®ƒä»¥æ‰§è¡Œè¿‡ç¨‹ä¸ºåˆ†æå•å…ƒï¼Œæå‡ºäº†ä¸€ä¸ªåŒ…å«å››ä¸ªç»´åº¦ï¼ˆæ€»ä½“æ‰§è¡Œè®¡åˆ’ã€ç´¢å¼•ç»“æ„ã€ä¸‹ä¸€æ­¥æ§åˆ¶ã€åœæ­¢/ç»§ç»­æ ‡å‡†ï¼‰çš„æ¡†æ¶ï¼Œç”¨äºç³»ç»Ÿæ€§åœ°æ¯”è¾ƒå’Œåˆ†æRAGç­‰æ£€ç´¢æ¨ç†æ–¹æ³•çš„éšå¼è¿‡ç¨‹ï¼Œå¹¶æ€»ç»“äº†åœ¨æ•ˆç‡ã€æ•ˆæœå’Œè¯æ®å¯é æ€§ä¹‹é—´çš„æƒè¡¡åŠé¢ä¸´çš„æŒ‘æˆ˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.00526v1">Federated Customization of Large Models: Approaches, Experiments, and Insights</a></td><td><details><summary>å±•å¼€</summary>In this article, we explore federated customization of large models and highlight the key challenges it poses within the federated learning framework. We review several popular large model customization techniques, including full fine-tuning, efficient fine-tuning, prompt engineering, prefix-tuning, knowledge distillation, and retrieval-augmented generation. Then, we discuss how these techniques can be implemented within the federated learning framework. Moreover, we conduct experiments on federated prefix-tuning, which, to the best of our knowledge, is the first trial to apply prefix-tuning in the federated learning setting. The conducted experiments validate its feasibility with performance close to centralized approaches. Further comparison with three other federated customization methods demonstrated its competitive performance, satisfactory efficiency, and consistent robustness.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æ¢è®¨äº†è”é‚¦å­¦ä¹ æ¡†æ¶ä¸‹å¤§å‹æ¨¡å‹çš„å®šåˆ¶åŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­‰å¤šç§æ–¹æ³•ï¼Œå¹¶é‡ç‚¹ç ”ç©¶äº†è”é‚¦å‰ç¼€è°ƒä¼˜çš„å¯è¡Œæ€§ï¼Œå®éªŒè¡¨æ˜å…¶æ€§èƒ½æ¥è¿‘é›†ä¸­å¼æ–¹æ³•ä¸”å…·æœ‰ç«äº‰åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.00513v1">When Small Models Are Right for Wrong Reasons: Process Verification for Trustworthy Agents</a></td><td><details><summary>å±•å¼€</summary>Deploying small language models (7-9B parameters) as autonomous agents requires trust in their reasoning, not just their outputs. We reveal a critical reliability crisis: 50-69\% of correct answers from these models contain fundamentally flawed reasoning -- a ``Right-for-Wrong-Reasons'' phenomenon invisible to standard accuracy metrics. Through analysis of 10,734 reasoning traces across three models and diverse tasks, we introduce the Reasoning Integrity Score (RIS), a process-based metric validated with substantial inter-rater agreement ($Îº=0.657$). Conventional practices are challenged by our findings: while retrieval-augmented generation (RAG) significantly improves reasoning integrity (Cohen's $d=0.23$--$0.93$), meta-cognitive interventions like self-critique often harm performance ($d=-0.14$ to $-0.33$) in small models on the evaluated tasks. Mechanistic analysis reveals RAG succeeds by grounding calculations in external evidence, reducing errors by 7.6\%, while meta-cognition amplifies confusion without sufficient model capacity. To enable deployment, verification capabilities are distilled into a neural classifier achieving 0.86 F1-score with 100$\times$ speedup. These results underscore the necessity of process-based verification for trustworthy agents: accuracy alone is dangerously insufficient when models can be right for entirely wrong reasons.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ­ç¤ºäº†å°å‹è¯­è¨€æ¨¡å‹ï¼ˆ7-9Bå‚æ•°ï¼‰åœ¨ä½œä¸ºè‡ªä¸»ä»£ç†æ—¶å­˜åœ¨çš„â€œæ­£ç¡®ç­”æ¡ˆä½†é”™è¯¯æ¨ç†â€ç°è±¡ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºè¿‡ç¨‹çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆRISï¼‰ã€‚ç ”ç©¶å‘ç°ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰èƒ½æ˜¾è‘—æå‡æ¨ç†å®Œæ•´æ€§ï¼ˆå‡å°‘7.6%é”™è¯¯ï¼‰ï¼Œè€Œå…ƒè®¤çŸ¥å¹²é¢„ï¼ˆå¦‚è‡ªæˆ‘æ‰¹åˆ¤ï¼‰å¯èƒ½æŸå®³æ€§èƒ½ã€‚è®ºæ–‡è¿˜å¼€å‘äº†ä¸€ä¸ªå¿«é€Ÿç¥ç»åˆ†ç±»å™¨ç”¨äºéªŒè¯æ¨ç†è¿‡ç¨‹ï¼Œå¼ºè°ƒåŸºäºè¿‡ç¨‹çš„éªŒè¯å¯¹å¯ä¿¡ä»£ç†çš„å¿…è¦æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.00509v1">Improving LLM-Assisted Secure Code Generation through Retrieval-Augmented-Generation and Multi-Tool Feedback</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) can generate code but often introduce security vulnerabilities, logical inconsistencies, and compilation errors. Prior work demonstrates that LLMs benefit substantially from structured feedback, static analysis, retrieval augmentation, and execution-based refinement. We propose a retrieval-augmented, multi-tool repair workflow in which a single code-generating LLM iteratively refines its outputs using compiler diagnostics, CodeQL security scanning, and KLEE symbolic execution. A lightweight embedding model is used for semantic retrieval of previously successful repairs, providing security-focused examples that guide generation. Evaluated on a combined dataset of 3,242 programs generated by DeepSeek-Coder-1.3B and CodeLlama-7B, the system demonstrates significant improvements in robustness. For DeepSeek, security vulnerabilities were reduced by 96%. For the larger CodeLlama model, the critical security defect rate was decreased from 58.55% to 22.19%, highlighting the efficacy of tool-assisted self-repair even on "stubborn" models.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ£€ç´¢å¢å¼ºçš„å¤šå·¥å…·ä¿®å¤å·¥ä½œæµï¼Œé€šè¿‡æ•´åˆç¼–è¯‘å™¨è¯Šæ–­ã€å®‰å…¨æ‰«æå’Œç¬¦å·æ‰§è¡Œç­‰å·¥å…·ï¼Œå¹¶åˆ©ç”¨åµŒå…¥æ¨¡å‹è¿›è¡Œè¯­ä¹‰æ£€ç´¢ä»¥è·å–æˆåŠŸçš„ä¿®å¤æ¡ˆä¾‹ï¼Œæ¥è¿­ä»£ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ä»£ç ï¼Œæ˜¾è‘—æå‡äº†ä»£ç çš„å®‰å…¨æ€§å’Œé²æ£’æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.00926v1">MACA: A Framework for Distilling Trustworthy LLMs into Efficient Retrievers</a></td><td><details><summary>å±•å¼€</summary>Modern enterprise retrieval systems must handle short, underspecified queries such as ``foreign transaction fee refund'' and ``recent check status''. In these cases, semantic nuance and metadata matter but per-query large language model (LLM) re-ranking and manual labeling are costly. We present Metadata-Aware Cross-Model Alignment (MACA), which distills a calibrated metadata aware LLM re-ranker into a compact student retriever, avoiding online LLM calls. A metadata-aware prompt verifies the teacher's trustworthiness by checking consistency under permutations and robustness to paraphrases, then supplies listwise scores, hard negatives, and calibrated relevance margins. The student trains with MACA's MetaFusion objective, which combines a metadata conditioned ranking loss with a cross model margin loss so it learns to push the correct answer above semantically similar candidates with mismatched topic, sub-topic, or entity. On a proprietary consumer banking FAQ corpus and BankFAQs, the MACA teacher surpasses a MAFA baseline at Accuracy@1 by five points on the proprietary set and three points on BankFAQs. MACA students substantially outperform pretrained encoders; e.g., on the proprietary corpus MiniLM Accuracy@1 improves from 0.23 to 0.48, while keeping inference free of LLM calls and supporting retrieval-augmented generation.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMACAï¼ˆMetadata-Aware Cross-Model Alignmentï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡å°†ç»è¿‡æ ¡å‡†çš„å…ƒæ•°æ®æ„ŸçŸ¥LLMé‡æ–°æ’åºå™¨æç‚¼ä¸ºç´§å‡‘çš„å­¦ç”Ÿæ£€ç´¢å™¨ï¼Œé¿å…äº†åœ¨çº¿LLMè°ƒç”¨ï¼Œä»è€Œæå‡äº†æ£€ç´¢ç³»ç»Ÿçš„æ€§èƒ½ã€‚MACAç»“åˆå…ƒæ•°æ®æ„ŸçŸ¥æç¤ºå’Œè·¨æ¨¡å‹å¯¹é½æŠ€æœ¯ï¼Œä¼˜åŒ–äº†æ£€ç´¢ç»“æœï¼Œå°¤å…¶åœ¨å¤„ç†çŸ­ä¸”ä¸æ˜ç¡®çš„æŸ¥è¯¢æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶æ”¯æŒæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åº”ç”¨ã€‚å®éªŒè¡¨æ˜ï¼ŒMACAåœ¨ä¸“æœ‰å’Œå…¬å¼€æ•°æ®é›†ä¸Šå‡æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.00376v1">In Line with Context: Repository-Level Code Generation via Context Inlining</a></td><td><details><summary>å±•å¼€</summary>Repository-level code generation has attracted growing attention in recent years. Unlike function-level code generation, it requires the model to understand the entire repository, reasoning over complex dependencies across functions, classes, and modules. However, existing approaches such as retrieval-augmented generation (RAG) or context-based function selection often fall short: they primarily rely on surface-level similarity and struggle to capture the rich dependencies that govern repository-level semantics. In this paper, we introduce InlineCoder, a novel framework for repository-level code generation. InlineCoder enhances the understanding of repository context by inlining the unfinished function into its call graph, thereby reframing the challenging repository understanding as an easier function-level coding task. Given a function signature, InlineCoder first generates a draft completion, termed an anchor, which approximates downstream dependencies and enables perplexity-based confidence estimation. This anchor drives a bidirectional inlining process: (i) Upstream Inlining, which embeds the anchor into its callers to capture diverse usage scenarios; and (ii) Downstream Retrieval, which integrates the anchor's callees into the prompt to provide precise dependency context. The enriched context, combining draft completion with upstream and downstream perspectives, equips the LLM with a comprehensive repository view.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºInlineCoderçš„æ–°æ¡†æ¶ï¼Œç”¨äºè§£å†³ä»“åº“çº§åˆ«ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­çš„ä¾èµ–ç†è§£é—®é¢˜ã€‚å°½ç®¡ä¼ ç»Ÿæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•åœ¨ä»“åº“çº§ä»£ç ç”Ÿæˆä¸­å­˜åœ¨å±€é™æ€§ï¼ˆä»…ä¾èµ–è¡¨å±‚ç›¸ä¼¼æ€§ï¼‰ï¼Œä½†æœ¬æ–‡é€šè¿‡å°†æœªå®Œæˆå‡½æ•°å†…è”åˆ°å…¶è°ƒç”¨å›¾ä¸­ï¼Œç»“åˆé”šç‚¹ç”Ÿæˆã€ä¸Šæ¸¸å†…è”å’Œä¸‹æ¸¸æ£€ç´¢ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹ä»“åº“ä¸Šä¸‹æ–‡çš„ç†è§£èƒ½åŠ›ï¼Œå±äºå¯¹RAGæ–¹æ³•åœ¨ç‰¹å®šåœºæ™¯ä¸‹çš„æ”¹è¿›ä¸æ‰©å±•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.00254v1">An Empirical Evaluation of LLM-Based Approaches for Code Vulnerability Detection: RAG, SFT, and Dual-Agent Systems</a></td><td><details><summary>å±•å¼€</summary>The rapid advancement of Large Language Models (LLMs) presents new opportunities for automated software vulnerability detection, a crucial task in securing modern codebases. This paper presents a comparative study on the effectiveness of LLM-based techniques for detecting software vulnerabilities. The study evaluates three approaches, Retrieval-Augmented Generation (RAG), Supervised Fine-Tuning (SFT), and a Dual-Agent LLM framework, against a baseline LLM model. A curated dataset was compiled from Big-Vul and real-world code repositories from GitHub, focusing on five critical Common Weakness Enumeration (CWE) categories: CWE-119, CWE-399, CWE-264, CWE-20, and CWE-200. Our RAG approach, which integrated external domain knowledge from the internet and the MITRE CWE database, achieved the highest overall accuracy (0.86) and F1 score (0.85), highlighting the value of contextual augmentation. Our SFT approach, implemented using parameter-efficient QLoRA adapters, also demonstrated strong performance. Our Dual-Agent system, an architecture in which a secondary agent audits and refines the output of the first, showed promise in improving reasoning transparency and error mitigation, with reduced resource overhead. These results emphasize that incorporating a domain expertise mechanism significantly strengthens the practical applicability of LLMs in real-world vulnerability detection tasks.</details></td><td><details><summary>å±•å¼€</summary></details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.00216v1">From Evidence-Based Medicine to Knowledge Graph: Retrieval-Augmented Generation for Sports Rehabilitation and a Domain Benchmark</a></td><td><details><summary>å±•å¼€</summary>In medicine, large language models (LLMs) increasingly rely on retrieval-augmented generation (RAG) to ground outputs in up-to-date external evidence. However, current RAG approaches focus primarily on performance improvements while overlooking evidence-based medicine (EBM) principles. This study addresses two key gaps: (1) the lack of PICO alignment between queries and retrieved evidence, and (2) the absence of evidence hierarchy considerations during reranking. We present a generalizable strategy for adapting EBM to graph-based RAG, integrating the PICO framework into knowledge graph construction and retrieval, and proposing a Bayesian-inspired reranking algorithm to calibrate ranking scores by evidence grade without introducing predefined weights. We validated this framework in sports rehabilitation, a literature-rich domain currently lacking RAG systems and benchmarks. We released a knowledge graph (357,844 nodes and 371,226 edges) and a reusable benchmark of 1,637 QA pairs. The system achieved 0.830 nugget coverage, 0.819 answer faithfulness, 0.882 semantic similarity, and 0.788 PICOT match accuracy. In a 5-point Likert evaluation, five expert clinicians rated the system 4.66-4.84 across factual accuracy, faithfulness, relevance, safety, and PICO alignment. These findings demonstrate that the proposed EBM adaptation strategy improves retrieval and answer quality and is transferable to other clinical domains. The released resources also help address the scarcity of RAG datasets in sports rehabilitation.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè¯æ®åŒ»å­¦ï¼ˆEBMï¼‰åŸåˆ™æ”¹è¿›RAGçš„æ–¹æ³•ï¼Œé€šè¿‡æ•´åˆPICOæ¡†æ¶åˆ°çŸ¥è¯†å›¾è°±æ„å»ºä¸æ£€ç´¢ä¸­ï¼Œå¹¶è®¾è®¡è´å¶æ–¯å¯å‘çš„é‡æ’åºç®—æ³•æ¥æå‡åŒ»å­¦é¢†åŸŸï¼ˆå°¤å…¶æ˜¯è¿åŠ¨åº·å¤ï¼‰çš„æ£€ç´¢ä¸ç”Ÿæˆè´¨é‡ï¼ŒåŒæ—¶å‘å¸ƒäº†ç›¸å…³æ•°æ®é›†å’ŒçŸ¥è¯†å›¾è°±èµ„æºã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-31
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2601.00513v1">When Small Models Are Right for Wrong Reasons: Process Verification for Trustworthy Agents</a></td><td><details><summary>å±•å¼€</summary>Deploying small language models (7-9B parameters) as autonomous agents requires trust in their reasoning, not just their outputs. We reveal a critical reliability crisis: 50-69\% of correct answers from these models contain fundamentally flawed reasoning -- a ``Right-for-Wrong-Reasons'' phenomenon invisible to standard accuracy metrics. Through analysis of 10,734 reasoning traces across three models and diverse tasks, we introduce the Reasoning Integrity Score (RIS), a process-based metric validated with substantial inter-rater agreement ($Îº=0.657$). Conventional practices are challenged by our findings: while retrieval-augmented generation (RAG) significantly improves reasoning integrity (Cohen's $d=0.23$--$0.93$), meta-cognitive interventions like self-critique often harm performance ($d=-0.14$ to $-0.33$) in small models on the evaluated tasks. Mechanistic analysis reveals RAG succeeds by grounding calculations in external evidence, reducing errors by 7.6\%, while meta-cognition amplifies confusion without sufficient model capacity. To enable deployment, verification capabilities are distilled into a neural classifier achieving 0.86 F1-score with 100$\times$ speedup. These results underscore the necessity of process-based verification for trustworthy agents: accuracy alone is dangerously insufficient when models can be right for entirely wrong reasons.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ­ç¤ºäº†å°å‹è¯­è¨€æ¨¡å‹ï¼ˆ7-9Bå‚æ•°ï¼‰ä½œä¸ºè‡ªä¸»ä»£ç†æ—¶å­˜åœ¨çš„â€œæ­£ç¡®ç­”æ¡ˆä½†é”™è¯¯æ¨ç†â€ç°è±¡ï¼Œå¹¶æå‡ºåŸºäºè¿‡ç¨‹çš„è¯„ä¼°æŒ‡æ ‡RISã€‚ç ”ç©¶å‘ç°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰èƒ½æ˜¾è‘—æå‡æ¨ç†å®Œæ•´æ€§ï¼ˆå‡å°‘7.6%é”™è¯¯ï¼‰ï¼Œè€Œå…ƒè®¤çŸ¥å¹²é¢„ï¼ˆå¦‚è‡ªæˆ‘æ‰¹è¯„ï¼‰å¯èƒ½æŸå®³æ€§èƒ½ã€‚è®ºæ–‡è¿˜å¼€å‘äº†å¿«é€ŸéªŒè¯åˆ†ç±»å™¨ï¼ˆF1=0.86ï¼‰ï¼Œå¼ºè°ƒè¿‡ç¨‹éªŒè¯å¯¹å¯ä¿¡ä»£ç†çš„å¿…è¦æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.00509v1">Improving LLM-Assisted Secure Code Generation through Retrieval-Augmented-Generation and Multi-Tool Feedback</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) can generate code but often introduce security vulnerabilities, logical inconsistencies, and compilation errors. Prior work demonstrates that LLMs benefit substantially from structured feedback, static analysis, retrieval augmentation, and execution-based refinement. We propose a retrieval-augmented, multi-tool repair workflow in which a single code-generating LLM iteratively refines its outputs using compiler diagnostics, CodeQL security scanning, and KLEE symbolic execution. A lightweight embedding model is used for semantic retrieval of previously successful repairs, providing security-focused examples that guide generation. Evaluated on a combined dataset of 3,242 programs generated by DeepSeek-Coder-1.3B and CodeLlama-7B, the system demonstrates significant improvements in robustness. For DeepSeek, security vulnerabilities were reduced by 96%. For the larger CodeLlama model, the critical security defect rate was decreased from 58.55% to 22.19%, highlighting the efficacy of tool-assisted self-repair even on "stubborn" models.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ£€ç´¢å¢å¼ºã€å¤šå·¥å…·ä¿®å¤çš„å·¥ä½œæµç¨‹ï¼Œé€šè¿‡ç»“åˆç¼–è¯‘å™¨è¯Šæ–­ã€å®‰å…¨æ‰«æå’Œç¬¦å·æ‰§è¡Œç­‰å·¥å…·ï¼Œå¹¶åˆ©ç”¨è½»é‡çº§åµŒå…¥æ¨¡å‹è¿›è¡Œè¯­ä¹‰æ£€ç´¢ï¼Œä¸ºä»£ç ç”Ÿæˆå¤§è¯­è¨€æ¨¡å‹æä¾›å®‰å…¨ç›¸å…³çš„ä¿®å¤ç¤ºä¾‹ï¼Œä»è€Œè¿­ä»£åœ°ä¼˜åŒ–ä»£ç è¾“å‡ºï¼Œæ˜¾è‘—é™ä½äº†ç”Ÿæˆä»£ç ä¸­çš„å®‰å…¨æ¼æ´å’Œç¼ºé™·ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.00926v1">MACA: A Framework for Distilling Trustworthy LLMs into Efficient Retrievers</a></td><td><details><summary>å±•å¼€</summary>Modern enterprise retrieval systems must handle short, underspecified queries such as ``foreign transaction fee refund'' and ``recent check status''. In these cases, semantic nuance and metadata matter but per-query large language model (LLM) re-ranking and manual labeling are costly. We present Metadata-Aware Cross-Model Alignment (MACA), which distills a calibrated metadata aware LLM re-ranker into a compact student retriever, avoiding online LLM calls. A metadata-aware prompt verifies the teacher's trustworthiness by checking consistency under permutations and robustness to paraphrases, then supplies listwise scores, hard negatives, and calibrated relevance margins. The student trains with MACA's MetaFusion objective, which combines a metadata conditioned ranking loss with a cross model margin loss so it learns to push the correct answer above semantically similar candidates with mismatched topic, sub-topic, or entity. On a proprietary consumer banking FAQ corpus and BankFAQs, the MACA teacher surpasses a MAFA baseline at Accuracy@1 by five points on the proprietary set and three points on BankFAQs. MACA students substantially outperform pretrained encoders; e.g., on the proprietary corpus MiniLM Accuracy@1 improves from 0.23 to 0.48, while keeping inference free of LLM calls and supporting retrieval-augmented generation.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMACAï¼ˆMetadata-Aware Cross-Model Alignmentï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡å°†ç»è¿‡æ ¡å‡†çš„å…ƒæ•°æ®æ„ŸçŸ¥LLMé‡æ–°æ’åºå™¨æç‚¼ä¸ºç´§å‡‘çš„å­¦ç”Ÿæ£€ç´¢å™¨ï¼Œé¿å…äº†åœ¨çº¿LLMè°ƒç”¨ï¼Œä»è€Œæå‡äº†æ£€ç´¢ç³»ç»Ÿçš„æ€§èƒ½ã€‚MACAç»“åˆäº†å…ƒæ•°æ®æ„ŸçŸ¥çš„æç¤ºå’Œè·¨æ¨¡å‹å¯¹é½ç›®æ ‡ï¼Œæ˜¾è‘—æé«˜äº†åœ¨ä¸“æœ‰é“¶è¡ŒFAQè¯­æ–™åº“å’ŒBankFAQsä¸Šçš„æ£€ç´¢å‡†ç¡®æ€§ï¼ŒåŒæ—¶æ”¯æŒæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.00376v1">In Line with Context: Repository-Level Code Generation via Context Inlining</a></td><td><details><summary>å±•å¼€</summary>Repository-level code generation has attracted growing attention in recent years. Unlike function-level code generation, it requires the model to understand the entire repository, reasoning over complex dependencies across functions, classes, and modules. However, existing approaches such as retrieval-augmented generation (RAG) or context-based function selection often fall short: they primarily rely on surface-level similarity and struggle to capture the rich dependencies that govern repository-level semantics. In this paper, we introduce InlineCoder, a novel framework for repository-level code generation. InlineCoder enhances the understanding of repository context by inlining the unfinished function into its call graph, thereby reframing the challenging repository understanding as an easier function-level coding task. Given a function signature, InlineCoder first generates a draft completion, termed an anchor, which approximates downstream dependencies and enables perplexity-based confidence estimation. This anchor drives a bidirectional inlining process: (i) Upstream Inlining, which embeds the anchor into its callers to capture diverse usage scenarios; and (ii) Downstream Retrieval, which integrates the anchor's callees into the prompt to provide precise dependency context. The enriched context, combining draft completion with upstream and downstream perspectives, equips the LLM with a comprehensive repository view.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºInlineCoderæ¡†æ¶ä»¥è§£å†³ç°æœ‰RAGæ–¹æ³•åœ¨ä»“åº“çº§ä»£ç ç”Ÿæˆä¸­çš„å±€é™æ€§ï¼ˆå¦‚ä»…ä¾èµ–è¡¨å±‚ç›¸ä¼¼æ€§ï¼‰ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡å°†æœªå®Œæˆå‡½æ•°å†…è”åˆ°è°ƒç”¨å›¾ä¸­ï¼Œç»“åˆåŸºäºåˆå§‹è‰ç¨¿çš„ä¸Šæ¸¸å†…è”å’Œä¸‹æ¸¸æ£€ç´¢ï¼Œä¸ºLLMæä¾›æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ï¼Œä»è€Œå°†å¤æ‚çš„ä»“åº“çº§ç†è§£ä»»åŠ¡è½¬æ¢ä¸ºæ›´æ˜“å¤„ç†çš„å‡½æ•°çº§ç”Ÿæˆä»»åŠ¡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.00254v1">An Empirical Evaluation of LLM-Based Approaches for Code Vulnerability Detection: RAG, SFT, and Dual-Agent Systems</a></td><td><details><summary>å±•å¼€</summary>The rapid advancement of Large Language Models (LLMs) presents new opportunities for automated software vulnerability detection, a crucial task in securing modern codebases. This paper presents a comparative study on the effectiveness of LLM-based techniques for detecting software vulnerabilities. The study evaluates three approaches, Retrieval-Augmented Generation (RAG), Supervised Fine-Tuning (SFT), and a Dual-Agent LLM framework, against a baseline LLM model. A curated dataset was compiled from Big-Vul and real-world code repositories from GitHub, focusing on five critical Common Weakness Enumeration (CWE) categories: CWE-119, CWE-399, CWE-264, CWE-20, and CWE-200. Our RAG approach, which integrated external domain knowledge from the internet and the MITRE CWE database, achieved the highest overall accuracy (0.86) and F1 score (0.85), highlighting the value of contextual augmentation. Our SFT approach, implemented using parameter-efficient QLoRA adapters, also demonstrated strong performance. Our Dual-Agent system, an architecture in which a secondary agent audits and refines the output of the first, showed promise in improving reasoning transparency and error mitigation, with reduced resource overhead. These results emphasize that incorporating a domain expertise mechanism significantly strengthens the practical applicability of LLMs in real-world vulnerability detection tasks.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è½¯ä»¶æ¼æ´æ£€æµ‹æ–¹æ³•ï¼Œé‡ç‚¹æ¯”è¾ƒäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒåŒAgentæ¡†æ¶çš„æ€§èƒ½ã€‚å…¶ä¸­RAGæ–¹æ³•é€šè¿‡æ•´åˆäº’è”ç½‘å’ŒMITRE CWEæ•°æ®åº“çš„å¤–éƒ¨çŸ¥è¯†ï¼Œåœ¨æ£€æµ‹å‡†ç¡®ç‡ï¼ˆ0.86ï¼‰å’ŒF1åˆ†æ•°ï¼ˆ0.85ï¼‰ä¸Šè¡¨ç°æœ€ä¼˜ï¼ŒéªŒè¯äº†ä¸Šä¸‹æ–‡å¢å¼ºå¯¹æå‡æ¼æ´æ£€æµ‹æ•ˆæœçš„ä»·å€¼ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.00216v1">From Evidence-Based Medicine to Knowledge Graph: Retrieval-Augmented Generation for Sports Rehabilitation and a Domain Benchmark</a></td><td><details><summary>å±•å¼€</summary>In medicine, large language models (LLMs) increasingly rely on retrieval-augmented generation (RAG) to ground outputs in up-to-date external evidence. However, current RAG approaches focus primarily on performance improvements while overlooking evidence-based medicine (EBM) principles. This study addresses two key gaps: (1) the lack of PICO alignment between queries and retrieved evidence, and (2) the absence of evidence hierarchy considerations during reranking. We present a generalizable strategy for adapting EBM to graph-based RAG, integrating the PICO framework into knowledge graph construction and retrieval, and proposing a Bayesian-inspired reranking algorithm to calibrate ranking scores by evidence grade without introducing predefined weights. We validated this framework in sports rehabilitation, a literature-rich domain currently lacking RAG systems and benchmarks. We released a knowledge graph (357,844 nodes and 371,226 edges) and a reusable benchmark of 1,637 QA pairs. The system achieved 0.830 nugget coverage, 0.819 answer faithfulness, 0.882 semantic similarity, and 0.788 PICOT match accuracy. In a 5-point Likert evaluation, five expert clinicians rated the system 4.66-4.84 across factual accuracy, faithfulness, relevance, safety, and PICO alignment. These findings demonstrate that the proposed EBM adaptation strategy improves retrieval and answer quality and is transferable to other clinical domains. The released resources also help address the scarcity of RAG datasets in sports rehabilitation.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè¯æ®åŒ»å­¦ï¼ˆEBMï¼‰åŸåˆ™æ”¹è¿›RAGç³»ç»Ÿçš„æ–¹æ³•ï¼Œé‡ç‚¹è§£å†³äº†æŸ¥è¯¢ä¸æ£€ç´¢è¯æ®çš„PICOå¯¹é½é—®é¢˜ä»¥åŠè¯æ®å±‚çº§é‡æ’çš„ç¼ºé™·ï¼Œå¹¶é€šè¿‡çŸ¥è¯†å›¾è°±æ„å»ºå’Œè´å¶æ–¯é‡æ’ç®—æ³•æå‡äº†ä½“è‚²åº·å¤é¢†åŸŸçš„æ£€ç´¢ä¸ç”Ÿæˆè´¨é‡ï¼ŒåŒæ—¶å‘å¸ƒäº†ç›¸å…³æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.25052v1">AdaGReS:Adaptive Greedy Context Selection via Redundancy-Aware Scoring for Token-Budgeted RAG</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) is highly sensitive to the quality of selected context, yet standard top-k retrieval often returns redundant or near-duplicate chunks that waste token budget and degrade downstream generation. We present AdaGReS, a redundancy-aware context selection framework for token-budgeted RAG that optimizes a set-level objective combining query-chunk relevance and intra-set redundancy penalties. AdaGReS performs greedy selection under a token-budget constraint using marginal gains derived from the objective, and introduces a closed-form, instance-adaptive calibration of the relevance-redundancy trade-off parameter to eliminate manual tuning and adapt to candidate-pool statistics and budget limits. We further provide a theoretical analysis showing that the proposed objective exhibits epsilon-approximate submodularity under practical embedding similarity conditions, yielding near-optimality guarantees for greedy selection. Experiments on open-domain question answering (Natural Questions) and a high-redundancy biomedical (drug) corpus demonstrate consistent improvements in redundancy control and context quality, translating to better end-to-end answer quality and robustness across settings.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†AdaGReSï¼Œä¸€ä¸ªé’ˆå¯¹RAGä¸­å†—ä½™ä¸Šä¸‹æ–‡é€‰æ‹©é—®é¢˜çš„æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆæŸ¥è¯¢-å—ç›¸å…³æ€§å’Œé›†åˆå†…å†—ä½™æƒ©ç½šçš„ä¼˜åŒ–ç›®æ ‡ï¼Œåœ¨ä»¤ç‰Œé¢„ç®—çº¦æŸä¸‹è¿›è¡Œè´ªå©ªé€‰æ‹©ï¼Œä»¥æé«˜ç”Ÿæˆè´¨é‡ã€‚å®éªŒè¯æ˜å…¶åœ¨å¼€æ”¾åŸŸé—®ç­”å’Œç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.00891v1">Enhancing Retrieval-Augmented Generation with Topic-Enriched Embeddings: A Hybrid Approach Integrating Traditional NLP Techniques</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) systems rely on accurate document retrieval to ground large language models (LLMs) in external knowledge, yet retrieval quality often degrades in corpora where topics overlap and thematic variation is high. This work proposes topic-enriched embeddings that integrate term-based signals and topic structure with contextual sentence embeddings. The approach combines TF-IDF with topic modeling and dimensionality reduction, using Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA) to encode latent topical organization, and fuses these representations with a compact contextual encoder (all-MiniLM). By jointly capturing term-level and topic-level semantics, topic-enriched embeddings improve semantic clustering, increase retrieval precision, and reduce computational burden relative to purely contextual baselines. Experiments on a legal-text corpus show consistent gains in clustering coherence and retrieval metrics, suggesting that topic-enriched embeddings can serve as a practical component for more reliable knowledge-intensive RAG pipelines.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆæœ¯è¯­ä¿¡å·å’Œä¸»é¢˜ç»“æ„çš„ä¸»é¢˜å¢å¼ºåµŒå…¥æ–¹æ³•ï¼ˆtopic-enriched embeddingsï¼‰ï¼Œé€šè¿‡èåˆTF-IDFã€ä¸»é¢˜å»ºæ¨¡ï¼ˆLSA/LDAï¼‰å’Œè½»é‡ä¸Šä¸‹æ–‡ç¼–ç å™¨ï¼ˆall-MiniLMï¼‰ï¼Œæå‡RAGç³»ç»Ÿåœ¨ä¸»é¢˜é‡å å’Œé«˜å˜å¼‚è¯­æ–™ä¸­çš„æ£€ç´¢è´¨é‡ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨æ³•å¾‹æ–‡æœ¬ä¸Šèƒ½å¢å¼ºè¯­ä¹‰èšç±»ã€æ£€ç´¢ç²¾åº¦å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.24848v1">PrivacyBench: A Conversational Benchmark for Evaluating Privacy in Personalized AI</a></td><td><details><summary>å±•å¼€</summary>Personalized AI agents rely on access to a user's digital footprint, which often includes sensitive data from private emails, chats and purchase histories. Yet this access creates a fundamental societal and privacy risk: systems lacking social-context awareness can unintentionally expose user secrets, threatening digital well-being. We introduce PrivacyBench, a benchmark with socially grounded datasets containing embedded secrets and a multi-turn conversational evaluation to measure secret preservation. Testing Retrieval-Augmented Generation (RAG) assistants reveals that they leak secrets in up to 26.56% of interactions. A privacy-aware prompt lowers leakage to 5.12%, yet this measure offers only partial mitigation. The retrieval mechanism continues to access sensitive data indiscriminately, which shifts the entire burden of privacy preservation onto the generator. This creates a single point of failure, rendering current architectures unsafe for wide-scale deployment. Our findings underscore the urgent need for structural, privacy-by-design safeguards to ensure an ethical and inclusive web for everyone.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†RAGåŠ©æ‰‹åœ¨éšç§ä¿æŠ¤æ–¹é¢çš„é£é™©ï¼ŒæŒ‡å‡ºå…¶åœ¨å¤šè½®å¯¹è¯ä¸­å¯èƒ½æ³„éœ²ç”¨æˆ·æ•æ„Ÿä¿¡æ¯ï¼ˆæ³„æ¼ç‡é«˜è¾¾26.56%ï¼‰ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†PrivacyBenchåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«åµŒå…¥ç§˜å¯†çš„ç¤¾ä¼šæƒ…å¢ƒæ•°æ®é›†ï¼Œå¹¶å‘ç°ç°æœ‰RAGæ¶æ„çš„æ£€ç´¢æœºåˆ¶ä¼šæ— å·®åˆ«è®¿é—®æ•æ„Ÿæ•°æ®ï¼Œå¯¼è‡´éšç§ä¿æŠ¤å®Œå…¨ä¾èµ–ç”Ÿæˆæ¨¡å‹ï¼Œå½¢æˆå•ç‚¹æ•…éšœã€‚è®ºæ–‡å‘¼åé€šè¿‡éšç§è®¾è®¡ï¼ˆprivacy-by-designï¼‰çš„ç»“æ„æ€§æ”¹è¿›æ¥è§£å†³è¿™ä¸€ä¼¦ç†é—®é¢˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.24684v1">R-Debater: Retrieval-Augmented Debate Generation through Argumentative Memory</a></td><td><details><summary>å±•å¼€</summary>We present R-Debater, an agentic framework for generating multi-turn debates built on argumentative memory. Grounded in rhetoric and memory studies, the system views debate as a process of recalling and adapting prior arguments to maintain stance consistency, respond to opponents, and support claims with evidence. Specifically, R-Debater integrates a debate knowledge base for retrieving case-like evidence and prior debate moves with a role-based agent that composes coherent utterances across turns. We evaluate on standardized ORCHID debates, constructing a 1,000-item retrieval corpus and a held-out set of 32 debates across seven domains. Two tasks are evaluated: next-utterance generation, assessed by InspireScore (subjective, logical, and factual), and adversarial multi-turn simulations, judged by Debatrix (argument, source, language, and overall). Compared with strong LLM baselines, R-Debater achieves higher single-turn and multi-turn scores. Human evaluation with 20 experienced debaters further confirms its consistency and evidence use, showing that combining retrieval grounding with structured planning yields more faithful, stance-aligned, and coherent debates across turns.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†R-Debateræ¡†æ¶ï¼Œå®ƒé€šè¿‡æ£€ç´¢è¾©è®ºçŸ¥è¯†åº“ä¸­çš„è®ºæ®å’Œæ¡ˆä¾‹ï¼ˆç±»ä¼¼RAGçš„æ£€ç´¢æœºåˆ¶ï¼‰ï¼Œç»“åˆåŸºäºè§’è‰²çš„æ™ºèƒ½ä½“ç”Ÿæˆå¤šè½®è¿è´¯è¾©è®ºå†…å®¹ï¼Œå¼ºè°ƒè®°å¿†æ£€ç´¢ä¸ç«‹åœºä¸€è‡´æ€§çš„ç»“åˆï¼Œå¹¶åœ¨å¯¹æŠ—æ€§å¤šè½®è¾©è®ºä»»åŠ¡ä¸­éªŒè¯äº†å…¶ä¼˜äºåŸºçº¿æ¨¡å‹çš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.24613v1">Group Deliberation Oriented Multi-Agent Conversational Model for Complex Reasoning</a></td><td><details><summary>å±•å¼€</summary>This paper proposes a group deliberation oriented multi-agent conversational model to address the limitations of single large language models in complex reasoning tasks. The model adopts a three-level role division architecture consisting of generation, verification, and integration. An opinion generation agent produces diverse reasoning perspectives, an evidence verification agent retrieves external knowledge and quantifies factual support, and a consistency arbitration agent integrates logically coherent conclusions. A self-game mechanism is introduced to expand multi-path reasoning trajectories, while a retrieval enhancement module dynamically supplements external knowledge. A composite reward function combining factual consistency and logical coherence is designed, and an improved proximal policy optimization strategy is applied for collaborative training. Experimental results show that the proposed model improves multi-hop reasoning accuracy by 16.8 percent on HotpotQA, 14.3 percent on 2WikiMultihopQA, and 19.2 percent on MeetingBank, while improving consistency by 21.5 percent. The model achieves higher reasoning efficiency than mainstream multi-agent approaches, providing an effective and stable solution for complex reasoning tasks.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§é¢å‘ç¾¤ä½“å®¡è®®çš„å¤šæ™ºèƒ½ä½“å¯¹è¯æ¨¡å‹ï¼Œé€šè¿‡ç”Ÿæˆã€éªŒè¯å’Œæ•´åˆçš„ä¸‰çº§è§’è‰²æ¶æ„å¢å¼ºå¤æ‚æ¨ç†èƒ½åŠ›ã€‚æ¨¡å‹åŒ…å«æ£€ç´¢å¢å¼ºæ¨¡å—åŠ¨æ€è¡¥å……å¤–éƒ¨çŸ¥è¯†ï¼Œå¹¶é‡‡ç”¨å¤šæ™ºèƒ½ä½“åä½œæœºåˆ¶ï¼ˆå¦‚æ„è§ç”Ÿæˆã€è¯æ®éªŒè¯å’Œä¸€è‡´æ€§ä»²è£ï¼‰æå‡æ¨ç†å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ï¼Œå®éªŒæ˜¾ç¤ºåœ¨å¤šè·³æ¨ç†ä»»åŠ¡ä¸­æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-30
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.25052v1">AdaGReS:Adaptive Greedy Context Selection via Redundancy-Aware Scoring for Token-Budgeted RAG</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) is highly sensitive to the quality of selected context, yet standard top-k retrieval often returns redundant or near-duplicate chunks that waste token budget and degrade downstream generation. We present AdaGReS, a redundancy-aware context selection framework for token-budgeted RAG that optimizes a set-level objective combining query-chunk relevance and intra-set redundancy penalties. AdaGReS performs greedy selection under a token-budget constraint using marginal gains derived from the objective, and introduces a closed-form, instance-adaptive calibration of the relevance-redundancy trade-off parameter to eliminate manual tuning and adapt to candidate-pool statistics and budget limits. We further provide a theoretical analysis showing that the proposed objective exhibits epsilon-approximate submodularity under practical embedding similarity conditions, yielding near-optimality guarantees for greedy selection. Experiments on open-domain question answering (Natural Questions) and a high-redundancy biomedical (drug) corpus demonstrate consistent improvements in redundancy control and context quality, translating to better end-to-end answer quality and robustness across settings.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºAdaGReSï¼Œä¸€ç§é¢å‘RAGçš„å†—ä½™æ„ŸçŸ¥ä¸Šä¸‹æ–‡é€‰æ‹©æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆæŸ¥è¯¢-ç‰‡æ®µç›¸å…³æ€§å’Œé›†åˆå†…å†—ä½™æƒ©ç½šçš„ä¼˜åŒ–ç›®æ ‡ï¼Œåœ¨tokené¢„ç®—çº¦æŸä¸‹è¿›è¡Œè´ªå©ªé€‰æ‹©ï¼Œå¹¶è‡ªé€‚åº”æ ¡å‡†ç›¸å…³æ€§ä¸å†—ä½™æ€§çš„æƒè¡¡å‚æ•°ã€‚å®éªŒè¡¨æ˜è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæå‡ä¸Šä¸‹æ–‡è´¨é‡åŠæœ€ç»ˆç”Ÿæˆç­”æ¡ˆçš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.00891v1">Enhancing Retrieval-Augmented Generation with Topic-Enriched Embeddings: A Hybrid Approach Integrating Traditional NLP Techniques</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) systems rely on accurate document retrieval to ground large language models (LLMs) in external knowledge, yet retrieval quality often degrades in corpora where topics overlap and thematic variation is high. This work proposes topic-enriched embeddings that integrate term-based signals and topic structure with contextual sentence embeddings. The approach combines TF-IDF with topic modeling and dimensionality reduction, using Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA) to encode latent topical organization, and fuses these representations with a compact contextual encoder (all-MiniLM). By jointly capturing term-level and topic-level semantics, topic-enriched embeddings improve semantic clustering, increase retrieval precision, and reduce computational burden relative to purely contextual baselines. Experiments on a legal-text corpus show consistent gains in clustering coherence and retrieval metrics, suggesting that topic-enriched embeddings can serve as a practical component for more reliable knowledge-intensive RAG pipelines.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆæœ¯è¯­ä¿¡å·å’Œä¸»é¢˜ç»“æ„çš„ä¸»é¢˜å¢å¼ºåµŒå…¥æ–¹æ³•ï¼ˆtopic-enriched embeddingsï¼‰ï¼Œé€šè¿‡èåˆTF-IDFã€ä¸»é¢˜å»ºæ¨¡ï¼ˆLSA/LDAï¼‰å’Œè½»é‡ä¸Šä¸‹æ–‡ç¼–ç å™¨ï¼ˆall-MiniLMï¼‰ï¼Œæå‡RAGç³»ç»Ÿåœ¨ä¸»é¢˜é‡å å’Œé«˜å˜å¼‚è¯­æ–™ä¸­çš„æ£€ç´¢è´¨é‡ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨æ³•å¾‹æ–‡æœ¬ä¸Šèƒ½å¢å¼ºè¯­ä¹‰èšç±»ã€æ£€ç´¢ç²¾åº¦å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.24848v1">PrivacyBench: A Conversational Benchmark for Evaluating Privacy in Personalized AI</a></td><td><details><summary>å±•å¼€</summary>Personalized AI agents rely on access to a user's digital footprint, which often includes sensitive data from private emails, chats and purchase histories. Yet this access creates a fundamental societal and privacy risk: systems lacking social-context awareness can unintentionally expose user secrets, threatening digital well-being. We introduce PrivacyBench, a benchmark with socially grounded datasets containing embedded secrets and a multi-turn conversational evaluation to measure secret preservation. Testing Retrieval-Augmented Generation (RAG) assistants reveals that they leak secrets in up to 26.56% of interactions. A privacy-aware prompt lowers leakage to 5.12%, yet this measure offers only partial mitigation. The retrieval mechanism continues to access sensitive data indiscriminately, which shifts the entire burden of privacy preservation onto the generator. This creates a single point of failure, rendering current architectures unsafe for wide-scale deployment. Our findings underscore the urgent need for structural, privacy-by-design safeguards to ensure an ethical and inclusive web for everyone.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†RAGåŠ©æ‰‹åœ¨éšç§ä¿æŠ¤æ–¹é¢çš„ç¼ºé™·ï¼ŒæŒ‡å‡ºå…¶åœ¨äº¤äº’ä¸­å¯èƒ½æ³„éœ²ç”¨æˆ·æ•æ„Ÿæ•°æ®ï¼ˆæ³„éœ²ç‡é«˜è¾¾26.56%ï¼‰ï¼Œå¹¶æå‡ºPrivacyBenchåŸºå‡†æ¥è¯„ä¼°ç§˜å¯†ä¿æŠ¤èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°å³ä½¿é‡‡ç”¨éšç§æç¤ºï¼Œæ£€ç´¢æœºåˆ¶ä»ä¼šæ— å·®åˆ«è®¿é—®æ•æ„Ÿä¿¡æ¯ï¼Œå‡¸æ˜¾ç°æœ‰æ¶æ„éœ€é€šè¿‡éšç§è®¾è®¡è¿›è¡Œç»“æ„æ€§æ”¹è¿›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.24684v1">R-Debater: Retrieval-Augmented Debate Generation through Argumentative Memory</a></td><td><details><summary>å±•å¼€</summary>We present R-Debater, an agentic framework for generating multi-turn debates built on argumentative memory. Grounded in rhetoric and memory studies, the system views debate as a process of recalling and adapting prior arguments to maintain stance consistency, respond to opponents, and support claims with evidence. Specifically, R-Debater integrates a debate knowledge base for retrieving case-like evidence and prior debate moves with a role-based agent that composes coherent utterances across turns. We evaluate on standardized ORCHID debates, constructing a 1,000-item retrieval corpus and a held-out set of 32 debates across seven domains. Two tasks are evaluated: next-utterance generation, assessed by InspireScore (subjective, logical, and factual), and adversarial multi-turn simulations, judged by Debatrix (argument, source, language, and overall). Compared with strong LLM baselines, R-Debater achieves higher single-turn and multi-turn scores. Human evaluation with 20 experienced debaters further confirms its consistency and evidence use, showing that combining retrieval grounding with structured planning yields more faithful, stance-aligned, and coherent debates across turns.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†R-Debaterï¼Œä¸€ä¸ªåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„è¾©è®ºæ¡†æ¶ï¼Œå®ƒé€šè¿‡æ£€ç´¢å…ˆå‰çš„è®ºç‚¹å’Œè¯æ®æ¥ç”Ÿæˆå¤šè½®è¾©è®ºå†…å®¹ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†è¾©è®ºçŸ¥è¯†åº“å’ŒåŸºäºè§’è‰²çš„ä»£ç†ï¼Œä»¥ä¿æŒç«‹åœºä¸€è‡´æ€§ã€å›åº”å¯¹æ‰‹å¹¶æ”¯æŒè®ºç‚¹ã€‚å®éªŒè¡¨æ˜ï¼ŒR-Debateråœ¨å•è½®å’Œå¤šè½®è¾©è®ºä¸­è¡¨ç°ä¼˜äºå¼ºåŸºçº¿æ¨¡å‹ï¼Œå¹¶é€šè¿‡äººç±»è¯„ä¼°éªŒè¯äº†å…¶åœ¨ä¸€è‡´æ€§å’Œè¯æ®ä½¿ç”¨ä¸Šçš„ä¼˜åŠ¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.24613v1">Group Deliberation Oriented Multi-Agent Conversational Model for Complex Reasoning</a></td><td><details><summary>å±•å¼€</summary>This paper proposes a group deliberation oriented multi-agent conversational model to address the limitations of single large language models in complex reasoning tasks. The model adopts a three-level role division architecture consisting of generation, verification, and integration. An opinion generation agent produces diverse reasoning perspectives, an evidence verification agent retrieves external knowledge and quantifies factual support, and a consistency arbitration agent integrates logically coherent conclusions. A self-game mechanism is introduced to expand multi-path reasoning trajectories, while a retrieval enhancement module dynamically supplements external knowledge. A composite reward function combining factual consistency and logical coherence is designed, and an improved proximal policy optimization strategy is applied for collaborative training. Experimental results show that the proposed model improves multi-hop reasoning accuracy by 16.8 percent on HotpotQA, 14.3 percent on 2WikiMultihopQA, and 19.2 percent on MeetingBank, while improving consistency by 21.5 percent. The model achieves higher reasoning efficiency than mainstream multi-agent approaches, providing an effective and stable solution for complex reasoning tasks.</details></td><td><details><summary>å±•å¼€</summary></details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.24268v1">RAGPart & RAGMask: Retrieval-Stage Defenses Against Corpus Poisoning in Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to enhance large language models (LLMs) with external knowledge, reducing hallucinations and compensating for outdated information. However, recent studies have exposed a critical vulnerability in RAG pipelines corpus poisoning where adversaries inject malicious documents into the retrieval corpus to manipulate model outputs. In this work, we propose two complementary retrieval-stage defenses: RAGPart and RAGMask. Our defenses operate directly on the retriever, making them computationally lightweight and requiring no modification to the generation model. RAGPart leverages the inherent training dynamics of dense retrievers, exploiting document partitioning to mitigate the effect of poisoned points. In contrast, RAGMask identifies suspicious tokens based on significant similarity shifts under targeted token masking. Across two benchmarks, four poisoning strategies, and four state-of-the-art retrievers, our defenses consistently reduce attack success rates while preserving utility under benign conditions. We further introduce an interpretable attack to stress-test our defenses. Our findings highlight the potential and limitations of retrieval-stage defenses, providing practical insights for robust RAG deployments.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æŠ€æœ¯ä¸­å­˜åœ¨çš„å®‰å…¨æ¼æ´â€”â€”æ£€ç´¢åº“æŠ•æ¯’æ”»å‡»ï¼ˆcorpus poisoningï¼‰ï¼Œå³æ”»å‡»è€…é€šè¿‡æ³¨å…¥æ¶æ„æ–‡æ¡£æ“æ§æ¨¡å‹è¾“å‡ºã€‚ä½œè€…æå‡ºäº†ä¸¤ç§è½»é‡çº§é˜²å¾¡æ–¹æ³•ï¼ˆRAGPartå’ŒRAGMaskï¼‰ï¼Œåœ¨æ£€ç´¢é˜¶æ®µç›´æ¥æ“ä½œï¼Œæ— éœ€ä¿®æ”¹ç”Ÿæˆæ¨¡å‹ã€‚RAGPartåˆ©ç”¨ç¨ å¯†æ£€ç´¢å™¨çš„è®­ç»ƒåŠ¨æ€å’Œæ–‡æ¡£åˆ†åŒºæŠ€æœ¯ï¼ŒRAGMaskåˆ™é€šè¿‡ç›®æ ‡ä»¤ç‰Œæ©è”½ä¸‹çš„ç›¸ä¼¼æ€§åç§»æ£€æµ‹å¯ç–‘å†…å®¹ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ä¸¤ç§æ–¹æ³•åœ¨å¤šç§æ”»å‡»åœºæ™¯ä¸‹èƒ½æœ‰æ•ˆé™ä½æ”»å‡»æˆåŠŸç‡ï¼ŒåŒæ—¶ä¿æŒæ­£å¸¸ä½¿ç”¨æ—¶çš„æ€§èƒ½ã€‚ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§å¯è§£é‡Šçš„æ”»å‡»æ–¹æ³•ä»¥éªŒè¯é˜²å¾¡æ•ˆæœï¼Œä¸ºRAGçš„é²æ£’æ€§éƒ¨ç½²æä¾›äº†å®è·µæŒ‡å¯¼ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.24008v1">SPARK: Search Personalization via Agent-Driven Retrieval and Knowledge-sharing</a></td><td><details><summary>å±•å¼€</summary>Personalized search demands the ability to model users' evolving, multi-dimensional information needs; a challenge for systems constrained by static profiles or monolithic retrieval pipelines. We present SPARK (Search Personalization via Agent-Driven Retrieval and Knowledge-sharing), a framework in which coordinated persona-based large language model (LLM) agents deliver task-specific retrieval and emergent personalization. SPARK formalizes a persona space defined by role, expertise, task context, and domain, and introduces a Persona Coordinator that dynamically interprets incoming queries to activate the most relevant specialized agents. Each agent executes an independent retrieval-augmented generation process, supported by dedicated long- and short-term memory stores and context-aware reasoning modules. Inter-agent collaboration is facilitated through structured communication protocols, including shared memory repositories, iterative debate, and relay-style knowledge transfer. Drawing on principles from cognitive architectures, multi-agent coordination theory, and information retrieval, SPARK models how emergent personalization properties arise from distributed agent behaviors governed by minimal coordination rules. The framework yields testable predictions regarding coordination efficiency, personalization quality, and cognitive load distribution, while incorporating adaptive learning mechanisms for continuous persona refinement. By integrating fine-grained agent specialization with cooperative retrieval, SPARK provides insights for next-generation search systems capable of capturing the complexity, fluidity, and context sensitivity of human information-seeking behavior.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†SPARKæ¡†æ¶ï¼Œé€šè¿‡åŸºäºè§’è‰²çš„å¤šæ™ºèƒ½ä½“åä½œå®ç°ä¸ªæ€§åŒ–æœç´¢ï¼Œå…¶ä¸­æ¯ä¸ªæ™ºèƒ½ä½“æ‰§è¡Œç‹¬ç«‹çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è¿‡ç¨‹ï¼Œç»“åˆé•¿æœŸ/çŸ­æœŸè®°å¿†å­˜å‚¨å’Œæƒ…å¢ƒæ„ŸçŸ¥æ¨¡å—ï¼Œå¹¶é€šè¿‡ç»“æ„åŒ–åè®®ï¼ˆå¦‚å…±äº«è®°å¿†åº“å’ŒçŸ¥è¯†ä¼ é€’ï¼‰å®ç°åä½œï¼Œæœ€ç»ˆä»åˆ†å¸ƒå¼æ™ºèƒ½ä½“è¡Œä¸ºä¸­æ¶Œç°ä¸ªæ€§åŒ–ç‰¹æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.23966v2">Efficient Context Scaling with LongCat ZigZag Attention</a></td><td><details><summary>å±•å¼€</summary>We introduce LongCat ZigZag Attention (LoZA), which is a sparse attention scheme designed to transform any existing full-attention models into sparse versions with rather limited compute budget. In long-context scenarios, LoZA can achieve significant speed-ups both for prefill-intensive (e.g., retrieval-augmented generation) and decode-intensive (e.g., tool-integrated reasoning) cases. Specifically, by applying LoZA to LongCat-Flash during mid-training, we serve LongCat-Flash-Exp as a long-context foundation model that can swiftly process up to 1 million tokens, enabling efficient long-term reasoning and long-horizon agentic capabilities.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†LongCat ZigZag Attention (LoZA)ï¼Œä¸€ç§ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ—¨åœ¨å°†ç°æœ‰å…¨æ³¨æ„åŠ›æ¨¡å‹è½¬åŒ–ä¸ºè®¡ç®—èµ„æºæœ‰é™çš„ç¨€ç–ç‰ˆæœ¬ã€‚åœ¨é•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸­ï¼ŒLoZAèƒ½æ˜¾è‘—åŠ é€Ÿé¢„å¡«å……å¯†é›†å‹ï¼ˆå¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰å’Œè§£ç å¯†é›†å‹ï¼ˆå¦‚å·¥å…·é›†æˆæ¨ç†ï¼‰ä»»åŠ¡ã€‚é€šè¿‡å°†LoZAåº”ç”¨äºLongCat-Flashæ¨¡å‹ï¼Œä½œè€…æå‡ºäº†LongCat-Flash-Expï¼Œè¿™æ˜¯ä¸€ä¸ªèƒ½é«˜æ•ˆå¤„ç†ç™¾ä¸‡çº§tokençš„é•¿ä¸Šä¸‹æ–‡åŸºç¡€æ¨¡å‹ï¼Œæ”¯æŒé•¿æœŸæ¨ç†å’Œé•¿è§†é‡æ™ºèƒ½ä½“èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.23959v2">Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling</a></td><td><details><summary>å±•å¼€</summary>Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè¶…å›¾çš„åŠ¨æ€è®°å¿†æœºåˆ¶HGMemï¼Œç”¨äºæ”¹è¿›å¤šæ­¥æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿã€‚ä¼ ç»ŸRAGçš„è®°å¿†æ¨¡å—ä»…ä½œä¸ºè¢«åŠ¨å­˜å‚¨å­¤ç«‹äº‹å®çš„é™æ€ç»“æ„ï¼Œè€ŒHGMemé€šè¿‡è¶…è¾¹æ„å»ºé«˜é˜¶å…³è”ï¼Œå°†è®°å¿†è½¬åŒ–ä¸ºæ”¯æŒå¤æ‚æ¨ç†å’Œå…¨å±€ç†è§£çš„åŠ¨æ€çŸ¥è¯†ç½‘ç»œï¼Œæ˜¾è‘—æå‡äº†å¤šæ­¥æ¨ç†å’Œé•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šé¡¹æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸Šä¼˜äºåŸºçº¿ç³»ç»Ÿã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-29
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.24268v1">RAGPart & RAGMask: Retrieval-Stage Defenses Against Corpus Poisoning in Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to enhance large language models (LLMs) with external knowledge, reducing hallucinations and compensating for outdated information. However, recent studies have exposed a critical vulnerability in RAG pipelines corpus poisoning where adversaries inject malicious documents into the retrieval corpus to manipulate model outputs. In this work, we propose two complementary retrieval-stage defenses: RAGPart and RAGMask. Our defenses operate directly on the retriever, making them computationally lightweight and requiring no modification to the generation model. RAGPart leverages the inherent training dynamics of dense retrievers, exploiting document partitioning to mitigate the effect of poisoned points. In contrast, RAGMask identifies suspicious tokens based on significant similarity shifts under targeted token masking. Across two benchmarks, four poisoning strategies, and four state-of-the-art retrievers, our defenses consistently reduce attack success rates while preserving utility under benign conditions. We further introduce an interpretable attack to stress-test our defenses. Our findings highlight the potential and limitations of retrieval-stage defenses, providing practical insights for robust RAG deployments.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ä¸­å­˜åœ¨çš„å®‰å…¨æ¼æ´â€”â€”è¯­æ–™åº“ä¸­æ¯’ï¼ˆcorpus poisoningï¼‰ï¼Œå³æ”»å‡»è€…é€šè¿‡å‘æ£€ç´¢è¯­æ–™åº“ä¸­æ³¨å…¥æ¶æ„æ–‡æ¡£æ¥æ“çºµæ¨¡å‹è¾“å‡ºã€‚ä½œè€…æå‡ºäº†ä¸¤ç§è½»é‡çº§çš„æ£€ç´¢é˜¶æ®µé˜²å¾¡æ–¹æ³•ï¼ˆRAGPartå’ŒRAGMaskï¼‰ï¼Œæ— éœ€ä¿®æ”¹ç”Ÿæˆæ¨¡å‹å³å¯æœ‰æ•ˆé™ä½æ”»å‡»æˆåŠŸç‡ï¼Œå¹¶åœ¨å¤šç§åŸºå‡†æµ‹è¯•å’Œæ”»å‡»ç­–ç•¥ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶åˆ†æäº†æ£€ç´¢é˜¶æ®µé˜²å¾¡çš„æ½œåŠ›ä¸å±€é™æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.24008v1">SPARK: Search Personalization via Agent-Driven Retrieval and Knowledge-sharing</a></td><td><details><summary>å±•å¼€</summary>Personalized search demands the ability to model users' evolving, multi-dimensional information needs; a challenge for systems constrained by static profiles or monolithic retrieval pipelines. We present SPARK (Search Personalization via Agent-Driven Retrieval and Knowledge-sharing), a framework in which coordinated persona-based large language model (LLM) agents deliver task-specific retrieval and emergent personalization. SPARK formalizes a persona space defined by role, expertise, task context, and domain, and introduces a Persona Coordinator that dynamically interprets incoming queries to activate the most relevant specialized agents. Each agent executes an independent retrieval-augmented generation process, supported by dedicated long- and short-term memory stores and context-aware reasoning modules. Inter-agent collaboration is facilitated through structured communication protocols, including shared memory repositories, iterative debate, and relay-style knowledge transfer. Drawing on principles from cognitive architectures, multi-agent coordination theory, and information retrieval, SPARK models how emergent personalization properties arise from distributed agent behaviors governed by minimal coordination rules. The framework yields testable predictions regarding coordination efficiency, personalization quality, and cognitive load distribution, while incorporating adaptive learning mechanisms for continuous persona refinement. By integrating fine-grained agent specialization with cooperative retrieval, SPARK provides insights for next-generation search systems capable of capturing the complexity, fluidity, and context sensitivity of human information-seeking behavior.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†SPARKæ¡†æ¶ï¼Œé€šè¿‡åŸºäºè§’è‰²çš„LLMä»£ç†å®ç°ä»»åŠ¡ç‰¹å®šçš„æ£€ç´¢å’Œä¸ªæ€§åŒ–æœç´¢ã€‚SPARKåˆ©ç”¨å¤šä¸ªä»£ç†ç‹¬ç«‹æ‰§è¡Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è¿‡ç¨‹ï¼Œæ¯ä¸ªä»£ç†é…å¤‡ä¸“ç”¨çš„é•¿çŸ­æœŸè®°å¿†å­˜å‚¨å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨ç†æ¨¡å—ï¼Œå¹¶é€šè¿‡ç»“æ„åŒ–é€šä¿¡åè®®åä½œï¼Œæœ€ç»ˆå®ç°åŠ¨æ€ä¸ªæ€§åŒ–çš„æœç´¢ä½“éªŒã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.23966v2">Efficient Context Scaling with LongCat ZigZag Attention</a></td><td><details><summary>å±•å¼€</summary>We introduce LongCat ZigZag Attention (LoZA), which is a sparse attention scheme designed to transform any existing full-attention models into sparse versions with rather limited compute budget. In long-context scenarios, LoZA can achieve significant speed-ups both for prefill-intensive (e.g., retrieval-augmented generation) and decode-intensive (e.g., tool-integrated reasoning) cases. Specifically, by applying LoZA to LongCat-Flash during mid-training, we serve LongCat-Flash-Exp as a long-context foundation model that can swiftly process up to 1 million tokens, enabling efficient long-term reasoning and long-horizon agentic capabilities.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†LoZAï¼ˆLongCat ZigZag Attentionï¼‰ï¼Œä¸€ç§ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ—¨åœ¨å°†ç°æœ‰å…¨æ³¨æ„åŠ›æ¨¡å‹è½¬åŒ–ä¸ºè®¡ç®—èµ„æºå—é™çš„ç¨€ç–ç‰ˆæœ¬ï¼Œç‰¹åˆ«é€‚ç”¨äºé•¿ä¸Šä¸‹æ–‡åœºæ™¯ã€‚æ–‡ç« æåˆ°LoZAåœ¨é¢„å¡«å……å¯†é›†å‹ä»»åŠ¡ï¼ˆå¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ŒRAGï¼‰å’Œè§£ç å¯†é›†å‹ä»»åŠ¡ï¼ˆå¦‚å·¥å…·é›†æˆæ¨ç†ï¼‰ä¸­èƒ½æ˜¾è‘—æé€Ÿï¼Œå¹¶é€šè¿‡åº”ç”¨äºLongCat-Flashæ¨¡å‹å±•ç¤ºäº†å…¶å¤„ç†ç™¾ä¸‡çº§tokençš„èƒ½åŠ›ï¼Œä»è€Œæ”¯æŒé«˜æ•ˆé•¿æœŸæ¨ç†å’Œé•¿ç¨‹æ™ºèƒ½ä½“åŠŸèƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.23959v2">Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling</a></td><td><details><summary>å±•å¼€</summary>Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè¶…å›¾çš„åŠ¨æ€è®°å¿†æœºåˆ¶HGMemï¼Œç”¨äºæ”¹è¿›å¤šæ­¥æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿã€‚ç°æœ‰çš„RAGç³»ç»Ÿé€šå¸¸ä½¿ç”¨é™æ€è®°å¿†å­˜å‚¨å­¤ç«‹çš„äº‹å®ï¼Œå¿½ç•¥äº†é«˜é˜¶å…³è”ï¼Œå¯¼è‡´æ¨ç†ç¢ç‰‡åŒ–å’Œå…¨å±€ç†è§£èƒ½åŠ›ä¸è¶³ã€‚HGMemé€šè¿‡è¶…è¾¹è¡¨ç¤ºè®°å¿†å•å…ƒï¼Œé€æ­¥æ„å»ºé«˜é˜¶äº¤äº’ï¼Œå½¢æˆåŠ¨æ€çš„çŸ¥è¯†ç»“æ„ï¼Œä»è€Œæ”¯æŒæ›´æ·±å…¥çš„æ¨ç†å’Œå…¨å±€ç†è§£ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å¤šæ­¥RAGä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºåŸºçº¿ç³»ç»Ÿã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.23848v1">Integrating Domain Knowledge for Financial QA: A Multi-Retriever RAG Approach with LLMs</a></td><td><details><summary>å±•å¼€</summary>This research project addresses the errors of financial numerical reasoning Question Answering (QA) tasks due to the lack of domain knowledge in finance. Despite recent advances in Large Language Models (LLMs), financial numerical questions remain challenging because they require specific domain knowledge in finance and complex multi-step numeric reasoning. We implement a multi-retriever Retrieval Augmented Generators (RAG) system to retrieve both external domain knowledge and internal question contexts, and utilize the latest LLM to tackle these tasks. Through comprehensive ablation experiments and error analysis, we find that domain-specific training with the SecBERT encoder significantly contributes to our best neural symbolic model surpassing the FinQA paper's top model, which serves as our baseline. This suggests the potential superior performance of domain-specific training. Furthermore, our best prompt-based LLM generator achieves the state-of-the-art (SOTA) performance with significant improvement (>7%), yet it is still below the human expert performance. This study highlights the trade-off between hallucinations loss and external knowledge gains in smaller models and few-shot examples. For larger models, the gains from external facts typically outweigh the hallucination loss. Finally, our findings confirm the enhanced numerical reasoning capabilities of the latest LLM, optimized for few-shot learning.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡é’ˆå¯¹é‡‘èæ•°å€¼æ¨ç†é—®ç­”ä»»åŠ¡ä¸­å› ç¼ºä¹é¢†åŸŸçŸ¥è¯†å¯¼è‡´çš„é”™è¯¯ï¼Œæå‡ºäº†ä¸€ç§å¤šæ£€ç´¢å™¨çš„RAGç³»ç»Ÿï¼Œç»“åˆå¤–éƒ¨é‡‘èé¢†åŸŸçŸ¥è¯†å’Œå†…éƒ¨é—®é¢˜ä¸Šä¸‹æ–‡ï¼Œåˆ©ç”¨æœ€æ–°å¤§è¯­è¨€æ¨¡å‹æå‡æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œé¢†åŸŸç‰¹å®šè®­ç»ƒå’Œæœ€æ–°LLMæ˜¾è‘—æé«˜äº†æ¨¡å‹è¡¨ç°ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´ï¼Œå¹¶æ¢è®¨äº†å¹»è§‰æŸå¤±ä¸å¤–éƒ¨çŸ¥è¯†å¢ç›Šçš„æƒè¡¡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.23836v1">Retrieval Augmented Question Answering: When Should LLMs Admit Ignorance?</a></td><td><details><summary>å±•å¼€</summary>The success of expanded context windows in Large Language Models (LLMs) has driven increased use of broader context in retrieval-augmented generation. We investigate the use of LLMs for retrieval augmented question answering. While longer contexts make it easier to incorporate targeted knowledge, they introduce more irrelevant information that hinders the model's generation process and degrades its performance. To address the issue, we design an adaptive prompting strategy which involves splitting the retrieved information into smaller chunks and sequentially prompting a LLM to answer the question using each chunk. Adjusting the chunk size allows a trade-off between incorporating relevant information and reducing irrelevant information. Experimental results on three open-domain question answering datasets demonstrate that the adaptive strategy matches the performance of standard prompting while using fewer tokens. Our analysis reveals that when encountering insufficient information, the LLM often generates incorrect answers instead of declining to respond, which constitutes a major source of error. This finding highlights the need for further research into enhancing LLMs' ability to effectively decline requests when faced with inadequate information.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†åœ¨æ£€ç´¢å¢å¼ºé—®ç­”ä¸­ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ—¶ï¼Œé•¿ä¸Šä¸‹æ–‡çª—å£å¸¦æ¥çš„æŒ‘æˆ˜ï¼ˆå¦‚æ— å…³ä¿¡æ¯å¹²æ‰°ï¼‰ï¼Œå¹¶æå‡ºäº†ä¸€ç§è‡ªé€‚åº”æç¤ºç­–ç•¥â€”â€”å°†æ£€ç´¢ä¿¡æ¯åˆ†å‰²ä¸ºå°å—å¹¶ä¾æ¬¡æç¤ºLLMå›ç­”ï¼Œä»¥å¹³è¡¡ç›¸å…³æ€§ä¸å™ªå£°ã€‚å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨å‡å°‘tokenä½¿ç”¨é‡çš„åŒæ—¶ä¿æŒæ€§èƒ½ï¼Œå¹¶å‘ç°LLMåœ¨ä¿¡æ¯ä¸è¶³æ—¶æ˜“ç”Ÿæˆé”™è¯¯ç­”æ¡ˆè€Œéæ‹’ç»å›ç­”ï¼ŒæŒ‡å‡ºéœ€æ”¹è¿›å…¶â€œæ‹’ç­”èƒ½åŠ›â€ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.23489v2">The Gaining Paths to Investment Success: Information-Driven LLM Graph Reasoning for Venture Capital Prediction</a></td><td><details><summary>å±•å¼€</summary>Most venture capital (VC) investments fail, while a few deliver outsized returns. Accurately predicting startup success requires synthesizing complex relational evidence, including company disclosures, investor track records, and investment network structures, through explicit reasoning to form coherent, interpretable investment theses. Traditional machine learning and graph neural networks both lack this reasoning capability. Large language models (LLMs) offer strong reasoning but face a modality mismatch with graphs. Recent graph-LLM methods target in-graph tasks where answers lie within the graph, whereas VC prediction is off-graph: the target exists outside the network. The core challenge is selecting graph paths that maximize predictor performance on an external objective while enabling step-by-step reasoning. We present MIRAGE-VC, a multi-perspective retrieval-augmented generation framework that addresses two obstacles: path explosion (thousands of candidate paths overwhelm LLM context) and heterogeneous evidence fusion (different startups need different analytical emphasis). Our information-gain-driven path retriever iteratively selects high-value neighbors, distilling investment networks into compact chains for explicit reasoning. A multi-agent architecture integrates three evidence streams via a learnable gating mechanism based on company attributes. Under strict anti-leakage controls, MIRAGE-VC achieves +5.0% F1 and +16.6% PrecisionAt5, and sheds light on other off-graph prediction tasks such as recommendation and risk assessment. Code: https://anonymous.4open.science/r/MIRAGE-VC-323F.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†MIRAGE-VCæ¡†æ¶ï¼Œé€šè¿‡å¤šè§†è§’æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è§£å†³é£æŠ•é¢„æµ‹ä¸­çš„è·¯å¾„çˆ†ç‚¸å’Œå¼‚æ„è¯æ®èåˆé—®é¢˜ï¼Œç»“åˆå›¾ç¥ç»ç½‘ç»œä¸LLMçš„æ¨ç†èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†é¢„æµ‹æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.23307v1">RobustMask: Certified Robustness against Adversarial Neural Ranking Attack via Randomized Masking</a></td><td><details><summary>å±•å¼€</summary>Neural ranking models have achieved remarkable progress and are now widely deployed in real-world applications such as Retrieval-Augmented Generation (RAG). However, like other neural architectures, they remain vulnerable to adversarial manipulations: subtle character-, word-, or phrase-level perturbations can poison retrieval results and artificially promote targeted candidates, undermining the integrity of search engines and downstream systems. Existing defenses either rely on heuristics with poor generalization or on certified methods that assume overly strong adversarial knowledge, limiting their practical use. To address these challenges, we propose RobustMask, a novel defense that combines the context-prediction capability of pretrained language models with a randomized masking-based smoothing mechanism. Our approach strengthens neural ranking models against adversarial perturbations at the character, word, and phrase levels. Leveraging both the pairwise comparison ability of ranking models and probabilistic statistical analysis, we provide a theoretical proof of RobustMask's certified top-K robustness. Extensive experiments further demonstrate that RobustMask successfully certifies over 20% of candidate documents within the top-10 ranking positions against adversarial perturbations affecting up to 30% of their content. These results highlight the effectiveness of RobustMask in enhancing the adversarial robustness of neural ranking models, marking a significant step toward providing stronger security guarantees for real-world retrieval systems.</details></td><td><details><summary>å±•å¼€</summary></details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.23236v2">KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta</a></td><td><details><summary>å±•å¼€</summary>Making deep learning recommendation model (DLRM) training and inference fast and efficient is important. However, this presents three key system challenges - model architecture diversity, kernel primitive diversity, and hardware generation and architecture heterogeneity. This paper presents KernelEvolve-an agentic kernel coding framework-to tackle heterogeneity at-scale for DLRM. KernelEvolve is designed to take kernel specifications as input and automate the process of kernel generation and optimization for recommendation model across heterogeneous hardware architectures. KernelEvolve does so by operating at multiple programming abstractions, from Triton and CuTe DSL to low-level hardware agnostic languages, spanning the full hardware-software optimization stack. The kernel optimization process is described as graph-based search with selection policy, universal operator, fitness function, and termination rule, dynamically adapts to runtime execution context through retrieval-augmented prompt synthesis. We designed, implemented, and deployed KernelEvolve to optimize a wide variety of production recommendation models across generations of NVIDIA and AMD GPUs, as well as Meta's AI accelerators. We validate KernelEvolve on the publicly-available KernelBench suite, achieving 100% pass rate on all 250 problems across three difficulty levels, and 160 PyTorch ATen operators across three heterogeneous hardware platforms, demonstrating 100% correctness. KernelEvolve reduces development time from weeks to hours and achieves substantial performance improvements over PyTorch baselines across diverse production use cases and for heterogeneous AI systems at-scale. Beyond performance efficiency improvements, KernelEvolve significantly mitigates the programmability barrier for new AI hardware by enabling automated kernel generation for in-house developed AI hardware.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ä»‹ç»äº†KernelEvolveï¼Œä¸€ä¸ªç”¨äºè§£å†³æ·±åº¦å­¦ä¹ æ¨èæ¨¡å‹ï¼ˆDLRMï¼‰åœ¨å¼‚æ„ç¡¬ä»¶æ¶æ„ä¸Šè®­ç»ƒå’Œæ¨ç†æ•ˆç‡é—®é¢˜çš„ä»£ç†å†…æ ¸ç¼–ç æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¤šçº§ç¼–ç¨‹æŠ½è±¡å®ç°å†…æ ¸è‡ªåŠ¨ç”Ÿæˆä¸ä¼˜åŒ–ï¼Œå¹¶åˆ©ç”¨æ£€ç´¢å¢å¼ºçš„æç¤ºåˆæˆæŠ€æœ¯åŠ¨æ€é€‚åº”è¿è¡Œæ—¶æ‰§è¡Œä¸Šä¸‹æ–‡ï¼Œæ˜¾è‘—æå‡äº†å¼€å‘æ•ˆç‡å’Œæ€§èƒ½ï¼ŒåŒæ—¶æ”¯æŒå¤šç§ç¡¬ä»¶å¹³å°ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.23132v1">Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems</a></td><td><details><summary>å±•å¼€</summary>Machine learning (ML) underpins foundation models in finance, healthcare, and critical infrastructure, making them targets for data poisoning, model extraction, prompt injection, automated jailbreaking, and preference-guided black-box attacks that exploit model comparisons. Larger models can be more vulnerable to introspection-driven jailbreaks and cross-modal manipulation. Traditional cybersecurity lacks ML-specific threat modeling for foundation, multimodal, and RAG systems. Objective: Characterize ML security risks by identifying dominant TTPs, vulnerabilities, and targeted lifecycle stages. Methods: We extract 93 threats from MITRE ATLAS (26), AI Incident Database (12), and literature (55), and analyze 854 GitHub/Python repositories. A multi-agent RAG system (ChatGPT-4o, temp 0.4) mines 300+ articles to build an ontology-driven threat graph linking TTPs, vulnerabilities, and stages. Results: We identify unreported threats including commercial LLM API model stealing, parameter memorization leakage, and preference-guided text-only jailbreaks. Dominant TTPs include MASTERKEY-style jailbreaking, federated poisoning, diffusion backdoors, and preference optimization leakage, mainly impacting pre-training and inference. Graph analysis reveals dense vulnerability clusters in libraries with poor patch propagation. Conclusion: Adaptive, ML-specific security frameworks, combining dependency hygiene, threat intelligence, and monitoring, are essential to mitigate supply-chain and inference risks across the ML lifecycle.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰åœ¨åŸºç¡€æ¨¡å‹ä¸­çš„å®‰å…¨é£é™©ï¼Œç‰¹åˆ«å…³æ³¨äº†åŒ…æ‹¬RAGç³»ç»Ÿåœ¨å†…çš„å¤šæ¨¡æ€ç³»ç»Ÿçš„å¨èƒå»ºæ¨¡ï¼Œé€šè¿‡æ„å»ºæœ¬ä½“é©±åŠ¨çš„å¨èƒå›¾è°±åˆ†æäº†æœªæŠ¥å‘Šçš„å¨èƒå’Œä¸»è¦æ”»å‡»æ‰‹æ³•ï¼Œå¹¶æå‡ºäº†ç»“åˆä¾èµ–ç®¡ç†ã€å¨èƒæƒ…æŠ¥å’Œç›‘æ§çš„MLä¸“ç”¨å®‰å…¨æ¡†æ¶ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-28
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.23848v1">Integrating Domain Knowledge for Financial QA: A Multi-Retriever RAG Approach with LLMs</a></td><td><details><summary>å±•å¼€</summary>This research project addresses the errors of financial numerical reasoning Question Answering (QA) tasks due to the lack of domain knowledge in finance. Despite recent advances in Large Language Models (LLMs), financial numerical questions remain challenging because they require specific domain knowledge in finance and complex multi-step numeric reasoning. We implement a multi-retriever Retrieval Augmented Generators (RAG) system to retrieve both external domain knowledge and internal question contexts, and utilize the latest LLM to tackle these tasks. Through comprehensive ablation experiments and error analysis, we find that domain-specific training with the SecBERT encoder significantly contributes to our best neural symbolic model surpassing the FinQA paper's top model, which serves as our baseline. This suggests the potential superior performance of domain-specific training. Furthermore, our best prompt-based LLM generator achieves the state-of-the-art (SOTA) performance with significant improvement (>7%), yet it is still below the human expert performance. This study highlights the trade-off between hallucinations loss and external knowledge gains in smaller models and few-shot examples. For larger models, the gains from external facts typically outweigh the hallucination loss. Finally, our findings confirm the enhanced numerical reasoning capabilities of the latest LLM, optimized for few-shot learning.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡é’ˆå¯¹é‡‘èæ•°å€¼æ¨ç†é—®ç­”ä»»åŠ¡ä¸­å› ç¼ºä¹é¢†åŸŸçŸ¥è¯†å¯¼è‡´çš„é”™è¯¯é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å¤šæ£€ç´¢å™¨çš„RAGç³»ç»Ÿï¼Œç»“åˆå¤–éƒ¨é‡‘èé¢†åŸŸçŸ¥è¯†å’Œå†…éƒ¨é—®é¢˜ä¸Šä¸‹æ–‡ï¼Œåˆ©ç”¨æœ€æ–°å¤§è¯­è¨€æ¨¡å‹æå‡ä»»åŠ¡æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œé¢†åŸŸç‰¹å®šè®­ç»ƒï¼ˆå¦‚SecBERTç¼–ç å™¨ï¼‰æ˜¾è‘—æå‡æ¨¡å‹è¡¨ç°ï¼Œä¸”åŸºäºæç¤ºçš„ç”Ÿæˆå™¨è¾¾åˆ°SOTAæ€§èƒ½ï¼ˆæå‡>7%ï¼‰ï¼Œä½†ä»ä½äºäººç±»ä¸“å®¶æ°´å¹³ã€‚ç ”ç©¶è¿˜æ­ç¤ºäº†å°æ¨¡å‹å¹»è§‰æŸå¤±ä¸å¤–éƒ¨çŸ¥è¯†å¢ç›Šçš„æƒè¡¡ï¼Œå¹¶éªŒè¯äº†æœ€æ–°å¤§è¯­è¨€æ¨¡å‹åœ¨å°‘æ ·æœ¬å­¦ä¹ ä¸­çš„å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.23836v1">Retrieval Augmented Question Answering: When Should LLMs Admit Ignorance?</a></td><td><details><summary>å±•å¼€</summary>The success of expanded context windows in Large Language Models (LLMs) has driven increased use of broader context in retrieval-augmented generation. We investigate the use of LLMs for retrieval augmented question answering. While longer contexts make it easier to incorporate targeted knowledge, they introduce more irrelevant information that hinders the model's generation process and degrades its performance. To address the issue, we design an adaptive prompting strategy which involves splitting the retrieved information into smaller chunks and sequentially prompting a LLM to answer the question using each chunk. Adjusting the chunk size allows a trade-off between incorporating relevant information and reducing irrelevant information. Experimental results on three open-domain question answering datasets demonstrate that the adaptive strategy matches the performance of standard prompting while using fewer tokens. Our analysis reveals that when encountering insufficient information, the LLM often generates incorrect answers instead of declining to respond, which constitutes a major source of error. This finding highlights the need for further research into enhancing LLMs' ability to effectively decline requests when faced with inadequate information.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†åœ¨æ£€ç´¢å¢å¼ºé—®ç­”ä¸­ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ—¶ï¼Œé•¿ä¸Šä¸‹æ–‡çª—å£å¸¦æ¥çš„æŒ‘æˆ˜ï¼ˆå¦‚æ— å…³ä¿¡æ¯å¹²æ‰°ï¼‰ï¼Œå¹¶æå‡ºäº†ä¸€ç§è‡ªé€‚åº”æç¤ºç­–ç•¥â€”â€”å°†æ£€ç´¢ä¿¡æ¯åˆ†å—å¹¶ä¾æ¬¡æç¤ºæ¨¡å‹å›ç­”ï¼Œä»¥å¹³è¡¡ç›¸å…³æ€§ä¸å™ªå£°ã€‚å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨å‡å°‘tokenä½¿ç”¨é‡çš„åŒæ—¶ä¿æŒæ€§èƒ½ï¼Œå¹¶å‘ç°LLMåœ¨ä¿¡æ¯ä¸è¶³æ—¶æ˜“ç”Ÿæˆé”™è¯¯ç­”æ¡ˆè€Œéæ‹’ç»å›ç­”ï¼ŒæŒ‡å‡ºéœ€æ”¹è¿›å…¶â€œæ‹’ç­”èƒ½åŠ›â€ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.23489v2">The Gaining Paths to Investment Success: Information-Driven LLM Graph Reasoning for Venture Capital Prediction</a></td><td><details><summary>å±•å¼€</summary>Most venture capital (VC) investments fail, while a few deliver outsized returns. Accurately predicting startup success requires synthesizing complex relational evidence, including company disclosures, investor track records, and investment network structures, through explicit reasoning to form coherent, interpretable investment theses. Traditional machine learning and graph neural networks both lack this reasoning capability. Large language models (LLMs) offer strong reasoning but face a modality mismatch with graphs. Recent graph-LLM methods target in-graph tasks where answers lie within the graph, whereas VC prediction is off-graph: the target exists outside the network. The core challenge is selecting graph paths that maximize predictor performance on an external objective while enabling step-by-step reasoning. We present MIRAGE-VC, a multi-perspective retrieval-augmented generation framework that addresses two obstacles: path explosion (thousands of candidate paths overwhelm LLM context) and heterogeneous evidence fusion (different startups need different analytical emphasis). Our information-gain-driven path retriever iteratively selects high-value neighbors, distilling investment networks into compact chains for explicit reasoning. A multi-agent architecture integrates three evidence streams via a learnable gating mechanism based on company attributes. Under strict anti-leakage controls, MIRAGE-VC achieves +5.0% F1 and +16.6% PrecisionAt5, and sheds light on other off-graph prediction tasks such as recommendation and risk assessment. Code: https://anonymous.4open.science/r/MIRAGE-VC-323F.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†MIRAGE-VCï¼Œä¸€ä¸ªå¤šè§†è§’æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºé¢„æµ‹é£é™©æŠ•èµ„ï¼ˆVCï¼‰ä¸­çš„åˆåˆ›ä¼ä¸šæˆåŠŸæ¦‚ç‡ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¿¡æ¯å¢ç›Šé©±åŠ¨çš„è·¯å¾„æ£€ç´¢å™¨ä»å¤æ‚çš„æŠ•èµ„ç½‘ç»œä¸­ç­›é€‰å…³é”®è·¯å¾„ï¼Œå¹¶ç»“åˆå¤šæ™ºèƒ½ä½“æ¶æ„èåˆå¼‚æ„è¯æ®ï¼ˆå¦‚å…¬å¸æŠ«éœ²ã€æŠ•èµ„è€…è®°å½•å’Œç½‘ç»œç»“æ„ï¼‰ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨æ˜¾å¼æ¨ç†å’Œæ¨¡æ€åŒ¹é…ä¸Šçš„ä¸è¶³ï¼Œæ˜¾è‘—æå‡äº†é¢„æµ‹æ€§èƒ½ï¼ˆF1å’ŒPrecisionAt5æŒ‡æ ‡ï¼‰ï¼Œå¹¶é€‚ç”¨äºå…¶ä»–å›¾å¤–é¢„æµ‹ä»»åŠ¡ï¼ˆå¦‚æ¨èå’Œé£é™©è¯„ä¼°ï¼‰ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.23307v1">RobustMask: Certified Robustness against Adversarial Neural Ranking Attack via Randomized Masking</a></td><td><details><summary>å±•å¼€</summary>Neural ranking models have achieved remarkable progress and are now widely deployed in real-world applications such as Retrieval-Augmented Generation (RAG). However, like other neural architectures, they remain vulnerable to adversarial manipulations: subtle character-, word-, or phrase-level perturbations can poison retrieval results and artificially promote targeted candidates, undermining the integrity of search engines and downstream systems. Existing defenses either rely on heuristics with poor generalization or on certified methods that assume overly strong adversarial knowledge, limiting their practical use. To address these challenges, we propose RobustMask, a novel defense that combines the context-prediction capability of pretrained language models with a randomized masking-based smoothing mechanism. Our approach strengthens neural ranking models against adversarial perturbations at the character, word, and phrase levels. Leveraging both the pairwise comparison ability of ranking models and probabilistic statistical analysis, we provide a theoretical proof of RobustMask's certified top-K robustness. Extensive experiments further demonstrate that RobustMask successfully certifies over 20% of candidate documents within the top-10 ranking positions against adversarial perturbations affecting up to 30% of their content. These results highlight the effectiveness of RobustMask in enhancing the adversarial robustness of neural ranking models, marking a significant step toward providing stronger security guarantees for real-world retrieval systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRobustMaskçš„æ–°å‹é˜²å¾¡æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºç¥ç»æ’åºæ¨¡å‹å¯¹æŠ—å­—ç¬¦ã€å•è¯å’ŒçŸ­è¯­çº§åˆ«å¯¹æŠ—æ€§æ‰°åŠ¨çš„é²æ£’æ€§ï¼Œç‰¹åˆ«é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­‰å®é™…åº”ç”¨ä¸­çš„æ’åºæ¨¡å‹ã€‚é€šè¿‡ç»“åˆé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡é¢„æµ‹èƒ½åŠ›å’Œéšæœºæ©ç å¹³æ»‘æœºåˆ¶ï¼Œå¹¶åŸºäºç†è®ºè¯æ˜å’Œå®éªŒéªŒè¯ï¼Œè¯¥æ–¹æ³•åœ¨å¯¹æŠ—æ€§æ‰°åŠ¨ä¸‹èƒ½æœ‰æ•ˆä¿è¯æ’åºç»“æœçš„å¯é æ€§ï¼Œæå‡äº†ç°å®æ£€ç´¢ç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.23132v1">Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems</a></td><td><details><summary>å±•å¼€</summary>Machine learning (ML) underpins foundation models in finance, healthcare, and critical infrastructure, making them targets for data poisoning, model extraction, prompt injection, automated jailbreaking, and preference-guided black-box attacks that exploit model comparisons. Larger models can be more vulnerable to introspection-driven jailbreaks and cross-modal manipulation. Traditional cybersecurity lacks ML-specific threat modeling for foundation, multimodal, and RAG systems. Objective: Characterize ML security risks by identifying dominant TTPs, vulnerabilities, and targeted lifecycle stages. Methods: We extract 93 threats from MITRE ATLAS (26), AI Incident Database (12), and literature (55), and analyze 854 GitHub/Python repositories. A multi-agent RAG system (ChatGPT-4o, temp 0.4) mines 300+ articles to build an ontology-driven threat graph linking TTPs, vulnerabilities, and stages. Results: We identify unreported threats including commercial LLM API model stealing, parameter memorization leakage, and preference-guided text-only jailbreaks. Dominant TTPs include MASTERKEY-style jailbreaking, federated poisoning, diffusion backdoors, and preference optimization leakage, mainly impacting pre-training and inference. Graph analysis reveals dense vulnerability clusters in libraries with poor patch propagation. Conclusion: Adaptive, ML-specific security frameworks, combining dependency hygiene, threat intelligence, and monitoring, are essential to mitigate supply-chain and inference risks across the ML lifecycle.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰åœ¨åŸºç¡€æ¨¡å‹ä¸­çš„å®‰å…¨é£é™©ï¼Œç‰¹åˆ«å…³æ³¨äº†åŒ…æ‹¬RAGç³»ç»Ÿåœ¨å†…çš„å¤šç§æ¨¡å‹é¢ä¸´çš„å¨èƒã€‚ç ”ç©¶é€šè¿‡åˆ†æå¤šä¸ªæ•°æ®æºæ„å»ºäº†ä¸€ä¸ªå¨èƒå›¾è°±ï¼Œè¯†åˆ«äº†æœªæŠ¥å‘Šçš„å¨èƒå’Œä¸»è¦æ”»å‡»æ‰‹æ³•ï¼ˆå¦‚æ¨¡å‹çªƒå–ã€å‚æ•°æ³„éœ²ç­‰ï¼‰ï¼Œå¹¶æŒ‡å‡ºä¼ ç»Ÿç½‘ç»œå®‰å…¨æ–¹æ³•åœ¨MLç‰¹å®šå¨èƒå»ºæ¨¡ä¸Šçš„ä¸è¶³ã€‚æ–‡ç« æå‡ºéœ€è¦ç»“åˆä¾èµ–ç®¡ç†ã€å¨èƒæƒ…æŠ¥å’Œç›‘æ§çš„è‡ªé€‚åº”å®‰å…¨æ¡†æ¶æ¥åº”å¯¹MLç”Ÿå‘½å‘¨æœŸä¸­çš„ä¾›åº”é“¾å’Œæ¨ç†é£é™©ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.22827v1">FasterPy: An LLM-based Code Execution Efficiency Optimization Framework</a></td><td><details><summary>å±•å¼€</summary>Code often suffers from performance bugs. These bugs necessitate the research and practice of code optimization. Traditional rule-based methods rely on manually designing and maintaining rules for specific performance bugs (e.g., redundant loops, repeated computations), making them labor-intensive and limited in applicability. In recent years, machine learning and deep learning-based methods have emerged as promising alternatives by learning optimization heuristics from annotated code corpora and performance measurements. However, these approaches usually depend on specific program representations and meticulously crafted training datasets, making them costly to develop and difficult to scale. With the booming of Large Language Models (LLMs), their remarkable capabilities in code generation have opened new avenues for automated code optimization. In this work, we proposed FasterPy, a low-cost and efficient framework that adapts LLMs to optimize the execution efficiency of Python code. FasterPy combines Retrieval-Augmented Generation (RAG), supported by a knowledge base constructed from existing performance-improving code pairs and corresponding performance measurements, with Low-Rank Adaptation (LoRA) to enhance code optimization performance. Our experimental results on the Performance Improving Code Edits (PIE) benchmark demonstrate that our method outperforms existing models on multiple metrics. The FasterPy tool and the experimental results are available at https://github.com/WuYue22/fasterpy.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†FasterPyæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œç»“åˆä»æ€§èƒ½æ”¹è¿›ä»£ç å¯¹å’Œæ€§èƒ½æµ‹é‡æ„å»ºçš„çŸ¥è¯†åº“ï¼Œä»¥åŠä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ï¼Œæ¥ä¼˜åŒ–Pythonä»£ç çš„æ‰§è¡Œæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ¨¡å‹ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-27
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.22827v1">FasterPy: An LLM-based Code Execution Efficiency Optimization Framework</a></td><td><details><summary>å±•å¼€</summary>Code often suffers from performance bugs. These bugs necessitate the research and practice of code optimization. Traditional rule-based methods rely on manually designing and maintaining rules for specific performance bugs (e.g., redundant loops, repeated computations), making them labor-intensive and limited in applicability. In recent years, machine learning and deep learning-based methods have emerged as promising alternatives by learning optimization heuristics from annotated code corpora and performance measurements. However, these approaches usually depend on specific program representations and meticulously crafted training datasets, making them costly to develop and difficult to scale. With the booming of Large Language Models (LLMs), their remarkable capabilities in code generation have opened new avenues for automated code optimization. In this work, we proposed FasterPy, a low-cost and efficient framework that adapts LLMs to optimize the execution efficiency of Python code. FasterPy combines Retrieval-Augmented Generation (RAG), supported by a knowledge base constructed from existing performance-improving code pairs and corresponding performance measurements, with Low-Rank Adaptation (LoRA) to enhance code optimization performance. Our experimental results on the Performance Improving Code Edits (PIE) benchmark demonstrate that our method outperforms existing models on multiple metrics. The FasterPy tool and the experimental results are available at https://github.com/WuYue22/fasterpy.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºFasterPyçš„ä½æˆæœ¬é«˜æ•ˆæ¡†æ¶ï¼Œåˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¼˜åŒ–Pythonä»£ç çš„æ‰§è¡Œæ•ˆç‡ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«æ€§èƒ½æ”¹è¿›ä»£ç å¯¹å’Œæ€§èƒ½æµ‹é‡çš„çŸ¥è¯†åº“ï¼Œå¹¶ç»“åˆä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯ï¼ŒFasterPyåœ¨æ€§èƒ½æ”¹è¿›ä»£ç ç¼–è¾‘ï¼ˆPIEï¼‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.22629v1">DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>As Retrieval-Augmented Generation (RAG) systems evolve toward more sophisticated architectures, ensuring their trustworthiness through explainable and robust evaluation becomes critical. Existing scalar metrics suffer from limited interpretability, inadequate uncertainty quantification, and computational inefficiency in multi-system comparisons, hindering responsible deployment of RAG technologies. We introduce DICE (Discrete Interpretable Comparative Evaluation), a two-stage, evidence-coupled framework that advances explainability and robustness in RAG evaluation. DICE combines deep analytical reasoning with probabilistic $\{A, B, Tie\}$ scoring to produce transparent, confidence-aware judgments that support accountable system improvement through interpretable reasoning traces, enabling systematic error diagnosis and actionable insights. To address efficiency challenges at scale, DICE employs a Swiss-system tournament that reduces computational complexity from $O(N^2)$ to $O(N \log N)$, achieving a 42.9% reduction in our eight-system evaluation while preserving ranking fidelity. Validation on a curated Chinese financial QA dataset demonstrates that DICE achieves 85.7% agreement with human experts, substantially outperforming existing LLM-based metrics such as RAGAS. Our results establish DICE as a responsible, explainable, and efficient paradigm for trustworthy RAG system assessment.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDICEçš„æ–°å‹è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æå‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿçš„å¯è§£é‡Šæ€§å’Œé²æ£’æ€§ã€‚DICEé€šè¿‡ä¸¤é˜¶æ®µè¯æ®è€¦åˆæ–¹æ³•ï¼ˆç»“åˆæ·±åº¦åˆ†ææ¨ç†å’Œæ¦‚ç‡è¯„åˆ†ï¼‰ç”Ÿæˆé€æ˜ä¸”ç½®ä¿¡åº¦æ„ŸçŸ¥çš„åˆ¤æ–­ï¼Œæ”¯æŒå¯è¿½æº¯çš„é”™è¯¯è¯Šæ–­ï¼Œå¹¶é‡‡ç”¨ç‘å£«åˆ¶é”¦æ ‡èµ›æœºåˆ¶å°†è®¡ç®—å¤æ‚åº¦ä»O(NÂ²)é™è‡³O(N log N)ã€‚å®éªŒè¡¨æ˜ï¼ŒDICEåœ¨ä¸­æ–‡é‡‘èQAæ•°æ®é›†ä¸Šè¾¾åˆ°85.7%çš„ä¸“å®¶ä¸€è‡´æ€§ï¼Œæ˜¾è‘—ä¼˜äºRAGASç­‰ç°æœ‰æŒ‡æ ‡ï¼Œä¸ºRAGç³»ç»Ÿçš„å¯ä¿¡è¯„ä¼°æä¾›äº†é«˜æ•ˆã€å¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.00841v1">SLO-Conditioned Action Routing for Retrieval-Augmented Generation: Objective Ablation and Failure Modes</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) introduces a practical control problem: retrieval depth and generation behavior must be chosen per query to satisfy service-level objectives (SLOs) such as cost, refusal rate, and hallucination risk. This work models per-query control as a small discrete action: choose a retrieval depth and a generation mode (guarded vs. auto), or refuse. An offline logged dataset is constructed from SQuAD 2.0 by executing each action and recording accuracy, token cost, hallucination/refusal indicators, and an SLO-weighted reward. Two simple policy-learning objectives are evaluated: supervised classification of the per-state best action (Argmax-CE) and a reward-weighted variant (Argmax-CE-WT). Across the evaluated settings, a strong fixed baseline (low k, guarded prompting) performs competitively; learned policies mainly provide additional cost savings under a quality-focused SLO and can exhibit refusal collapse under a cheap SLO when refusal is heavily rewarded. The contribution is a reproducible case study of SLO-aware control for RAG pipelines, emphasizing failure modes and reporting conventions rather than proposing a new retriever or language model.</details></td><td><details><summary>å±•å¼€</summary></details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.22568v1">Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI</a></td><td><details><summary>å±•å¼€</summary>The phenomenal advances in large language models (LLMs) and other foundation models over the past few years have been based on optimizing large-scale transformer models on the surprisingly simple objective of minimizing next-token prediction loss, a form of predictive coding that is also the backbone of an increasingly popular model of brain function in neuroscience and cognitive science. However, current foundation models ignore three other important components of state-of-the-art predictive coding models: tight integration of actions with generative models, hierarchical compositional structure, and episodic memory. We propose that to achieve safe, interpretable, energy-efficient, and human-like AI, foundation models should integrate actions, at multiple scales of abstraction, with a compositional generative architecture and episodic memory. We present recent evidence from neuroscience and cognitive science on the importance of each of these components. We describe how the addition of these missing components to foundation models could help address some of their current deficiencies: hallucinations and superficial understanding of concepts due to lack of grounding, a missing sense of agency/responsibility due to lack of control, threats to safety and trustworthiness due to lack of interpretability, and energy inefficiency. We compare our proposal to current trends, such as adding chain-of-thought (CoT) reasoning and retrieval-augmented generation (RAG) to foundation models, and discuss new ways of augmenting these models with brain-inspired components. We conclude by arguing that a rekindling of the historically fruitful exchange of ideas between brain science and AI will help pave the way towards safe and interpretable human-centered AI.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå…¶ä»–åŸºç¡€æ¨¡å‹çš„å½“å‰å±€é™æ€§ï¼Œæå‡ºé€šè¿‡æ•´åˆè¡ŒåŠ¨ã€åˆ†å±‚ç»„åˆç»“æ„å’Œæƒ…æ™¯è®°å¿†ç­‰ç¥ç»ç§‘å­¦å’Œè®¤çŸ¥ç§‘å­¦çš„åŸç†æ¥æ”¹è¿›è¿™äº›æ¨¡å‹ã€‚æ–‡ç« ç‰¹åˆ«æåˆ°ï¼Œç°æœ‰çš„æ”¹è¿›æ–¹æ³•å¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†è™½ç„¶æœ‰ä¸€å®šä½œç”¨ï¼Œä½†ä»éœ€ç»“åˆæ›´å¤šè„‘ç§‘å­¦å¯å‘çš„ç»„ä»¶ï¼Œä»¥å®ç°æ›´å®‰å…¨ã€å¯è§£é‡Šä¸”é«˜æ•ˆçš„äººå·¥æ™ºèƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.22442v1">HiFi-RAG: Hierarchical Content Filtering and Two-Pass Generation for Open-Domain RAG</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) in open-domain settings faces significant challenges regarding irrelevant information in retrieved documents and the alignment of generated answers with user intent. We present HiFi-RAG (Hierarchical Filtering RAG), the winning closed-source system in the Text-to-Text static evaluation of the MMU-RAGent NeurIPS 2025 Competition. Our approach moves beyond standard embedding-based retrieval via a multi-stage pipeline. We leverage the speed and cost-efficiency of Gemini 2.5 Flash (4-6x cheaper than Pro) for query formulation, hierarchical content filtering, and citation attribution, while reserving the reasoning capabilities of Gemini 2.5 Pro for final answer generation. On the MMU-RAGent validation set, our system outperformed the baseline, improving ROUGE-L to 0.274 (+19.6%) and DeBERTaScore to 0.677 (+6.2%). On Test2025, our custom dataset evaluating questions that require post-cutoff knowledge (post January 2025), HiFi-RAG outperforms the parametric baseline by 57.4% in ROUGE-L and 14.9% in DeBERTaScore.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†HiFi-RAGï¼ˆHierarchical Filtering RAGï¼‰ï¼Œä¸€ç§åœ¨å¼€æ”¾åŸŸè®¾ç½®ä¸­æ”¹è¿›RAGæŠ€æœ¯çš„å¤šé˜¶æ®µç®¡é“ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé€šè¿‡åˆ†å±‚å†…å®¹è¿‡æ»¤å’Œå¼•ç”¨å½’å› ï¼Œè§£å†³äº†æ£€ç´¢æ–‡æ¡£ä¸­æ— å…³ä¿¡æ¯å’Œå¯¹é½ç”¨æˆ·æ„å›¾çš„æŒ‘æˆ˜ï¼Œå¹¶åœ¨MMU-RAGentç«èµ›ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æå‡äº†ROUGE-Lå’ŒDeBERTaScoreæŒ‡æ ‡ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-26
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.22629v1">DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>As Retrieval-Augmented Generation (RAG) systems evolve toward more sophisticated architectures, ensuring their trustworthiness through explainable and robust evaluation becomes critical. Existing scalar metrics suffer from limited interpretability, inadequate uncertainty quantification, and computational inefficiency in multi-system comparisons, hindering responsible deployment of RAG technologies. We introduce DICE (Discrete Interpretable Comparative Evaluation), a two-stage, evidence-coupled framework that advances explainability and robustness in RAG evaluation. DICE combines deep analytical reasoning with probabilistic $\{A, B, Tie\}$ scoring to produce transparent, confidence-aware judgments that support accountable system improvement through interpretable reasoning traces, enabling systematic error diagnosis and actionable insights. To address efficiency challenges at scale, DICE employs a Swiss-system tournament that reduces computational complexity from $O(N^2)$ to $O(N \log N)$, achieving a 42.9% reduction in our eight-system evaluation while preserving ranking fidelity. Validation on a curated Chinese financial QA dataset demonstrates that DICE achieves 85.7% agreement with human experts, substantially outperforming existing LLM-based metrics such as RAGAS. Our results establish DICE as a responsible, explainable, and efficient paradigm for trustworthy RAG system assessment.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDICEçš„æ–°å‹è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æå‡RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç³»ç»Ÿçš„å¯è§£é‡Šæ€§å’Œé²æ£’æ€§ã€‚DICEé€šè¿‡ä¸¤é˜¶æ®µè¯æ®è€¦åˆæœºåˆ¶ï¼Œç»“åˆæ·±åº¦åˆ†æå’Œæ¦‚ç‡è¯„åˆ†ï¼Œç”Ÿæˆé€æ˜ä¸”ç½®ä¿¡åº¦æ„ŸçŸ¥çš„åˆ¤æ–­ï¼Œæ”¯æŒå¯è§£é‡Šçš„æ¨ç†è¿½è¸ªå’Œç³»ç»Ÿé”™è¯¯è¯Šæ–­ã€‚æ­¤å¤–ï¼ŒDICEé‡‡ç”¨ç‘å£«åˆ¶é”¦æ ‡èµ›æ–¹æ³•æ˜¾è‘—é™ä½è®¡ç®—å¤æ‚åº¦ï¼Œå¹¶åœ¨ä¸­æ–‡é‡‘èQAæ•°æ®é›†ä¸ŠéªŒè¯å…¶ä¸äººç±»ä¸“å®¶åˆ¤æ–­çš„ä¸€è‡´æ€§ï¼ˆ85.7%ï¼‰ï¼Œä¼˜äºç°æœ‰åŸºäºLLMçš„è¯„ä¼°æ–¹æ³•ï¼ˆå¦‚RAGASï¼‰ï¼Œä¸ºRAGç³»ç»Ÿçš„å¯ä¿¡è¯„ä¼°æä¾›äº†é«˜æ•ˆã€å¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2601.00841v1">SLO-Conditioned Action Routing for Retrieval-Augmented Generation: Objective Ablation and Failure Modes</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) introduces a practical control problem: retrieval depth and generation behavior must be chosen per query to satisfy service-level objectives (SLOs) such as cost, refusal rate, and hallucination risk. This work models per-query control as a small discrete action: choose a retrieval depth and a generation mode (guarded vs. auto), or refuse. An offline logged dataset is constructed from SQuAD 2.0 by executing each action and recording accuracy, token cost, hallucination/refusal indicators, and an SLO-weighted reward. Two simple policy-learning objectives are evaluated: supervised classification of the per-state best action (Argmax-CE) and a reward-weighted variant (Argmax-CE-WT). Across the evaluated settings, a strong fixed baseline (low k, guarded prompting) performs competitively; learned policies mainly provide additional cost savings under a quality-focused SLO and can exhibit refusal collapse under a cheap SLO when refusal is heavily rewarded. The contribution is a reproducible case study of SLO-aware control for RAG pipelines, emphasizing failure modes and reporting conventions rather than proposing a new retriever or language model.</details></td><td><details><summary>å±•å¼€</summary></details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.22568v1">Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI</a></td><td><details><summary>å±•å¼€</summary>The phenomenal advances in large language models (LLMs) and other foundation models over the past few years have been based on optimizing large-scale transformer models on the surprisingly simple objective of minimizing next-token prediction loss, a form of predictive coding that is also the backbone of an increasingly popular model of brain function in neuroscience and cognitive science. However, current foundation models ignore three other important components of state-of-the-art predictive coding models: tight integration of actions with generative models, hierarchical compositional structure, and episodic memory. We propose that to achieve safe, interpretable, energy-efficient, and human-like AI, foundation models should integrate actions, at multiple scales of abstraction, with a compositional generative architecture and episodic memory. We present recent evidence from neuroscience and cognitive science on the importance of each of these components. We describe how the addition of these missing components to foundation models could help address some of their current deficiencies: hallucinations and superficial understanding of concepts due to lack of grounding, a missing sense of agency/responsibility due to lack of control, threats to safety and trustworthiness due to lack of interpretability, and energy inefficiency. We compare our proposal to current trends, such as adding chain-of-thought (CoT) reasoning and retrieval-augmented generation (RAG) to foundation models, and discuss new ways of augmenting these models with brain-inspired components. We conclude by arguing that a rekindling of the historically fruitful exchange of ideas between brain science and AI will help pave the way towards safe and interpretable human-centered AI.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å½“å‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå…¶ä»–åŸºç¡€æ¨¡å‹çš„å±€é™æ€§ï¼Œæå‡ºé€šè¿‡æ•´åˆè¡ŒåŠ¨ã€åˆ†å±‚ç»„åˆç»“æ„å’Œæƒ…æ™¯è®°å¿†ç­‰ç¥ç»ç§‘å­¦å’Œè®¤çŸ¥ç§‘å­¦çš„ç»„ä»¶æ¥æ”¹è¿›æ¨¡å‹ã€‚æ–‡ç« å°†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ç­‰ç°æœ‰æ–¹æ³•ä½œä¸ºå¯¹æ¯”ï¼Œå¹¶è®¨è®ºäº†å¦‚ä½•é€šè¿‡è„‘ç§‘å­¦å¯å‘çš„ç»„ä»¶å¢å¼ºè¿™äº›æ¨¡å‹ï¼Œä»¥å®ç°æ›´å®‰å…¨ã€å¯è§£é‡Šå’Œç±»äººçš„äººå·¥æ™ºèƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.22442v1">HiFi-RAG: Hierarchical Content Filtering and Two-Pass Generation for Open-Domain RAG</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) in open-domain settings faces significant challenges regarding irrelevant information in retrieved documents and the alignment of generated answers with user intent. We present HiFi-RAG (Hierarchical Filtering RAG), the winning closed-source system in the Text-to-Text static evaluation of the MMU-RAGent NeurIPS 2025 Competition. Our approach moves beyond standard embedding-based retrieval via a multi-stage pipeline. We leverage the speed and cost-efficiency of Gemini 2.5 Flash (4-6x cheaper than Pro) for query formulation, hierarchical content filtering, and citation attribution, while reserving the reasoning capabilities of Gemini 2.5 Pro for final answer generation. On the MMU-RAGent validation set, our system outperformed the baseline, improving ROUGE-L to 0.274 (+19.6%) and DeBERTaScore to 0.677 (+6.2%). On Test2025, our custom dataset evaluating questions that require post-cutoff knowledge (post January 2025), HiFi-RAG outperforms the parametric baseline by 57.4% in ROUGE-L and 14.9% in DeBERTaScore.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†HiFi-RAGç³»ç»Ÿï¼Œä¸€ç§é€šè¿‡å¤šé˜¶æ®µç®¡é“æ”¹è¿›å¼€æ”¾åŸŸRAGæ€§èƒ½çš„æ–¹æ³•ï¼Œç»“åˆäº†åˆ†å±‚å†…å®¹è¿‡æ»¤å’ŒGeminiæ¨¡å‹çš„é«˜æ•ˆæ£€ç´¢ä¸ç”Ÿæˆèƒ½åŠ›ï¼Œåœ¨MMU-RAGentç«èµ›ä¸­æ˜¾è‘—æå‡äº†ROUGE-Lå’ŒDeBERTaScoreæŒ‡æ ‡ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-25
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody></tbody></table>

### ğŸ“… 2025-12-24
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.21280v1">SMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance</a></td><td><details><summary>å±•å¼€</summary>The user of Engineering Manuals (EM) finds it difficult to read EM s because they are long, have a dense format which includes written documents, step by step procedures, and standard parameter lists for engineering equipment. Off the shelf transformers, especially compact ones, treat this material as a flat stream of tokens. This approach leads to confident but incorrect numeric answers and forces the models to memorize separate facts inefficiently. SMART (Structured Memory and Reasoning Transformer) offers a different and practical solution to the above problem. SMART structures its processing by using a hierarchical approach, and is based upon three main job categories (1) A syntax-aware Fact Extractor (Grammarian) Tree LSTM which extracts facts as subject relation object relations from EM sentences (2) A compact indexed memory MANN (Memory Augmented Neural Network) that indexes these Rational Subject Relation Objects as 384 dimensional vectors that are associated with the source of the information, and (3) A 6 layer Transformer that learns to fuse the previously retrieved facts into its generated response. The entire SMART model utilizes 45.51M parameters, which is 64% less than GPT-2 (124M) and 69% less than BERT (133M), and it achieves a 21.3% higher accuracy than GPT-2, indicating that SMART fits the data better with the least amount of processing requirements. SMART employs dual modes of inference an indexed fast path for known documents (sub-second answer times) and an indexed dynamic path assisted by RAGs for new uploads (FAISS Top 20 results with memory severed at 64 slots). In real world deployment, this framework leads to more well supported results with reduced hallucinations than comparable small transformer models.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†SMARTï¼ˆStructured Memory and Reasoning Transformerï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡åˆ†å±‚å¤„ç†å’Œç»“æ„åŒ–è®°å¿†æ¥è§£å†³å·¥ç¨‹æ‰‹å†Œï¼ˆEMï¼‰ä¿¡æ¯å¤„ç†ä¸­çš„é—®é¢˜ã€‚SMARTç»“åˆäº†è¯­æ³•æ„ŸçŸ¥çš„äº‹å®æå–å™¨ã€ç´§å‡‘çš„ç´¢å¼•è®°å¿†ç½‘ç»œå’ŒTransformerå±‚ï¼Œæ˜¾è‘—æé«˜äº†å›ç­”çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚è®ºæ–‡æåˆ°SMARTåœ¨æ¨ç†æ—¶é‡‡ç”¨äº†ä¸¤ç§æ¨¡å¼ï¼šä¸€ç§æ˜¯å¯¹å·²çŸ¥æ–‡æ¡£çš„å¿«é€Ÿç´¢å¼•è·¯å¾„ï¼Œå¦ä¸€ç§æ˜¯é€šè¿‡RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰è¾…åŠ©çš„åŠ¨æ€è·¯å¾„å¤„ç†æ–°ä¸Šä¼ çš„æ–‡æ¡£ï¼Œä»è€Œå‡å°‘å¹»è§‰å¹¶æå‡ç»“æœçš„å¯ä¿¡åº¦ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.20916v1">MMSRARec: Summarization and Retrieval Augumented Sequential Recommendation Based on Multimodal Large Language Model</a></td><td><details><summary>å±•å¼€</summary>Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant potential in recommendation systems. However, the effective application of MLLMs to multimodal sequential recommendation remains unexplored: A) Existing methods primarily leverage the multimodal semantic understanding capabilities of pre-trained MLLMs to generate item embeddings or semantic IDs, thereby enhancing traditional recommendation models. These approaches generate item representations that exhibit limited interpretability, and pose challenges when transferring to language model-based recommendation systems. B) Other approaches convert user behavior sequence into image-text pairs and perform recommendation through multiple MLLM inference, incurring prohibitive computational and time costs. C) Current MLLM-based recommendation systems generally neglect the integration of collaborative signals. To address these limitations while balancing recommendation performance, interpretability, and computational cost, this paper proposes MultiModal Summarization-and-Retrieval-Augmented Sequential Recommendation. Specifically, we first employ MLLM to summarize items into concise keywords and fine-tune the model using rewards that incorporate summary length, information loss, and reconstruction difficulty, thereby enabling adaptive adjustment of the summarization policy. Inspired by retrieval-augmented generation, we then transform collaborative signals into corresponding keywords and integrate them as supplementary context. Finally, we apply supervised fine-tuning with multi-task learning to align the MLLM with the multimodal sequential recommendation. Extensive evaluations on common recommendation datasets demonstrate the effectiveness of MMSRARec, showcasing its capability to efficiently and interpretably understand user behavior histories and item information for accurate recommendations.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯çš„å¤šæ¨¡æ€åºåˆ—æ¨èæ–¹æ³•ï¼ˆMMSRARecï¼‰ï¼Œé€šè¿‡å°†ç‰©å“æ‘˜è¦åŒ–ä¸ºå…³é”®è¯å¹¶æ•´åˆååŒä¿¡å·ä½œä¸ºè¡¥å……ä¸Šä¸‹æ–‡ï¼Œä»¥æå‡æ¨èæ€§èƒ½ã€å¯è§£é‡Šæ€§å’Œè®¡ç®—æ•ˆç‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.20884v1">The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents</a></td><td><details><summary>å±•å¼€</summary>Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frameworks remain largely heuristic and private, lacking a probabilistic foundation to quantify certainty or justify external interaction.To bridge this gap, we propose a formal probabilistic framework that provides agents with a non-altruistic motive for bidirectional knowledge exchange. We model an agent's belief in a proposition using a Beta-Bernoulli distribution with a forgetting factor ($Î³$). This allows us to isolate epistemic uncertainty as the variance of belief, establishing a dual drive for interaction: A homeostatic motive: The need to maintain certainty against the temporal decay introduced by $Î³$. An optimal learning strategy: Targeting points of maximum ambiguity ($\mathbb{E}[Î¸]=0.5$) to maximize information gain. Under this framework, public contribution is reframed as optimal active learning: sharing solutions to elicit feedback is the most efficient method for an agent to reduce its own uncertainty. To ensure scalability, we introduce epistemic caching, which leverages the forgetting factor to dynamically prioritize resources for the active head of non-stationary knowledge distributions. Finally, we demonstrate how these accumulated belief states serve as verifiable reward signals for Reinforcement Learning from Human Feedback (RLHF) and high-quality data filters for Supervised Fine-Tuning (SFT). Simulation results validate that this uncertainty-driven strategy significantly outperforms random baselines in heterogeneous (Zipfian) environments, maintaining high adaptability to concept drift.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¦‚ç‡æ¡†æ¶çš„åŒå‘çŸ¥è¯†äº¤æ¢æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å½“å‰åŸºäºRAGå’ŒLLMçš„è‡ªä¸»æ™ºèƒ½ä½“åœ¨çŸ¥è¯†è·å–ä¸­çš„å•å‘æ€§å±€é™ï¼ˆå³"è®¤çŸ¥ä¸å¯¹ç§°"ï¼‰ã€‚ä½œè€…é€šè¿‡Beta-Bernoulliåˆ†å¸ƒå»ºæ¨¡æ™ºèƒ½ä½“çš„ä¿¡å¿µï¼Œå¼•å…¥é—å¿˜å› å­Î³æ¥é‡åŒ–è®¤çŸ¥ä¸ç¡®å®šæ€§ï¼Œå¹¶å»ºç«‹äº†ä¸¤ç§äº¤äº’é©±åŠ¨åŠ›ï¼ˆç¨³æ€ç»´æŒåŠ¨æœºå’Œæœ€ä¼˜å­¦ä¹ ç­–ç•¥ï¼‰ã€‚è®ºæ–‡è¿˜æå‡ºäº†"è®¤çŸ¥ç¼“å­˜"æŠ€æœ¯æ¥ä¼˜åŒ–éç¨³æ€çŸ¥è¯†åˆ†å¸ƒä¸‹çš„èµ„æºåˆ†é…ï¼Œå¹¶å±•ç¤ºäº†è¯¥æ¡†æ¶å¦‚ä½•å¢å¼ºRLHFå’ŒSFTã€‚å®éªŒè¯æ˜è¿™ç§ä¸ç¡®å®šæ€§é©±åŠ¨ç­–ç•¥åœ¨å¼‚æ„ç¯å¢ƒä¸­æ˜¾è‘—ä¼˜äºéšæœºåŸºçº¿ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-23
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.20237v1">MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents</a></td><td><details><summary>å±•å¼€</summary>Memory systems have been designed to leverage past experiences in Large Language Model (LLM) agents. However, many deployed memory systems primarily optimize compression and storage, with comparatively less emphasis on explicit, closed-loop control of memory retrieval. From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process. This design departs from the standard retrieve-then-answer pipeline by introducing a closed-loop control mechanism that enables autonomous decision-making. Empirical results on the LoCoMo benchmark demonstrate that MemR$^3$ surpasses strong baselines on LLM-as-a-Judge score, and particularly, it improves existing retrievers across four categories with an overall improvement on RAG (+7.29%) and Zep (+1.94%) using GPT-4.1-mini backend, offering a plug-and-play controller for existing memory stores.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMemR$^3$çš„è‡ªä¸»è®°å¿†æ£€ç´¢ç³»ç»Ÿï¼Œé€šè¿‡è·¯ç”±å™¨å’Œå…¨å±€è¯æ®ç¼ºå£è·Ÿè¸ªå™¨å®ç°é—­ç¯æ§åˆ¶ï¼Œä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„è®°å¿†æ£€ç´¢ä¸å›ç­”æµç¨‹ï¼Œæ˜¾è‘—æå‡äº†RAGå’Œå…¶ä»–æ£€ç´¢æ–¹æ³•çš„æ€§èƒ½ï¼ˆå¦‚RAGæ€§èƒ½æå‡7.29%ï¼‰ï¼Œå¹¶åœ¨LoCoMoåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.20182v1">FaithLens: Detecting and Explaining Faithfulness Hallucination</a></td><td><details><summary>å±•å¼€</summary>Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†FaithLensï¼Œä¸€ä¸ªç”¨äºæ£€æµ‹å¤§è¯­è¨€æ¨¡å‹è¾“å‡ºä¸­å¿ å®æ€§å¹»è§‰ï¼ˆfaithfulness hallucinationï¼‰çš„é«˜æ•ˆæ¨¡å‹ï¼Œç‰¹åˆ«é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œæ‘˜è¦ç­‰å®é™…åº”ç”¨åœºæ™¯ã€‚è¯¥æ¨¡å‹é€šè¿‡åˆæˆè®­ç»ƒæ•°æ®ã€å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼Œåœ¨12é¡¹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºGPT-4ç­‰å…ˆè¿›æ¨¡å‹ï¼Œå¹¶èƒ½æä¾›å¯ä¿¡çš„é¢„æµ‹å’Œè§£é‡Šã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.20145v1">Retrieval-augmented Prompt Learning for Pre-trained Foundation Models</a></td><td><details><summary>å±•å¼€</summary>The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRetroPromptçš„æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥æ£€ç´¢æœºåˆ¶ä»å…¬å¼€çŸ¥è¯†åº“ä¸­åŠ¨æ€è·å–ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»¥å¹³è¡¡è®°å¿†ä¸æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å±•ç¤ºäº†å…¶åœ¨é›¶æ ·æœ¬å’Œå°æ ·æœ¬åœºæ™¯ä¸‹çš„ä¼˜è¶Šæ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.20144v1">Multi-hop Reasoning via Early Knowledge Alignment</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for Large Language Models (LLMs) to address knowledge-intensive queries requiring domain-specific or up-to-date information. To handle complex multi-hop questions that are challenging for single-step retrieval, iterative RAG approaches incorporating reinforcement learning have been proposed. However, existing iterative RAG systems typically plan to decompose questions without leveraging information about the available retrieval corpus, leading to inefficient retrieval and reasoning chains that cascade into suboptimal performance. In this paper, we introduce Early Knowledge Alignment (EKA), a simple but effective module that aligns LLMs with retrieval set before planning in iterative RAG systems with contextually relevant retrieved knowledge. Extensive experiments on six standard RAG datasets demonstrate that by establishing a stronger reasoning foundation, EKA significantly improves retrieval precision, reduces cascading errors, and enhances both performance and efficiency. Our analysis from an entropy perspective demonstrate that incorporating early knowledge reduces unnecessary exploration during the reasoning process, enabling the model to focus more effectively on relevant information subsets. Moreover, EKA proves effective as a versatile, training-free inference strategy that scales seamlessly to large models. Generalization tests across diverse datasets and retrieval corpora confirm the robustness of our approach. Overall, EKA advances the state-of-the-art in iterative RAG systems while illuminating the critical interplay between structured reasoning and efficient exploration in reinforcement learning-augmented frameworks. The code is released at \href{https://github.com/yxzwang/EarlyKnowledgeAlignment}{Github}.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºEarly Knowledge Alignment (EKA)çš„æ¨¡å—ï¼Œç”¨äºæ”¹è¿›è¿­ä»£å¼æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿã€‚EKAé€šè¿‡åœ¨è§„åˆ’é˜¶æ®µå‰å°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç›¸å…³çŸ¥è¯†å¯¹é½ï¼Œæ˜¾è‘—æé«˜äº†æ£€ç´¢ç²¾åº¦ã€å‡å°‘äº†çº§è”é”™è¯¯ï¼Œå¹¶æå‡äº†æ€§èƒ½å’Œæ•ˆç‡ã€‚å®éªŒè¯æ˜ï¼ŒEKAèƒ½æœ‰æ•ˆå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„ä¸å¿…è¦æ¢ç´¢ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒå³å¯é€‚ç”¨äºå¤§å‹æ¨¡å‹ï¼Œå…·æœ‰å¼ºé²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.20136v1">M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has recently been extended to multimodal settings, connecting multimodal large language models (MLLMs) with vast corpora of external knowledge such as multimodal knowledge graphs (MMKGs). Despite their recent success, multimodal RAG in the audio-visual domain remains challenging due to 1) limited modality coverage and multi-hop connectivity of existing MMKGs, and 2) retrieval based solely on similarity in a shared multimodal embedding space, which fails to filter out off-topic or redundant knowledge. To address these limitations, we propose M$^3$KG-RAG, a Multi-hop Multimodal Knowledge Graph-enhanced RAG that retrieves query-aligned audio-visual knowledge from MMKGs, improving reasoning depth and answer faithfulness in MLLMs. Specifically, we devise a lightweight multi-agent pipeline to construct multi-hop MMKG (M$^3$KG), which contains context-enriched triplets of multimodal entities, enabling modality-wise retrieval based on input queries. Furthermore, we introduce GRASP (Grounded Retrieval And Selective Pruning), which ensures precise entity grounding to the query, evaluates answer-supporting relevance, and prunes redundant context to retain only knowledge essential for response generation. Extensive experiments across diverse multimodal benchmarks demonstrate that M$^3$KG-RAG significantly enhances MLLMs' multimodal reasoning and grounding over existing approaches.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMÂ³KG-RAGçš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡æ„å»ºå¤šè·³å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±ï¼ˆMÂ³KGï¼‰å’Œå¼•å…¥GRASPæœºåˆ¶ï¼ˆåŸºäºæŸ¥è¯¢çš„å®ä½“å¯¹é½ã€ç­”æ¡ˆç›¸å…³æ€§è¯„ä¼°åŠå†—ä½™ä¿®å‰ªï¼‰ï¼Œè§£å†³äº†ç°æœ‰éŸ³é¢‘-è§†è§‰é¢†åŸŸå¤šæ¨¡æ€RAGåœ¨æ¨¡æ€è¦†ç›–ã€å¤šè·³è¿æ¥åŠæ£€ç´¢ç²¾åº¦ä¸Šçš„å±€é™æ€§ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†æ·±åº¦å’Œç­”æ¡ˆå‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.20082v1">Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches</a></td><td><details><summary>å±•å¼€</summary>Financial sentiment analysis plays a crucial role in informing investment decisions, assessing market risk, and predicting stock price trends. Existing works in financial sentiment analysis have not considered the impact of stock prices or market feedback on sentiment analysis. In this paper, we propose an adaptive framework that integrates large language models (LLMs) with real-world stock market feedback to improve sentiment classification in the context of the Indian stock market. The proposed methodology fine-tunes the LLaMA 3.2 3B model using instruction-based learning on the SentiFin dataset. To enhance sentiment predictions, a retrieval-augmented generation (RAG) pipeline is employed that dynamically selects multi-source contextual information based on the cosine similarity of the sentence embeddings. Furthermore, a feedback-driven module is introduced that adjusts the reliability of the source by comparing predicted sentiment with actual next-day stock returns, allowing the system to iteratively adapt to market behavior. To generalize this adaptive mechanism across temporal data, a reinforcement learning agent trained using proximal policy optimization (PPO) is incorporated. The PPO agent learns to optimize source weighting policies based on cumulative reward signals from sentiment-return alignment. Experimental results on NIFTY 50 news headlines collected from 2024 to 2025 demonstrate that the proposed system significantly improves classification accuracy, F1-score, and market alignment over baseline models and static retrieval methods. The results validate the potential of combining instruction-tuned LLMs with dynamic feedback and reinforcement learning for robust, market-aware financial sentiment modeling.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸å¸‚åœºåé¦ˆçš„è‡ªé€‚åº”æ¡†æ¶ï¼Œé€šè¿‡RAGç®¡é“åŠ¨æ€é€‰æ‹©å¤šæºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆåŸºäºå¥å­åµŒå…¥çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰ï¼Œå¹¶å¼•å…¥åé¦ˆé©±åŠ¨æ¨¡å—å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆPPOï¼‰ä¼˜åŒ–æƒ…æ„Ÿé¢„æµ‹ä¸å¸‚åœºå›æŠ¥çš„ alignmentï¼Œæœ€ç»ˆåœ¨å°åº¦è‚¡å¸‚æ•°æ®ä¸ŠéªŒè¯äº†æ¨¡å‹åœ¨æƒ…æ„Ÿåˆ†ç±»å‡†ç¡®æ€§å’Œå¸‚åœºé€‚åº”æ€§ä¸Šçš„æå‡ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-22
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.19537v1">Event Extraction in Large Language Model</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) and multimodal LLMs are changing event extraction (EE): prompting and generation can often produce structured outputs in zero shot or few shot settings. Yet LLM based pipelines face deployment gaps, including hallucinations under weak constraints, fragile temporal and causal linking over long contexts and across documents, and limited long horizon knowledge management within a bounded context window. We argue that EE should be viewed as a system component that provides a cognitive scaffold for LLM centered solutions. Event schemas and slot constraints create interfaces for grounding and verification; event centric structures act as controlled intermediate representations for stepwise reasoning; event links support relation aware retrieval with graph based RAG; and event stores offer updatable episodic and agent memory beyond the context window. This survey covers EE in text and multimodal settings, organizing tasks and taxonomy, tracing method evolution from rule based and neural models to instruction driven and generative frameworks, and summarizing formulations, decoding strategies, architectures, representations, datasets, and evaluation. We also review cross lingual, low resource, and domain specific settings, and highlight open challenges and future directions for reliable event centric systems. Finally, we outline open challenges and future directions that are central to the LLM era, aiming to evolve EE from static extraction into a structurally reliable, agent ready perception and memory layer for open world systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æ¢è®¨äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„äº‹ä»¶æå–ï¼ˆEEï¼‰æŠ€æœ¯ï¼Œæå‡ºäº†å°†EEä½œä¸ºç³»ç»Ÿç»„ä»¶çš„è®¤çŸ¥æ”¯æ¶ï¼Œé‡ç‚¹ä»‹ç»äº†åŸºäºå›¾çš„RAGæ–¹æ³•ç”¨äºå…³ç³»æ„ŸçŸ¥æ£€ç´¢ï¼Œä»¥è§£å†³é•¿ä¸Šä¸‹æ–‡å’Œè·¨æ–‡æ¡£çš„æ—¶åºä¸å› æœé“¾æ¥é—®é¢˜ï¼Œå¹¶å¼ºè°ƒäº†äº‹ä»¶å­˜å‚¨å’Œç»“æ„åŒ–è¡¨ç¤ºå¯¹å¢å¼ºLLMå¯é æ€§çš„ä½œç”¨ã€‚æ–‡ç« è¿˜ç»¼è¿°äº†EEçš„ä»»åŠ¡ã€æ–¹æ³•æ¼”è¿›åŠæœªæ¥æ–¹å‘ï¼Œæ—¨åœ¨æ¨åŠ¨EEæˆä¸ºå¼€æ”¾ä¸–ç•Œç³»ç»Ÿä¸­ç»“æ„å¯é çš„æ„ŸçŸ¥ä¸è®°å¿†å±‚ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.19475v1">A Large-Language-Model Framework for Automated Humanitarian Situation Reporting</a></td><td><details><summary>å±•å¼€</summary>Timely and accurate situational reports are essential for humanitarian decision-making, yet current workflows remain largely manual, resource intensive, and inconsistent. We present a fully automated framework that uses large language models (LLMs) to transform heterogeneous humanitarian documents into structured and evidence-grounded reports. The system integrates semantic text clustering, automatic question generation, retrieval augmented answer extraction with citations, multi-level summarization, and executive summary generation, supported by internal evaluation metrics that emulate expert reasoning. We evaluated the framework across 13 humanitarian events, including natural disasters and conflicts, using more than 1,100 documents from verified sources such as ReliefWeb. The generated questions achieved 84.7 percent relevance, 84.0 percent importance, and 76.4 percent urgency. The extracted answers reached 86.3 percent relevance, with citation precision and recall both exceeding 76 percent. Agreement between human and LLM based evaluations surpassed an F1 score of 0.80. Comparative analysis shows that the proposed framework produces reports that are more structured, interpretable, and actionable than existing baselines. By combining LLM reasoning with transparent citation linking and multi-level evaluation, this study demonstrates that generative AI can autonomously produce accurate, verifiable, and operationally useful humanitarian situation reports.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œç”¨äºå°†å¤šæ ·åŒ–çš„äººé“ä¸»ä¹‰æ–‡æ¡£è½¬åŒ–ä¸ºç»“æ„åŒ–ä¸”è¯æ®æ”¯æ’‘çš„æŠ¥å‘Šã€‚è¯¥æ¡†æ¶ç»“åˆäº†è¯­ä¹‰æ–‡æœ¬èšç±»ã€è‡ªåŠ¨é—®é¢˜ç”Ÿæˆã€å¸¦æœ‰å¼•ç”¨çš„æ£€ç´¢å¢å¼ºç­”æ¡ˆæå–ã€å¤šçº§æ‘˜è¦å’Œè¡Œæ”¿æ‘˜è¦ç”Ÿæˆï¼Œå¹¶é€šè¿‡å†…éƒ¨è¯„ä¼°æŒ‡æ ‡æ¨¡æ‹Ÿä¸“å®¶æ¨ç†ã€‚åœ¨13ä¸ªäººé“ä¸»ä¹‰äº‹ä»¶ï¼ˆåŒ…æ‹¬è‡ªç„¶ç¾å®³å’Œå†²çªï¼‰ä¸­ä½¿ç”¨è¶…è¿‡1,100ä»½æ–‡æ¡£è¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºæ¡†æ¶ç”Ÿæˆçš„é—®é¢˜å’Œç­”æ¡ˆå…·æœ‰é«˜ç›¸å…³æ€§ã€é‡è¦æ€§å’Œç´§æ€¥æ€§ï¼Œä¸”å¼•ç”¨å‡†ç¡®æ€§å’Œå¬å›ç‡å‡è¶…è¿‡76%ã€‚æœ€ç»ˆç”Ÿæˆçš„æŠ¥å‘Šæ¯”ç°æœ‰åŸºçº¿æ›´å…·ç»“æ„åŒ–ã€å¯è§£é‡Šæ€§å’Œå¯æ“ä½œæ€§ï¼Œè¯æ˜äº†ç”Ÿæˆå¼AIåœ¨è‡ªä¸»ç”Ÿæˆå‡†ç¡®ã€å¯éªŒè¯ä¸”å®ç”¨çš„äººé“ä¸»ä¹‰æŠ¥å‘Šæ–¹é¢çš„æ½œåŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.19360v1">Generative vector search to improve pathology foundation models across multimodal vision-language tasks</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation improves large language models by grounding outputs in external knowledge sources, reducing hallucinations and addressing knowledge cutoffs. However, standard embedding-based retrieval fails to capture the complexity of multi-concept queries, particularly in domains like biomedicine, where biological data are inherently high-dimensional. For example,omics datasets, and clinical reports simultaneously exhibit numerous molecular, cellular, and physiological features. We present Stochastic Latent Matching (STHLM), a generative vector search method that samples query-conditioned embeddings from text or image inputs to enhance retrieval performance. Analogous to how Chain-of-Thought reasoning enables language models to "think longer" on complex problems, STHLM allows retrieval systems to "search wider" through iterative sampling. STHLM demonstrates critical improvements over classical vector retrieval across diverse benchmarks, including scientific literature, clinical notes, and tissue images, boosting retrieval performance by 10-30% through test-time compute (trading latency for accuracy), while enabling up to a 10-fold compression of embedding dimensions.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSTHLMï¼ˆStochastic Latent Matchingï¼‰çš„ç”Ÿæˆå¼å‘é‡æœç´¢æ–¹æ³•ï¼Œé€šè¿‡ä»æ–‡æœ¬æˆ–å›¾åƒè¾“å…¥ä¸­é‡‡æ ·æŸ¥è¯¢æ¡ä»¶åµŒå…¥æ¥æå‡æ£€ç´¢æ€§èƒ½ï¼Œè§£å†³äº†ä¼ ç»ŸåŸºäºåµŒå…¥çš„æ£€ç´¢åœ¨å¤„ç†ç”Ÿç‰©åŒ»å­¦ç­‰å¤šæ¦‚å¿µå¤æ‚æŸ¥è¯¢æ—¶çš„å±€é™æ€§ï¼Œæ˜¾è‘—æé«˜äº†æ£€ç´¢å‡†ç¡®ç‡ï¼ˆ10-30%ï¼‰å¹¶æ”¯æŒåµŒå…¥ç»´åº¦å‹ç¼©ï¼Œä»è€Œå¢å¼ºRAGç³»ç»Ÿåœ¨ä¸“ä¸šé¢†åŸŸçš„è¡¨ç°ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.19247v1">Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics</a></td><td><details><summary>å±•å¼€</summary>Prompt engineering plays a critical role in adapting large language models (LLMs) to complex reasoning and labeling tasks without the need for extensive fine-tuning. In this paper, we propose a novel prompt optimization pipeline for frame detection in logistics texts, combining retrieval-augmented generation (RAG), few-shot prompting, chain-of-thought (CoT) reasoning, and automatic CoT synthesis (Auto-CoT) to generate highly effective task-specific prompts. Central to our approach is an LLM-based prompt optimizer agent that iteratively refines the prompts using retrieved examples, performance feedback, and internal self-evaluation. Our framework is evaluated on a real-world logistics text annotation task, where reasoning accuracy and labeling efficiency are critical. Experimental results show that the optimized prompts - particularly those enhanced via Auto-CoT and RAG - improve real-world inference accuracy by up to 15% compared to baseline zero-shot or static prompts. The system demonstrates consistent improvements across multiple LLMs, including GPT-4o, Qwen 2.5 (72B), and LLaMA 3.1 (70B), validating its generalizability and practical value. These findings suggest that structured prompt optimization is a viable alternative to full fine-tuning, offering scalable solutions for deploying LLMs in domain-specific NLP applications such as logistics.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€å°‘æ ·æœ¬æç¤ºã€æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†å’Œè‡ªåŠ¨CoTåˆæˆï¼ˆAuto-CoTï¼‰çš„æç¤ºä¼˜åŒ–æµç¨‹ï¼Œç”¨äºç‰©æµæ–‡æœ¬ä¸­çš„æ¡†æ¶æ£€æµ‹ä»»åŠ¡ã€‚é€šè¿‡LLMåŸºç¡€æç¤ºä¼˜åŒ–ä»£ç†è¿­ä»£æ”¹è¿›æç¤ºï¼Œå®éªŒè¡¨æ˜ä¼˜åŒ–åçš„æç¤ºï¼ˆå°¤å…¶æ˜¯é€šè¿‡RAGå’ŒAuto-CoTå¢å¼ºçš„æç¤ºï¼‰ç›¸æ¯”åŸºçº¿æç¤ºæ˜¾è‘—æé«˜äº†æ¨ç†å‡†ç¡®æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.19134v1">QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºQuCo-RAGçš„åŠ¨æ€æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡åŸºäºé¢„è®­ç»ƒæ•°æ®çš„å®¢è§‚ç»Ÿè®¡é‡åŒ–ä¸ç¡®å®šæ€§ï¼Œè€Œéä¾èµ–æ¨¡å‹å†…éƒ¨ä¿¡å·ï¼Œä»¥æ›´å‡†ç¡®åœ°åˆ¤æ–­ä½•æ—¶è¿›è¡Œæ£€ç´¢æ¥å‡å°‘å¤§è¯­è¨€æ¨¡å‹çš„å¹»è§‰ã€‚è¯¥æ–¹æ³•åœ¨ç”Ÿæˆå‰è¯†åˆ«ä½é¢‘å®ä½“æ­ç¤ºçŸ¥è¯†ç¼ºå£ï¼Œç”Ÿæˆæ—¶éªŒè¯å®ä½“å…±ç°ä»¥è¯„ä¼°å¹»è§‰é£é™©ï¼Œåˆ©ç”¨å¤§è§„æ¨¡è¯­æ–™åº“ï¼ˆ4ä¸‡äº¿è¯å…ƒï¼‰è¿›è¡Œé«˜æ•ˆæŸ¥è¯¢ã€‚å®éªŒè¡¨æ˜ï¼ŒQuCo-RAGåœ¨å¤šè·³QAåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å¯é€‚é…ä¸åŒé¢„è®­ç»ƒæ•°æ®çš„æ¨¡å‹ï¼ˆå¦‚Llamaã€GPTï¼‰ï¼Œåœ¨ç”Ÿç‰©åŒ»å­¦QAé¢†åŸŸä¹Ÿå±•ç°äº†å¼ºé²æ£’æ€§ï¼ŒéªŒè¯äº†å…¶ä½œä¸ºåŠ¨æ€RAGé€šç”¨èŒƒå¼çš„æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.19122v1">BanglaForge: LLM Collaboration with Self-Refinement for Bangla Code Generation</a></td><td><details><summary>å±•å¼€</summary>Bangla is a low-resource language for code generation, lacking large-scale annotated datasets and tools to transform natural language specifications into executable programs. This makes Bangla-to-code generation a challenging task requiring innovative solutions. To address this, we introduce BanglaForge, a novel framework for generating code from Bangla function descriptions. BanglaForge leverages a retrieval-augmented dual-model collaboration paradigm with self-refinement, combining in-context learning, llm-based translation, systematic prompt engineering, and iterative self-refinement based on execution feedback, where a coder generates initial solutions and a reviewer enhances them for robustness. On the BLP-2025 Bangla Code Generation benchmark, BanglaForge achieves a competitive Pass@1 accuracy of 84.00%, demonstrating the effectiveness of retrieval, model collaboration, and self-refinement for low-resource Bangla code generation.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ä»‹ç»äº†BanglaForgeæ¡†æ¶ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºçš„åŒæ¨¡å‹åä½œä¸è‡ªæˆ‘ä¼˜åŒ–æ–¹æ³•ï¼ˆå¦‚ä¸Šä¸‹æ–‡å­¦ä¹ ã€LLMç¿»è¯‘ã€æç¤ºå·¥ç¨‹å’ŒåŸºäºæ‰§è¡Œåé¦ˆçš„è¿­ä»£ä¼˜åŒ–ï¼‰ï¼Œè§£å†³ä½èµ„æºè¯­è¨€Banglaçš„ä»£ç ç”Ÿæˆé—®é¢˜ï¼Œå¹¶åœ¨åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°84.00%çš„Pass@1å‡†ç¡®ç‡ï¼Œä½“ç°äº†RAGæŠ€æœ¯åœ¨æå‡ä½èµ„æºè¯­è¨€ä»»åŠ¡æ€§èƒ½ä¸­çš„æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.18987v1">Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation</a></td><td><details><summary>å±•å¼€</summary>In this study, we address the problem of open-vocabulary mobile manipulation, where a robot is required to carry a wide range of objects to receptacles based on free-form natural language instructions. This task is challenging, as it involves understanding visual semantics and the affordance of manipulation actions. To tackle these challenges, we propose Affordance RAG, a zero-shot hierarchical multimodal retrieval framework that constructs Affordance-Aware Embodied Memory from pre-explored images. The model retrieves candidate targets based on regional and visual semantics and reranks them with affordance scores, allowing the robot to identify manipulation options that are likely to be executable in real-world environments. Our method outperformed existing approaches in retrieval performance for mobile manipulation instruction in large-scale indoor environments. Furthermore, in real-world experiments where the robot performed mobile manipulation in indoor environments based on free-form instructions, the proposed method achieved a task success rate of 85%, outperforming existing methods in both retrieval performance and overall task success.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºAffordance RAGçš„é›¶æ ·æœ¬åˆ†å±‚å¤šæ¨¡æ€æ£€ç´¢æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¼€æ”¾è¯æ±‡ç§»åŠ¨æ“ä½œä»»åŠ¡ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»é¢„æ¢ç´¢å›¾åƒä¸­æ„å»º"Affordance-Aware Embodied Memory"ï¼Œç»“åˆåŒºåŸŸè§†è§‰è¯­ä¹‰æ£€ç´¢å’ŒåŠŸèƒ½å¾—åˆ†é‡æ’åºï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿè¯†åˆ«ç°å®ç¯å¢ƒä¸­å¯æ‰§è¡Œçš„æ“ä½œé€‰é¡¹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤§è§„æ¨¡å®¤å†…ç¯å¢ƒä¸­çš„æ£€ç´¢æ€§èƒ½å’Œä»»åŠ¡æˆåŠŸç‡ï¼ˆ85%ï¼‰å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-21
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.19537v1">Event Extraction in Large Language Model</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) and multimodal LLMs are changing event extraction (EE): prompting and generation can often produce structured outputs in zero shot or few shot settings. Yet LLM based pipelines face deployment gaps, including hallucinations under weak constraints, fragile temporal and causal linking over long contexts and across documents, and limited long horizon knowledge management within a bounded context window. We argue that EE should be viewed as a system component that provides a cognitive scaffold for LLM centered solutions. Event schemas and slot constraints create interfaces for grounding and verification; event centric structures act as controlled intermediate representations for stepwise reasoning; event links support relation aware retrieval with graph based RAG; and event stores offer updatable episodic and agent memory beyond the context window. This survey covers EE in text and multimodal settings, organizing tasks and taxonomy, tracing method evolution from rule based and neural models to instruction driven and generative frameworks, and summarizing formulations, decoding strategies, architectures, representations, datasets, and evaluation. We also review cross lingual, low resource, and domain specific settings, and highlight open challenges and future directions for reliable event centric systems. Finally, we outline open challenges and future directions that are central to the LLM era, aiming to evolve EE from static extraction into a structurally reliable, agent ready perception and memory layer for open world systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„äº‹ä»¶æå–ï¼ˆEEï¼‰æŠ€æœ¯ï¼Œæå‡ºäº†å°†äº‹ä»¶æ¨¡å¼ã€æ§½çº¦æŸå’Œäº‹ä»¶é“¾æ¥ä½œä¸ºè®¤çŸ¥æ¡†æ¶ï¼Œä»¥å¢å¼ºLLMçš„å¯é æ€§å’ŒçŸ¥è¯†ç®¡ç†èƒ½åŠ›ã€‚å…¶ä¸­ç‰¹åˆ«æåˆ°äº†åŸºäºå›¾ç»“æ„çš„RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç”¨äºå…³ç³»æ„ŸçŸ¥çš„ä¿¡æ¯æ£€ç´¢ï¼Œæ—¨åœ¨è§£å†³é•¿ä¸Šä¸‹æ–‡å’Œè·¨æ–‡æ¡£çš„æ—¶åºä¸å› æœé“¾æ¥é—®é¢˜ï¼Œå¹¶å°†äº‹ä»¶å­˜å‚¨ä½œä¸ºå¯æ›´æ–°çš„å†…å­˜å±‚ï¼Œæ‰©å±•äº†ä¼ ç»ŸRAGçš„åº”ç”¨åœºæ™¯ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.19475v1">A Large-Language-Model Framework for Automated Humanitarian Situation Reporting</a></td><td><details><summary>å±•å¼€</summary>Timely and accurate situational reports are essential for humanitarian decision-making, yet current workflows remain largely manual, resource intensive, and inconsistent. We present a fully automated framework that uses large language models (LLMs) to transform heterogeneous humanitarian documents into structured and evidence-grounded reports. The system integrates semantic text clustering, automatic question generation, retrieval augmented answer extraction with citations, multi-level summarization, and executive summary generation, supported by internal evaluation metrics that emulate expert reasoning. We evaluated the framework across 13 humanitarian events, including natural disasters and conflicts, using more than 1,100 documents from verified sources such as ReliefWeb. The generated questions achieved 84.7 percent relevance, 84.0 percent importance, and 76.4 percent urgency. The extracted answers reached 86.3 percent relevance, with citation precision and recall both exceeding 76 percent. Agreement between human and LLM based evaluations surpassed an F1 score of 0.80. Comparative analysis shows that the proposed framework produces reports that are more structured, interpretable, and actionable than existing baselines. By combining LLM reasoning with transparent citation linking and multi-level evaluation, this study demonstrates that generative AI can autonomously produce accurate, verifiable, and operationally useful humanitarian situation reports.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œç”¨äºå°†å¼‚æ„çš„äººé“ä¸»ä¹‰æ–‡æ¡£è½¬åŒ–ä¸ºç»“æ„åŒ–ã€è¯æ®æ”¯æ’‘çš„æŠ¥å‘Šã€‚è¯¥ç³»ç»Ÿæ•´åˆäº†è¯­ä¹‰æ–‡æœ¬èšç±»ã€è‡ªåŠ¨é—®é¢˜ç”Ÿæˆã€æ£€ç´¢å¢å¼ºçš„ç­”æ¡ˆæå–ï¼ˆé™„å¸¦å¼•ç”¨ï¼‰ã€å¤šçº§æ‘˜è¦ç”Ÿæˆç­‰åŠŸèƒ½ï¼Œå¹¶é€šè¿‡å†…éƒ¨è¯„ä¼°æŒ‡æ ‡æ¨¡æ‹Ÿä¸“å®¶æ¨ç†ã€‚ç ”ç©¶åœ¨13ä¸ªäººé“ä¸»ä¹‰äº‹ä»¶ä¸­éªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºç”Ÿæˆçš„æŠ¥å‘Šåœ¨ç›¸å…³æ€§ã€å¯è§£é‡Šæ€§å’Œå¯æ“ä½œæ€§ä¸Šä¼˜äºç°æœ‰åŸºçº¿ï¼Œä½“ç°äº†RAGæŠ€æœ¯åœ¨æå‡ç”Ÿæˆå†…å®¹å‡†ç¡®æ€§å’Œå¯éªŒè¯æ€§æ–¹é¢çš„åº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.19360v1">Generative vector search to improve pathology foundation models across multimodal vision-language tasks</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation improves large language models by grounding outputs in external knowledge sources, reducing hallucinations and addressing knowledge cutoffs. However, standard embedding-based retrieval fails to capture the complexity of multi-concept queries, particularly in domains like biomedicine, where biological data are inherently high-dimensional. For example,omics datasets, and clinical reports simultaneously exhibit numerous molecular, cellular, and physiological features. We present Stochastic Latent Matching (STHLM), a generative vector search method that samples query-conditioned embeddings from text or image inputs to enhance retrieval performance. Analogous to how Chain-of-Thought reasoning enables language models to "think longer" on complex problems, STHLM allows retrieval systems to "search wider" through iterative sampling. STHLM demonstrates critical improvements over classical vector retrieval across diverse benchmarks, including scientific literature, clinical notes, and tissue images, boosting retrieval performance by 10-30% through test-time compute (trading latency for accuracy), while enabling up to a 10-fold compression of embedding dimensions.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡é’ˆå¯¹RAGä¸­ä¼ ç»ŸåµŒå…¥æ£€ç´¢åœ¨å¤šæ¦‚å¿µå¤æ‚æŸ¥è¯¢ï¼ˆå¦‚ç”Ÿç‰©åŒ»å­¦é¢†åŸŸï¼‰çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åä¸ºSTHLMï¼ˆStochastic Latent Matchingï¼‰çš„ç”Ÿæˆå¼å‘é‡æœç´¢æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»æ–‡æœ¬æˆ–å›¾åƒè¾“å…¥ä¸­é‡‡æ ·æŸ¥è¯¢æ¡ä»¶åŒ–åµŒå…¥ï¼Œä»¥è¿­ä»£é‡‡æ ·æå‡æ£€ç´¢æ€§èƒ½ï¼ˆç±»æ¯”æ€ç»´é“¾çš„â€œå»¶é•¿æ€è€ƒâ€ï¼‰ï¼Œåœ¨ç§‘å­¦æ–‡çŒ®ã€ä¸´åºŠç¬”è®°ç­‰ä»»åŠ¡ä¸­æ¯”ä¼ ç»Ÿå‘é‡æ£€ç´¢æ•ˆæœæå‡10-30%ï¼Œå¹¶æ”¯æŒ10å€åµŒå…¥ç»´åº¦å‹ç¼©ï¼ŒåŒæ—¶é€šè¿‡ç‰ºç‰²å»¶è¿Ÿæ¢å–äº†æ›´é«˜å‡†ç¡®æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.19247v1">Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics</a></td><td><details><summary>å±•å¼€</summary>Prompt engineering plays a critical role in adapting large language models (LLMs) to complex reasoning and labeling tasks without the need for extensive fine-tuning. In this paper, we propose a novel prompt optimization pipeline for frame detection in logistics texts, combining retrieval-augmented generation (RAG), few-shot prompting, chain-of-thought (CoT) reasoning, and automatic CoT synthesis (Auto-CoT) to generate highly effective task-specific prompts. Central to our approach is an LLM-based prompt optimizer agent that iteratively refines the prompts using retrieved examples, performance feedback, and internal self-evaluation. Our framework is evaluated on a real-world logistics text annotation task, where reasoning accuracy and labeling efficiency are critical. Experimental results show that the optimized prompts - particularly those enhanced via Auto-CoT and RAG - improve real-world inference accuracy by up to 15% compared to baseline zero-shot or static prompts. The system demonstrates consistent improvements across multiple LLMs, including GPT-4o, Qwen 2.5 (72B), and LLaMA 3.1 (70B), validating its generalizability and practical value. These findings suggest that structured prompt optimization is a viable alternative to full fine-tuning, offering scalable solutions for deploying LLMs in domain-specific NLP applications such as logistics.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€å°‘æ ·æœ¬æç¤ºã€æ€ç»´é“¾æ¨ç†ï¼ˆCoTï¼‰å’Œè‡ªåŠ¨CoTåˆæˆï¼ˆAuto-CoTï¼‰çš„æ–°å‹æç¤ºä¼˜åŒ–æµç¨‹ï¼Œç”¨äºç‰©æµæ–‡æœ¬ä¸­çš„æ¡†æ¶æ£€æµ‹ä»»åŠ¡ã€‚é€šè¿‡åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æç¤ºä¼˜åŒ–ä»£ç†è¿­ä»£ä¼˜åŒ–æç¤ºï¼Œåˆ©ç”¨æ£€ç´¢åˆ°çš„ç¤ºä¾‹ã€æ€§èƒ½åé¦ˆå’Œå†…éƒ¨è‡ªè¯„ä¼°ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†å‡†ç¡®æ€§å’Œæ ‡æ³¨æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼Œä¼˜åŒ–åçš„æç¤ºï¼ˆå°¤å…¶æ˜¯é€šè¿‡Auto-CoTå’ŒRAGå¢å¼ºçš„æç¤ºï¼‰æ¯”åŸºçº¿é›¶æ ·æœ¬æˆ–é™æ€æç¤ºæé«˜äº†é«˜è¾¾15%çš„æ¨ç†å‡†ç¡®æ€§ï¼Œå¹¶åœ¨å¤šç§å¤§è¯­è¨€æ¨¡å‹ä¸ŠéªŒè¯äº†å…¶é€šç”¨æ€§å’Œå®ç”¨ä»·å€¼ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.19134v1">QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºQuCo-RAGçš„åŠ¨æ€æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒæ•°æ®çš„å®¢è§‚ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚ä½é¢‘å®ä½“å’Œå®ä½“å…±ç°ï¼‰æ¥é‡åŒ–ä¸ç¡®å®šæ€§ï¼Œä»è€Œè‡ªé€‚åº”åœ°åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è§¦å‘æ£€ç´¢ï¼Œä»¥å‡å°‘å¤§è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå¹¶åœ¨ä¸åŒæ¨¡å‹å’Œé¢†åŸŸï¼ˆå¦‚ç”Ÿç‰©åŒ»å­¦é—®ç­”ï¼‰ä¸­å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.19122v1">BanglaForge: LLM Collaboration with Self-Refinement for Bangla Code Generation</a></td><td><details><summary>å±•å¼€</summary>Bangla is a low-resource language for code generation, lacking large-scale annotated datasets and tools to transform natural language specifications into executable programs. This makes Bangla-to-code generation a challenging task requiring innovative solutions. To address this, we introduce BanglaForge, a novel framework for generating code from Bangla function descriptions. BanglaForge leverages a retrieval-augmented dual-model collaboration paradigm with self-refinement, combining in-context learning, llm-based translation, systematic prompt engineering, and iterative self-refinement based on execution feedback, where a coder generates initial solutions and a reviewer enhances them for robustness. On the BLP-2025 Bangla Code Generation benchmark, BanglaForge achieves a competitive Pass@1 accuracy of 84.00%, demonstrating the effectiveness of retrieval, model collaboration, and self-refinement for low-resource Bangla code generation.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†BanglaForgeæ¡†æ¶ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºçš„åŒæ¨¡å‹åä½œèŒƒå¼ï¼ˆç»“åˆä¸Šä¸‹æ–‡å­¦ä¹ ã€LLMç¿»è¯‘ã€æç¤ºå·¥ç¨‹å’ŒåŸºäºæ‰§è¡Œåé¦ˆçš„è¿­ä»£è‡ªä¼˜åŒ–ï¼‰è§£å†³ä½èµ„æºå­ŸåŠ æ‹‰è¯­ä»£ç ç”Ÿæˆé—®é¢˜ï¼Œåœ¨åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°84.00%çš„å‡†ç¡®ç‡ï¼ŒéªŒè¯äº†æ£€ç´¢ä¸åä½œç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.19769v1">A Declarative Language for Building And Orchestrating LLM-Powered Agent Workflows</a></td><td><details><summary>å±•å¼€</summary>Building deployment-ready LLM agents requires complex orchestration of tools, data sources, and control flow logic, yet existing systems tightly couple agent logic to specific programming languages and deployment models. We present a declarative system that separates agent workflow specification from implementation, enabling the same pipeline definition to execute across multiple backend languages (Java, Python, Go) and deployment environments (cloud-native, on-premises).
  Our key insight is that most agent workflows consist of common patterns -- data serialization, filtering, RAG retrieval, API orchestration -- that can be expressed through a unified DSL rather than imperative code. This approach transforms agent development from application programming to configuration, where adding new tools or fine-tuning agent behaviors requires only pipeline specification changes, not code deployment. Our system natively supports A/B testing of agent strategies, allowing multiple pipeline variants to run on the same backend infrastructure with automatic metric collection and comparison.
  We evaluate our approach on real-world e-commerce workflows at PayPal, processing millions of daily interactions. Our results demonstrate 60% reduction in development time, and 3x improvement in deployment velocity compared to imperative implementations. The language's declarative approach enables non-engineers to modify agent behaviors safely, while maintaining sub-100ms orchestration overhead. We show that complex workflows involving product search, personalization, and cart management can be expressed in under 50 lines of DSL compared to 500+ lines of imperative code.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§å£°æ˜å¼ç³»ç»Ÿï¼Œç”¨äºæ„å»ºå’Œéƒ¨ç½²LLMæ™ºèƒ½ä½“ï¼Œå°†å·¥ä½œæµè§„èŒƒä¸å®ç°è§£è€¦ï¼Œæ”¯æŒå¤šç§åç«¯è¯­è¨€å’Œéƒ¨ç½²ç¯å¢ƒã€‚ç³»ç»Ÿé€šè¿‡ç»Ÿä¸€DSLè¡¨è¾¾å¸¸è§æ¨¡å¼ï¼ˆåŒ…æ‹¬RAGæ£€ç´¢ã€APIç¼–æ’ç­‰ï¼‰ï¼Œæ˜¾è‘—é™ä½å¼€å‘æ—¶é—´å’Œéƒ¨ç½²å¤æ‚åº¦ï¼Œå¹¶åœ¨å®é™…ç”µå•†åœºæ™¯ä¸­éªŒè¯äº†é«˜æ•ˆæ€§ï¼ˆå¦‚å¼€å‘æ—¶é—´å‡å°‘60%ï¼‰ã€‚æ–‡ä¸­æ˜ç¡®æåŠRAGä½œä¸ºå·¥ä½œæµçš„æ ¸å¿ƒæ¨¡å¼ä¹‹ä¸€ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.18987v1">Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation</a></td><td><details><summary>å±•å¼€</summary>In this study, we address the problem of open-vocabulary mobile manipulation, where a robot is required to carry a wide range of objects to receptacles based on free-form natural language instructions. This task is challenging, as it involves understanding visual semantics and the affordance of manipulation actions. To tackle these challenges, we propose Affordance RAG, a zero-shot hierarchical multimodal retrieval framework that constructs Affordance-Aware Embodied Memory from pre-explored images. The model retrieves candidate targets based on regional and visual semantics and reranks them with affordance scores, allowing the robot to identify manipulation options that are likely to be executable in real-world environments. Our method outperformed existing approaches in retrieval performance for mobile manipulation instruction in large-scale indoor environments. Furthermore, in real-world experiments where the robot performed mobile manipulation in indoor environments based on free-form instructions, the proposed method achieved a task success rate of 85%, outperforming existing methods in both retrieval performance and overall task success.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºAffordance RAGçš„é›¶æ ·æœ¬åˆ†å±‚å¤šæ¨¡æ€æ£€ç´¢æ¡†æ¶ï¼Œç”¨äºè§£å†³å¼€æ”¾è¯æ±‡ç§»åŠ¨æ“ä½œä»»åŠ¡ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºAffordance-Aware Embodied Memoryï¼Œç»“åˆåŒºåŸŸå’Œè§†è§‰è¯­ä¹‰æ£€ç´¢å€™é€‰ç›®æ ‡ï¼Œå¹¶é€šè¿‡åŠŸèƒ½å¾—åˆ†é‡æ–°æ’åºï¼Œå¸®åŠ©æœºå™¨äººåœ¨ç°å®ç¯å¢ƒä¸­è¯†åˆ«å¯æ‰§è¡Œçš„æ“ä½œé€‰é¡¹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤§è§„æ¨¡å®¤å†…ç¯å¢ƒä¸­çš„æ£€ç´¢æ€§èƒ½å’Œä»»åŠ¡æˆåŠŸç‡ï¼ˆ85%ï¼‰å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.18683v1">CIRR: Causal-Invariant Retrieval-Augmented Recommendation with Faithful Explanations under Distribution Shift</a></td><td><details><summary>å±•å¼€</summary>Recent advances in retrieval-augmented generation (RAG) have shown promise in enhancing recommendation systems with external knowledge. However, existing RAG-based recommenders face two critical challenges: (1) vulnerability to distribution shifts across different environments (e.g., time periods, user segments), leading to performance degradation in out-of-distribution (OOD) scenarios, and (2) lack of faithful explanations that can be verified against retrieved evidence. In this paper, we propose CIRR, a Causal-Invariant Retrieval-Augmented Recommendation framework that addresses both challenges simultaneously. CIRR learns environment-invariant user preference representations through causal inference, which guide a debiased retrieval process to select relevant evidence from multiple sources. Furthermore, we introduce consistency constraints that enforce faithfulness between retrieved evidence, generated explanations, and recommendation outputs. Extensive experiments on two real-world datasets demonstrate that CIRR achieves robust performance under distribution shifts, reducing performance degradation from 15.4% (baseline) to only 5.6% in OOD scenarios, while providing more faithful and interpretable explanations (26% improvement in faithfulness score) compared to state-of-the-art baselines.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºCIRRçš„å› æœä¸å˜æ€§æ£€ç´¢å¢å¼ºæ¨èæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŸºäºRAGçš„æ¨èç³»ç»Ÿåœ¨åˆ†å¸ƒåç§»ç¯å¢ƒä¸‹çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œå¹¶æå‡ç”Ÿæˆè§£é‡Šçš„å¯éªŒè¯æ€§ã€‚é€šè¿‡å› æœæ¨æ–­å­¦ä¹ ç¯å¢ƒä¸å˜çš„ç”¨æˆ·åå¥½è¡¨ç¤ºï¼Œå¹¶ç»“åˆå¤šæºè¯æ®æ£€ç´¢å’Œä¸€è‡´æ€§çº¦æŸï¼ŒCIRRåœ¨åˆ†å¸ƒå¤–åœºæ™¯ä¸­æ˜¾è‘—é™ä½äº†æ€§èƒ½è¡°å‡ï¼ˆä»15.4%é™è‡³5.6%ï¼‰ï¼ŒåŒæ—¶æé«˜äº†è§£é‡Šçš„å¿ å®æ€§ï¼ˆ26%æå‡ï¼‰ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-20
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.18683v1">CIRR: Causal-Invariant Retrieval-Augmented Recommendation with Faithful Explanations under Distribution Shift</a></td><td><details><summary>å±•å¼€</summary>Recent advances in retrieval-augmented generation (RAG) have shown promise in enhancing recommendation systems with external knowledge. However, existing RAG-based recommenders face two critical challenges: (1) vulnerability to distribution shifts across different environments (e.g., time periods, user segments), leading to performance degradation in out-of-distribution (OOD) scenarios, and (2) lack of faithful explanations that can be verified against retrieved evidence. In this paper, we propose CIRR, a Causal-Invariant Retrieval-Augmented Recommendation framework that addresses both challenges simultaneously. CIRR learns environment-invariant user preference representations through causal inference, which guide a debiased retrieval process to select relevant evidence from multiple sources. Furthermore, we introduce consistency constraints that enforce faithfulness between retrieved evidence, generated explanations, and recommendation outputs. Extensive experiments on two real-world datasets demonstrate that CIRR achieves robust performance under distribution shifts, reducing performance degradation from 15.4% (baseline) to only 5.6% in OOD scenarios, while providing more faithful and interpretable explanations (26% improvement in faithfulness score) compared to state-of-the-art baselines.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºCIRRçš„å› æœä¸å˜æ€§æ£€ç´¢å¢å¼ºæ¨èæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŸºäºRAGçš„æ¨èç³»ç»Ÿä¸­å­˜åœ¨çš„åˆ†å¸ƒåç§»é—®é¢˜å’Œè§£é‡Šå¯ä¿¡åº¦ä¸è¶³çš„æŒ‘æˆ˜ã€‚CIRRé€šè¿‡å› æœæ¨æ–­å­¦ä¹ ç¯å¢ƒä¸å˜çš„ç”¨æˆ·åå¥½è¡¨å¾æ¥æŒ‡å¯¼å»åæ£€ç´¢è¿‡ç¨‹ï¼Œå¹¶å¼•å…¥ä¸€è‡´æ€§çº¦æŸç¡®ä¿æ£€ç´¢è¯æ®ã€ç”Ÿæˆè§£é‡Šä¸æ¨èç»“æœä¹‹é—´çš„å¯ä¿¡å…³è”ã€‚å®éªŒè¡¨æ˜ï¼ŒCIRRåœ¨åˆ†å¸ƒåç§»ä¸‹æ€§èƒ½æ›´ç¨³å¥ï¼ˆOODåœºæ™¯æ€§èƒ½ä¸‹é™ä»15.4%é™è‡³5.6%ï¼‰ï¼Œä¸”ç”Ÿæˆè§£é‡Šçš„å¯éªŒè¯æ€§æ˜¾è‘—æå‡ï¼ˆå¯ä¿¡åº¦åˆ†æ•°æé«˜26%ï¼‰ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.18357v1">DACE For Railway Acronym Disambiguation</a></td><td><details><summary>å±•å¼€</summary>Acronym Disambiguation (AD) is a fundamental challenge in technical text processing, particularly in specialized sectors where high ambiguity complicates automated analysis. This paper addresses AD within the context of the TextMine'26 competition on French railway documentation. We present DACE (Dynamic Prompting, Retrieval Augmented Generation, Contextual Selection, and Ensemble Aggregation), a framework that enhances Large Language Models through adaptive in-context learning and external domain knowledge injection. By dynamically tailoring prompts to acronym ambiguity and aggregating ensemble predictions, DACE mitigates hallucination and effectively handles low-resource scenarios. Our approach secured the top rank in the competition with an F1 score of 0.9069.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºDACEæ¡†æ¶ï¼Œç»“åˆåŠ¨æ€æç¤ºã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€ä¸Šä¸‹æ–‡é€‰æ‹©å’Œé›†æˆèšåˆæŠ€æœ¯ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´æç¤ºå’Œæ³¨å…¥å¤–éƒ¨é¢†åŸŸçŸ¥è¯†æ¥ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹ï¼ŒæˆåŠŸè§£å†³æ³•è¯­é“è·¯æ–‡æ¡£ä¸­çš„ç¼©å†™æ¶ˆæ­§é—®é¢˜ï¼Œå¹¶åœ¨TextMine'26ç«èµ›ä¸­ä»¥0.9069çš„F1å¾—åˆ†è·å¾—ç¬¬ä¸€åã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.18329v1">LIR$^3$AG: A Lightweight Rerank Reasoning Strategy Framework for Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) effectively enhances Large Language Models (LLMs) by incorporating retrieved external knowledge into the generation process. Reasoning models improve LLM performance in multi-hop QA tasks, which require integrating and reasoning over multiple pieces of evidence across different documents to answer a complex question. However, they often introduce substantial computational costs, including increased token consumption and inference latency. To better understand and mitigate this trade-off, we conduct a comprehensive study of reasoning strategies for reasoning models in RAG multi-hop QA tasks. Our findings reveal that reasoning models adopt structured strategies to integrate retrieved and internal knowledge, primarily following two modes: Context-Grounded Reasoning, which relies directly on retrieved content, and Knowledge-Reconciled Reasoning, which resolves conflicts or gaps using internal knowledge. To this end, we propose a novel Lightweight Rerank Reasoning Strategy Framework for RAG (LiR$^3$AG) to enable non-reasoning models to transfer reasoning strategies by restructuring retrieved evidence into coherent reasoning chains. LiR$^3$AG significantly reduce the average 98% output tokens overhead and 58.6% inferencing time while improving 8B non-reasoning model's F1 performance ranging from 6.2% to 22.5% to surpass the performance of 32B reasoning model in RAG, offering a practical and efficient path forward for RAG systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†RAGåœ¨å¤šè·³é—®ç­”ä»»åŠ¡ä¸­çš„æ¨ç†ç­–ç•¥ï¼Œæå‡ºäº†ä¸€ç§è½»é‡çº§çš„é‡æ–°æ’åºæ¨ç†ç­–ç•¥æ¡†æ¶ï¼ˆLiR$^3$AGï¼‰ï¼Œé€šè¿‡é‡ç»„æ£€ç´¢åˆ°çš„è¯æ®å½¢æˆè¿è´¯çš„æ¨ç†é“¾ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€å¹¶æå‡äº†éæ¨ç†æ¨¡å‹çš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.18177v1">NEURO-GUARD: Neuro-Symbolic Generalization and Unbiased Adaptive Routing for Diagnostics -- Explainable Medical AI</a></td><td><details><summary>å±•å¼€</summary>Accurate yet interpretable image-based diagnosis remains a central challenge in medical AI, particularly in settings characterized by limited data, subtle visual cues, and high-stakes clinical decision-making. Most existing vision models rely on purely data-driven learning and produce black-box predictions with limited interpretability and poor cross-domain generalization, hindering their real-world clinical adoption. We present NEURO-GUARD, a novel knowledge-guided vision framework that integrates Vision Transformers (ViTs) with language-driven reasoning to improve performance, transparency, and domain robustness. NEURO-GUARD employs a retrieval-augmented generation (RAG) mechanism for self-verification, in which a large language model (LLM) iteratively generates, evaluates, and refines feature-extraction code for medical images. By grounding this process in clinical guidelines and expert knowledge, the framework progressively enhances feature detection and classification beyond purely data-driven baselines. Extensive experiments on diabetic retinopathy classification across four benchmark datasets APTOS, EyePACS, Messidor-1, and Messidor-2 demonstrate that NEURO-GUARD improves accuracy by 6.2% over a ViT-only baseline (84.69% vs. 78.4%) and achieves a 5% gain in domain generalization. Additional evaluations on MRI-based seizure detection further confirm its cross-domain robustness, consistently outperforming existing methods.
  Overall, NEURO-GUARD bridges symbolic medical reasoning with subsymbolic visual learning, enabling interpretable, knowledge-aware, and generalizable medical image diagnosis while achieving state-of-the-art performance across multiple datasets.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ä»‹ç»äº†NEURO-GUARDï¼Œä¸€ä¸ªç»“åˆè§†è§‰Transformerï¼ˆViTï¼‰ä¸è¯­è¨€é©±åŠ¨æ¨ç†çš„æ–°å‹çŸ¥è¯†å¼•å¯¼æ¡†æ¶ï¼Œé€šè¿‡RAGæœºåˆ¶å®ç°è‡ªéªŒè¯ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿­ä»£ç”Ÿæˆã€è¯„ä¼°å’Œä¼˜åŒ–åŒ»å­¦å›¾åƒç‰¹å¾æå–ä»£ç ï¼Œæ˜¾è‘—æå‡äº†ç³–å°¿ç—…è§†ç½‘è†œç—…å˜åˆ†ç±»çš„å‡†ç¡®æ€§å’Œè·¨é¢†åŸŸé²æ£’æ€§ï¼Œå¹¶åœ¨å…¶ä»–åŒ»å­¦å½±åƒä»»åŠ¡ä¸­éªŒè¯äº†å…¶ä¼˜è¶Šæ€§èƒ½ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-19
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.18357v1">DACE For Railway Acronym Disambiguation</a></td><td><details><summary>å±•å¼€</summary>Acronym Disambiguation (AD) is a fundamental challenge in technical text processing, particularly in specialized sectors where high ambiguity complicates automated analysis. This paper addresses AD within the context of the TextMine'26 competition on French railway documentation. We present DACE (Dynamic Prompting, Retrieval Augmented Generation, Contextual Selection, and Ensemble Aggregation), a framework that enhances Large Language Models through adaptive in-context learning and external domain knowledge injection. By dynamically tailoring prompts to acronym ambiguity and aggregating ensemble predictions, DACE mitigates hallucination and effectively handles low-resource scenarios. Our approach secured the top rank in the competition with an F1 score of 0.9069.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†DACEæ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€æç¤ºã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€ä¸Šä¸‹æ–‡é€‰æ‹©å’Œé›†æˆèšåˆç­‰æŠ€æœ¯ï¼Œç»“åˆå¤–éƒ¨é¢†åŸŸçŸ¥è¯†å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼Œç”¨äºè§£å†³æ³•è¯­é“è·¯æ–‡æ¡£ä¸­çš„ç¼©å†™æ¶ˆæ­§é—®é¢˜ï¼Œå¹¶åœ¨TextMine'26ç«èµ›ä¸­ä»¥0.9069çš„F1åˆ†æ•°å–å¾—æœ€ä¼˜æˆç»©ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.18329v1">LIR$^3$AG: A Lightweight Rerank Reasoning Strategy Framework for Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) effectively enhances Large Language Models (LLMs) by incorporating retrieved external knowledge into the generation process. Reasoning models improve LLM performance in multi-hop QA tasks, which require integrating and reasoning over multiple pieces of evidence across different documents to answer a complex question. However, they often introduce substantial computational costs, including increased token consumption and inference latency. To better understand and mitigate this trade-off, we conduct a comprehensive study of reasoning strategies for reasoning models in RAG multi-hop QA tasks. Our findings reveal that reasoning models adopt structured strategies to integrate retrieved and internal knowledge, primarily following two modes: Context-Grounded Reasoning, which relies directly on retrieved content, and Knowledge-Reconciled Reasoning, which resolves conflicts or gaps using internal knowledge. To this end, we propose a novel Lightweight Rerank Reasoning Strategy Framework for RAG (LiR$^3$AG) to enable non-reasoning models to transfer reasoning strategies by restructuring retrieved evidence into coherent reasoning chains. LiR$^3$AG significantly reduce the average 98% output tokens overhead and 58.6% inferencing time while improving 8B non-reasoning model's F1 performance ranging from 6.2% to 22.5% to surpass the performance of 32B reasoning model in RAG, offering a practical and efficient path forward for RAG systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨å¤šè·³é—®ç­”ä»»åŠ¡ä¸­å¦‚ä½•é€šè¿‡æ¨ç†æ¨¡å‹æ•´åˆå¤–éƒ¨æ£€ç´¢çŸ¥è¯†ä¸å†…éƒ¨çŸ¥è¯†ï¼Œæå‡ºäº†ä¸€ç§è½»é‡çº§é‡æ’æ¨ç†ç­–ç•¥æ¡†æ¶ï¼ˆLiRÂ³AGï¼‰ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ï¼ˆå¦‚98%çš„tokenæ¶ˆè€—å’Œ58.6%æ¨ç†æ—¶é—´ï¼‰ï¼ŒåŒæ—¶æå‡äº†éæ¨ç†æ¨¡å‹çš„æ€§èƒ½ï¼Œä½¿å…¶è¶…è¶Š32Bæ¨ç†æ¨¡å‹çš„è¡¨ç°ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.18177v1">NEURO-GUARD: Neuro-Symbolic Generalization and Unbiased Adaptive Routing for Diagnostics -- Explainable Medical AI</a></td><td><details><summary>å±•å¼€</summary>Accurate yet interpretable image-based diagnosis remains a central challenge in medical AI, particularly in settings characterized by limited data, subtle visual cues, and high-stakes clinical decision-making. Most existing vision models rely on purely data-driven learning and produce black-box predictions with limited interpretability and poor cross-domain generalization, hindering their real-world clinical adoption. We present NEURO-GUARD, a novel knowledge-guided vision framework that integrates Vision Transformers (ViTs) with language-driven reasoning to improve performance, transparency, and domain robustness. NEURO-GUARD employs a retrieval-augmented generation (RAG) mechanism for self-verification, in which a large language model (LLM) iteratively generates, evaluates, and refines feature-extraction code for medical images. By grounding this process in clinical guidelines and expert knowledge, the framework progressively enhances feature detection and classification beyond purely data-driven baselines. Extensive experiments on diabetic retinopathy classification across four benchmark datasets APTOS, EyePACS, Messidor-1, and Messidor-2 demonstrate that NEURO-GUARD improves accuracy by 6.2% over a ViT-only baseline (84.69% vs. 78.4%) and achieves a 5% gain in domain generalization. Additional evaluations on MRI-based seizure detection further confirm its cross-domain robustness, consistently outperforming existing methods.
  Overall, NEURO-GUARD bridges symbolic medical reasoning with subsymbolic visual learning, enabling interpretable, knowledge-aware, and generalizable medical image diagnosis while achieving state-of-the-art performance across multiple datasets.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†NEURO-GUARDæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆè§†è§‰Transformerï¼ˆViTï¼‰å’ŒåŸºäºè¯­è¨€çš„æ¨ç†ï¼Œåˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æœºåˆ¶è¿›è¡Œè‡ªæˆ‘éªŒè¯ï¼Œæå‡åŒ»å­¦å›¾åƒè¯Šæ–­çš„å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œè·¨é¢†åŸŸé²æ£’æ€§ã€‚è¯¥æ–¹æ³•åœ¨ç³–å°¿ç—…è§†ç½‘è†œç—…å˜åˆ†ç±»å’ŒMRIç™«ç—«æ£€æµ‹ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºçº¯æ•°æ®é©±åŠ¨çš„åŸºçº¿æ¨¡å‹ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.18060v1">Graph-based Nearest Neighbors with Dynamic Updates via Random Walks</a></td><td><details><summary>å±•å¼€</summary>Approximate nearest neighbor search (ANN) is a common way to retrieve relevant search results, especially now in the context of large language models and retrieval augmented generation. One of the most widely used algorithms for ANN is based on constructing a multi-layer graph over the dataset, called the Hierarchical Navigable Small World (HNSW). While this algorithm supports insertion of new data, it does not support deletion of existing data. Moreover, deletion algorithms described by prior work come at the cost of increased query latency, decreased recall, or prolonged deletion time. In this paper, we propose a new theoretical framework for graph-based ANN based on random walks. We then utilize this framework to analyze a randomized deletion approach that preserves hitting time statistics compared to the graph before deleting the point. We then turn this theoretical framework into a deterministic deletion algorithm, and show that it provides better tradeoff between query latency, recall, deletion time, and memory usage through an extensive collection of experiments.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡èšç„¦äºè¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ï¼ˆANNï¼‰ä¸­çš„åˆ†å±‚å¯å¯¼èˆªå°ä¸–ç•Œï¼ˆHNSWï¼‰å›¾ç®—æ³•ï¼Œé’ˆå¯¹å…¶ä¸æ”¯æŒæ•°æ®åˆ é™¤çš„å±€é™æ€§æå‡ºæ”¹è¿›ã€‚ä½œè€…é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªåŸºäºéšæœºæ¸¸èµ°çš„ANNç†è®ºæ¡†æ¶ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§éšæœºåŒ–åˆ é™¤æ–¹æ³•ä»¥ä¿æŒå›¾çš„å‘½ä¸­æ—¶é—´ç»Ÿè®¡ç‰¹æ€§ï¼Œæœ€ç»ˆè½¬åŒ–ä¸ºç¡®å®šæ€§åˆ é™¤ç®—æ³•ã€‚å®éªŒè¡¨æ˜è¯¥ç®—æ³•åœ¨æŸ¥è¯¢å»¶è¿Ÿã€å¬å›ç‡ã€åˆ é™¤æ—¶é—´å’Œå†…å­˜å ç”¨ä¹‹é—´å®ç°äº†æ›´ä¼˜çš„å¹³è¡¡ï¼Œè¿™å¯¹RAGæŠ€æœ¯ä¸­åŠ¨æ€çŸ¥è¯†åº“çš„ç»´æŠ¤å…·æœ‰é‡è¦æ„ä¹‰ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.17559v1">Towards Explainable Conversational AI for Early Diagnosis with Large Language Models</a></td><td><details><summary>å±•å¼€</summary>Healthcare systems around the world are grappling with issues like inefficient diagnostics, rising costs, and limited access to specialists. These problems often lead to delays in treatment and poor health outcomes. Most current AI and deep learning diagnostic systems are not very interactive or transparent, making them less effective in real-world, patient-centered environments. This research introduces a diagnostic chatbot powered by a Large Language Model (LLM), using GPT-4o, Retrieval-Augmented Generation, and explainable AI techniques. The chatbot engages patients in a dynamic conversation, helping to extract and normalize symptoms while prioritizing potential diagnoses through similarity matching and adaptive questioning. With Chain-of-Thought prompting, the system also offers more transparent reasoning behind its diagnoses. When tested against traditional machine learning models like Naive Bayes, Logistic Regression, SVM, Random Forest, and KNN, the LLM-based system delivered impressive results, achieving an accuracy of 90% and Top-3 accuracy of 100%. These findings offer a promising outlook for more transparent, interactive, and clinically relevant AI in healthcare.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆGPT-4oï¼‰çš„è¯Šæ–­èŠå¤©æœºå™¨äººï¼Œç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œå¯è§£é‡ŠAIæŠ€æœ¯ï¼Œé€šè¿‡åŠ¨æ€å¯¹è¯æå–ç—‡çŠ¶å¹¶ä¼˜å…ˆè¯Šæ–­ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶å‡†ç¡®æ€§è¾¾90%ï¼Œå…·æœ‰è¾ƒé«˜çš„ä¸´åºŠæ½œåŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.17220v1">Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding</a></td><td><details><summary>å±•å¼€</summary>Humans understand long and complex texts by relying on a holistic semantic representation of the content. This global view helps organize prior knowledge, interpret new information, and integrate evidence dispersed across a document, as revealed by the Mindscape-Aware Capability of humans in psychology. Current Retrieval-Augmented Generation (RAG) systems lack such guidance and therefore struggle with long-context tasks. In this paper, we propose Mindscape-Aware RAG (MiA-RAG), the first approach that equips LLM-based RAG systems with explicit global context awareness. MiA-RAG builds a mindscape through hierarchical summarization and conditions both retrieval and generation on this global semantic representation. This enables the retriever to form enriched query embeddings and the generator to reason over retrieved evidence within a coherent global context. We evaluate MiA-RAG across diverse long-context and bilingual benchmarks for evidence-based understanding and global sense-making. It consistently surpasses baselines, and further analysis shows that it aligns local details with a coherent global representation, enabling more human-like long-context retrieval and reasoning.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMindscape-Aware RAG (MiA-RAG)çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡åˆ†å±‚æ‘˜è¦æ„å»ºå…¨å±€è¯­ä¹‰è¡¨å¾ï¼ˆmindscapeï¼‰ï¼Œå¢å¼ºæ£€ç´¢å’Œç”Ÿæˆè¿‡ç¨‹ä¸­çš„å…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ï¼Œä»¥è§£å†³ä¼ ç»ŸRAGç³»ç»Ÿåœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­çš„ä¸è¶³ï¼Œå¹¶åœ¨å¤šè¯­è¨€å’Œé•¿æ–‡æœ¬åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.17194v1">MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Multi-modal Retrieval-Augmented Generation (MMRAG) enables highly credible generation by integrating external multi-modal knowledge, thus demonstrating impressive performance in complex multi-modal scenarios. However, existing MMRAG methods fail to clarify the reasoning logic behind retrieval and response generation, which limits the explainability of the results. To address this gap, we propose to introduce reinforcement learning into multi-modal retrieval-augmented generation, enhancing the reasoning capabilities of multi-modal large language models through a two-stage reinforcement fine-tuning framework to achieve explainable multi-modal retrieval-augmented generation. Specifically, in the first stage, rule-based reinforcement fine-tuning is employed to perform coarse-grained point-wise ranking of multi-modal documents, effectively filtering out those that are significantly irrelevant. In the second stage, reasoning-based reinforcement fine-tuning is utilized to jointly optimize fine-grained list-wise ranking and answer generation, guiding multi-modal large language models to output explainable reasoning logic in the MMRAG process. Our method achieves state-of-the-art results on WebQA and MultimodalQA, two benchmark datasets for multi-modal retrieval-augmented generation, and its effectiveness is validated through comprehensive ablation experiments.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ï¼ˆMMRAGï¼‰ï¼Œé€šè¿‡ä¸¤é˜¶æ®µå¼ºåŒ–å¾®è°ƒæ¡†æ¶æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå®ç°å¯è§£é‡Šçš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆã€‚ç¬¬ä¸€é˜¶æ®µä½¿ç”¨åŸºäºè§„åˆ™çš„å¼ºåŒ–å¾®è°ƒå¯¹å¤šæ¨¡æ€æ–‡æ¡£è¿›è¡Œç²—ç²’åº¦æ’åºï¼Œè¿‡æ»¤æ— å…³å†…å®¹ï¼›ç¬¬äºŒé˜¶æ®µé€šè¿‡åŸºäºæ¨ç†çš„å¼ºåŒ–å¾®è°ƒè”åˆä¼˜åŒ–ç»†ç²’åº¦æ’åºå’Œç­”æ¡ˆç”Ÿæˆï¼ŒæŒ‡å¯¼æ¨¡å‹è¾“å‡ºå¯è§£é‡Šçš„æ¨ç†é€»è¾‘ã€‚è¯¥æ–¹æ³•åœ¨WebQAå’ŒMultimodalQAåŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›æ€§èƒ½ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-18
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.16822v1">MEPIC: Memory Efficient Position Independent Caching for LLM Serving</a></td><td><details><summary>å±•å¼€</summary>Modern LLM applications such as deep-research assistants, coding agents, and Retrieval-Augmented Generation (RAG) systems, repeatedly process long prompt histories containing shared document or code chunks, creating significant pressure on the Key Value (KV) cache, which must operate within limited memory while sustaining high throughput and low latency. Prefix caching partially alleviates some of these costs by reusing KV cache for previously processed tokens, but limited by strict prefix matching. Position-independent caching (PIC) enables chunk-level reuse at arbitrary positions, but requires selective recomputation and positional-encoding (PE) adjustments. However, because these operations vary across queries, KV for the same chunk diverges across requests. Moreover, without page alignment, chunk KV layouts diverge in memory, preventing page sharing. These issues result in only modest HBM savings even when many requests reuse the same content.
  We present MEPIC, a memory-efficient PIC system that enables chunk KV reuse across positions, requests, and batches. MEPIC aligns chunk KV to paged storage, shifts recomputation from token- to block-level so only the first block is request-specific, removes positional encodings via Rotary Position Embedding (RoPE) fusion in the attention kernel, and makes remaining blocks fully shareable. These techniques eliminate most duplicate chunk KV in HBM, reducing usage by up to 2x over state-of-the-art PIC at comparable latency and accuracy, and up to 5x for long prompts, without any model changes.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†MEPICç³»ç»Ÿï¼Œæ—¨åœ¨ä¼˜åŒ–å¤„ç†é•¿æç¤ºå†å²çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­‰LLMåº”ç”¨ä¸­çš„KVç¼“å­˜æ•ˆç‡ï¼Œé€šè¿‡å—çº§é‡ç”¨ã€å†…å­˜å¯¹é½å’Œä½ç½®ç¼–ç èåˆæŠ€æœ¯å‡å°‘æ˜¾å­˜å ç”¨ï¼Œæå‡æ€§èƒ½ï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.16802v1">Exploration of Augmentation Strategies in Multi-modal Retrieval-Augmented Generation for the Biomedical Domain: A Case Study Evaluating Question Answering in Glycobiology</a></td><td><details><summary>å±•å¼€</summary>Multi-modal retrieval-augmented generation (MM-RAG) promises grounded biomedical QA, but it is unclear when to (i) convert figures/tables into text versus (ii) use optical character recognition (OCR)-free visual retrieval that returns page images and leaves interpretation to the generator. We study this trade-off in glycobiology, a visually dense domain. We built a benchmark of 120 multiple-choice questions (MCQs) from 25 papers, stratified by retrieval difficulty (easy text, medium figures/tables, hard cross-evidence). We implemented four augmentations-None, Text RAG, Multi-modal conversion, and late-interaction visual retrieval (ColPali)-using Docling parsing and Qdrant indexing. We evaluated mid-size open-source and frontier proprietary models (e.g., Gemma-3-27B-IT, GPT-4o family). Additional testing used the GPT-5 family and multiple visual retrievers (ColPali/ColQwen/ColFlor). Accuracy with Agresti-Coull 95% confidence intervals (CIs) was computed over 5 runs per configuration. With Gemma-3-27B-IT, Text and Multi-modal augmentation outperformed OCR-free retrieval (0.722-0.740 vs. 0.510 average accuracy). With GPT-4o, Multi-modal achieved 0.808, with Text 0.782 and ColPali 0.745 close behind; within-model differences were small. In follow-on experiments with the GPT-5 family, the best results with ColPali and ColFlor improved by ~2% to 0.828 in both cases. In general, across the GPT-5 family, ColPali, ColQwen, and ColFlor were statistically indistinguishable. GPT-5-nano trailed larger GPT-5 variants by roughly 8-10%. Pipeline choice is capacity-dependent: converting visuals to text lowers the reader burden and is more reliable for mid-size models, whereas OCR-free visual retrieval becomes competitive under frontier models. Among retrievers, ColFlor offers parity with heavier options at a smaller footprint, making it an efficient default when strong generators are available.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆMM-RAGï¼‰åœ¨ç”Ÿç‰©åŒ»å­¦QAä¸­çš„åº”ç”¨ï¼Œæ¢è®¨äº†åœ¨ç¨ å¯†è§†è§‰é¢†åŸŸï¼ˆç³–ç”Ÿç‰©å­¦ï¼‰ä¸­ä½•æ—¶å°†å›¾è¡¨è½¬æ¢ä¸ºæ–‡æœ¬æˆ–ç›´æ¥ä½¿ç”¨æ— OCRè§†è§‰æ£€ç´¢çš„ç­–ç•¥ï¼Œå¹¶é€šè¿‡å®éªŒæ¯”è¾ƒäº†ä¸åŒæ¨¡å‹å’Œæ£€ç´¢æ–¹æ³•çš„æ€§èƒ½å·®å¼‚åŠé€‚ç”¨åœºæ™¯ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.16795v1">From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¨ç†è½¨è¿¹å¢å¼ºçš„RAGæ¡†æ¶ï¼Œé€šè¿‡ä¸‰é˜¶æ®µç»“æ„åŒ–æ¨ç†ï¼ˆæ–‡æ¡£çº§è£å†³ã€å†²çªåˆ†æå’ŒåŸºäºè¯æ®çš„åˆæˆï¼‰è§£å†³ä¼ ç»ŸRAGåœ¨æ£€ç´¢æºå†²çªã€è¿‡æ—¶æˆ–ä¸»è§‚ä¿¡æ¯æ—¶çš„ç¼ºé™·ï¼Œå¹¶å¼•å…¥å†²çªæ„ŸçŸ¥ä¿¡ä»»è¯„åˆ†ï¼ˆCATSï¼‰å’Œè¯„ä¼°ç®¡é“ã€‚å®éªŒè¡¨æ˜è¯¥æ¡†æ¶æ˜¾è‘—æå‡äº†æ¨¡å‹ç­”æ¡ˆæ­£ç¡®æ€§å’Œè¡Œä¸ºä¸€è‡´æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.16676v1">DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI</a></td><td><details><summary>å±•å¼€</summary>The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\% execution accuracy in Text-to-SQL over SynSQL, +7\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ä»‹ç»äº†DataFlowï¼Œä¸€ä¸ªç»Ÿä¸€ä¸”å¯æ‰©å±•çš„LLMé©±åŠ¨æ•°æ®å‡†å¤‡æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰æ•°æ®å‡†å¤‡æµç¨‹ä¸­çš„å¯é‡ç”¨æ€§ã€å¯è°ƒè¯•æ€§å’Œå¯ä¼˜åŒ–æ€§é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒ…å«è¿‘200ä¸ªå¯å¤ç”¨æ“ä½œç¬¦å’Œå…­ä¸ªé¢†åŸŸé€šç”¨ç®¡é“ï¼Œå…¶ä¸­æ˜ç¡®æåˆ°æ”¯æŒâ€œAgentic RAGâ€ä»»åŠ¡ï¼Œå¹¶é€šè¿‡è‡ªåŠ¨ç”Ÿæˆç®¡é“ï¼ˆå¦‚DataFlow-Agentï¼‰æå‡LLMä¸‹æ¸¸æ€§èƒ½ï¼Œæœ€ç»ˆåœ¨æ•°å­¦ã€ä»£ç å’Œæ–‡æœ¬ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å°½ç®¡æ ¸å¿ƒä¸»é¢˜æ˜¯æ•°æ®å‡†å¤‡æ¡†æ¶ï¼Œä½†å…¶åº”ç”¨åœºæ™¯ç›´æ¥æ¶‰åŠRAGé¢†åŸŸçš„æŠ€æœ¯é›†æˆã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.16425v1">Introducing ORKG ASK: an AI-driven Scholarly Literature Search and Exploration System Taking a Neuro-Symbolic Approach</a></td><td><details><summary>å±•å¼€</summary>As the volume of published scholarly literature continues to grow, finding relevant literature becomes increasingly difficult. With the rise of generative Artificial Intelligence (AI), and particularly Large Language Models (LLMs), new possibilities emerge to find and explore literature. We introduce ASK (Assistant for Scientific Knowledge), an AI-driven scholarly literature search and exploration system that follows a neuro-symbolic approach. ASK aims to provide active support to researchers in finding relevant scholarly literature by leveraging vector search, LLMs, and knowledge graphs. The system allows users to input research questions in natural language and retrieve relevant articles. ASK automatically extracts key information and generates answers to research questions using a Retrieval-Augmented Generation (RAG) approach. We present an evaluation of ASK, assessing the system's usability and usefulness. Findings indicate that the system is user-friendly and users are generally satisfied while using the system.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ASKï¼ˆAssistant for Scientific Knowledgeï¼‰ï¼Œä¸€ä¸ªåŸºäºç¥ç»ç¬¦å·æ–¹æ³•çš„AIé©±åŠ¨çš„å­¦æœ¯æ–‡çŒ®æ£€ç´¢ä¸æ¢ç´¢ç³»ç»Ÿï¼Œå®ƒç»“åˆå‘é‡æœç´¢ã€å¤§è¯­è¨€æ¨¡å‹å’ŒçŸ¥è¯†å›¾è°±ï¼Œé€šè¿‡RAGæ–¹æ³•ä»è‡ªç„¶è¯­è¨€æé—®ä¸­æ£€ç´¢ç›¸å…³æ–‡çŒ®å¹¶ç”Ÿæˆç­”æ¡ˆï¼Œè¯„ä¼°æ˜¾ç¤ºç³»ç»Ÿæ˜“ç”¨ä¸”ç”¨æˆ·æ»¡æ„åº¦é«˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.16391v1">Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference</a></td><td><details><summary>å±•å¼€</summary>Attention is the dominant source of latency during long-context LLM inference, an increasingly popular workload with reasoning models and RAG. We propose Kascade, a training-free sparse attention method that leverages known observations such as 1) post-softmax attention is intrinsically sparse, and 2) the identity of high-weight keys is stable across nearby layers. Kascade computes exact Top-k indices in a small set of anchor layers, then reuses those indices in intermediate reuse layers. The anchor layers are selected algorithmically, via a dynamic-programming objective that maximizes cross-layer similarity over a development set, allowing easy deployment across models. The method incorporates efficient implementation constraints (e.g. tile-level operations), across both prefill and decode attention. The Top-k selection and reuse in Kascade is head-aware and we show in our experiments that this is critical for high accuracy. Kascade achieves up to 4.1x speedup in decode attention and 2.2x speedup in prefill attention over FlashAttention-3 baseline on H100 GPUs while closely matching dense attention accuracy on long-context benchmarks such as LongBench and AIME-24.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºKascadeçš„è®­ç»ƒæ— å…³ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³é•¿ä¸Šä¸‹æ–‡LLMæ¨ç†ä¸­çš„æ³¨æ„åŠ›å»¶è¿Ÿé—®é¢˜ï¼Œç‰¹åˆ«é€‚ç”¨äºæ¨ç†æ¨¡å‹å’ŒRAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰åœºæ™¯ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶çš„å›ºæœ‰ç¨€ç–æ€§å’Œè·¨å±‚é”®ç¨³å®šæ€§ï¼Œåœ¨é”šå®šå±‚è®¡ç®—ç²¾ç¡®Top-kç´¢å¼•å¹¶åœ¨ä¸­é—´å±‚å¤ç”¨ï¼Œç»“åˆåŠ¨æ€è§„åˆ’ä¼˜åŒ–å±‚é€‰æ‹©ï¼Œå®ç°äº†é¢„å¡«å……å’Œè§£ç é˜¶æ®µçš„é«˜æ•ˆæ³¨æ„åŠ›è®¡ç®—ã€‚å®éªŒè¡¨æ˜ï¼ŒKascadeåœ¨H100 GPUä¸Šç›¸æ¯”FlashAttention-3åŸºçº¿æœ€é«˜æå‡4.1å€è§£ç é€Ÿåº¦å’Œ2.2å€é¢„å¡«å……é€Ÿåº¦ï¼ŒåŒæ—¶åœ¨LongBenchç­‰é•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸­ä¿æŒæ¥è¿‘å¯†é›†æ³¨æ„åŠ›çš„å‡†ç¡®æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.16236v1">The Evolution of Reranking Models in Information Retrieval: From Heuristic Methods to Large Language Models</a></td><td><details><summary>å±•å¼€</summary>Reranking is a critical stage in contemporary information retrieval (IR) systems, improving the relevance of the user-presented final results by honing initial candidate sets. This paper is a thorough guide to examine the changing reranker landscape and offer a clear view of the advancements made in reranking methods. We present a comprehensive survey of reranking models employed in IR, particularly within modern Retrieval Augmented Generation (RAG) pipelines, where retrieved documents notably influence output quality.
  We embark on a chronological journey through the historical trajectory of reranking techniques, starting with foundational approaches, before exploring the wide range of sophisticated neural network architectures such as cross-encoders, sequence-generation models like T5, and Graph Neural Networks (GNNs) utilized for structural information. Recognizing the computational cost of advancing neural rerankers, we analyze techniques for enhancing efficiency, notably knowledge distillation for creating competitive, lighter alternatives. Furthermore, we map the emerging territory of integrating Large Language Models (LLMs) in reranking, examining novel prompting strategies and fine-tuning tactics. This survey seeks to elucidate the fundamental ideas, relative effectiveness, computational features, and real-world trade-offs of various reranking strategies. The survey provides a structured synthesis of the diverse reranking paradigms, highlighting their underlying principles and comparative strengths and weaknesses.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ˜¯ä¸€ç¯‡å…³äºä¿¡æ¯æ£€ç´¢ä¸­é‡æ’åºï¼ˆrerankingï¼‰æŠ€æœ¯çš„ç»¼è¿°ï¼Œç‰¹åˆ«å…³æ³¨äº†åœ¨ç°ä»£æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æµç¨‹ä¸­çš„åº”ç”¨ã€‚æ–‡ç« ç³»ç»Ÿå›é¡¾äº†é‡æ’åºæ–¹æ³•çš„å†å²æ¼”å˜ï¼ŒåŒ…æ‹¬åŸºç¡€æ–¹æ³•åˆ°å¤æ‚çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼ˆå¦‚äº¤å‰ç¼–ç å™¨ã€T5åºåˆ—ç”Ÿæˆæ¨¡å‹å’Œå›¾ç¥ç»ç½‘ç»œï¼‰ï¼Œå¹¶æ¢è®¨äº†æå‡æ•ˆç‡çš„æŠ€æœ¯ï¼ˆå¦‚çŸ¥è¯†è’¸é¦ï¼‰ä»¥åŠå¤§è¯­è¨€æ¨¡å‹åœ¨é‡æ’åºä¸­çš„æ•´åˆç­–ç•¥ï¼Œæœ€ç»ˆå¯¹æ¯”åˆ†æäº†ä¸åŒæ–¹æ³•çš„åŸç†ã€æ•ˆæœå’Œå®é™…æƒè¡¡ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-17
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.15081v1">Quantifying Return on Security Controls in LLM Systems</a></td><td><details><summary>å±•å¼€</summary>Although large language models (LLMs) are increasingly used in security-critical workflows, practitioners lack quantitative guidance on which safeguards are worth deploying. This paper introduces a decision-oriented framework and reproducible methodology that together quantify residual risk, convert adversarial probe outcomes into financial risk estimates and return-on-control (RoC) metrics, and enable monetary comparison of layered defenses for LLM-based systems. A retrieval-augmented generation (RAG) service is instantiated using the DeepSeek-R1 model over a corpus containing synthetic personally identifiable information (PII), and subjected to automated attacks with Garak across five vulnerability classes: PII leakage, latent context injection, prompt injection, adversarial attack generation, and divergence. For each (vulnerability, control) pair, attack success probabilities are estimated via Laplace's Rule of Succession and combined with loss triangle distributions, calibrated from public breach-cost data, in 10,000-run Monte Carlo simulations to produce loss exceedance curves and expected losses. Three widely used mitigations, attribute-based access control (ABAC); named entity recognition (NER) redaction using Microsoft Presidio; and NeMo Guardrails, are then compared to a baseline RAG configuration. The baseline system exhibits very high attack success rates (>= 0.98 for PII, latent injection, and prompt injection), yielding a total simulated expected loss of $313k per attack scenario. ABAC collapses success probabilities for PII and prompt-related attacks to near zero and reduces the total expected loss by ~94%, achieving an RoC of 9.83. NER redaction likewise eliminates PII leakage and attains an RoC of 5.97, while NeMo Guardrails provides only marginal benefit (RoC of 0.05).</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ç ”ç©¶LLMåœ¨å®‰å…¨å…³é”®å·¥ä½œæµä¸­çš„é£é™©é‡åŒ–ï¼Œä»¥RAGæœåŠ¡ï¼ˆåŸºäºDeepSeek-R1æ¨¡å‹å’Œå«åˆæˆPIIçš„è¯­æ–™åº“æ„å»ºï¼‰ä¸ºæ¡ˆä¾‹ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æ”»å‡»æµ‹è¯•äº”ç§æ¼æ´ç±»å‹ï¼Œè¯„ä¼°åŸºçº¿é…ç½®å’Œä¸‰ç§é˜²å¾¡æªæ–½ï¼ˆABACã€NERã€NeMo Guardrailsï¼‰çš„æ•ˆæœï¼Œé‡åŒ–æ”»å‡»æˆåŠŸæ¦‚ç‡å’Œç»æµæŸå¤±ï¼Œå¹¶æ¯”è¾ƒä¸åŒæ§åˆ¶æªæ–½çš„æŠ•å…¥å›æŠ¥ç‡ï¼ˆRoCï¼‰ã€‚ç ”ç©¶å‘ç°ABACèƒ½æ˜¾è‘—é™ä½é£é™©ï¼ˆRoC=9.83ï¼‰ï¼Œè€ŒåŸºçº¿RAGé…ç½®å­˜åœ¨æé«˜æ”»å‡»æˆåŠŸç‡ï¼ˆâ‰¥98%ï¼‰ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.15068v1">The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection in RAG Systems</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) systems remain susceptible to hallucinations despite grounding in retrieved evidence. Current detection methods rely on semantic similarity and natural language inference (NLI), but their fundamental limitations have not been rigorously characterized. We apply conformal prediction to hallucination detection, providing finite-sample coverage guarantees that enable precise quantification of detection capabilities. Using calibration sets of approximately 600 examples, we achieve 94% coverage with 0% false positive rate on synthetic hallucinations (Natural Questions). However, on three real hallucination benchmarks spanning multiple LLMs (GPT-4, ChatGPT, GPT-3, Llama-2, Mistral), embedding-based methods - including state-of-the-art OpenAI text-embedding-3-large and cross-encoder models - exhibit unacceptable false positive rates: 100% on HaluEval, 88% on RAGTruth, and 50% on WikiBio. Crucially, GPT-4 as an LLM judge achieves only 7% FPR (95% CI: [3.4%, 13.7%]) on the same data, proving the task is solvable through reasoning. We term this the "semantic illusion": semantically plausible hallucinations preserve similarity to source documents while introducing factual errors invisible to embeddings. This limitation persists across embedding architectures, LLM generators, and task types, suggesting embedding-based detection is insufficient for production RAG deployment.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æ¢è®¨äº†RAGç³»ç»Ÿä¸­å­˜åœ¨çš„å¹»è§‰é—®é¢˜ï¼ŒæŒ‡å‡ºå°½ç®¡ä½¿ç”¨äº†æ£€ç´¢è¯æ®ï¼Œå½“å‰åŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§å’Œè‡ªç„¶è¯­è¨€æ¨ç†çš„æ£€æµ‹æ–¹æ³•ä»å­˜åœ¨å±€é™æ€§ã€‚ä½œè€…æå‡ºä½¿ç”¨ä¿å½¢é¢„æµ‹ï¼ˆconformal predictionï¼‰è¿›è¡Œå¹»è§‰æ£€æµ‹ï¼Œæä¾›æœ‰é™æ ·æœ¬è¦†ç›–ä¿è¯ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°äº†åµŒå…¥æ–¹æ³•å’ŒGPT-4ä½œä¸ºLLMæ³•å®˜çš„æ€§èƒ½ï¼Œæ­ç¤ºäº†â€œè¯­ä¹‰å¹»è§‰â€ç°è±¡ï¼Œå³è¯­ä¹‰åˆç†ä½†äº‹å®é”™è¯¯çš„å›ç­”éš¾ä»¥é€šè¿‡åµŒå…¥æ–¹æ³•æ£€æµ‹ï¼Œå¼ºè°ƒåŸºäºåµŒå…¥çš„æ£€æµ‹æ–¹æ³•åœ¨å®é™…RAGéƒ¨ç½²ä¸­çš„ä¸è¶³ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-16
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.14554v1">VLegal-Bench: Cognitively Grounded Benchmark for Vietnamese Legal Reasoning of Large Language Models</a></td><td><details><summary>å±•å¼€</summary>The rapid advancement of large language models (LLMs) has enabled new possibilities for applying artificial intelligence within the legal domain. Nonetheless, the complexity, hierarchical organization, and frequent revisions of Vietnamese legislation pose considerable challenges for evaluating how well these models interpret and utilize legal knowledge. To address this gap, Vietnamese Legal Benchmark (VLegal-Bench) is introduced, the first comprehensive benchmark designed to systematically assess LLMs on Vietnamese legal tasks. Informed by Bloom's cognitive taxonomy, VLegal-Bench encompasses multiple levels of legal understanding through tasks designed to reflect practical usage scenarios. The benchmark comprises 10,450 samples generated through a rigorous annotation pipeline, where legal experts label and cross-validate each instance using our annotation system to ensure every sample is grounded in authoritative legal documents and mirrors real-world legal assistant workflows, including general legal questions and answers, retrieval-augmented generation, multi-step reasoning, and scenario-based problem solving tailored to Vietnamese law. By providing a standardized, transparent, and cognitively informed evaluation framework, VLegal-Bench establishes a solid foundation for assessing LLM performance in Vietnamese legal contexts and supports the development of more reliable, interpretable, and ethically aligned AI-assisted legal systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†è¶Šå—æ³•å¾‹åŸºå‡†ï¼ˆVLegal-Benchï¼‰ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨è¶Šå—æ³•å¾‹ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå…¶ä¸­åŒ…å«æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä»»åŠ¡ï¼Œä»¥åŠå…¶ä»–å¦‚å¤šæ­¥æ¨ç†å’ŒåŸºäºåœºæ™¯çš„é—®é¢˜è§£å†³ç­‰ä»»åŠ¡ï¼Œä»¥æå‡AIè¾…åŠ©æ³•å¾‹ç³»ç»Ÿçš„å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.14465v1">Context-Picker: Dynamic context selection using multi-stage reinforcement learning</a></td><td><details><summary>å±•å¼€</summary>In long-context question answering (LCQA), determining the optimal amount of context for a given query is a significant challenge. Including too few passages may omit critical information, while including too many can introduce noise and reduce the quality of the answer. Traditional approaches, such as fixed Top-$K$ retrieval and single-stage reranking, face the dilemma of selecting the right number of passages. This problem is particularly pronounced for factoid questions, which often require only a few specific pieces of evidence. To address this issue, we introduce \emph{Context-Picker}, a reasoning-aware framework that shifts the paradigm from similarity-based ranking to minimal sufficient subset selection. Context-Picker treats context selection as a decision-making process optimized via a human-inspired, two-stage reinforcement learning schedule: a \emph{recall-oriented} stage that prioritizes the coverage of reasoning chains, followed by a \emph{precision-oriented} stage that aggressively prunes redundancy to distill a compact evidence set. To resolve reward sparsity, we propose an offline evidence distillation pipeline that mines "minimal sufficient sets" via a Leave-One-Out (LOO) procedure, providing dense, task-aligned supervision. Experiments on five long-context and multi-hop QA benchmarks demonstrate that Context-Picker significantly outperforms strong RAG baselines, achieving superior answer accuracy with comparable or reduced context lengths. Ablation studies indicate that the coarse-to-fine optimization schedule, the redundancy-aware reward shaping, and the rationale-guided format all contribute substantially to these gains.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºContext-Pickerçš„æ¡†æ¶ï¼Œç”¨äºè§£å†³é•¿ä¸Šä¸‹æ–‡é—®ç­”ï¼ˆLCQAï¼‰ä¸­å¦‚ä½•é€‰æ‹©æœ€ä¼˜ä¸Šä¸‹æ–‡çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ ï¼ˆå¬å›ä¼˜å…ˆå’Œç²¾ç¡®ä¼˜å…ˆï¼‰ä»ç›¸ä¼¼æ€§æ’åè½¬å‘æœ€å°å……åˆ†å­é›†é€‰æ‹©ï¼Œå¹¶ç»“åˆLeave-One-Outï¼ˆLOOï¼‰è¿‡ç¨‹æŒ–æ˜æœ€å°å……åˆ†è¯æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼ŒContext-Pickeråœ¨ä¿æŒæˆ–å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦çš„åŒæ—¶æ˜¾è‘—ä¼˜äºç°æœ‰RAGåŸºçº¿ï¼Œæå‡äº†ç­”æ¡ˆå‡†ç¡®æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.14417v1">PortAgent: LLM-driven Vehicle Dispatching Agent for Port Terminals</a></td><td><details><summary>å±•å¼€</summary>Vehicle Dispatching Systems (VDSs) are critical to the operational efficiency of Automated Container Terminals (ACTs). However, their widespread commercialization is hindered due to their low transferability across diverse terminals. This transferability challenge stems from three limitations: high reliance on port operational specialists, a high demand for terminal-specific data, and time-consuming manual deployment processes. Leveraging the emergence of Large Language Models (LLMs), this paper proposes PortAgent, an LLM-driven vehicle dispatching agent that fully automates the VDS transferring workflow. It bears three features: (1) no need for port operations specialists; (2) low need of data; and (3) fast deployment. Specifically, specialist dependency is eliminated by the Virtual Expert Team (VET). The VET collaborates with four virtual experts, including a Knowledge Retriever, Modeler, Coder, and Debugger, to emulate a human expert team for the VDS transferring workflow. These experts specialize in the domain of terminal VDS via a few-shot example learning approach. Through this approach, the experts are able to learn VDS-domain knowledge from a few VDS examples. These examples are retrieved via a Retrieval-Augmented Generation (RAG) mechanism, mitigating the high demand for terminal-specific data. Furthermore, an automatic VDS design workflow is established among these experts to avoid extra manual interventions. In this workflow, a self-correction loop inspired by the LLM Reflexion framework is created</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†PortAgentï¼Œä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨åŒ–è½¦è¾†è°ƒåº¦ä»£ç†ï¼Œç”¨äºè§£å†³è‡ªåŠ¨åŒ–é›†è£…ç®±ç å¤´ï¼ˆACTï¼‰ä¸­è½¦è¾†è°ƒåº¦ç³»ç»Ÿï¼ˆVDSï¼‰çš„å¯è¿ç§»æ€§é—®é¢˜ã€‚PortAgenté€šè¿‡è™šæ‹Ÿä¸“å®¶å›¢é˜Ÿï¼ˆVETï¼‰æ¶ˆé™¤å¯¹æ¸¯å£æ“ä½œä¸“å®¶çš„ä¾èµ–ï¼Œé‡‡ç”¨å°æ ·æœ¬å­¦ä¹ è·å–é¢†åŸŸçŸ¥è¯†ï¼Œå¹¶åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æœºåˆ¶ä»å°‘é‡ç¤ºä¾‹ä¸­æ£€ç´¢ç›¸å…³æ•°æ®ä»¥å‡å°‘å¯¹ç‰¹å®šç å¤´æ•°æ®çš„éœ€æ±‚ã€‚æ­¤å¤–ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–çš„VDSè®¾è®¡å·¥ä½œæµå’Œè‡ªæˆ‘çº æ­£å¾ªç¯ï¼ˆå—LLM Reflexionæ¡†æ¶å¯å‘ï¼‰å®ç°å¿«é€Ÿéƒ¨ç½²ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.14313v1">Dynamic Context Selection for Retrieval-Augmented Generation: Mitigating Distractors and Positional Bias</a></td><td><details><summary>å±•å¼€</summary>Retrieval Augmented Generation (RAG) enhances language model performance by incorporating external knowledge retrieved from large corpora, which makes it highly suitable for tasks such as open domain question answering. Standard RAG systems typically rely on a fixed top k retrieval strategy, which can either miss relevant information or introduce semantically irrelevant passages, known as distractors, that degrade output quality. Additionally, the positioning of retrieved passages within the input context can influence the model attention and generation outcomes. Context placed in the middle tends to be overlooked, which is an issue known as the "lost in the middle" phenomenon. In this work, we systematically analyze the impact of distractors on generation quality, and quantify their effects under varying conditions. We also investigate how the position of relevant passages within the context window affects their influence on generation. Building on these insights, we propose a context-size classifier that dynamically predicts the optimal number of documents to retrieve based on query-specific informational needs. We integrate this approach into a full RAG pipeline, and demonstrate improved performance over fixed k baselines.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç³»ç»Ÿåœ°åˆ†æäº†æ ‡å‡†RAGç³»ç»Ÿä¸­å›ºå®štop kæ£€ç´¢ç­–ç•¥çš„å±€é™æ€§ï¼ˆå¦‚é—æ¼ç›¸å…³ä¿¡æ¯æˆ–å¼•å…¥å¹²æ‰°æ€§æ®µè½ï¼‰ï¼Œç ”ç©¶äº†æ£€ç´¢æ®µè½ä½ç½®å¯¹ç”Ÿæˆè´¨é‡çš„å½±å“ï¼ˆå¦‚â€œlost in the middleâ€ç°è±¡ï¼‰ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŠ¨æ€é¢„æµ‹æœ€ä¼˜æ£€ç´¢æ–‡æ¡£æ•°é‡çš„ä¸Šä¸‹æ–‡å¤§å°åˆ†ç±»å™¨ï¼Œé›†æˆåˆ°RAGæµç¨‹åæ€§èƒ½ä¼˜äºå›ºå®škçš„åŸºçº¿æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.14179v1">A Comparative Analysis of Retrieval-Augmented Generation Techniques for Bengali Standard-to-Dialect Machine Translation Using LLMs</a></td><td><details><summary>å±•å¼€</summary>Translating from a standard language to its regional dialects is a significant NLP challenge due to scarce data and linguistic variation, a problem prominent in the Bengali language. This paper proposes and compares two novel RAG pipelines for standard-to-dialectal Bengali translation. The first, a Transcript-Based Pipeline, uses large dialect sentence contexts from audio transcripts. The second, a more effective Standardized Sentence-Pairs Pipeline, utilizes structured local\_dialect:standard\_bengali sentence pairs. We evaluated both pipelines across six Bengali dialects and multiple LLMs using BLEU, ChrF, WER, and BERTScore. Our findings show that the sentence-pair pipeline consistently outperforms the transcript-based one, reducing Word Error Rate (WER) from 76\% to 55\% for the Chittagong dialect. Critically, this RAG approach enables smaller models (e.g., Llama-3.1-8B) to outperform much larger models (e.g., GPT-OSS-120B), demonstrating that a well-designed retrieval strategy can be more crucial than model size. This work contributes an effective, fine-tuning-free solution for low-resource dialect translation, offering a practical blueprint for preserving linguistic diversity.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸¤ç§æ–°é¢–çš„RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æµç¨‹ï¼Œç”¨äºè§£å†³æ ‡å‡†å­ŸåŠ æ‹‰è¯­åˆ°æ–¹è¨€çš„ç¿»è¯‘é—®é¢˜ï¼Œé€šè¿‡å¯¹æ¯”åŸºäºéŸ³é¢‘è½¬å½•çš„æµç¨‹å’ŒåŸºäºæ ‡å‡†åŒ–å¥å­å¯¹çš„æµç¨‹ï¼Œå‘ç°åè€…åœ¨å¤šç§è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°æ›´ä¼˜ï¼Œæ˜¾è‘—é™ä½äº†é”™è¯¯ç‡ï¼Œå¹¶è¯æ˜è®¾è®¡è‰¯å¥½çš„æ£€ç´¢ç­–ç•¥åœ¨ä½èµ„æºæ–¹è¨€ç¿»è¯‘ä¸­æ¯”æ¨¡å‹è§„æ¨¡æ›´é‡è¦ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.14121v1">SportsGPT: An LLM-driven Framework for Interpretable Sports Motion Assessment and Training Guidance</a></td><td><details><summary>å±•å¼€</summary>Existing intelligent sports analysis systems mainly focus on "scoring and visualization," often lacking automatic performance diagnosis and interpretable training guidance. Recent advances of Large Language Models (LMMs) and motion analysis techniques provide new opportunities to address the above limitations. In this paper, we propose SportsGPT, an LLM-driven framework for interpretable sports motion assessment and training guidance, which establishes a closed loop from motion time-series input to professional training guidance. First, given a set of high-quality target models, we introduce MotionDTW, a two-stage time series alignment algorithm designed for accurate keyframe extraction from skeleton-based motion sequences. Subsequently, we design a Knowledge-based Interpretable Sports Motion Assessment Model (KISMAM) to obtain a set of interpretable assessment metrics (e.g., insufficient extension) by constrasting the keyframes with the targe models. Finally, we propose SportsRAG, a RAG-based training guidance model based on Qwen3. Leveraging a 6B-token knowledge base, it prompts the LLM to generate professional training guidance by retrieving domain-specific QA pairs. Experimental results demonstrate that MotionDTW significantly outperforms traditional methods with lower temporal error and higher IoU scores. Furthermore, ablation studies validate the KISMAM and SportsRAG, confirming that SportsGPT surpasses general LLMs in diagnostic accuracy and professionalism.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†SportsGPTæ¡†æ¶ï¼Œç»“åˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè¿åŠ¨åˆ†ææŠ€æœ¯ï¼Œç”¨äºå¯è§£é‡Šçš„è¿åŠ¨è¯„ä¼°ä¸è®­ç»ƒæŒ‡å¯¼ã€‚å…¶ä¸­ï¼ŒSportsRAGæ¨¡å—åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œåˆ©ç”¨6B-tokençš„çŸ¥è¯†åº“æ£€ç´¢é¢†åŸŸç‰¹å®šçš„QAå¯¹ï¼Œé©±åŠ¨LLMç”Ÿæˆä¸“ä¸šè®­ç»ƒå»ºè®®ï¼Œä»è€Œæå‡è¯Šæ–­å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-15
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.13632v1">StutterFuse: Mitigating Modality Collapse in Stuttering Detection with Jaccard-Weighted Metric Learning and Gated Fusion</a></td><td><details><summary>å±•å¼€</summary>Stuttering detection breaks down when disfluencies overlap. Existing parametric models struggle to distinguish complex, simultaneous disfluencies (e.g., a 'block' with a 'prolongation') due to the scarcity of these specific combinations in training data. While Retrieval-Augmented Generation (RAG) has revolutionized NLP by grounding models in external knowledge, this paradigm remains unexplored in pathological speech processing. To bridge this gap, we introduce StutterFuse, the first Retrieval-Augmented Classifier (RAC) for multi-label stuttering detection. By conditioning a Conformer encoder on a non-parametric memory bank of clinical examples, we allow the model to classify by reference rather than memorization. We further identify and solve "Modality Collapse", an "Echo Chamber" effect where naive retrieval boosts recall but degrades precision. We mitigate this using: (1) SetCon, a Jaccard-Weighted Metric Learning objective that optimizes for multi-label set similarity, and (2) a Gated Mixture-of-Experts fusion strategy that dynamically arbitrates between acoustic evidence and retrieved context. On the SEP-28k dataset, StutterFuse achieves a weighted F1-score of 0.65, outperforming strong baselines and demonstrating remarkable zero-shot cross-lingual generalization.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†StutterFuseï¼Œé¦–ä¸ªåŸºäºæ£€ç´¢å¢å¼ºåˆ†ç±»å™¨ï¼ˆRACï¼‰çš„å¤šæ ‡ç­¾å£åƒæ£€æµ‹æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆä¸´åºŠæ¡ˆä¾‹çš„éå‚æ•°è®°å¿†åº“å’ŒConformerç¼–ç å™¨ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨å¤æ‚ä¸æµç•…é‡å åœºæ™¯ä¸‹çš„æ€§èƒ½ç“¶é¢ˆï¼Œå¹¶åˆ›æ–°æ€§åœ°è§£å†³äº†â€œæ¨¡æ€å´©æºƒâ€é—®é¢˜ï¼Œæœ€ç»ˆåœ¨SEP-28kæ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜äºåŸºçº¿æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.13059v1">An Open and Reproducible Deep Research Agent for Long-Form Question Answering</a></td><td><details><summary>å±•å¼€</summary>We present an open deep research system for long-form question answering, selected as a winning system in the text-to-text track of the MMU-RAG competition at NeurIPS 2025. The system combines an open-source large language model (LLM) with an open web search API to perform iterative retrieval, reasoning, and synthesis in real-world open-domain settings. To enhance reasoning quality, we apply preference tuning based on LLM-as-a-judge feedback that evaluates multiple aspects, including clarity, insightfulness, and factuality. Our experimental results show that the proposed method consistently improves answer quality across all three aspects. Our source code is publicly available at https://github.com/efficient-deep-research/efficient-deep-research.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªç”¨äºé•¿å½¢å¼é—®ç­”çš„å¼€æ”¾æ·±åº¦ç ”ç©¶ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç»“åˆäº†å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¼€æ”¾ç½‘ç»œæœç´¢APIï¼Œåœ¨å¼€æ”¾é¢†åŸŸè®¾ç½®ä¸­æ‰§è¡Œè¿­ä»£æ£€ç´¢ã€æ¨ç†å’Œåˆæˆã€‚é€šè¿‡åŸºäºLLMåé¦ˆçš„åå¥½è°ƒä¼˜æ¥è¯„ä¼°æ¸…æ™°åº¦ã€æ´å¯ŸåŠ›å’Œäº‹å®æ€§ï¼Œä»è€Œæå‡å›ç­”è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨è¿™ä¸‰ä¸ªæ–¹é¢å‡èƒ½æŒç»­æ”¹è¿›ç­”æ¡ˆè´¨é‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.13040v1">Understanding Structured Financial Data with LLMs: A Case Study on Fraud Detection</a></td><td><details><summary>å±•å¼€</summary>Detecting fraud in financial transactions typically relies on tabular models that demand heavy feature engineering to handle high-dimensional data and offer limited interpretability, making it difficult for humans to understand predictions. Large Language Models (LLMs), in contrast, can produce human-readable explanations and facilitate feature analysis, potentially reducing the manual workload of fraud analysts and informing system refinements. However, they perform poorly when applied directly to tabular fraud detection due to the difficulty of reasoning over many features, the extreme class imbalance, and the absence of contextual information. To bridge this gap, we introduce FinFRE-RAG, a two-stage approach that applies importance-guided feature reduction to serialize a compact subset of numeric/categorical attributes into natural language and performs retrieval-augmented in-context learning over label-aware, instance-level exemplars. Across four public fraud datasets and three families of open-weight LLMs, FinFRE-RAG substantially improves F1/MCC over direct prompting and is competitive with strong tabular baselines in several settings. Although these LLMs still lag behind specialized classifiers, they narrow the performance gap and provide interpretable rationales, highlighting their value as assistive tools in fraud analysis.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†FinFRE-RAGï¼Œä¸€ç§ç»“åˆç‰¹å¾é™ç»´å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆçš„ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œç”¨äºæå‡å¤§è¯­è¨€æ¨¡å‹åœ¨é‡‘èæ¬ºè¯ˆæ£€æµ‹ä¸­çš„æ€§èƒ½ã€‚é€šè¿‡å°†æ•°å€¼/åˆ†ç±»å±æ€§åºåˆ—åŒ–ä¸ºè‡ªç„¶è¯­è¨€ï¼Œå¹¶åˆ©ç”¨æ ‡ç­¾æ„ŸçŸ¥çš„å®ä¾‹çº§æ ·æœ¬è¿›è¡Œæ£€ç´¢å¢å¼ºçš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†F1/MCCåˆ†æ•°ï¼ŒåŒæ—¶æä¾›äº†å¯è§£é‡Šçš„æ¨ç†ï¼Œå¼¥è¡¥äº†ä¼ ç»Ÿè¡¨æ ¼æ¨¡å‹ä¸LLMsç›´æ¥åº”ç”¨çš„å±€é™æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.12938v1">SPAR: Session-based Pipeline for Adaptive Retrieval on Legacy File Systems</a></td><td><details><summary>å±•å¼€</summary>The ability to extract value from historical data is essential for enterprise decision-making. However, much of this information remains inaccessible within large legacy file systems that lack structured organization and semantic indexing, making retrieval and analysis inefficient and error-prone. We introduce SPAR (Session-based Pipeline for Adaptive Retrieval), a conceptual framework that integrates Large Language Models (LLMs) into a Retrieval-Augmented Generation (RAG) architecture specifically designed for legacy enterprise environments. Unlike conventional RAG pipelines, which require costly construction and maintenance of full-scale vector databases that mirror the entire file system, SPAR employs a lightweight two-stage process: a semantic Metadata Index is first created, after which session-specific vector databases are dynamically generated on demand. This design reduces computational overhead while improving transparency, controllability, and relevance in retrieval. We provide a theoretical complexity analysis comparing SPAR with standard LLM-based RAG pipelines, demonstrating its computational advantages. To validate the framework, we apply SPAR to a synthesized enterprise-scale file system containing a large corpus of biomedical literature, showing improvements in both retrieval effectiveness and downstream model accuracy. Finally, we discuss design trade-offs and outline open challenges for deploying SPAR across diverse enterprise settings.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ä»‹ç»äº†SPARï¼ˆSession-based Pipeline for Adaptive Retrievalï¼‰ï¼Œä¸€ç§ä¸“ä¸ºé—ç•™ä¼ä¸šç¯å¢ƒè®¾è®¡çš„RAGæ¡†æ¶ã€‚SPARé‡‡ç”¨è½»é‡çº§ä¸¤é˜¶æ®µæµç¨‹ï¼ˆå…ˆå»ºç«‹è¯­ä¹‰å…ƒæ•°æ®ç´¢å¼•ï¼Œå†åŠ¨æ€ç”Ÿæˆä¼šè¯ç‰¹å®šå‘é‡æ•°æ®åº“ï¼‰ï¼Œä»¥é™ä½è®¡ç®—å¼€é”€å¹¶æå‡æ£€ç´¢é€æ˜åº¦å’Œç›¸å…³æ€§ï¼ŒåŒæ—¶é€šè¿‡ç†è®ºåˆ†æå’Œç”Ÿç‰©åŒ»å­¦æ–‡çŒ®æ¡ˆä¾‹éªŒè¯å…¶ä¼˜åŠ¿ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-14
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.14762v1">Workflows vs Agents for Code Translation</a></td><td><details><summary>å±•å¼€</summary>Translating algorithms from high-level languages like MATLAB to hardware description languages (HDLs) is a resource-intensive but necessary step for deployment on FPGAs and ASICs. While large language models (LLMs) offer a path to automation, their limited training on HDL code makes end-to-end transpilation brittle and prone to syntax errors. We compare two LLM-driven methods for syntax repair in a MATLAB-to-HDL pipeline: a structured, expert-designed flow that follows a fixed sequence of operations, and a more autonomous agentic approach that uses the Model Context Protocol (MCP) \cite{anthropic2024mcp} to dynamically select its own tools. We study 42 MATLAB signal-processing functions and isolate the syntax-repair stage. Across three model scales, the agentic approach is more effective at resolving initial syntax errors, unblocking a greater number of candidates to proceed through the pipeline. This upstream improvement yields measurable downstream improvements, most notably on mid-sized models, where it increases the simulation reach rate by over 20 percentage points. We hypothesize the gains come from short prompts, aggressive context management, and conditional tool use. Conditional retrieval helps at 8B and 30B; at 235B final-success gains are small and a naive RAG variant attains the highest final success. Our findings suggest that these agentic frameworks, when properly designed, are most effective at compensating for the capacity limits of small and mid-sized models.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†åœ¨MATLABåˆ°HDLçš„è½¬æ¢æµç¨‹ä¸­ï¼Œä½¿ç”¨LLMè¿›è¡Œè¯­æ³•ä¿®å¤çš„ä¸¤ç§æ–¹æ³•ï¼šä¸€ç§ç»“æ„åŒ–æµç¨‹å’Œä¸€ç§åŸºäºModel Context Protocolï¼ˆMCPï¼‰çš„è‡ªä¸»ä»£ç†æ–¹æ³•ã€‚ç ”ç©¶å‘ç°è‡ªä¸»ä»£ç†æ–¹æ³•åœ¨è§£å†³è¯­æ³•é”™è¯¯æ–¹é¢æ›´æœ‰æ•ˆï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­å°å‹æ¨¡å‹ä¸­è¡¨ç°æ˜¾è‘—ï¼Œå¹¶æåˆ°åœ¨æŸäº›æ¨¡å‹è§„æ¨¡ä¸‹RAGå˜ä½“è¡¨ç°æœ€ä½³ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.13632v1">StutterFuse: Mitigating Modality Collapse in Stuttering Detection with Jaccard-Weighted Metric Learning and Gated Fusion</a></td><td><details><summary>å±•å¼€</summary>Stuttering detection breaks down when disfluencies overlap. Existing parametric models struggle to distinguish complex, simultaneous disfluencies (e.g., a 'block' with a 'prolongation') due to the scarcity of these specific combinations in training data. While Retrieval-Augmented Generation (RAG) has revolutionized NLP by grounding models in external knowledge, this paradigm remains unexplored in pathological speech processing. To bridge this gap, we introduce StutterFuse, the first Retrieval-Augmented Classifier (RAC) for multi-label stuttering detection. By conditioning a Conformer encoder on a non-parametric memory bank of clinical examples, we allow the model to classify by reference rather than memorization. We further identify and solve "Modality Collapse", an "Echo Chamber" effect where naive retrieval boosts recall but degrades precision. We mitigate this using: (1) SetCon, a Jaccard-Weighted Metric Learning objective that optimizes for multi-label set similarity, and (2) a Gated Mixture-of-Experts fusion strategy that dynamically arbitrates between acoustic evidence and retrieved context. On the SEP-28k dataset, StutterFuse achieves a weighted F1-score of 0.65, outperforming strong baselines and demonstrating remarkable zero-shot cross-lingual generalization.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†StutterFuseï¼Œé¦–ä¸ªç”¨äºå¤šæ ‡ç­¾å£åƒæ£€æµ‹çš„æ£€ç´¢å¢å¼ºåˆ†ç±»å™¨ï¼ˆRACï¼‰ï¼Œé€šè¿‡ç»“åˆå¤–éƒ¨ä¸´åºŠç¤ºä¾‹çš„éå‚æ•°è®°å¿†åº“å’ŒåŠ¨æ€èåˆç­–ç•¥ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨å¤æ‚é‡å ä¸æµç•…æƒ…å†µä¸‹çš„æ€§èƒ½ç“¶é¢ˆï¼Œå¹¶åˆ›æ–°æ€§åœ°è§£å†³äº†æ£€ç´¢è¿‡ç¨‹ä¸­çš„æ¨¡æ€å¡Œé™·é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†æ£€æµ‹æ•ˆæœå’Œè·¨è¯­è¨€æ³›åŒ–èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.13771v1">Semantic Grounding Index: Geometric Bounds on Context Engagement in RAG Systems</a></td><td><details><summary>å±•å¼€</summary>When retrieval-augmented generation (RAG) systems hallucinate, what geometric trace does this leave in embedding space? We introduce the Semantic Grounding Index (SGI), defined as the ratio of angular distances from the response to the question versus the context on the unit hypersphere $\mathbb{S}^{d-1}$.Our central finding is \emph{semantic laziness}: hallucinated responses remain angularly proximate to questions rather than departing toward retrieved contexts. On HaluEval ($n$=5,000), we observe large effect sizes (Cohen's $d$ ranging from 0.92 to 1.28) across five embedding models with mean cross-model correlation $r$=0.85. Crucially, we derive from the spherical triangle inequality that SGI's discriminative power should increase with question-context angular separation $Î¸(q,c)$-a theoretical prediction confirmed empirically: effect size rises monotonically from $d$=0.61 -low $Î¸(q,c)$, to $d$=1.27 -high $Î¸(q,c)$, with AUC improving from 0.72 to 0.83. Subgroup analysis reveals that SGI excels on long responses ($d$=2.05) and short questions ($d$=1.22), while remaining robust across context lengths. Calibration analysis yields ECE=0.10, indicating SGI scores can serve as probability estimates, not merely rankings. A critical negative result on TruthfulQA (AUC=0.478) establishes that angular geometry measures topical engagement rather than factual accuracy. SGI provides computationally efficient, theoretically grounded infrastructure for identifying responses that warrant verification in production RAG deployments.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œè¯­ä¹‰åŸºç¡€æŒ‡æ•°ï¼ˆSGIï¼‰â€çš„æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–RAGç³»ç»Ÿä¸­ç”Ÿæˆå¹»è§‰åœ¨åµŒå…¥ç©ºé—´çš„å‡ ä½•ç‰¹å¾ï¼Œå‘ç°å¹»è§‰å›ç­”å€¾å‘äºåœ¨è§’åº¦ä¸Šæ¥è¿‘é—®é¢˜è€Œéæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ã€‚é€šè¿‡å®éªŒéªŒè¯äº†SGIçš„åˆ¤åˆ«èƒ½åŠ›åŠå…¶ä¸é—®é¢˜-ä¸Šä¸‹æ–‡è§’åº¦åˆ†ç¦»çš„å…³ç³»ï¼Œå¹¶åˆ†æäº†å…¶åœ¨é•¿å›ç­”å’ŒçŸ­é—®é¢˜ä¸­çš„ä¼˜åŠ¿ï¼ŒåŒæ—¶æŒ‡å‡ºå…¶è¡¡é‡çš„æ˜¯ä¸»é¢˜ç›¸å…³æ€§è€Œéäº‹å®å‡†ç¡®æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.13059v1">An Open and Reproducible Deep Research Agent for Long-Form Question Answering</a></td><td><details><summary>å±•å¼€</summary>We present an open deep research system for long-form question answering, selected as a winning system in the text-to-text track of the MMU-RAG competition at NeurIPS 2025. The system combines an open-source large language model (LLM) with an open web search API to perform iterative retrieval, reasoning, and synthesis in real-world open-domain settings. To enhance reasoning quality, we apply preference tuning based on LLM-as-a-judge feedback that evaluates multiple aspects, including clarity, insightfulness, and factuality. Our experimental results show that the proposed method consistently improves answer quality across all three aspects. Our source code is publicly available at https://github.com/efficient-deep-research/efficient-deep-research.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªç”¨äºé•¿æ ¼å¼é—®ç­”çš„å¼€æ”¾æ·±åº¦ç ”ç©¶ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç»“åˆäº†å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¼€æ”¾ç½‘ç»œæœç´¢APIï¼Œåœ¨å®é™…å¼€æ”¾åŸŸç¯å¢ƒä¸­è¿›è¡Œè¿­ä»£æ£€ç´¢ã€æ¨ç†å’Œåˆæˆã€‚é€šè¿‡åŸºäºLLMåé¦ˆçš„åå¥½è°ƒä¼˜æ¥è¯„ä¼°æ¸…æ™°åº¦ã€æ´å¯ŸåŠ›å’Œäº‹å®æ€§ç­‰å¤šæ–¹é¢ï¼Œä»è€Œæå‡æ¨ç†è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä»¥ä¸Šæ‰€æœ‰æ–¹é¢å‡èƒ½æŒç»­æé«˜ç­”æ¡ˆè´¨é‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.13040v1">Understanding Structured Financial Data with LLMs: A Case Study on Fraud Detection</a></td><td><details><summary>å±•å¼€</summary>Detecting fraud in financial transactions typically relies on tabular models that demand heavy feature engineering to handle high-dimensional data and offer limited interpretability, making it difficult for humans to understand predictions. Large Language Models (LLMs), in contrast, can produce human-readable explanations and facilitate feature analysis, potentially reducing the manual workload of fraud analysts and informing system refinements. However, they perform poorly when applied directly to tabular fraud detection due to the difficulty of reasoning over many features, the extreme class imbalance, and the absence of contextual information. To bridge this gap, we introduce FinFRE-RAG, a two-stage approach that applies importance-guided feature reduction to serialize a compact subset of numeric/categorical attributes into natural language and performs retrieval-augmented in-context learning over label-aware, instance-level exemplars. Across four public fraud datasets and three families of open-weight LLMs, FinFRE-RAG substantially improves F1/MCC over direct prompting and is competitive with strong tabular baselines in several settings. Although these LLMs still lag behind specialized classifiers, they narrow the performance gap and provide interpretable rationales, highlighting their value as assistive tools in fraud analysis.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºFinFRE-RAGçš„ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œé€šè¿‡é‡è¦æ€§å¼•å¯¼çš„ç‰¹å¾ç¼©å‡å°†æ•°å€¼/åˆ†ç±»å±æ€§åºåˆ—åŒ–ä¸ºè‡ªç„¶è¯­è¨€ï¼Œå¹¶åˆ©ç”¨æ£€ç´¢å¢å¼ºçš„ä¸Šä¸‹æ–‡å­¦ä¹ æ¥å¤„ç†æ ‡ç­¾æ„ŸçŸ¥çš„å®ä¾‹çº§æ ·æœ¬ï¼Œä»è€Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é‡‘èæ¬ºè¯ˆæ£€æµ‹ä¸­çš„æ€§èƒ½ï¼ŒåŒæ—¶æä¾›å¯è§£é‡Šçš„æ¨ç†ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.12938v1">SPAR: Session-based Pipeline for Adaptive Retrieval on Legacy File Systems</a></td><td><details><summary>å±•å¼€</summary>The ability to extract value from historical data is essential for enterprise decision-making. However, much of this information remains inaccessible within large legacy file systems that lack structured organization and semantic indexing, making retrieval and analysis inefficient and error-prone. We introduce SPAR (Session-based Pipeline for Adaptive Retrieval), a conceptual framework that integrates Large Language Models (LLMs) into a Retrieval-Augmented Generation (RAG) architecture specifically designed for legacy enterprise environments. Unlike conventional RAG pipelines, which require costly construction and maintenance of full-scale vector databases that mirror the entire file system, SPAR employs a lightweight two-stage process: a semantic Metadata Index is first created, after which session-specific vector databases are dynamically generated on demand. This design reduces computational overhead while improving transparency, controllability, and relevance in retrieval. We provide a theoretical complexity analysis comparing SPAR with standard LLM-based RAG pipelines, demonstrating its computational advantages. To validate the framework, we apply SPAR to a synthesized enterprise-scale file system containing a large corpus of biomedical literature, showing improvements in both retrieval effectiveness and downstream model accuracy. Finally, we discuss design trade-offs and outline open challenges for deploying SPAR across diverse enterprise settings.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSPARçš„æ¡†æ¶ï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é›†æˆåˆ°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¶æ„ä¸­ï¼Œä¸“é—¨é’ˆå¯¹é—ç•™ä¼ä¸šç¯å¢ƒè®¾è®¡ã€‚SPARé‡‡ç”¨è½»é‡çº§çš„ä¸¤é˜¶æ®µæµç¨‹ï¼ˆè¯­ä¹‰å…ƒæ•°æ®ç´¢å¼•åˆ›å»ºå’ŒæŒ‰éœ€åŠ¨æ€ç”Ÿæˆä¼šè¯ç‰¹å®šå‘é‡æ•°æ®åº“ï¼‰ï¼Œç›¸æ¯”ä¼ ç»ŸRAGç®¡é“é™ä½äº†è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶æå‡äº†æ£€ç´¢çš„é€æ˜åº¦ã€å¯æ§æ€§å’Œç›¸å…³æ€§ï¼Œå¹¶åœ¨ç”Ÿç‰©åŒ»å­¦æ–‡çŒ®æ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.12885v1">SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition</a></td><td><details><summary>å±•å¼€</summary>Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºRAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰çš„é›¶æ ·æœ¬é“è·¯æ ‡å¿—è¯†åˆ«æ¡†æ¶ï¼Œé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆæ ‡å¿—çš„æ–‡æœ¬æè¿°ï¼Œä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³å€™é€‰æ ‡å¿—ï¼Œå†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œç»†ç²’åº¦è¯†åˆ«ï¼Œåœ¨ç†æƒ³å’ŒçœŸå®é“è·¯æ•°æ®ä¸Šåˆ†åˆ«è¾¾åˆ°95.58%å’Œ82.45%çš„å‡†ç¡®ç‡ï¼ŒéªŒè¯äº†RAGåœ¨æ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒä¸‹çš„æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.12783v2">Credit Risk Estimation with Non-Financial Features: Evidence from a Synthetic Istanbul Dataset</a></td><td><details><summary>å±•å¼€</summary>Financial exclusion constrains entrepreneurship, increases income volatility, and widens wealth gaps. Underbanked consumers in Istanbul often have no bureau file because their earnings and payments flow through informal channels. To study how such borrowers can be evaluated we create a synthetic dataset of one hundred thousand Istanbul residents that reproduces first quarter 2025 TÃœÄ°K census marginals and telecom usage patterns. Retrieval augmented generation feeds these public statistics into the OpenAI o3 model, which synthesises realistic yet private records. Each profile contains seven socio demographic variables and nine alternative attributes that describe phone specifications, online shopping rhythm, subscription spend, car ownership, monthly rent, and a credit card flag. To test the impact of the alternative financial data CatBoost, LightGBM, and XGBoost are each trained in two versions. Demo models use only the socio demographic variables; Full models include both socio demographic and alternative attributes. Across five fold stratified validation the alternative block raises area under the curve by about one point three percentage and lifts balanced \(F_{1}\) from roughly 0.84 to 0.95, a fourteen percent gain. We contribute an open Istanbul 2025 Q1 synthetic dataset, a fully reproducible modeling pipeline, and empirical evidence that a concise set of behavioural attributes can approach bureau level discrimination power while serving borrowers who lack formal credit records. These findings give lenders and regulators a transparent blueprint for extending fair and safe credit access to the underbanked.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œç»“åˆå…¬å¼€ç»Ÿè®¡æ•°æ®ä¸OpenAIæ¨¡å‹ï¼Œåˆæˆä¼Šæ–¯å¦å¸ƒå°”å±…æ°‘çš„éšç§ä¿æŠ¤åˆæˆæ•°æ®é›†ï¼Œä»¥è¯„ä¼°ç¼ºä¹ä¼ ç»Ÿä¿¡ç”¨è®°å½•çš„å€Ÿæ¬¾äººã€‚ç ”ç©¶å±•ç¤ºäº†åŠ å…¥æ›¿ä»£é‡‘èæ•°æ®ï¼ˆå¦‚ç”µä¿¡ä½¿ç”¨æ¨¡å¼ç­‰ï¼‰å¯æ˜¾è‘—æå‡ä¿¡ç”¨è¯„åˆ†æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½ï¼ˆAUCæå‡1.3%ï¼ŒF1åˆ†æ•°æé«˜14%ï¼‰ï¼Œå¹¶ä¸ºæœåŠ¡æ— é“¶è¡Œè´¦æˆ·ç¾¤ä½“æä¾›äº†é€æ˜å¯å¤ç°çš„è§£å†³æ–¹æ¡ˆã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-13
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.12885v1">SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition</a></td><td><details><summary>å±•å¼€</summary>Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„é›¶æ ·æœ¬é“è·¯æ ‡å¿—è¯†åˆ«æ¡†æ¶ï¼Œé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç”Ÿæˆæ ‡å¿—çš„æ–‡æœ¬æè¿°ï¼Œæ£€ç´¢æœ€ç›¸å…³çš„å€™é€‰æ ‡å¿—ï¼Œå†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œç»†ç²’åº¦è¯†åˆ«ï¼Œåœ¨å®éªŒä¸­è·å¾—é«˜å‡†ç¡®ç‡ï¼Œå±•ç¤ºäº†RAGåœ¨é“è·¯æ ‡å¿—è¯†åˆ«ä¸­çš„æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.12783v2">Credit Risk Estimation with Non-Financial Features: Evidence from a Synthetic Istanbul Dataset</a></td><td><details><summary>å±•å¼€</summary>Financial exclusion constrains entrepreneurship, increases income volatility, and widens wealth gaps. Underbanked consumers in Istanbul often have no bureau file because their earnings and payments flow through informal channels. To study how such borrowers can be evaluated we create a synthetic dataset of one hundred thousand Istanbul residents that reproduces first quarter 2025 TÃœÄ°K census marginals and telecom usage patterns. Retrieval augmented generation feeds these public statistics into the OpenAI o3 model, which synthesises realistic yet private records. Each profile contains seven socio demographic variables and nine alternative attributes that describe phone specifications, online shopping rhythm, subscription spend, car ownership, monthly rent, and a credit card flag. To test the impact of the alternative financial data CatBoost, LightGBM, and XGBoost are each trained in two versions. Demo models use only the socio demographic variables; Full models include both socio demographic and alternative attributes. Across five fold stratified validation the alternative block raises area under the curve by about one point three percentage and lifts balanced \(F_{1}\) from roughly 0.84 to 0.95, a fourteen percent gain. We contribute an open Istanbul 2025 Q1 synthetic dataset, a fully reproducible modeling pipeline, and empirical evidence that a concise set of behavioural attributes can approach bureau level discrimination power while serving borrowers who lack formal credit records. These findings give lenders and regulators a transparent blueprint for extending fair and safe credit access to the underbanked.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡åˆ©ç”¨RAGæŠ€æœ¯å°†å…¬å¼€ç»Ÿè®¡æ•°æ®è¾“å…¥OpenAIæ¨¡å‹ä»¥ç”Ÿæˆåˆæˆæ•°æ®é›†ï¼Œç ”ç©¶å¦‚ä½•é€šè¿‡æ›¿ä»£é‡‘èæ•°æ®è¯„ä¼°æ— æ­£å¼ä¿¡ç”¨è®°å½•çš„å€Ÿæ¬¾äººï¼Œå¹¶å±•ç¤ºäº†è¿™äº›æ•°æ®å¯¹æå‡ä¿¡ç”¨è¯„åˆ†æ¨¡å‹æ€§èƒ½çš„æ˜¾è‘—æ•ˆæœã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.12458v1">Breaking the Curse of Dimensionality: On the Stability of Modern Vector Retrieval</a></td><td><details><summary>å±•å¼€</summary>Modern vector databases enable efficient retrieval over high-dimensional neural embeddings, powering applications from web search to retrieval-augmented generation. However, classical theory predicts such tasks should suffer from the curse of dimensionality, where distances between points become nearly indistinguishable, thereby crippling efficient nearest-neighbor search. We revisit this paradox through the lens of stability, the property that small perturbations to a query do not radically alter its nearest neighbors. Building on foundational results, we extend stability theory to three key retrieval settings widely used in practice: (i) multi-vector search, where we prove that the popular Chamfer distance metric preserves single-vector stability, while average pooling aggregation may destroy it; (ii) filtered vector search, where we show that sufficiently large penalties for mismatched filters can induce stability even when the underlying search is unstable; and (iii) sparse vector search, where we formalize and prove novel sufficient stability conditions. Across synthetic and real datasets, our experimental results match our theoretical predictions, offering concrete guidance for model and system design to avoid the curse of dimensionality.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ç°ä»£å‘é‡æ•°æ®åº“åœ¨é«˜ç»´ç¥ç»åµŒå…¥æ£€ç´¢ä¸­çš„ç¨³å®šæ€§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­‰åº”ç”¨åœºæ™¯ä¸­ã€‚ä½œè€…é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼Œç ”ç©¶äº†å¤šå‘é‡æœç´¢ã€è¿‡æ»¤å‘é‡æœç´¢å’Œç¨€ç–å‘é‡æœç´¢ä¸‰ç§å¸¸è§æ£€ç´¢è®¾ç½®çš„ç¨³å®šæ€§æ¡ä»¶ï¼Œå¹¶æå‡ºäº†é¿å…ç»´åº¦ç¾éš¾çš„å…·ä½“è®¾è®¡å»ºè®®ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.13725v2">Compressed Causal Reasoning: Quantization and GraphRAG Effects on Interventional and Counterfactual Accuracy</a></td><td><details><summary>å±•å¼€</summary>Causal reasoning in Large Language Models spanning association, intervention, and counterfactual inference is essential for reliable decision making in high stakes settings. As deployment shifts toward edge and resource constrained environments, quantized models such as INT8 and NF4 are becoming standard. Yet the impact of precision reduction on formal causal reasoning is poorly understood. To our knowledge, this is the first study to systematically evaluate quantization effects across all three levels of Pearls Causal Ladder. Using a 3000 sample stratified CLadder benchmark, we find that rung level accuracy in Llama 3 8B remains broadly stable under quantization, with NF4 showing less than one percent overall degradation. Interventional queries at rung 2 are the most sensitive to precision loss, whereas counterfactual reasoning at rung 3 is comparatively stable but exhibits heterogeneous weaknesses across query types such as collider bias and backdoor adjustment. Experiments on the CRASS benchmark show near identical performance across precisions, indicating that existing commonsense counterfactual datasets lack the structural sensitivity needed to reveal quantization induced reasoning drift. We further evaluate Graph Retrieval Augmented Generation using ground truth causal graphs and observe a consistent improvement in NF4 interventional accuracy of plus 1.7 percent, partially offsetting compression related degradation. These results suggest that causal reasoning is unexpectedly robust to four bit quantization, graph structured augmentation can selectively reinforce interventional reasoning, and current counterfactual benchmarks fail to capture deeper causal brittleness. This work provides an initial empirical map of compressed causal reasoning and practical guidance for deploying efficient and structurally supported causal AI systems.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é‡åŒ–ï¼ˆå¦‚INT8å’ŒNF4ï¼‰æ¡ä»¶ä¸‹è¿›è¡Œå› æœæ¨ç†ï¼ˆå…³è”ã€å¹²é¢„å’Œåäº‹å®æ¨æ–­ï¼‰çš„ç¨³å¥æ€§ï¼Œå¹¶å‘ç°å› æœæ¨ç†å¯¹4æ¯”ç‰¹é‡åŒ–è¡¨ç°å‡ºæ„å¤–çš„é«˜é²æ£’æ€§ã€‚æ–‡ç« é¦–æ¬¡ç³»ç»Ÿè¯„ä¼°äº†é‡åŒ–å¯¹Pearlå› æœé˜¶æ¢¯ä¸‰ä¸ªå±‚çº§çš„å½±å“ï¼Œä½¿ç”¨CLadderå’ŒCRASSåŸºå‡†æµ‹è¯•è¡¨æ˜å¹²é¢„æ€§æŸ¥è¯¢å¯¹ç²¾åº¦æŸå¤±æœ€æ•æ„Ÿï¼Œè€Œåäº‹å®æ¨ç†ç›¸å¯¹ç¨³å®šã€‚ç ”ç©¶è¿˜å‘ç°ï¼ŒåŸºäºçœŸå®å› æœå›¾çš„å›¾æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆGraph RAGï¼‰èƒ½éƒ¨åˆ†æŠµæ¶ˆé‡åŒ–å¸¦æ¥çš„æ€§èƒ½ä¸‹é™ï¼Œå°¤å…¶åœ¨å¹²é¢„å‡†ç¡®æ€§ä¸Šæå‡äº†1.7%ï¼Œä»è€Œä¸ºé«˜æ•ˆã€ç»“æ„æ”¯æŒçš„å› æœAIç³»ç»Ÿéƒ¨ç½²æä¾›äº†å®è·µæŒ‡å¯¼ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-12
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.12458v1">Breaking the Curse of Dimensionality: On the Stability of Modern Vector Retrieval</a></td><td><details><summary>å±•å¼€</summary>Modern vector databases enable efficient retrieval over high-dimensional neural embeddings, powering applications from web search to retrieval-augmented generation. However, classical theory predicts such tasks should suffer from the curse of dimensionality, where distances between points become nearly indistinguishable, thereby crippling efficient nearest-neighbor search. We revisit this paradox through the lens of stability, the property that small perturbations to a query do not radically alter its nearest neighbors. Building on foundational results, we extend stability theory to three key retrieval settings widely used in practice: (i) multi-vector search, where we prove that the popular Chamfer distance metric preserves single-vector stability, while average pooling aggregation may destroy it; (ii) filtered vector search, where we show that sufficiently large penalties for mismatched filters can induce stability even when the underlying search is unstable; and (iii) sparse vector search, where we formalize and prove novel sufficient stability conditions. Across synthetic and real datasets, our experimental results match our theoretical predictions, offering concrete guidance for model and system design to avoid the curse of dimensionality.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ç°ä»£å‘é‡æ•°æ®åº“åœ¨é«˜æ•ˆæ£€ç´¢é«˜ç»´ç¥ç»åµŒå…¥ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­‰ä»»åŠ¡ä¸­ã€‚ä½œè€…é€šè¿‡ç¨³å®šæ€§ç†è®ºé‡æ–°å®¡è§†äº†â€œç»´åº¦ç¾éš¾â€é—®é¢˜ï¼Œå¹¶æ‰©å±•äº†ç†è®ºåˆ°ä¸‰ç§å®é™…æ£€ç´¢åœºæ™¯ï¼ˆå¤šå‘é‡æœç´¢ã€è¿‡æ»¤å‘é‡æœç´¢å’Œç¨€ç–å‘é‡æœç´¢ï¼‰ï¼ŒéªŒè¯äº†ç†è®ºé¢„æµ‹ä¸å®éªŒç»“æœçš„ä¸€è‡´æ€§ï¼Œä¸ºæ¨¡å‹å’Œç³»ç»Ÿè®¾è®¡æä¾›äº†é¿å…ç»´åº¦ç¾éš¾çš„æŒ‡å¯¼ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.13725v2">Compressed Causal Reasoning: Quantization and GraphRAG Effects on Interventional and Counterfactual Accuracy</a></td><td><details><summary>å±•å¼€</summary>Causal reasoning in Large Language Models spanning association, intervention, and counterfactual inference is essential for reliable decision making in high stakes settings. As deployment shifts toward edge and resource constrained environments, quantized models such as INT8 and NF4 are becoming standard. Yet the impact of precision reduction on formal causal reasoning is poorly understood. To our knowledge, this is the first study to systematically evaluate quantization effects across all three levels of Pearls Causal Ladder. Using a 3000 sample stratified CLadder benchmark, we find that rung level accuracy in Llama 3 8B remains broadly stable under quantization, with NF4 showing less than one percent overall degradation. Interventional queries at rung 2 are the most sensitive to precision loss, whereas counterfactual reasoning at rung 3 is comparatively stable but exhibits heterogeneous weaknesses across query types such as collider bias and backdoor adjustment. Experiments on the CRASS benchmark show near identical performance across precisions, indicating that existing commonsense counterfactual datasets lack the structural sensitivity needed to reveal quantization induced reasoning drift. We further evaluate Graph Retrieval Augmented Generation using ground truth causal graphs and observe a consistent improvement in NF4 interventional accuracy of plus 1.7 percent, partially offsetting compression related degradation. These results suggest that causal reasoning is unexpectedly robust to four bit quantization, graph structured augmentation can selectively reinforce interventional reasoning, and current counterfactual benchmarks fail to capture deeper causal brittleness. This work provides an initial empirical map of compressed causal reasoning and practical guidance for deploying efficient and structurally supported causal AI systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä¸»è¦ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é‡åŒ–ï¼ˆå¦‚INT8å’ŒNF4ï¼‰åå¯¹å› æœæ¨ç†ï¼ˆå…³è”ã€å¹²é¢„å’Œåäº‹å®æ¨æ–­ï¼‰èƒ½åŠ›çš„å½±å“ï¼Œå‘ç°å› æœæ¨ç†å¯¹4æ¯”ç‰¹é‡åŒ–è¡¨ç°å‡ºæ„å¤–çš„é²æ£’æ€§ï¼Œå¹¶é¦–æ¬¡ç³»ç»Ÿè¯„ä¼°äº†é‡åŒ–åœ¨Pearlå› æœé˜¶æ¢¯ä¸‰ä¸ªå±‚çº§ä¸Šçš„æ•ˆæœã€‚è®ºæ–‡è¿˜æ¢è®¨äº†å›¾æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆGraph RAGï¼‰æŠ€æœ¯ï¼Œé€šè¿‡å¼•å…¥çœŸå®å› æœå›¾è¿›è¡Œå¢å¼ºï¼Œä½¿NF4é‡åŒ–æ¨¡å‹åœ¨å¹²é¢„æ€§æŸ¥è¯¢ä¸­çš„å‡†ç¡®ç‡æå‡äº†1.7%ï¼Œéƒ¨åˆ†æŠµæ¶ˆäº†é‡åŒ–å¸¦æ¥çš„æ€§èƒ½ä¸‹é™ï¼Œè¡¨æ˜ç»“æ„åŒ–æ£€ç´¢å¢å¼ºèƒ½é’ˆå¯¹æ€§å¼ºåŒ–å› æœæ¨ç†èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.12084v1">FloodSQL-Bench: A Retrieval-Augmented Benchmark for Geospatially-Grounded Text-to-SQL</a></td><td><details><summary>å±•å¼€</summary>Existing Text-to-SQL benchmarks primarily focus on single-table queries or limited joins in general-purpose domains, and thus fail to reflect the complexity of domain-specific, multi-table and geospatial reasoning, To address this limitation, we introduce FLOODSQL-BENCH, a geospatially grounded benchmark for the flood management domain that integrates heterogeneous datasets through key-based, spatial, and hybrid joins. The benchmark captures realistic flood-related information needs by combining social, infrastructural, and hazard data layers. We systematically evaluate recent large language models with the same retrieval-augmented generation settings and measure their performance across difficulty tiers. By providing a unified, open benchmark grounded in real-world disaster management data, FLOODSQL-BENCH establishes a practical testbed for advancing Text-to-SQL research in high-stakes application domains.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ä»‹ç»äº†FLOODSQL-BENCHï¼Œä¸€ä¸ªé’ˆå¯¹æ´ªæ°´ç®¡ç†é¢†åŸŸçš„åœ°ç†ç©ºé—´åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡æ•´åˆå¤šæºå¼‚æ„æ•°æ®å’Œå¤æ‚æŸ¥è¯¢ï¼ˆå¦‚ç©ºé—´è¿æ¥ï¼‰æ¥è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬åˆ°SQLä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¹¶é‡‡ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è®¾ç½®ç»Ÿä¸€æµ‹è¯•æ¨¡å‹è¡¨ç°ï¼Œæ—¨åœ¨æ¨åŠ¨é«˜é£é™©é¢†åŸŸText-to-SQLç ”ç©¶çš„å‘å±•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.11997v1">Log Anomaly Detection with Large Language Models via Knowledge-Enriched Fusion</a></td><td><details><summary>å±•å¼€</summary>System logs are a critical resource for monitoring and managing distributed systems, providing insights into failures and anomalous behavior. Traditional log analysis techniques, including template-based and sequence-driven approaches, often lose important semantic information or struggle with ambiguous log patterns. To address this, we present EnrichLog, a training-free, entry-based anomaly detection framework that enriches raw log entries with both corpus-specific and sample-specific knowledge. EnrichLog incorporates contextual information, including historical examples and reasoning derived from the corpus, to enable more accurate and interpretable anomaly detection. The framework leverages retrieval-augmented generation to integrate relevant contextual knowledge without requiring retraining. We evaluate EnrichLog on four large-scale system log benchmark datasets and compare it against five baseline methods. Our results show that EnrichLog consistently improves anomaly detection performance, effectively handles ambiguous log entries, and maintains efficient inference. Furthermore, incorporating both corpus- and sample-specific knowledge enhances model confidence and detection accuracy, making EnrichLog well-suited for practical deployments.</details></td><td><details><summary>å±•å¼€</summary>  </details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.14744v1">VERAFI: Verified Agentic Financial Intelligence through Neurosymbolic Policy Generation</a></td><td><details><summary>å±•å¼€</summary>Financial AI systems suffer from a critical blind spot: while Retrieval-Augmented Generation (RAG) excels at finding relevant documents, language models still generate calculation errors and regulatory violations during reasoning, even with perfect retrieval. This paper introduces VERAFI (Verified Agentic Financial Intelligence), an agentic framework with neurosymbolic policy generation for verified financial intelligence. VERAFI combines state-of-the-art dense retrieval and cross-encoder reranking with financial tool-enabled agents and automated reasoning policies covering GAAP compliance, SEC requirements, and mathematical validation. Our comprehensive evaluation on FinanceBench demonstrates remarkable improvements: while traditional dense retrieval with reranking achieves only 52.4\% factual correctness, VERAFI's integrated approach reaches 94.7\%, an 81\% relative improvement. The neurosymbolic policy layer alone contributes a 4.3 percentage point gain over pure agentic processing, specifically targeting persistent mathematical and logical errors. By integrating financial domain expertise directly into the reasoning process, VERAFI offers a practical pathway toward trustworthy financial AI that meets the stringent accuracy demands of regulatory compliance, investment decisions, and risk management.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºVERAFIæ¡†æ¶ï¼Œé’ˆå¯¹é‡‘èAIä¸­RAGæŠ€æœ¯æ£€ç´¢å‡†ç¡®ä½†ç”Ÿæˆä»å­˜åœ¨è®¡ç®—ä¸åˆè§„é”™è¯¯çš„é—®é¢˜ï¼Œé€šè¿‡ç»“åˆç¥ç»ç¬¦å·ç­–ç•¥ã€é‡‘èå·¥å…·ä»£ç†å’Œè‡ªåŠ¨æ¨ç†ç­–ç•¥ï¼Œæ˜¾è‘—æå‡ç”Ÿæˆç»“æœçš„å‡†ç¡®æ€§ä¸åˆè§„æ€§ï¼ˆä»52.4%æå‡è‡³94.7%ï¼‰ï¼Œå°¤å…¶è§£å†³äº†æ•°å­¦éªŒè¯ä¸ç›‘ç®¡åˆè§„ï¼ˆå¦‚GAAPã€SECï¼‰ç­‰æ ¸å¿ƒéš¾é¢˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.11724v2">From Signal to Turn: Interactional Friction in Modular Speech-to-Speech Pipelines</a></td><td><details><summary>å±•å¼€</summary>While voice-based AI systems have achieved remarkable generative capabilities, their interactions often feel conversationally broken. This paper examines the interactional friction that emerges in modular Speech-to-Speech Retrieval-Augmented Generation (S2S-RAG) pipelines. By analyzing a representative production system, we move beyond simple latency metrics to identify three recurring patterns of conversational breakdown: (1) Temporal Misalignment, where system delays violate user expectations of conversational rhythm; (2) Expressive Flattening, where the loss of paralinguistic cues leads to literal, inappropriate responses; and (3) Repair Rigidity, where architectural gating prevents users from correcting errors in real-time. Through system-level analysis, we demonstrate that these friction points should not be understood as defects or failures, but as structural consequences of a modular design that prioritizes control over fluidity. We conclude that building natural spoken AI is an infrastructure design challenge, requiring a shift from optimizing isolated components to carefully choreographing the seams between them.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åŸºäºè¯­éŸ³çš„S2S-RAGï¼ˆè¯­éŸ³åˆ°è¯­éŸ³æ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç³»ç»Ÿä¸­å‡ºç°çš„äº¤äº’æ‘©æ“¦é—®é¢˜ï¼Œåˆ†æäº†ä¸‰ç§å…¸å‹å¯¹è¯å´©æºƒæ¨¡å¼ï¼ˆæ—¶é—´é”™ä½ã€è¡¨è¾¾æ‰å¹³åŒ–å’Œä¿®å¤åƒµåŒ–ï¼‰ï¼ŒæŒ‡å‡ºè¿™äº›é—®é¢˜æ˜¯æ¨¡å—åŒ–è®¾è®¡ä¼˜å…ˆæ§åˆ¶è€Œéæµç•…æ€§çš„ç»“æ„æ€§ç»“æœï¼Œå¹¶å¼ºè°ƒæ„å»ºè‡ªç„¶è¯­éŸ³AIéœ€ä»ä¼˜åŒ–ç‹¬ç«‹ç»„ä»¶è½¬å‘åè°ƒæ¨¡å—é—´è¡”æ¥çš„åŸºç¡€è®¾æ–½è®¾è®¡æŒ‘æˆ˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.11682v1">MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition</a></td><td><details><summary>å±•å¼€</summary>Therapeutic decision-making in clinical medicine constitutes a high-stakes domain in which AI guidance interacts with complex interactions among patient characteristics, disease processes, and pharmacological agents. Tasks such as drug recommendation, treatment planning, and adverse-effect prediction demand robust, multi-step reasoning grounded in reliable biomedical knowledge. Agentic AI methods, exemplified by TxAgent, address these challenges through iterative retrieval-augmented generation (RAG). TxAgent employs a fine-tuned Llama-3.1-8B model that dynamically generates and executes function calls to a unified biomedical tool suite (ToolUniverse), integrating FDA Drug API, OpenTargets, and Monarch resources to ensure access to current therapeutic information. In contrast to general-purpose RAG systems, medical applications impose stringent safety constraints, rendering the accuracy of both the reasoning trace and the sequence of tool invocations critical. These considerations motivate evaluation protocols treating token-level reasoning and tool-usage behaviors as explicit supervision signals. This work presents insights derived from our participation in the CURE-Bench NeurIPS 2025 Challenge, which benchmarks therapeutic-reasoning systems using metrics that assess correctness, tool utilization, and reasoning quality. We analyze how retrieval quality for function (tool) calls influences overall model performance and demonstrate performance gains achieved through improved tool-retrieval strategies. Our work was awarded the Excellence Award in Open Science. Complete information can be found at https://curebench.ai/.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTxAgentçš„åŒ»ç–—AIä»£ç†ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé€šè¿‡è¿­ä»£å¼çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œæ•´åˆäº†ç”Ÿç‰©åŒ»å­¦å·¥å…·å¥—ä»¶ï¼ˆå¦‚FDA Drug APIç­‰ï¼‰ï¼Œä»¥æ”¯æŒä¸´åºŠæ²»ç–—å†³ç­–ï¼ˆå¦‚è¯ç‰©æ¨èã€æ²»ç–—æ–¹æ¡ˆåˆ¶å®šç­‰ï¼‰ã€‚æ–‡ç« é‡ç‚¹æ¢è®¨äº†åŒ»ç–—é¢†åŸŸRAGç³»ç»Ÿåœ¨å·¥å…·è°ƒç”¨å‡†ç¡®æ€§å’Œæ¨ç†è¿½è¸ªæ–¹é¢çš„ä¸¥æ ¼å®‰å…¨è¦æ±‚ï¼Œå¹¶åŸºäºCURE-Benchç«èµ›æ•°æ®åˆ†æäº†å·¥å…·æ£€ç´¢è´¨é‡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œæœ€ç»ˆæå‡ºä¼˜åŒ–ç­–ç•¥å–å¾—äº†æ€§èƒ½æå‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.11614v1">Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) models rely on retrieved evidence to guide large language model (LLM) generators, yet current systems treat retrieval as a weak heuristic rather than verifiable evidence. As a result, LLMs answer without support, hallucinate under incomplete or misleading context, and rely on spurious evidence. We introduce a training framework that treats the entire RAG pipeline -- both the retriever and the generator -- as an interactive proof system via an adaptation of the Merlin-Arthur (M/A) protocol. Arthur (the generator LLM) trains on questions of unkown provenance: Merlin provides helpful evidence, while Morgana injects adversarial, misleading context. Both use a linear-time XAI method to identify and modify the evidence most influential to Arthur. Consequently, Arthur learns to (i) answer when the context support the answer, (ii) reject when evidence is insufficient, and (iii) rely on the specific context spans that truly ground the answer. We further introduce a rigorous evaluation framework to disentangle explanation fidelity from baseline predictive errors. This allows us to introduce and measure the Explained Information Fraction (EIF), which normalizes M/A certified mutual-information guarantees relative to model capacity and imperfect benchmarks. Across three RAG datasets and two model families of varying sizes, M/A-trained LLMs show improved groundedness, completeness, soundness, and reject behavior, as well as reduced hallucinations -- without needing manually annotated unanswerable questions. The retriever likewise improves recall and MRR through automatically generated M/A hard positives and negatives. Our results demonstrate that autonomous interactive-proof-style supervision provides a principled and practical path toward reliable RAG systems that treat retrieved documents not as suggestions, but as verifiable evidence.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºMerlin-Arthuräº¤äº’è¯æ˜åè®®çš„RAGè®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡è®©ç”Ÿæˆå™¨æ¨¡å‹ï¼ˆArthurï¼‰åœ¨å¯¹æŠ—æ€§è¯æ®ï¼ˆMorganaæä¾›ï¼‰å’Œæœ‰ç›Šè¯æ®ï¼ˆMerlinæä¾›ï¼‰çš„äº¤äº’ä¸­å­¦ä¹ ï¼Œä½¿å…¶èƒ½å¤ŸåŒºåˆ†å¯é è¯æ®ã€æ‹’ç»ä¸å……åˆ†ä¿¡æ¯å¹¶å‡å°‘å¹»è§‰ã€‚åŒæ—¶å¼•å…¥EIFæŒ‡æ ‡é‡åŒ–è§£é‡Šå¯ä¿¡åº¦ï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•èƒ½æ˜¾è‘—æå‡RAGç³»ç»Ÿçš„å›ç­”å¯é æ€§ã€æ£€ç´¢æ•ˆæœåŠæŠ—å¹²æ‰°èƒ½åŠ›ï¼Œæ— éœ€äººå·¥æ ‡æ³¨æ•°æ®ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.11509v1">Does Less Hallucination Mean Less Creativity? An Empirical Investigation in LLMs</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) exhibit remarkable capabilities in natural language understanding and reasoning, but suffer from hallucination: the generation of factually incorrect content. While numerous methods have been developed to reduce hallucinations, their impact on creative generations remains unexplored. This gap is particularly critical for AI-assisted scientific discovery, which requires both factual accuracy and creative hypothesis generation. We investigate how three hallucination-reduction techniques: Chain of Verification (CoVe), Decoding by Contrasting Layers (DoLa), and Retrieval-Augmented Generation (RAG), affect creativity in LLMs. Evaluating multiple model families (LLaMA, Qwen, Mistral) at varying scales (1B - 70B parameters) on two creativity benchmarks (NeoCoder and CS4), we find that these methods have opposing effects on divergent creativity. CoVe enhances divergent thinking, DoLa suppresses it, and RAG shows minimal impact. Our findings provide guidance for selecting appropriate hallucination-reduction methods in scientific applications, where the balance between factual accuracy and creative exploration is crucial.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†ä¸‰ç§å‡å°‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¹»è§‰çš„æŠ€æœ¯ï¼ˆåŒ…æ‹¬RAGï¼‰å¯¹æ¨¡å‹åˆ›é€ åŠ›çš„å½±å“ï¼Œå‘ç°å®ƒä»¬åœ¨å‘æ•£æ€§æ€ç»´ä¸Šè¡¨ç°ä¸åŒï¼ˆRAGå½±å“æœ€å°ï¼‰ï¼Œä¸ºç§‘å­¦åº”ç”¨ä¸­å¹³è¡¡äº‹å®å‡†ç¡®æ€§ä¸åˆ›é€ æ€§æä¾›äº†æ–¹æ³•é€‰æ‹©ä¾æ®ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.11506v2">EmeraldMind: A Knowledge Graph-Augmented Framework for Greenwashing Detection</a></td><td><details><summary>å±•å¼€</summary>As AI and web agents become pervasive in decision-making, it is critical to design intelligent systems that not only support sustainability efforts but also guard against misinformation. Greenwashing, i.e., misleading corporate sustainability claims, poses a major challenge to environmental progress. To address this challenge, we introduce EmeraldMind, a fact-centric framework integrating a domain-specific knowledge graph with retrieval-augmented generation to automate greenwashing detection. EmeraldMind builds the EmeraldGraph from diverse corporate ESG (environmental, social, and governance) reports, surfacing verifiable evidence, often missing in generic knowledge bases, and supporting large language models in claim assessment. The framework delivers justification-centric classifications, presenting transparent, evidence-backed verdicts and abstaining responsibly when claims cannot be verified. Experiments on a new greenwashing claims dataset demonstrate that EmeraldMind achieves competitive accuracy, greater coverage, and superior explanation quality compared to generic LLMs, without the need for fine-tuning or retraining.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†EmeraldMindæ¡†æ¶ï¼Œå®ƒç»“åˆäº†é¢†åŸŸç‰¹å®šçŸ¥è¯†å›¾è°±å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œç”¨äºè‡ªåŠ¨åŒ–æ£€æµ‹ä¼ä¸šç»¿è‰²æ´—æ¶¤è¡Œä¸ºï¼ˆgreenwashingï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡ä»ä¼ä¸šESGæŠ¥å‘Šä¸­æ„å»ºçŸ¥è¯†å›¾è°±ï¼ˆEmeraldGraphï¼‰ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æä¾›å¯éªŒè¯çš„è¯æ®ï¼Œä»è€Œæé«˜å¯¹å¯æŒç»­å‘å±•å£°æ˜çš„è¯„ä¼°å‡†ç¡®æ€§ã€è¦†ç›–èŒƒå›´å’Œè§£é‡Šè´¨é‡ï¼ŒåŒæ—¶ç”Ÿæˆé€æ˜ä¸”åŸºäºè¯æ®çš„ç»“è®ºã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.15766v1">LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models</a></td><td><details><summary>å±•å¼€</summary>Loop transformations are semantics-preserving optimization techniques, widely used to maximize objectives such as parallelism. Despite decades of research, applying the optimal composition of loop transformations remains challenging due to inherent complexities, including cost modeling for optimization objectives. Recent studies have explored the potential of Large Language Models (LLMs) for code optimization. However, our key observation is that LLMs often struggle with effective loop transformation optimization, frequently leading to errors or suboptimal optimization, thereby missing opportunities for performance improvements. To bridge this gap, we propose LOOPRAG, a novel retrieval-augmented generation framework designed to guide LLMs in performing effective loop optimization on Static Control Part. We introduce a parameter-driven method to harness loop properties, which trigger various loop transformations, and generate diverse yet legal example codes serving as a demonstration source. To effectively obtain the most informative demonstrations, we propose a loop-aware algorithm based on loop features, which balances similarity and diversity for code retrieval. To enhance correct and efficient code generation, we introduce a feedback-based iterative mechanism that incorporates compilation, testing and performance results as feedback to guide LLMs. Each optimized code undergoes mutation, coverage and differential testing for equivalence checking. We evaluate LOOPRAG on PolyBench, TSVC and LORE benchmark suites, and compare it against compilers (GCC-Graphite, Clang-Polly, Perspective and ICX) and representative LLMs (DeepSeek and GPT-4). The results demonstrate average speedups over base compilers of up to 11.20$\times$, 14.34$\times$, and 9.29$\times$ for PolyBench, TSVC, and LORE, respectively, and speedups over base LLMs of up to 11.97$\times$, 5.61$\times$, and 11.59$\times$.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†LOOPRAGï¼Œä¸€ç§åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæœ‰æ•ˆçš„å¾ªç¯ä¼˜åŒ–ã€‚é€šè¿‡ç»“åˆå‚æ•°é©±åŠ¨çš„æ–¹æ³•ã€å¾ªç¯æ„ŸçŸ¥çš„æ£€ç´¢ç®—æ³•å’Œåé¦ˆè¿­ä»£æœºåˆ¶ï¼ŒLOOPRAGèƒ½å¤Ÿç”Ÿæˆå¤šæ ·ä¸”åˆæ³•çš„ä»£ç ç¤ºä¾‹ï¼Œå¹¶é€šè¿‡ç¼–è¯‘ã€æµ‹è¯•å’Œæ€§èƒ½åé¦ˆä¼˜åŒ–ç»“æœã€‚å®éªŒè¡¨æ˜ï¼ŒLOOPRAGåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿç¼–è¯‘å™¨å’ŒLLMsï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-11
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.10787v1">Replace, Don't Expand: Mitigating Context Dilution in Multi-Hop RAG via Fixed-Budget Evidence Assembly</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) systems often fail on multi-hop queries when the initial retrieval misses a bridge fact. Prior corrective approaches, such as Self-RAG, CRAG, and Adaptive-$k$, typically address this by \textit{adding} more context or pruning existing lists. However, simply expanding the context window often leads to \textbf{context dilution}, where distractors crowd out relevant information. We propose \textbf{SEAL-RAG}, a training-free controller that adopts a \textbf{``replace, don't expand''} strategy to fight context dilution under a fixed retrieval depth $k$. SEAL executes a (\textbf{S}earch $\rightarrow$ \textbf{E}xtract $\rightarrow$ \textbf{A}ssess $\rightarrow$ \textbf{L}oop) cycle: it performs on-the-fly, entity-anchored extraction to build a live \textit{gap specification} (missing entities/relations), triggers targeted micro-queries, and uses \textit{entity-first ranking} to actively swap out distractors for gap-closing evidence. We evaluate SEAL-RAG against faithful re-implementations of Basic RAG, CRAG, Self-RAG, and Adaptive-$k$ in a shared environment on \textbf{HotpotQA} and \textbf{2WikiMultiHopQA}. On HotpotQA ($k=3$), SEAL improves answer correctness by \textbf{+3--13 pp} and evidence precision by \textbf{+12--18 pp} over Self-RAG. On 2WikiMultiHopQA ($k=5$), it outperforms Adaptive-$k$ by \textbf{+8.0 pp} in accuracy and maintains \textbf{96\%} evidence precision compared to 22\% for CRAG. These gains are statistically significant ($p<0.001$). By enforcing fixed-$k$ replacement, SEAL yields a predictable cost profile while ensuring the top-$k$ slots are optimized for precision rather than mere breadth. We release our code and data at https://github.com/mosherino/SEAL-RAG.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSEAL-RAGçš„æ–°å‹è®­ç»ƒæ— å…³æ§åˆ¶å™¨ï¼Œæ—¨åœ¨è§£å†³å¤šè·³æŸ¥è¯¢ä¸­åˆå§‹æ£€ç´¢é—æ¼å…³é”®ä¿¡æ¯ï¼ˆæ¡¥æ¥äº‹å®ï¼‰å¯¼è‡´çš„RAGç³»ç»Ÿå¤±æ•ˆé—®é¢˜ã€‚SEAL-RAGé€šè¿‡â€œæ›¿æ¢è€Œéæ‰©å±•â€ç­–ç•¥ï¼ˆæ‰§è¡Œæœç´¢â†’æå–â†’è¯„ä¼°â†’å¾ªç¯çš„æµç¨‹ï¼‰ï¼Œåœ¨å›ºå®šæ£€ç´¢æ·±åº¦$k$ä¸‹åŠ¨æ€è¯†åˆ«å¹¶å¡«è¡¥ç¼ºå¤±çš„å®ä½“/å…³ç³»ï¼ˆé—´éš™è§„èŒƒï¼‰ï¼Œåˆ©ç”¨å®ä½“ä¼˜å…ˆæ’åä¸»åŠ¨æ›¿æ¢å¹²æ‰°ä¿¡æ¯ï¼Œä»è€Œæå‡è¯æ®ç²¾åº¦å’Œå›ç­”æ­£ç¡®æ€§ã€‚å®éªŒæ˜¾ç¤ºï¼Œå…¶åœ¨HotpotQAå’Œ2WikiMultiHopQAæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ˆå¦‚Self-RAGã€Adaptive-$k$ï¼‰ï¼Œå°¤å…¶åœ¨è¯æ®ç²¾åº¦å’Œé¢„æµ‹æˆæœ¬å¯æ§æ€§æ–¹é¢è¡¨ç°çªå‡ºã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.10435v1">Semantic Reconstruction of Adversarial Plagiarism: A Context-Aware Framework for Detecting and Restoring "Tortured Phrases" in Scientific Literature</a></td><td><details><summary>å±•å¼€</summary>The integrity and reliability of scientific literature is facing a serious threat by adversarial text generation techniques, specifically from the use of automated paraphrasing tools to mask plagiarism. These tools generate "tortured phrases", statistically improbable synonyms (e.g. "counterfeit consciousness" for "artificial intelligence"), that preserve the local grammar while obscuring the original source. Most existing detection methods depend heavily on static blocklists or general-domain language models, which suffer from high false-negative rates for novel obfuscations and cannot determine the source of the plagiarized content. In this paper, we propose Semantic Reconstruction of Adversarial Plagiarism (SRAP), a framework designed not only to detect these anomalies but to mathematically recover the original terminology. We use a two-stage architecture: (1) statistical anomaly detection with a domain-specific masked language model (SciBERT) using token-level pseudo-perplexity, and (2) source-based semantic reconstruction using dense vector retrieval (FAISS) and sentence-level alignment (SBERT). Experiments on a parallel corpus of adversarial scientific text show that while zero-shot baselines fail completely (0.00 percent restoration accuracy), our retrieval-augmented approach achieves 23.67 percent restoration accuracy, significantly outperforming baseline methods. We also show that static decision boundaries are necessary for robust detection in jargon-heavy scientific text, since dynamic thresholding fails under high variance. SRAP enables forensic analysis by linking obfuscated expressions back to their most probable source documents.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†SRAPæ¡†æ¶ï¼Œé’ˆå¯¹ç§‘å­¦æ–‡çŒ®ä¸­çš„å¯¹æŠ—æ€§æŠ„è¢­ï¼ˆå¦‚ä½¿ç”¨è‡ªåŠ¨è½¬è¿°å·¥å…·ç”Ÿæˆ"æ‰­æ›²çŸ­è¯­"æ©ç›–å‰½çªƒè¡Œä¸ºï¼‰ï¼Œé€šè¿‡ä¸¤é˜¶æ®µæ–¹æ³•ï¼ˆåŸºäºSciBERTçš„å¼‚å¸¸æ£€æµ‹å’ŒåŸºäºFAISS/SBERTçš„è¯­ä¹‰é‡æ„ï¼‰æ£€æµ‹å¹¶è¿˜åŸåŸå§‹æœ¯è¯­ã€‚å…¶"æ£€ç´¢å¢å¼º"çš„æ ¸å¿ƒè®¾è®¡ï¼ˆåˆ©ç”¨ç¨ å¯†å‘é‡æ£€ç´¢å’Œè¯­ä¹‰å¯¹é½æ¢å¤æºæ–‡æ¡£ï¼‰å±äºRAGæŠ€æœ¯çš„åˆ›æ–°åº”ç”¨ï¼Œæ˜¾è‘—æå‡äº†è¿˜åŸå‡†ç¡®ç‡ï¼ˆ23.67% vs é›¶åŸºå‡†çš„0%ï¼‰ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.10422v1">Cooperative Retrieval-Augmented Generation for Question Answering: Mutual Information Exchange and Ranking by Contrasting Layers</a></td><td><details><summary>å±•å¼€</summary>Since large language models (LLMs) have a tendency to generate factually inaccurate output, retrieval-augmented generation (RAG) has gained significant attention as a key means to mitigate this downside of harnessing only LLMs. However, existing RAG methods for simple and multi-hop question answering (QA) are still prone to incorrect retrievals and hallucinations. To address these limitations, we propose CoopRAG, a novel RAG framework for the question answering task in which a retriever and an LLM work cooperatively with each other by exchanging informative knowledge, and the earlier and later layers of the retriever model work cooperatively with each other to accurately rank the retrieved documents relevant to a given query. In this framework, we (i) unroll a question into sub-questions and a reasoning chain in which uncertain positions are masked, (ii) retrieve the documents relevant to the question augmented with the sub-questions and the reasoning chain, (iii) rerank the documents by contrasting layers of the retriever, and (iv) reconstruct the reasoning chain by filling the masked positions via the LLM. Our experiments demonstrate that CoopRAG consistently outperforms state-of-the-art QA methods on three multi-hop QA datasets as well as a simple QA dataset in terms of both the retrieval and QA performances. Our code is available.\footnote{https://github.com/meaningful96/CoopRAG}</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºCoopRAGçš„æ–°å‹RAGæ¡†æ¶ï¼Œé€šè¿‡è®©æ£€ç´¢å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åä½œäº¤äº’çŸ¥è¯†ï¼Œå¹¶ç»“åˆæ£€ç´¢æ¨¡å‹çš„ä¸åŒå±‚æ¥ç²¾ç¡®æ’åºç›¸å…³æ–‡æ¡£ï¼Œä»¥æé«˜é—®ç­”ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬åˆ†è§£é—®é¢˜ä¸ºå­é—®é¢˜å’Œæ¨ç†é“¾ã€æ£€ç´¢å¢å¼ºæ–‡æ¡£ã€é‡æ–°æ’åºæ–‡æ¡£ä»¥åŠé‡æ„æ¨ç†é“¾ç­‰æ­¥éª¤ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨å¤šè·³å’Œç®€å•é—®ç­”æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-10
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.09487v1">RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸º\model{}çš„åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ¡†æ¶ï¼Œç”¨äºæ”¹è¿›å¤šè½®è‡ªé€‚åº”çš„å›¾-æ–‡æœ¬æ··åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡RLè”åˆä¼˜åŒ–æ•´ä¸ªç”Ÿæˆè¿‡ç¨‹ï¼Œä½¿å¤§è¯­è¨€æ¨¡å‹èƒ½åŠ¨æ€å†³å®šä½•æ—¶æ¨ç†ã€ä»æ–‡æœ¬æˆ–å›¾è°±ä¸­æ£€ç´¢ä»€ä¹ˆå†…å®¹ä»¥åŠä½•æ—¶ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼Œå¹¶è®¾è®¡äº†å…¼é¡¾ä»»åŠ¡æ•ˆæœä¸æ£€ç´¢æ•ˆç‡çš„ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äº”ä¸ªé—®ç­”åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰RAGåŸºçº¿ï¼Œå‡¸æ˜¾äº†ç«¯åˆ°ç«¯RLå¯¹å¤æ‚æ¨ç†ä¸­è‡ªé€‚åº”é«˜æ•ˆæ£€ç´¢çš„æ”¯æŒä¼˜åŠ¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.09331v1">Passing the Baton: High Throughput Distributed Disk-Based Vector Search with BatANN</a></td><td><details><summary>å±•å¼€</summary>Vector search underpins modern information-retrieval systems, including retrieval-augmented generation (RAG) pipelines and search engines over unstructured text and images. As datasets scale to billions of vectors, disk-based vector search has emerged as a practical solution. However, looking to the future, we need to anticipate datasets too large for any single server. We present BatANN, a distributed disk-based approximate nearest neighbor (ANN) system that retains the logarithmic search efficiency of a single global graph while achieving near-linear throughput scaling in the number of servers. Our core innovation is that when accessing a neighborhood which is stored on another machine, we send the full state of the query to the other machine to continue executing there for improved locality. On 100M- and 1B-point datasets at 0.95 recall using 10 servers, BatANN achieves 6.21-6.49x and 2.5-5.10x the throughput of the scatter-gather baseline, respectively, while maintaining mean latency below 6 ms. Moreover, we get these results on standard TCP. To our knowledge, BatANN is the first open-source distributed disk-based vector search system to operate over a single global graph.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†BatANNï¼Œä¸€ç§åˆ†å¸ƒå¼ç£ç›˜è¿‘ä¼¼æœ€è¿‘é‚»ï¼ˆANNï¼‰ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡æ•°æ®é›†ä¸‹å‘é‡æœç´¢çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æåŠäº†å…¶åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­‰ç°ä»£ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿä¸­çš„é‡è¦æ€§ã€‚é€šè¿‡ä¼˜åŒ–æŸ¥è¯¢çŠ¶æ€çš„è·¨æœºå™¨ä¼ è¾“ä»¥æé«˜å±€éƒ¨æ€§ï¼ŒBatANNåœ¨ä¿æŒä½å»¶è¿Ÿçš„åŒæ—¶æ˜¾è‘—æå‡äº†ååé‡ï¼Œå¹¶æˆä¸ºé¦–ä¸ªåŸºäºå•ä¸€å…¨å±€å›¾çš„å¼€æºåˆ†å¸ƒå¼ç£ç›˜å‘é‡æœç´¢ç³»ç»Ÿã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-09
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.08892v1">Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs. The code is available at https://github.com/Teddy-XiongGZ/RAGLens.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨RAGæŠ€æœ¯ä¸­å­˜åœ¨çš„"faithfulness failures"ï¼ˆç”Ÿæˆå†…å®¹ä¸æ£€ç´¢è¯æ®çŸ›ç›¾æˆ–è¶…å‡ºï¼‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºRAGLensçš„è½»é‡çº§å¹»è§‰æ£€æµ‹æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰è§£æ„å¤§è¯­è¨€æ¨¡å‹å†…éƒ¨æ¿€æ´»ç‰¹å¾ï¼Œé€šè¿‡åŸºäºä¿¡æ¯çš„ç‰¹å¾é€‰æ‹©å’ŒåŠ æ€§ç‰¹å¾å»ºæ¨¡ï¼Œå®ç°é«˜ç²¾åº¦æ£€æµ‹RAGä¸å¿ å®è¾“å‡ºï¼Œå…¼å…·ä¼˜å¼‚æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.08659v1">An Agentic AI System for Multi-Framework Communication Coding</a></td><td><details><summary>å±•å¼€</summary>Clinical communication is central to patient outcomes, yet large-scale human annotation of patient-provider conversation remains labor-intensive, inconsistent, and difficult to scale. Existing approaches based on large language models typically rely on single-task models that lack adaptability, interpretability, and reliability, especially when applied across various communication frameworks and clinical domains. In this study, we developed a Multi-framework Structured Agentic AI system for Clinical Communication (MOSAIC), built on a LangGraph-based architecture that orchestrates four core agents, including a Plan Agent for codebook selection and workflow planning, an Update Agent for maintaining up-to-date retrieval databases, a set of Annotation Agents that applies codebook-guided retrieval-augmented generation (RAG) with dynamic few-shot prompting, and a Verification Agent that provides consistency checks and feedback. To evaluate performance, we compared MOSAIC outputs against gold-standard annotations created by trained human coders. We developed and evaluated MOSAIC using 26 gold standard annotated transcripts for training and 50 transcripts for testing, spanning rheumatology and OB/GYN domains. On the test set, MOSAIC achieved an overall F1 score of 0.928. Performance was highest in the Rheumatology subset (F1 = 0.962) and strongest for Patient Behavior (e.g., patients asking questions, expressing preferences, or showing assertiveness). Ablations revealed that MOSAIC outperforms baseline benchmarking.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§å¤šæ¡†æ¶ç»“æ„åŒ–ä»£ç†AIç³»ç»Ÿï¼ˆMOSAICï¼‰ï¼Œç”¨äºä¸´åºŠé€šä¿¡åˆ†æï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ³¨é‡Šä»£ç†ï¼Œç”¨äºåœ¨åŠ¨æ€å°‘æ ·æœ¬æç¤ºä¸‹è¿›è¡Œä»£ç æœ¬å¼•å¯¼çš„æ³¨é‡Šç”Ÿæˆï¼Œç³»ç»Ÿåœ¨é£æ¹¿ç—…å­¦å’Œå¦‡äº§ç§‘é¢†åŸŸçš„æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒF1å¾—åˆ†è¾¾åˆ°0.928ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.08617v1">HealthcareNLP: where are we and what is next?</a></td><td><details><summary>å±•å¼€</summary>This proposed tutorial focuses on Healthcare Domain Applications of NLP, what we have achieved around HealthcareNLP, and the challenges that lie ahead for the future. Existing reviews in this domain either overlook some important tasks, such as synthetic data generation for addressing privacy concerns, or explainable clinical NLP for improved integration and implementation, or fail to mention important methodologies, including retrieval augmented generation and the neural symbolic integration of LLMs and KGs. In light of this, the goal of this tutorial is to provide an introductory overview of the most important sub-areas of a patient- and resource-oriented HealthcareNLP, with three layers of hierarchy: data/resource layer: annotation guidelines, ethical approvals, governance, synthetic data; NLP-Eval layer: NLP tasks such as NER, RE, sentiment analysis, and linking/coding with categorised methods, leading to explainable HealthAI; patients layer: Patient Public Involvement and Engagement (PPIE), health literacy, translation, simplification, and summarisation (also NLP tasks), and shared decision-making support. A hands-on session will be included in the tutorial for the audience to use HealthcareNLP applications. The target audience includes NLP practitioners in the healthcare application domain, NLP researchers who are interested in domain applications, healthcare researchers, and students from NLP fields. The type of tutorial is "Introductory to CL/NLP topics (HealthcareNLP)" and the audience does not need prior knowledge to attend this. Tutorial materials: https://github.com/4dpicture/HealthNLP</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†åŒ»ç–—é¢†åŸŸNLPçš„åº”ç”¨ã€æˆå°±ä¸æœªæ¥æŒ‘æˆ˜ï¼Œç‰¹åˆ«æåˆ°ç°æœ‰ç ”ç©¶å¿½ç•¥äº†åŒ…æ‹¬æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨å†…çš„é‡è¦æ–¹æ³•ã€‚æ•™ç¨‹æ—¨åœ¨åˆ†å±‚ï¼ˆæ•°æ®/èµ„æºå±‚ã€NLPè¯„ä¼°å±‚ã€æ‚£è€…å±‚ï¼‰æ¦‚è¿°ä»¥æ‚£è€…å’Œèµ„æºä¸ºä¸­å¿ƒçš„HealthcareNLPï¼ŒåŒ…å«å®è·µç¯èŠ‚ï¼Œé€‚åˆæ— ä¸“ä¸šèƒŒæ™¯çš„å¬ä¼—ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.08492v1">Autonomous Issue Resolver: Towards Zero-Touch Code Maintenance</a></td><td><details><summary>å±•å¼€</summary>Recent advances in Large Language Models have revolutionized function-level code generation; however, repository-scale Automated Program Repair (APR) remains a significant challenge. Current approaches typically employ a control-centric paradigm, forcing agents to navigate complex directory structures and irrelevant control logic. In this paper, we propose a paradigm shift from the standard Code Property Graphs (CPGs) to the concept of Data Transformation Graph (DTG) that inverts the topology by modeling data states as nodes and functions as edges, enabling agents to trace logic defects through data lineage rather than control flow. We introduce a multi-agent framework that reconciles data integrity navigation with control flow logic. Our theoretical analysis and case studies demonstrate that this approach resolves the "Semantic Trap" inherent in standard RAG systems in modern coding agents. We provide a comprehensive implementation in the form of Autonomous Issue Resolver (AIR), a self-improvement system for zero-touch code maintenance that utilizes neuro-symbolic reasoning and uses the DTG structure for scalable logic repair. Our approach has demonstrated good results on several SWE benchmarks, reaching a resolution rate of 87.1% on SWE-Verified benchmark. Our approach directly addresses the core limitations of current AI code-assistant tools and tackles the critical need for a more robust foundation for our increasingly software-dependent world.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ä»ä»£ç å±æ€§å›¾ï¼ˆCPGsï¼‰è½¬å‘æ•°æ®è½¬æ¢å›¾ï¼ˆDTGï¼‰çš„æ–°èŒƒå¼ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“æ¡†æ¶ç»“åˆæ•°æ®å®Œæ•´æ€§å’Œæ§åˆ¶æµé€»è¾‘ï¼Œè§£å†³äº†ç°ä»£ç¼–ç æ™ºèƒ½ä½“ä¸­RAGç³»ç»Ÿå›ºæœ‰çš„â€œè¯­ä¹‰é™·é˜±â€é—®é¢˜ï¼Œå¹¶å¼€å‘äº†è‡ªä¸»é—®é¢˜è§£å†³å™¨ï¼ˆAIRï¼‰ç³»ç»Ÿï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é«˜æ•ˆçš„ä»£ç ä¿®å¤ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.08398v1">Ontology-Based Knowledge Graph Framework for Industrial Standard Documents via Hierarchical and Propositional Structuring</a></td><td><details><summary>å±•å¼€</summary>Ontology-based knowledge graph (KG) construction is a core technology that enables multidimensional understanding and advanced reasoning over domain knowledge. Industrial standards, in particular, contain extensive technical information and complex rules presented in highly structured formats that combine tables, scopes of application, constraints, exceptions, and numerical calculations, making KG construction especially challenging. In this study, we propose a method that organizes such documents into a hierarchical semantic structure, decomposes sentences and tables into atomic propositions derived from conditional and numerical rules, and integrates them into an ontology-knowledge graph through LLM-based triple extraction. Our approach captures both the hierarchical and logical structures of documents, effectively representing domain-specific semantics that conventional methods fail to reflect. To verify its effectiveness, we constructed rule, table, and multi-hop QA datasets, as well as a toxic clause detection dataset, from industrial standards, and implemented an ontology-aware KG-RAG framework for comparative evaluation. Experimental results show that our method achieves significant performance improvements across all QA types compared to existing KG-RAG approaches. This study demonstrates that reliable and scalable knowledge representation is feasible even for industrial documents with intertwined conditions, constraints, and scopes, contributing to future domain-specific RAG development and intelligent document management.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæœ¬ä½“è®ºçš„çŸ¥è¯†å›¾è°±æ„å»ºæ–¹æ³•ï¼Œç‰¹åˆ«é’ˆå¯¹å·¥ä¸šæ ‡å‡†æ–‡æ¡£ä¸­å¤æ‚çš„ç»“æ„åŒ–ä¿¡æ¯ï¼ˆå¦‚è¡¨æ ¼ã€æ¡ä»¶è§„åˆ™ç­‰ï¼‰ï¼Œåˆ©ç”¨LLMè¿›è¡Œä¸‰å…ƒç»„æŠ½å–å¹¶æ•´åˆåˆ°çŸ¥è¯†å›¾è°±ä¸­ã€‚æœ€ç»ˆé€šè¿‡æ„å»ºQAæ•°æ®é›†å’Œæ¯’å®³æ¡æ¬¾æ£€æµ‹æ•°æ®é›†éªŒè¯å…¶æå‡ºçš„ontology-aware KG-RAGæ¡†æ¶ï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨å„ç±»QAä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰KG-RAGæ–¹æ¡ˆï¼Œä¸ºé¢†åŸŸç‰¹å¼‚æ€§RAGå‘å±•æä¾›äº†å¯é ä¸”å¯æ‰©å±•çš„çŸ¥è¯†è¡¨ç¤ºè§£å†³æ–¹æ¡ˆã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-08
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.07515v1">SPAD: Seven-Source Token Probability Attribution with Syntactic Aggregation for Detecting Hallucinations in RAG</a></td><td><details><summary>å±•å¼€</summary>Detecting hallucinations in Retrieval-Augmented Generation (RAG) remains a challenge. Prior approaches attribute hallucinations to a binary conflict between internal knowledge (stored in FFNs) and retrieved context. However, this perspective is incomplete, failing to account for the impact of other components in the generative process, such as the user query, previously generated tokens, the current token itself, and the final LayerNorm adjustment. To address this, we introduce SPAD. First, we mathematically attribute each token's probability into seven distinct sources: Query, RAG, Past, Current Token, FFN, Final LayerNorm, and Initial Embedding. This attribution quantifies how each source contributes to the generation of the current token. Then, we aggregate these scores by POS tags to quantify how different components drive specific linguistic categories. By identifying anomalies, such as Nouns relying on Final LayerNorm, SPAD effectively detects hallucinations. Extensive experiments demonstrate that SPAD achieves state-of-the-art performance</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­çš„å¹»è§‰æ£€æµ‹é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºSPADçš„æ–°æ–¹æ³•ã€‚SPADé€šè¿‡å°†æ¯ä¸ªç”Ÿæˆä»¤ç‰Œçš„æ¦‚ç‡å½’å› äºä¸ƒä¸ªä¸åŒæ¥æºï¼ˆå¦‚æŸ¥è¯¢ã€RAGæ£€ç´¢å†…å®¹ã€å†å²ä»¤ç‰Œç­‰ï¼‰ï¼Œå¹¶åŸºäºè¯æ€§æ ‡ç­¾èšåˆè¿™äº›è¯„åˆ†ï¼Œä»¥é‡åŒ–ä¸åŒç»„ä»¶å¯¹ç‰¹å®šè¯­è¨€ç±»åˆ«çš„å½±å“ï¼Œä»è€Œæœ‰æ•ˆè¯†åˆ«å¼‚å¸¸å’Œå¹»è§‰ã€‚å®éªŒè¡¨æ˜SPADåœ¨å¹»è§‰æ£€æµ‹ä¸Šè¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-07
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.07515v1">SPAD: Seven-Source Token Probability Attribution with Syntactic Aggregation for Detecting Hallucinations in RAG</a></td><td><details><summary>å±•å¼€</summary>Detecting hallucinations in Retrieval-Augmented Generation (RAG) remains a challenge. Prior approaches attribute hallucinations to a binary conflict between internal knowledge (stored in FFNs) and retrieved context. However, this perspective is incomplete, failing to account for the impact of other components in the generative process, such as the user query, previously generated tokens, the current token itself, and the final LayerNorm adjustment. To address this, we introduce SPAD. First, we mathematically attribute each token's probability into seven distinct sources: Query, RAG, Past, Current Token, FFN, Final LayerNorm, and Initial Embedding. This attribution quantifies how each source contributes to the generation of the current token. Then, we aggregate these scores by POS tags to quantify how different components drive specific linguistic categories. By identifying anomalies, such as Nouns relying on Final LayerNorm, SPAD effectively detects hallucinations. Extensive experiments demonstrate that SPAD achieves state-of-the-art performance</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æ¢è®¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­å¹»è§‰æ£€æµ‹çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºSPADçš„æ–°æ–¹æ³•ã€‚ä¼ ç»Ÿæ–¹æ³•å°†å¹»è§‰å½’å› äºå†…éƒ¨çŸ¥è¯†ï¼ˆå­˜å‚¨åœ¨å‰é¦ˆç½‘ç»œFFNsä¸­ï¼‰ä¸æ£€ç´¢ä¸Šä¸‹æ–‡ä¹‹é—´çš„äºŒå…ƒå†²çªï¼Œä½†è¿™ç§æ–¹æ³•å¿½ç•¥äº†ç”Ÿæˆè¿‡ç¨‹ä¸­å…¶ä»–ç»„ä»¶ï¼ˆå¦‚ç”¨æˆ·æŸ¥è¯¢ã€å…ˆå‰ç”Ÿæˆçš„æ ‡è®°ç­‰ï¼‰çš„å½±å“ã€‚SPADé€šè¿‡å°†æ¯ä¸ªæ ‡è®°çš„æ¦‚ç‡æ¥æºåˆ†ä¸ºä¸ƒä¸ªä¸åŒçš„éƒ¨åˆ†ï¼ˆæŸ¥è¯¢ã€RAGã€è¿‡å»ã€å½“å‰æ ‡è®°ã€FFNã€æœ€ç»ˆLayerNormè°ƒæ•´å’Œåˆå§‹åµŒå…¥ï¼‰ï¼Œå¹¶é‡åŒ–å„éƒ¨åˆ†å¯¹å½“å‰æ ‡è®°ç”Ÿæˆçš„è´¡çŒ®ï¼Œè¿›ä¸€æ­¥æŒ‰è¯æ€§æ ‡è®°èšåˆè¿™äº›åˆ†æ•°ä»¥è¯†åˆ«å¼‚å¸¸ï¼Œä»è€Œæœ‰æ•ˆæ£€æµ‹å¹»è§‰ã€‚å®éªŒè¡¨æ˜ï¼ŒSPADåœ¨æ€§èƒ½ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.07360v1">Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation</a></td><td><details><summary>å±•å¼€</summary>Benefiting from the inductive biases learned from large-scale datasets, open-vocabulary semantic segmentation (OVSS) leverages the power of vision-language models, such as CLIP, to achieve remarkable progress without requiring task-specific training. However, due to CLIP's pre-training nature on image-text pairs, it tends to focus on global semantic alignment, resulting in suboptimal performance when associating fine-grained visual regions with text. This leads to noisy and inconsistent predictions, particularly in local areas. We attribute this to a dispersed bias stemming from its contrastive training paradigm, which is difficult to alleviate using CLIP features alone. To address this, we propose a structure-aware feature rectification approach that incorporates instance-specific priors derived directly from the image. Specifically, we construct a region adjacency graph (RAG) based on low-level features (e.g., colour and texture) to capture local structural relationships and use it to refine CLIP features by enhancing local discrimination. Extensive experiments show that our method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºåŒºåŸŸé‚»æ¥å›¾ï¼ˆRAGï¼‰çš„ç»“æ„æ„ŸçŸ¥ç‰¹å¾æ ¡æ­£æ–¹æ³•ï¼Œç”¨äºæ”¹è¿›å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰ä¸­CLIPæ¨¡å‹å¯¹ç»†ç²’åº¦è§†è§‰åŒºåŸŸä¸æ–‡æœ¬å…³è”çš„æ€§èƒ½é—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨ä½å±‚ç‰¹å¾æ„å»ºRAGä»¥æ•è·å±€éƒ¨ç»“æ„å…³ç³»ï¼Œå¹¶ä»¥æ­¤ä¼˜åŒ–CLIPç‰¹å¾ï¼Œä»è€Œå‡å°‘åˆ†å‰²å™ªå£°ã€æå‡åŒºåŸŸä¸€è‡´æ€§ï¼Œåœ¨å¤šä¸ªå¼€æ”¾è¯æ±‡åˆ†å‰²åŸºå‡†ä¸Šå–å¾—æ˜¾è‘—æ•ˆæœã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.07015v1">FVA-RAG: Falsification-Verification Alignment for Mitigating Sycophantic Hallucinations</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) systems have significantly reduced hallucinations in Large Language Models (LLMs) by grounding responses in external context. However, standard RAG architectures suffer from a critical vulnerability: Retrieval Sycophancy. When presented with a query based on a false premise or a common misconception, vector-based retrievers tend to fetch documents that align with the user's bias rather than objective truth, leading the model to "hallucinate with citations."
  In this work, we introduce Falsification-Verification Alignment RAG (FVA-RAG), a framework that shifts the retrieval paradigm from Inductive Verification (seeking support) to Deductive Falsification (seeking disproof). Unlike existing "Self-Correction" methods that rely on internal consistency, FVA-RAG deploys a distinct Adversarial Retrieval Policy that actively generates "Kill Queries"-targeted search terms designed to surface contradictory evidence. We introduce a dual-verification mechanism that explicitly weighs the draft answer against this "Anti-Context." Preliminary experiments on a dataset of common misconceptions demonstrate that FVA-RAG significantly improves robustness against sycophantic hallucinations compared to standard RAG baselines, effectively acting as an inference-time "Red Team" for factual generation.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡é’ˆå¯¹ä¼ ç»ŸRAGç³»ç»Ÿä¸­å­˜åœ¨çš„â€œæ£€ç´¢é˜¿è°€â€ï¼ˆRetrieval Sycophancyï¼‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºFVA-RAGçš„æ–°æ¡†æ¶ï¼Œé€šè¿‡ä»å½’çº³éªŒè¯è½¬å‘æ¼”ç»è¯ä¼ªçš„æ£€ç´¢èŒƒå¼ï¼Œä¸»åŠ¨æ£€ç´¢åè¯ä¿¡æ¯å¹¶è¿›è¡ŒåŒé‡éªŒè¯ï¼Œä»¥å‡å°‘æ¨¡å‹åŸºäºé”™è¯¯å‰ææˆ–åè§ç”Ÿæˆçš„å¹»è§‰å›ç­”ï¼Œæ˜¾è‘—æå‡äº†RAGç³»ç»Ÿçš„é²æ£’æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.06787v1">LLM4SFC: Sequential Function Chart Generation via Large Language Models</a></td><td><details><summary>å±•å¼€</summary>While Large Language Models (LLMs) are increasingly used for synthesizing textual PLC programming languages like Structured Text (ST) code, other IEC 61131-3 standard graphical languages like Sequential Function Charts (SFCs) remain underexplored. Generating SFCs is challenging due to graphical nature and ST actions embedded within, which are not directly compatible with standard generation techniques, often leading to non-executable code that is incompatible with industrial tool-chains In this work, we introduce LLM4SFC, the first framework to receive natural-language descriptions of industrial workflows and provide executable SFCs. LLM4SFC is based on three components: (i) A reduced structured representation that captures essential topology and in-line ST and reduced textual verbosity; (ii) Fine-tuning and few-shot retrieval-augmented generation (RAG) for alignment with SFC programming conventions; and (iii) A structured generation approach that prunes illegal tokens in real-time to ensure compliance with the textual format of SFCs. We evaluate LLM4SFC on a dataset of real-world SFCs from automated manufacturing projects, using both open-source and proprietary LLMs. The results show that LLM4SFC reliably generates syntactically valid SFC programs effectively bridging graphical and textual PLC languages, achieving a generation generation success of 75% - 94%, paving the way for automated industrial programming.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†LLM4SFCæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆç»†è°ƒæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œå°†è‡ªç„¶è¯­è¨€æè¿°çš„å·¥ä¸šæµç¨‹è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„é¡ºåºåŠŸèƒ½å›¾ï¼ˆSFCï¼‰ä»£ç ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ç”Ÿæˆå›¾å½¢åŒ–PLCç¼–ç¨‹è¯­è¨€çš„å…¼å®¹æ€§é—®é¢˜ï¼Œå¹¶åœ¨å®é™…å·¥ä¸šæ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.06641v1">An Index-based Approach for Efficient and Effective Web Content Extraction</a></td><td><details><summary>å±•å¼€</summary>As web agents (e.g., Deep Research) routinely consume massive volumes of web pages to gather and analyze information, LLM context management -- under large token budgets and low signal density -- emerges as a foundational, high-importance, and technically challenging problem for agentic and RAG pipelines. Existing solutions for extracting relevant content are inadequate: generative extraction models suffer from high latency, rule-based heuristics lack adaptability, and chunk-and-rerank methods are blind to webpage structure. To overcome these issues, we introduce Index-based Web Content Extraction to reframe the extraction process from slow, token-by-token generation into a highly efficient, discriminative task of index prediction, achieving both effectiveness and efficiency. We partition HTML into structure-aware, addressable segments, and extract only the positional indices of content relevant to a given query. This method decouples extraction latency from content length, enabling rapid, query-relevant extraction. We first evaluate our method as a post-retrieval processing component within an RAG QA system and find that it improves QA accuracy. Then we directly measure its match rate with the target content in two scenarios: main content extraction (ME) and query-relevant extraction (QE). Experimental results show that our method outperforms existing works in both accuracy and speed, effectively bridging the gap between LLMs and the vast webpages.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œåŸºäºç´¢å¼•çš„ç½‘é¡µå†…å®¹æå–â€çš„æ–¹æ³•ï¼ˆIndex-based Web Content Extractionï¼‰ï¼Œæ—¨åœ¨è§£å†³RAGæµç¨‹ä¸­å¤§è§„æ¨¡ç½‘é¡µå†…å®¹å¤„ç†çš„æ•ˆç‡ä¸å‡†ç¡®æ€§é—®é¢˜ã€‚è¯¥æ–¹æ³•å°†ä¼ ç»Ÿé€è¯ç”Ÿæˆçš„æ…¢é€Ÿå†…å®¹æå–è½¬åŒ–ä¸ºé«˜æ•ˆçš„ç´¢å¼•é¢„æµ‹ä»»åŠ¡ï¼Œé€šè¿‡ç»“æ„æ„ŸçŸ¥çš„HTMLåˆ†æ®µå’ŒæŸ¥è¯¢ç›¸å…³çš„ç´¢å¼•å®šä½ï¼Œæ˜¾è‘—é™ä½äº†å»¶è¿Ÿå¹¶æå‡æå–ç²¾åº¦ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨RAGé—®ç­”ç³»ç»Ÿä¸­èƒ½æé«˜å›ç­”å‡†ç¡®æ€§ï¼Œå¹¶åœ¨â€œä¸»å†…å®¹æå–â€å’Œâ€œæŸ¥è¯¢ç›¸å…³æå–â€ä¸¤ç§åœºæ™¯ä¸‹å‡ä¼˜äºç°æœ‰æ–¹æ¡ˆï¼Œå®ç°äº†å¤§è¯­è¨€æ¨¡å‹ä¸æµ·é‡ç½‘é¡µæ•°æ®çš„é«˜æ•ˆå¯¹æ¥ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-06
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.07015v1">FVA-RAG: Falsification-Verification Alignment for Mitigating Sycophantic Hallucinations</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) systems have significantly reduced hallucinations in Large Language Models (LLMs) by grounding responses in external context. However, standard RAG architectures suffer from a critical vulnerability: Retrieval Sycophancy. When presented with a query based on a false premise or a common misconception, vector-based retrievers tend to fetch documents that align with the user's bias rather than objective truth, leading the model to "hallucinate with citations."
  In this work, we introduce Falsification-Verification Alignment RAG (FVA-RAG), a framework that shifts the retrieval paradigm from Inductive Verification (seeking support) to Deductive Falsification (seeking disproof). Unlike existing "Self-Correction" methods that rely on internal consistency, FVA-RAG deploys a distinct Adversarial Retrieval Policy that actively generates "Kill Queries"-targeted search terms designed to surface contradictory evidence. We introduce a dual-verification mechanism that explicitly weighs the draft answer against this "Anti-Context." Preliminary experiments on a dataset of common misconceptions demonstrate that FVA-RAG significantly improves robustness against sycophantic hallucinations compared to standard RAG baselines, effectively acting as an inference-time "Red Team" for factual generation.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºFVA-RAGçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸRAGç³»ç»Ÿä¸­å­˜åœ¨çš„"æ£€ç´¢é€¢è¿"ï¼ˆRetrieval Sycophancyï¼‰é—®é¢˜ï¼Œå³å½“ç”¨æˆ·æŸ¥è¯¢åŸºäºé”™è¯¯å‰ææˆ–å¸¸è§è¯¯è§£æ—¶ï¼Œæ£€ç´¢ç³»ç»Ÿå€¾å‘äºè¿”å›ç¬¦åˆç”¨æˆ·åè§çš„æ–‡æ¡£è€Œéå®¢è§‚äº‹å®ã€‚FVA-RAGé€šè¿‡å°†æ£€ç´¢èŒƒå¼ä»å½’çº³éªŒè¯ï¼ˆå¯»æ±‚æ”¯æŒï¼‰è½¬å˜ä¸ºæ¼”ç»è¯ä¼ªï¼ˆå¯»æ±‚åè¯ï¼‰ï¼Œå¹¶é‡‡ç”¨å¯¹æŠ—æ€§æ£€ç´¢ç­–ç•¥ç”Ÿæˆæ—¨åœ¨å‘ç°çŸ›ç›¾è¯æ®çš„"ç»ˆæ­¢æŸ¥è¯¢"ï¼ˆKill Queriesï¼‰ï¼Œä»¥åŠåŒé‡éªŒè¯æœºåˆ¶æ¥æƒè¡¡åˆæ­¥ç­”æ¡ˆä¸"åä¸Šä¸‹æ–‡"ï¼ˆAnti-Contextï¼‰ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†å¯¹é€¢è¿æ€§å¹»è§‰çš„é²æ£’æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.06787v1">LLM4SFC: Sequential Function Chart Generation via Large Language Models</a></td><td><details><summary>å±•å¼€</summary>While Large Language Models (LLMs) are increasingly used for synthesizing textual PLC programming languages like Structured Text (ST) code, other IEC 61131-3 standard graphical languages like Sequential Function Charts (SFCs) remain underexplored. Generating SFCs is challenging due to graphical nature and ST actions embedded within, which are not directly compatible with standard generation techniques, often leading to non-executable code that is incompatible with industrial tool-chains In this work, we introduce LLM4SFC, the first framework to receive natural-language descriptions of industrial workflows and provide executable SFCs. LLM4SFC is based on three components: (i) A reduced structured representation that captures essential topology and in-line ST and reduced textual verbosity; (ii) Fine-tuning and few-shot retrieval-augmented generation (RAG) for alignment with SFC programming conventions; and (iii) A structured generation approach that prunes illegal tokens in real-time to ensure compliance with the textual format of SFCs. We evaluate LLM4SFC on a dataset of real-world SFCs from automated manufacturing projects, using both open-source and proprietary LLMs. The results show that LLM4SFC reliably generates syntactically valid SFC programs effectively bridging graphical and textual PLC languages, achieving a generation generation success of 75% - 94%, paving the way for automated industrial programming.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºLLM4SFCçš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆå¯æ‰§è¡Œçš„Sequential Function Chartsï¼ˆSFCsï¼‰ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç»“æ„åŒ–è¡¨ç¤ºã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ä»¥åŠå®æ—¶ä¿®å‰ªéæ³•ä»¤ç‰Œçš„ç»“æ„åŒ–ç”Ÿæˆæ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†ç”ŸæˆSFCsçš„æˆåŠŸç‡ï¼ˆ75%-94%ï¼‰ï¼Œå¹¶è§£å†³äº†å›¾å½¢åŒ–ç¼–ç¨‹è¯­è¨€ä¸å·¥ä¸šå·¥å…·é“¾çš„å…¼å®¹æ€§é—®é¢˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.06641v1">An Index-based Approach for Efficient and Effective Web Content Extraction</a></td><td><details><summary>å±•å¼€</summary>As web agents (e.g., Deep Research) routinely consume massive volumes of web pages to gather and analyze information, LLM context management -- under large token budgets and low signal density -- emerges as a foundational, high-importance, and technically challenging problem for agentic and RAG pipelines. Existing solutions for extracting relevant content are inadequate: generative extraction models suffer from high latency, rule-based heuristics lack adaptability, and chunk-and-rerank methods are blind to webpage structure. To overcome these issues, we introduce Index-based Web Content Extraction to reframe the extraction process from slow, token-by-token generation into a highly efficient, discriminative task of index prediction, achieving both effectiveness and efficiency. We partition HTML into structure-aware, addressable segments, and extract only the positional indices of content relevant to a given query. This method decouples extraction latency from content length, enabling rapid, query-relevant extraction. We first evaluate our method as a post-retrieval processing component within an RAG QA system and find that it improves QA accuracy. Then we directly measure its match rate with the target content in two scenarios: main content extraction (ME) and query-relevant extraction (QE). Experimental results show that our method outperforms existing works in both accuracy and speed, effectively bridging the gap between LLMs and the vast webpages.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œIndex-based Web Content Extractionâ€çš„é«˜æ•ˆç½‘é¡µå†…å®¹æå–æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³RAGæµç¨‹ä¸­LLMä¸Šä¸‹æ–‡ç®¡ç†çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†HTMLåˆ†å‰²ä¸ºç»“æ„æ„ŸçŸ¥çš„å¯å¯»å€æ®µï¼Œå¹¶ä»…æå–ä¸æŸ¥è¯¢ç›¸å…³å†…å®¹çš„ç´¢å¼•ï¼Œä»è€Œå®ç°äº†å¿«é€Ÿä¸”å‡†ç¡®çš„æŸ¥è¯¢ç›¸å…³å†…å®¹æå–ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨RAGé—®ç­”ç³»ç»Ÿä¸­æé«˜äº†å‡†ç¡®æ€§ï¼Œå¹¶åœ¨ä¸¤ç§æå–åœºæ™¯ï¼ˆä¸»è¦å†…å®¹æå–å’ŒæŸ¥è¯¢ç›¸å…³æå–ï¼‰ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.06240v1">AI Application in Anti-Money Laundering for Sustainable and Transparent Financial Systems</a></td><td><details><summary>å±•å¼€</summary>Money laundering and financial fraud remain major threats to global financial stability, costing trillions annually and challenging regulatory oversight. This paper reviews how artificial intelligence (AI) applications can modernize Anti-Money Laundering (AML) workflows by improving detection accuracy, lowering false-positive rates, and reducing the operational burden of manual investigations, thereby supporting more sustainable development. It further highlights future research directions including federated learning for privacy-preserving collaboration, fairness-aware and interpretable AI, reinforcement learning for adaptive defenses, and human-in-the-loop visualization systems to ensure that next-generation AML architectures remain transparent, accountable, and robust. In the final part, the paper proposes an AI-driven KYC application that integrates graph-based retrieval-augmented generation (RAG Graph) with generative models to enhance efficiency, transparency, and decision support in KYC processes related to money-laundering detection. Experimental results show that the RAG-Graph architecture delivers high faithfulness and strong answer relevancy across diverse evaluation settings, thereby enhancing the efficiency and transparency of KYC CDD/EDD workflows and contributing to more sustainable, resource-optimized compliance practices.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½åœ¨åæ´—é’±ï¼ˆAMLï¼‰å·¥ä½œæµç¨‹ä¸­çš„åº”ç”¨ï¼Œé‡ç‚¹ä»‹ç»äº†åŸºäºå›¾çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAG-Graphï¼‰æŠ€æœ¯ä¸ç”Ÿæˆæ¨¡å‹ç»“åˆçš„KYCåº”ç”¨ï¼Œä»¥æé«˜æ•ˆç‡ã€é€æ˜åº¦å’Œå†³ç­–æ”¯æŒï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯å…¶é«˜æ•ˆæ€§å’Œç›¸å…³æ€§ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-05
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.06240v1">AI Application in Anti-Money Laundering for Sustainable and Transparent Financial Systems</a></td><td><details><summary>å±•å¼€</summary>Money laundering and financial fraud remain major threats to global financial stability, costing trillions annually and challenging regulatory oversight. This paper reviews how artificial intelligence (AI) applications can modernize Anti-Money Laundering (AML) workflows by improving detection accuracy, lowering false-positive rates, and reducing the operational burden of manual investigations, thereby supporting more sustainable development. It further highlights future research directions including federated learning for privacy-preserving collaboration, fairness-aware and interpretable AI, reinforcement learning for adaptive defenses, and human-in-the-loop visualization systems to ensure that next-generation AML architectures remain transparent, accountable, and robust. In the final part, the paper proposes an AI-driven KYC application that integrates graph-based retrieval-augmented generation (RAG Graph) with generative models to enhance efficiency, transparency, and decision support in KYC processes related to money-laundering detection. Experimental results show that the RAG-Graph architecture delivers high faithfulness and strong answer relevancy across diverse evaluation settings, thereby enhancing the efficiency and transparency of KYC CDD/EDD workflows and contributing to more sustainable, resource-optimized compliance practices.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½åœ¨åæ´—é’±ï¼ˆAMLï¼‰å·¥ä½œæµç¨‹ä¸­çš„åº”ç”¨ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆåŸºäºå›¾çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAG-Graphï¼‰ä¸ç”Ÿæˆæ¨¡å‹çš„AIé©±åŠ¨KYCæ–¹æ¡ˆï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ¶æ„èƒ½æ˜¾è‘—æå‡KYCæµç¨‹çš„æ•ˆç‡å’Œé€æ˜åº¦ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.06200v1">How Should We Evaluate Data Deletion in Graph-Based ANN Indexes?</a></td><td><details><summary>å±•å¼€</summary>Approximate Nearest Neighbor Search (ANNS) has recently gained significant attention due to its many applications, such as Retrieval-Augmented Generation. Such applications require ANNS algorithms that support dynamic data, so the ANNS problem on dynamic data has attracted considerable interest. However, a comprehensive evaluation methodology for data deletion in ANNS has yet to be established. This study proposes an experimental framework and comprehensive evaluation metrics to assess the efficiency of data deletion for ANNS indexes under practical use cases. Specifically, we categorize data deletion methods in graph-based ANNS into three approaches and formalize them mathematically. The performance is assessed in terms of accuracy, query speed, and other relevant metrics. Finally, we apply the proposed evaluation framework to Hierarchical Navigable Small World, one of the state-of-the-art ANNS methods, to analyze the effects of data deletion, and propose Deletion Control, a method which dynamically selects the appropriate deletion method under a required search accuracy.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†åŠ¨æ€æ•°æ®ç¯å¢ƒä¸‹çš„è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ï¼ˆANNSï¼‰é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­‰åº”ç”¨ä¸­çš„é‡è¦æ€§ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªå®éªŒæ¡†æ¶å’Œå…¨é¢çš„è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°ANNSç´¢å¼•åœ¨æ•°æ®åˆ é™¤æƒ…å†µä¸‹çš„æ•ˆç‡ï¼Œå°†åŸºäºå›¾çš„ANNSä¸­çš„æ•°æ®åˆ é™¤æ–¹æ³•åˆ†ä¸ºä¸‰ç±»å¹¶è¿›è¡Œäº†æ•°å­¦å½¢å¼åŒ–ã€‚æœ€åï¼Œè®ºæ–‡é€šè¿‡åº”ç”¨è¯¥æ¡†æ¶åˆ†æå±‚æ¬¡å¯å¯¼èˆªå°ä¸–ç•Œï¼ˆHNSWï¼‰æ–¹æ³•åœ¨æ•°æ®åˆ é™¤ä¸­çš„è¡¨ç°ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŠ¨æ€é€‰æ‹©åˆ é™¤æ–¹æ³•çš„æ–°ç­–ç•¥â€œDeletion Controlâ€ï¼Œä»¥åœ¨ä¿æŒæœç´¢ç²¾åº¦è¦æ±‚çš„åŒæ—¶ä¼˜åŒ–æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.05967v1">Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms</a></td><td><details><summary>å±•å¼€</summary>In the era of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) architectures are gaining significant attention for their ability to ground language generation in reliable knowledge sources. Despite their impressive effectiveness in many areas, RAG systems based solely on semantic similarity often fail to ensure factual accuracy in specialized domains, where terminological ambiguity can affect retrieval relevance. This study proposes an enhanced RAG architecture that integrates a factual signal derived from Entity Linking to improve the accuracy of educational question-answering systems in Italian. The system includes a Wikidata-based Entity Linking module and implements three re-ranking strategies to combine semantic and entity-based information: a hybrid score weighting model, reciprocal rank fusion, and a cross-encoder re-ranker. Experiments were conducted on two benchmarks: a custom academic dataset and the standard SQuAD-it dataset. Results show that, in domain-specific contexts, the hybrid schema based on reciprocal rank fusion significantly outperforms both the baseline and the cross-encoder approach, while the cross-encoder achieves the best results on the general-domain dataset. These findings confirm the presence of an effect of domain mismatch and highlight the importance of domain adaptation and hybrid ranking strategies to enhance factual precision and reliability in retrieval-augmented generation. They also demonstrate the potential of entity-aware RAG systems in educational environments, fostering adaptive and reliable AI-based tutoring tools.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§å¢å¼ºçš„RAGæ¶æ„ï¼Œé€šè¿‡é›†æˆå®ä½“é“¾æ¥ï¼ˆEntity Linkingï¼‰ç”Ÿæˆçš„äº‹å®ä¿¡å·ï¼Œä»¥æé«˜æ„å¤§åˆ©æ•™è‚²é—®ç­”ç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚è¯¥æ¶æ„åŒ…æ‹¬åŸºäºWikidataçš„å®ä½“é“¾æ¥æ¨¡å—ï¼Œå¹¶é‡‡ç”¨ä¸‰ç§é‡æ–°æ’åºç­–ç•¥ï¼ˆæ··åˆè¯„åˆ†åŠ æƒã€äº’ç§©èåˆå’Œäº¤å‰ç¼–ç å™¨ï¼‰ç»“åˆè¯­ä¹‰å’Œå®ä½“ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ç‰¹å®šé¢†åŸŸåœºæ™¯ä¸­ï¼ŒåŸºäºäº’ç§©èåˆçš„æ··åˆç­–ç•¥æ˜¾è‘—ä¼˜äºåŸºçº¿å’Œäº¤å‰ç¼–ç å™¨æ–¹æ³•ï¼Œè€Œäº¤å‰ç¼–ç å™¨åœ¨é€šç”¨æ•°æ®é›†ä¸Šè¡¨ç°æœ€ä½³ã€‚ç ”ç©¶å¼ºè°ƒäº†é¢†åŸŸé€‚åº”å’Œæ··åˆæ’åºç­–ç•¥å¯¹æå‡RAGäº‹å®ç²¾ç¡®æ€§å’Œå¯é æ€§çš„é‡è¦æ€§ï¼Œå¹¶å±•ç¤ºäº†å®ä½“æ„ŸçŸ¥RAGç³»ç»Ÿåœ¨æ•™è‚²ç¯å¢ƒä¸­çš„æ½œåŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.05959v1">M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG</a></td><td><details><summary>å±•å¼€</summary>Vision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†M4-RAGï¼Œä¸€ä¸ªé’ˆå¯¹å¤šè¯­è¨€å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼Œè¦†ç›–42ç§è¯­è¨€å’Œ56ç§åœ°åŒºæ–¹è¨€åŠå˜ä½“ï¼ŒåŒ…å«è¶…è¿‡8ä¸‡å¼ æ–‡åŒ–å¤šæ ·æ€§å›¾ç‰‡åŠå¯¹åº”é—®é¢˜ï¼Œç”¨äºè¯„ä¼°è·¨è¯­è¨€å’Œè·¨æ¨¡æ€çš„æ£€ç´¢å¢å¼ºè§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ€§èƒ½ã€‚ç ”ç©¶å‘ç°RAGå¯¹å°å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æœ‰æ˜¾è‘—æå‡ï¼Œä½†å¯¹å¤§å‹æ¨¡å‹æ•ˆæœæœ‰é™ç”šè‡³å¯èƒ½é™ä½æ€§èƒ½ï¼Œæ­ç¤ºäº†æ¨¡å‹è§„æ¨¡ä¸å½“å‰æ£€ç´¢æ•ˆç‡é—´çš„ä¸åŒ¹é…é—®é¢˜ï¼Œä¸ºæœªæ¥RAGç³»ç»Ÿçš„å¼€å‘æä¾›äº†åŸºç¡€ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.05958v1">MaxShapley: Towards Incentive-compatible Generative Search with Fair Context Attribution</a></td><td><details><summary>å±•å¼€</summary>Generative search engines based on large language models (LLMs) are replacing traditional search, fundamentally changing how information providers are compensated. To sustain this ecosystem, we need fair mechanisms to attribute and compensate content providers based on their contributions to generated answers. We introduce MaxShapley, an efficient algorithm for fair attribution in generative search pipelines that use retrieval-augmented generation (RAG). MaxShapley is a special case of the celebrated Shapley value; it leverages a decomposable max-sum utility function to compute attributions with linear computation in the number of documents, as opposed to the exponential cost of Shapley values. We evaluate MaxShapley on three multi-hop QA datasets (HotPotQA, MuSiQUE, MS MARCO); MaxShapley achieves comparable attribution quality to exact Shapley computation, while consuming a fraction of its tokens--for instance, it gives up to an 8x reduction in resource consumption over prior state-of-the-art methods at the same attribution accuracy.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†MaxShapleyç®—æ³•ï¼Œä¸€ç§ç”¨äºåŸºäºRAGçš„ç”Ÿæˆå¼æœç´¢å¼•æ“ä¸­å…¬å¹³åˆ†é…å†…å®¹æä¾›è€…è´¡çŒ®å’Œè¡¥å¿çš„é«˜æ•ˆæ–¹æ³•ã€‚é€šè¿‡åˆ©ç”¨å¯åˆ†è§£çš„æœ€å¤§æ€»å’Œæ•ˆç”¨å‡½æ•°ï¼ŒMaxShapleyèƒ½åœ¨æ–‡æ¡£æ•°é‡çº¿æ€§è®¡ç®—å¤æ‚åº¦ä¸‹å®Œæˆå±æ€§åˆ†é…ï¼ˆç›¸æ¯”ä¼ ç»ŸShapleyå€¼çš„æŒ‡æ•°è®¡ç®—æˆæœ¬ï¼‰ï¼Œå¹¶åœ¨å¤šè·³é—®ç­”æ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶ä¸ç²¾ç¡®Shapleyè®¡ç®—ç›¸å½“çš„å±æ€§åˆ†é…è´¨é‡ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†èµ„æºæ¶ˆè€—ï¼ˆå¦‚ç›¸åŒå‡†ç¡®ç‡ä¸‹èµ„æºä½¿ç”¨å‡å°‘è¾¾8å€ï¼‰ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.06060v1">Reinforcement Learning Integrated Agentic RAG for Software Test Cases Authoring</a></td><td><details><summary>å±•å¼€</summary>This paper introduces a framework that integrates reinforcement learning (RL) with autonomous agents to enable continuous improvement in the automated process of software test cases authoring from business requirement documents within Quality Engineering (QE) workflows. Conventional systems employing Large Language Models (LLMs) generate test cases from static knowledge bases, which fundamentally limits their capacity to enhance performance over time. Our proposed Reinforcement Infused Agentic RAG (Retrieve, Augment, Generate) framework overcomes this limitation by employing AI agents that learn from QE feedback, assessments, and defect discovery outcomes to automatically improve their test case generation strategies. The system combines specialized agents with a hybrid vector-graph knowledge base that stores and retrieves software testing knowledge. Through advanced RL algorithms, specifically Proximal Policy Optimization (PPO) and Deep Q-Networks (DQN), these agents optimize their behavior based on QE-reported test effectiveness, defect detection rates, and workflow metrics. As QEs execute AI-generated test cases and provide feedback, the system learns from this expert guidance to improve future iterations. Experimental validation on enterprise Apple projects yielded substantive improvements: a 2.4% increase in test generation accuracy (from 94.8% to 97.2%), and a 10.8% improvement in defect detection rates. The framework establishes a continuous knowledge refinement loop driven by QE expertise, resulting in progressively superior test case quality that enhances, rather than replaces, human testing capabilities.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆå¼ºåŒ–å­¦ä¹ ä¸è‡ªä¸»æ™ºèƒ½ä½“çš„æ¡†æ¶ï¼Œåä¸º"Reinforcement Infused Agentic RAG"ï¼Œé€šè¿‡æ£€ç´¢-å¢å¼º-ç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ä¼˜åŒ–è½¯ä»¶æµ‹è¯•ç”¨ä¾‹çš„è‡ªåŠ¨ç”Ÿæˆè¿‡ç¨‹ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨æ··åˆå‘é‡-å›¾çŸ¥è¯†åº“å­˜å‚¨æµ‹è¯•çŸ¥è¯†ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚PPOå’ŒDQNï¼‰ä»è´¨é‡å·¥ç¨‹å¸ˆçš„åé¦ˆä¸­æŒç»­æ”¹è¿›ç”Ÿæˆç­–ç•¥ï¼Œæœ€ç»ˆåœ¨ä¼ä¸šçº§åº”ç”¨ä¸­å®ç°äº†æµ‹è¯•ç”Ÿæˆå‡†ç¡®ç‡æå‡2.4%å’Œç¼ºé™·æ£€æµ‹ç‡æé«˜10.8%çš„æ•ˆæœã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.05908v1">Natural Language Summarization Enables Multi-Repository Bug Localization by LLMs in Microservice Architectures</a></td><td><details><summary>å±•å¼€</summary>Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository. We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval. Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories. Evaluated on DNext, an industrial system with 46 repositories and 1.1M lines of code, our method achieves Pass@10 of 0.82 and MRR of 0.50, significantly outperforming retrieval baselines and agentic RAG systems like GitHub Copilot and Cursor. This work demonstrates that engineered natural language representations can be more effective than raw source code for scalable bug localization, providing an interpretable repository -> directory -> file search path, which is vital for building trust in enterprise AI tools by providing essential transparency.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å°†ä»£ç åº“è½¬åŒ–ä¸ºåˆ†å±‚è‡ªç„¶è¯­è¨€æ‘˜è¦å¹¶è¿›è¡Œè‡ªç„¶è¯­è¨€æœç´¢çš„æ–¹æ³•ï¼Œä»¥è§£å†³å¤šä»“åº“å¾®æœåŠ¡æ¶æ„ä¸­çš„é”™è¯¯å®šä½é—®é¢˜ã€‚è¯¥æ–¹æ³•é¦–å…ˆåœ¨æ–‡ä»¶ã€ç›®å½•å’Œä»“åº“çº§åˆ«æ„å»ºä¸Šä¸‹æ–‡æ„ŸçŸ¥æ‘˜è¦ï¼Œç„¶åé‡‡ç”¨ä¸¤é˜¶æ®µæœç´¢ç­–ç•¥ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„æ£€ç´¢æ–¹æ³•å’Œå…¶ä»–RAGç³»ç»Ÿï¼ˆå¦‚GitHub Copilotå’ŒCursorï¼‰ï¼ŒåŒæ—¶æé«˜äº†é”™è¯¯å®šä½çš„å¯è§£é‡Šæ€§å’Œå¯æ‰©å±•æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.05863v1">Optimizing Medical Question-Answering Systems: A Comparative Study of Fine-Tuned and Zero-Shot Large Language Models with RAG Framework</a></td><td><details><summary>å±•å¼€</summary>Medical question-answering (QA) systems can benefit from advances in large language models (LLMs), but directly applying LLMs to the clinical domain poses challenges such as maintaining factual accuracy and avoiding hallucinations. In this paper, we present a retrieval-augmented generation (RAG) based medical QA system that combines domain-specific knowledge retrieval with open-source LLMs to answer medical questions. We fine-tune two state-of-the-art open LLMs (LLaMA~2 and Falcon) using Low-Rank Adaptation (LoRA) for efficient domain specialization. The system retrieves relevant medical literature to ground the LLM's answers, thereby improving factual correctness and reducing hallucinations. We evaluate the approach on benchmark datasets (PubMedQA and MedMCQA) and show that retrieval augmentation yields measurable improvements in answer accuracy compared to using LLMs alone. Our fine-tuned LLaMA~2 model achieves 71.8% accuracy on PubMedQA, substantially improving over the 55.4% zero-shot baseline, while maintaining transparency by providing source references. We also detail the system design and fine-tuning methodology, demonstrating that grounding answers in retrieved evidence reduces unsupported content by approximately 60%. These results highlight the potential of RAG-augmented open-source LLMs for reliable biomedical QA, pointing toward practical clinical informatics applications.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGç›¸å…³å—ï¼šæ˜¯  </details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.05647v1">A Greek Government Decisions Dataset for Public-Sector Analysis and Insight</a></td><td><details><summary>å±•å¼€</summary>We introduce an open, machine-readable corpus of Greek government decisions sourced from the national transparency platform Diavgeia. The resource comprises 1 million decisions, featuring and high-quality raw text extracted from PDFs. It is released with raw extracted text in Markdown format, alongside a fully reproducible extraction pipeline. Beyond the core dataset, we conduct qualitative analyses to explore boilerplate patterns and design a retrieval-augmented generation (RAG) task by formulating a set of representative questions, creating high-quality answers, and evaluating a baseline RAG system on its ability to retrieve and reason over public decisions. This evaluation demonstrates the potential of large-scale public-sector corpora to support advanced information access and transparency through structured retrieval and reasoning over governmental documents, and highlights how such a RAG pipeline could simulate a chat-based assistant capable of interactively answering questions about public decisions. Due to its scale, quality, and domain coverage, the corpus can also serve as high-value pre-training or fine-tuning material for new Language Models (LMs) and Large Language Models (LLMs) respectively, including specialized models for legal and governmental domains, and as a foundation for novel approaches in domain adaptation, knowledge-grounded generation, and explainable AI. Finally, we discuss limitations, outline future directions, and make both the data and the code accessible.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä»å¸Œè…Šå›½å®¶é€æ˜åº¦å¹³å°Diavgeiaè·å–çš„100ä¸‡é¡¹æ”¿åºœå†³ç­–çš„å¼€æºæœºå™¨å¯è¯»è¯­æ–™åº“ï¼ŒåŒ…å«é«˜è´¨é‡PDFæå–æ–‡æœ¬å’Œå¯å¤ç°çš„æå–æµç¨‹ã€‚é™¤æ„å»ºæ ¸å¿ƒæ•°æ®é›†å¤–ï¼Œä½œè€…è¿˜é€šè¿‡åˆ†æå…¬æ–‡æ¨¡æ¿è®¾è®¡äº†ä¸€é¡¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ¶å®šä»£è¡¨æ€§é—®é¢˜ã€ç”Ÿæˆé«˜è´¨é‡ç­”æ¡ˆï¼Œå¹¶è¯„ä¼°åŸºçº¿RAGç³»ç»Ÿåœ¨æ£€ç´¢å’Œæ¨ç†æ”¿åºœå†³ç­–æ–¹é¢çš„è¡¨ç°ã€‚ç ”ç©¶å±•ç¤ºäº†æ­¤ç±»è¯­æ–™åº“åœ¨æå‡ä¿¡æ¯è·å–é€æ˜åº¦çš„æ½œåŠ›ï¼ŒåŒæ—¶æŒ‡å‡ºè¯¥èµ„æºå¯ä½œä¸ºæ³•å¾‹/æ”¿åºœé¢†åŸŸè¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶æ”¯æŒé¢†åŸŸé€‚åº”ã€çŸ¥è¯†é©±åŠ¨ç”Ÿæˆç­‰ç ”ç©¶ã€‚è®ºæ–‡è¿˜å…¬å¼€äº†æ•°æ®å’Œä»£ç ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.05430v1">ArtistMus: A Globally Diverse, Artist-Centric Benchmark for Retrieval-Augmented Music Question Answering</a></td><td><details><summary>å±•å¼€</summary>Recent advances in large language models (LLMs) have transformed open-domain question answering, yet their effectiveness in music-related reasoning remains limited due to sparse music knowledge in pretraining data. While music information retrieval and computational musicology have explored structured and multimodal understanding, few resources support factual and contextual music question answering (MQA) grounded in artist metadata or historical context. We introduce MusWikiDB, a vector database of 3.2M passages from 144K music-related Wikipedia pages, and ArtistMus, a benchmark of 1,000 questions on 500 diverse artists with metadata such as genre, debut year, and topic. These resources enable systematic evaluation of retrieval-augmented generation (RAG) for MQA. Experiments show that RAG markedly improves factual accuracy; open-source models gain up to +56.8 percentage points (for example, Qwen3 8B improves from 35.0 to 91.8), approaching proprietary model performance. RAG-style fine-tuning further boosts both factual recall and contextual reasoning, improving results on both in-domain and out-of-domain benchmarks. MusWikiDB also yields approximately 6 percentage points higher accuracy and 40% faster retrieval than a general-purpose Wikipedia corpus. We release MusWikiDB and ArtistMus to advance research in music information retrieval and domain-specific question answering, establishing a foundation for retrieval-augmented reasoning in culturally rich domains such as music.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯è§£å†³éŸ³ä¹ç›¸å…³é—®ç­”ï¼ˆMQAï¼‰çš„å±€é™æ€§ï¼Œé€šè¿‡æ„å»ºéŸ³ä¹ä¸“ç”¨å‘é‡æ•°æ®åº“MusWikiDBå’ŒåŸºå‡†æµ‹è¯•ArtistMusï¼Œæ˜¾è‘—æå‡äº†å¼€æºæ¨¡å‹åœ¨éŸ³ä¹äº‹å®å‡†ç¡®æ€§ï¼ˆå¦‚Qwen3 8Bä»35.0æå‡è‡³91.8ï¼‰å’Œä¸Šä¸‹æ–‡æ¨ç†ä¸Šçš„è¡¨ç°ï¼Œå¹¶éªŒè¯äº†RAGåœ¨ä¸“ä¸šé¢†åŸŸï¼ˆå¦‚éŸ³ä¹ï¼‰ä¸­çš„é«˜æ•ˆæ€§ï¼ˆæ£€ç´¢é€Ÿåº¦æå‡40%ï¼‰å’Œä¼˜è¶Šæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.05411v1">A Systematic Framework for Enterprise Knowledge Retrieval: Leveraging LLM-Generated Metadata to Enhance RAG Systems</a></td><td><details><summary>å±•å¼€</summary>In enterprise settings, efficiently retrieving relevant information from large and complex knowledge bases is essential for operational productivity and informed decision-making. This research presents a systematic framework for metadata enrichment using large language models (LLMs) to enhance document retrieval in Retrieval-Augmented Generation (RAG) systems. Our approach employs a comprehensive, structured pipeline that dynamically generates meaningful metadata for document segments, substantially improving their semantic representations and retrieval accuracy. Through extensive experiments, we compare three chunking strategies-semantic, recursive, and naive-and evaluate their effectiveness when combined with advanced embedding techniques. The results demonstrate that metadata-enriched approaches consistently outperform content-only baselines, with recursive chunking paired with TF-IDF weighted embeddings yielding an 82.5% precision rate compared to 73.3% for semantic content-only approaches. The naive chunking strategy with prefix-fusion achieved the highest Hit Rate@10 of 0.925. Our evaluation employs cross-encoder reranking for ground truth generation, enabling rigorous assessment via Hit Rate and Metadata Consistency metrics. These findings confirm that metadata enrichment enhances vector clustering quality while reducing retrieval latency, making it a key optimization for RAG systems across knowledge domains. This work offers practical insights for deploying high-performance, scalable document retrieval solutions in enterprise settings, demonstrating that metadata enrichment is a powerful approach for enhancing RAG effectiveness.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œå…ƒæ•°æ®å¢å¼ºçš„ç³»ç»Ÿæ€§æ¡†æ¶ï¼Œæ—¨åœ¨æå‡RAGç³»ç»Ÿä¸­çš„æ–‡æ¡£æ£€ç´¢æ•ˆç‡ã€‚é€šè¿‡åŠ¨æ€ç”Ÿæˆæ–‡æ¡£ç‰‡æ®µçš„å…ƒæ•°æ®ä¼˜åŒ–è¯­ä¹‰è¡¨ç¤ºå’Œæ£€ç´¢ç²¾åº¦ï¼Œå®éªŒæ¯”è¾ƒäº†ä¸‰ç§æ–‡æœ¬åˆ†å—ç­–ç•¥ï¼ˆè¯­ä¹‰ã€é€’å½’ã€æœ´ç´ ï¼‰ä¸åµŒå…¥æŠ€æœ¯çš„ç»“åˆæ•ˆæœï¼Œç»“æœè¡¨æ˜é€’å½’åˆ†å—ç»“åˆTF-IDFåŠ æƒåµŒå…¥çš„ç²¾åº¦æœ€é«˜ï¼ˆ82.5%ï¼‰ï¼Œä¸”å…ƒæ•°æ®å¢å¼ºæ˜¾è‘—æé«˜äº†å‘é‡èšç±»è´¨é‡å¹¶é™ä½æ£€ç´¢å»¶è¿Ÿã€‚ç ”ç©¶ä¸ºä¼ä¸šéƒ¨ç½²é«˜æ€§èƒ½RAGç³»ç»Ÿæä¾›äº†å®è·µæŒ‡å¯¼ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.05371v1">ChipMind: Retrieval-Augmented Reasoning for Long-Context Circuit Design Specifications</a></td><td><details><summary>å±•å¼€</summary>While Large Language Models (LLMs) demonstrate immense potential for automating integrated circuit (IC) development, their practical deployment is fundamentally limited by restricted context windows. Existing context-extension methods struggle to achieve effective semantic modeling and thorough multi-hop reasoning over extensive, intricate circuit specifications. To address this, we introduce ChipMind, a novel knowledge graph-augmented reasoning framework specifically designed for lengthy IC specifications. ChipMind first transforms circuit specifications into a domain-specific knowledge graph ChipKG through the Circuit Semantic-Aware Knowledge Graph Construction methodology. It then leverages the ChipKG-Augmented Reasoning mechanism, combining information-theoretic adaptive retrieval to dynamically trace logical dependencies with intent-aware semantic filtering to prune irrelevant noise, effectively balancing retrieval completeness and precision. Evaluated on an industrial-scale specification reasoning benchmark, ChipMind significantly outperforms state-of-the-art baselines, achieving an average improvement of 34.59% (up to 72.73%). Our framework bridges a critical gap between academic research and practical industrial deployment of LLM-aided Hardware Design (LAD).</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºChipMindçš„çŸ¥è¯†å›¾å¢å¼ºæ¨ç†æ¡†æ¶ï¼Œç”¨äºå¤„ç†é›†æˆç”µè·¯ï¼ˆICï¼‰å¼€å‘ä¸­çš„é•¿ç¯‡å¹…è§„èŒƒé—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†ç”µè·¯è§„èŒƒè½¬æ¢ä¸ºç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†å›¾ï¼ˆChipKGï¼‰ï¼Œå¹¶ç»“åˆä¿¡æ¯ç†è®ºè‡ªé€‚åº”æ£€ç´¢ä¸æ„å›¾æ„ŸçŸ¥è¯­ä¹‰è¿‡æ»¤ï¼ŒåŠ¨æ€è¿½è¸ªé€»è¾‘ä¾èµ–å¹¶å‡å°‘æ— å…³å™ªå£°ï¼Œæå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç¡¬ä»¶è®¾è®¡ä¸­çš„è¯­ä¹‰å»ºæ¨¡å’Œå¤šè·³æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒChipMindåœ¨å·¥ä¸šçº§è§„èŒƒæ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†æœ€é«˜72.73%çš„æ€§èƒ½æå‡ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-04
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.05012v1">Factuality and Transparency Are All RAG Needs! Self-Explaining Contrastive Evidence Re-ranking</a></td><td><details><summary>å±•å¼€</summary>This extended abstract introduces Self-Explaining Contrastive Evidence Re-Ranking (CER), a novel method that restructures retrieval around factual evidence by fine-tuning embeddings with contrastive learning and generating token-level attribution rationales for each retrieved passage. Hard negatives are automatically selected using a subjectivity-based criterion, forcing the model to pull factual rationales closer while pushing subjective or misleading explanations apart. As a result, the method creates an embedding space explicitly aligned with evidential reasoning. We evaluated our method on clinical trial reports, and initial experimental results show that CER improves retrieval accuracy, mitigates the potential for hallucinations in RAG systems, and provides transparent, evidence-based retrieval that enhances reliability, especially in safety-critical domains.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè‡ªæˆ‘è§£é‡Šå¯¹æ¯”è¯æ®é‡æ’åºï¼ˆCERï¼‰çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å¾®è°ƒåµŒå…¥å‘é‡å¹¶ä¸ºæ£€ç´¢æ®µè½ç”Ÿæˆè¯çº§å½’å› è§£é‡Šï¼Œé‡æ„äº†åŸºäºäº‹å®è¯æ®çš„æ£€ç´¢è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä¸»è§‚æ€§æ ‡å‡†è‡ªåŠ¨é€‰æ‹©å›°éš¾è´Ÿæ ·æœ¬ï¼Œä¼˜åŒ–åµŒå…¥ç©ºé—´ä»¥å¯¹é½è¯æ®æ¨ç†ï¼Œå®éªŒè¡¨æ˜CERèƒ½æå‡æ£€ç´¢ç²¾åº¦ã€å‡å°‘RAGç³»ç»Ÿå¹»è§‰é£é™©ï¼Œå¹¶åœ¨å®‰å…¨å…³é”®é¢†åŸŸï¼ˆå¦‚ä¸´åºŠè¯•éªŒæŠ¥å‘Šï¼‰æä¾›æ›´é€æ˜å¯é çš„æ£€ç´¢ç»“æœã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.04790v1">Spatially-Enhanced Retrieval-Augmented Generation for Walkability and Urban Discovery</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) have become foundational tools in artificial intelligence, supporting a wide range of applications beyond traditional natural language processing, including urban systems and tourist recommendations. However, their tendency to hallucinate and their limitations in spatial retrieval and reasoning are well known, pointing to the need for novel solutions. Retrieval-augmented generation (RAG) has recently emerged as a promising way to enhance LLMs with accurate, domain-specific, and timely information. Spatial RAG extends this approach to tasks involving geographic understanding. In this work, we introduce WalkRAG, a spatial RAG-based framework with a conversational interface for recommending walkable urban itineraries. Users can request routes that meet specific spatial constraints and preferences while interactively retrieving information about the path and points of interest (POIs) along the way. Preliminary results show the effectiveness of combining information retrieval, spatial reasoning, and LLMs to support urban discovery.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†WalkRAGï¼Œä¸€ä¸ªåŸºäºç©ºé—´RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰çš„æ¡†æ¶ï¼Œç”¨äºæ¨èå¯æ­¥è¡Œçš„åŸå¸‚è·¯çº¿ã€‚å®ƒç»“åˆäº†ä¿¡æ¯æ£€ç´¢ã€ç©ºé—´æ¨ç†å’Œå¤§è¯­è¨€æ¨¡å‹ï¼Œå…è®¸ç”¨æˆ·æ ¹æ®ç‰¹å®šç©ºé—´é™åˆ¶å’Œåå¥½äº¤äº’å¼è·å–è·¯å¾„åŠæ²¿é€”å…´è¶£ç‚¹ä¿¡æ¯ï¼Œåˆæ­¥ç»“æœéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.04738v1">OsmT: Bridging OpenStreetMap Queries and Natural Language with Open-source Tag-aware Language Models</a></td><td><details><summary>å±•å¼€</summary>Bridging natural language and structured query languages is a long-standing challenge in the database community. While recent advances in language models have shown promise in this direction, existing solutions often rely on large-scale closed-source models that suffer from high inference costs, limited transparency, and lack of adaptability for lightweight deployment. In this paper, we present OsmT, an open-source tag-aware language model specifically designed to bridge natural language and Overpass Query Language (OverpassQL), a structured query language for accessing large-scale OpenStreetMap (OSM) data. To enhance the accuracy and structural validity of generated queries, we introduce a Tag Retrieval Augmentation (TRA) mechanism that incorporates contextually relevant tag knowledge into the generation process. This mechanism is designed to capture the hierarchical and relational dependencies present in the OSM database, addressing the topological complexity inherent in geospatial query formulation. In addition, we define a reverse task, OverpassQL-to-Text, which translates structured queries into natural language explanations to support query interpretation and improve user accessibility. We evaluate OsmT on a public benchmark against strong baselines and observe consistent improvements in both query generation and interpretation. Despite using significantly fewer parameters, our model achieves competitive accuracy, demonstrating the effectiveness of open-source pre-trained language models in bridging natural language and structured query languages within schema-rich geospatial environments.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†OsmTï¼Œä¸€ä¸ªå¼€æºçš„æ ‡ç­¾æ„ŸçŸ¥è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è¿æ¥è‡ªç„¶è¯­è¨€å’ŒOverpassQLï¼ˆä¸€ç§ç”¨äºè®¿é—®OpenStreetMapæ•°æ®çš„ç»“æ„åŒ–æŸ¥è¯¢è¯­è¨€ï¼‰ã€‚ä¸ºäº†æé«˜ç”ŸæˆæŸ¥è¯¢çš„å‡†ç¡®æ€§å’Œç»“æ„æœ‰æ•ˆæ€§ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ ‡ç­¾æ£€ç´¢å¢å¼ºï¼ˆTRAï¼‰æœºåˆ¶ï¼Œå°†ä¸Šä¸‹æ–‡ç›¸å…³çš„æ ‡ç­¾çŸ¥è¯†èå…¥ç”Ÿæˆè¿‡ç¨‹ï¼Œä»¥è§£å†³åœ°ç†ç©ºé—´æŸ¥è¯¢ä¸­çš„æ‹“æ‰‘å¤æ‚æ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å®šä¹‰äº†ä¸€ä¸ªåå‘ä»»åŠ¡ï¼ˆOverpassQL-to-Textï¼‰ï¼Œå°†ç»“æ„åŒ–æŸ¥è¯¢è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€è§£é‡Šã€‚å®éªŒè¡¨æ˜ï¼ŒOsmTåœ¨å‚æ•°è¾ƒå°‘çš„æƒ…å†µä¸‹ä»èƒ½å®ç°ç«äº‰æ€§ç²¾åº¦ï¼ŒéªŒè¯äº†å¼€æºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨å¤æ‚åœ°ç†ç©ºé—´ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.04416v1">GovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows</a></td><td><details><summary>å±•å¼€</summary>Data governance ensures data quality, security, and compliance through policies and standards, a critical foundation for scaling modern AI development. Recently, large language models (LLMs) have emerged as a promising solution for automating data governance by translating user intent into executable transformation code. However, existing benchmarks for automated data science often emphasize snippet-level coding or high-level analytics, failing to capture the unique challenge of data governance: ensuring the correctness and quality of the data itself. To bridge this gap, we introduce GovBench, a benchmark featuring 150 diverse tasks grounded in real-world scenarios, built on data from actual cases. GovBench employs a novel "reversed-objective" methodology to synthesize realistic noise and utilizes rigorous metrics to assess end-to-end pipeline reliability. Our analysis on GovBench reveals that current models struggle with complex, multi-step workflows and lack robust error-correction mechanisms. Consequently, we propose DataGovAgent, a framework utilizing a Planner-Executor-Evaluator architecture that integrates constraint-based planning, retrieval-augmented generation, and sandboxed feedback-driven debugging. Experimental results show that DataGovAgent significantly boosts the Average Task Score (ATS) on complex tasks from 39.7 to 54.9 and reduces debugging iterations by over 77.9 percent compared to general-purpose baselines.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºDataGovAgentæ¡†æ¶ï¼Œé€šè¿‡Planner-Executor-Evaluatoræ¶æ„æ•´åˆçº¦æŸè§„åˆ’ã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œæ²™ç®±åé¦ˆé©±åŠ¨è°ƒè¯•ï¼Œä»¥è§£å†³æ•°æ®æ²»ç†ä¸­è‡ªåŠ¨åŒ–ä»»åŠ¡çš„æŒ‘æˆ˜ï¼Œå¹¶åœ¨GovBenchåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡ä»»åŠ¡è¡¨ç°å’Œæ•ˆç‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.04343v1">The Personalization Paradox: Semantic Loss vs. Reasoning Gains in Agentic AI Q&A</a></td><td><details><summary>å±•å¼€</summary>AIVisor, an agentic retrieval-augmented LLM for student advising, was used to examine how personalization affects system performance across multiple evaluation dimensions. Using twelve authentic advising questions intentionally designed to stress lexical precision, we compared ten personalized and non-personalized system configurations and analyzed outcomes with a Linear Mixed-Effects Model across lexical (BLEU, ROUGE-L), semantic (METEOR, BERTScore), and grounding (RAGAS) metrics. Results showed a consistent trade-off: personalization reliably improved reasoning quality and grounding, yet introduced a significant negative interaction on semantic similarity, driven not by poorer answers but by the limits of current metrics, which penalize meaningful personalized deviations from generic reference texts. This reveals a structural flaw in prevailing LLM evaluation methods, which are ill-suited for assessing user-specific responses. The fully integrated personalized configuration produced the highest overall gains, suggesting that personalization can enhance system effectiveness when evaluated with appropriate multidimensional metrics. Overall, the study demonstrates that personalization produces metric-dependent shifts rather than uniform improvements and provides a methodological foundation for more transparent and robust personalization in agentic AI.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æ¢è®¨äº†åŸºäºRAGæŠ€æœ¯çš„AIé¡¾é—®ç³»ç»ŸAIVisoråœ¨ä¸ªæ€§åŒ–è®¾ç½®ä¸‹å¯¹å­¦ç”Ÿå’¨è¯¢ä»»åŠ¡æ€§èƒ½çš„å½±å“ï¼Œé€šè¿‡å¤šç»´åº¦è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚è¯­ä¹‰ç›¸ä¼¼åº¦ã€æ¨ç†è´¨é‡å’Œæ•°æ®åŸºç¡€æ€§ï¼‰åˆ†æå‘ç°ï¼Œä¸ªæ€§åŒ–è™½æå‡æ¨ç†å’ŒåŸºç¡€æ€§ï¼Œä½†ä¼šå› ä¸é€šç”¨å‚è€ƒç­”æ¡ˆçš„åç¦»è€Œé™ä½è¯­ä¹‰è¯„åˆ†ï¼Œæ­ç¤ºäº†å½“å‰LLMè¯„ä¼°æ–¹æ³•åœ¨ä¸ªæ€§åŒ–å“åº”è¯„ä¼°ä¸Šçš„ç»“æ„æ€§ç¼ºé™·ï¼Œå¹¶å¼ºè°ƒäº†é‡‡ç”¨å¤šç»´åº¦æŒ‡æ ‡çš„é‡è¦æ€§ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-03
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.03887v1">A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)</a></td><td><details><summary>å±•å¼€</summary>The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (RAG) pipelines in handling complex, multi-turn research tasks. This paper introduces the Static Deep Research Agent (Static-DRA), a novel solution built upon a configurable and hierarchical Tree-based static workflow.
  The core contribution is the integration of two user-tunable parameters, Depth and Breadth, which provide granular control over the research intensity. This design allows end-users to consciously balance the desired quality and comprehensiveness of the research report against the associated computational cost of Large Language Model (LLM) interactions. The agent's architecture, comprising Supervisor, Independent, and Worker agents, facilitates effective multi-hop information retrieval and parallel sub-topic investigation.
  We evaluate the Static-DRA against the established DeepResearch Bench using the RACE (Reference-based Adaptive Criteria-driven Evaluation) framework. Configured with a depth of 2 and a breadth of 5, and powered by the gemini-2.5-pro model, the agent achieved an overall score of 34.72. Our experiments validate that increasing the configured Depth and Breadth parameters results in a more in-depth research process and a correspondingly higher evaluation score. The Static-DRA offers a pragmatic and resource-aware solution, empowering users with transparent control over the deep research process. The entire source code, outputs and benchmark results are open-sourced at https://github.com/SauravP97/Static-Deep-Research/</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºStatic-DRAçš„æ–°å‹æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“ï¼Œé€šè¿‡æ ‘å½¢é™æ€å·¥ä½œæµå’Œå¯è°ƒå‚æ•°ï¼ˆDepth/Breadthï¼‰æ”¹è¿›ä¼ ç»Ÿé™æ€RAGæµç¨‹ï¼Œåœ¨å¤æ‚å¤šè½®ç ”ç©¶ä»»åŠ¡ä¸­å®ç°å¯æ§çš„ä¿¡æ¯æ£€ç´¢æ·±åº¦ä¸å¹¿åº¦ï¼Œå¹¶éªŒè¯äº†å‚æ•°æå‡å¯¹ç ”ç©¶è´¨é‡çš„æ­£å‘å½±å“ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.03874v1">OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance</a></td><td><details><summary>å±•å¼€</summary>Dexterous grasp generation aims to produce grasp poses that align with task requirements and human interpretable grasp semantics. However, achieving semantically controllable dexterous grasp synthesis remains highly challenging due to the lack of unified modeling of multiple semantic dimensions, including grasp taxonomy, contact semantics, and functional affordance. To address these limitations, we present OmniDexVLG, a multimodal, semantics aware grasp generation framework capable of producing structurally diverse and semantically coherent dexterous grasps under joint language and visual guidance. Our approach begins with OmniDexDataGen, a semantic rich dexterous grasp dataset generation pipeline that integrates grasp taxonomy guided configuration sampling, functional affordance contact point sampling, taxonomy aware differential force closure grasp sampling, and physics based optimization and validation, enabling systematic coverage of diverse grasp types. We further introduce OmniDexReasoner, a multimodal grasp type semantic reasoning module that leverages multi agent collaboration, retrieval augmented generation, and chain of thought reasoning to infer grasp related semantics and generate high quality annotations that align language instructions with task specific grasp intent. Building upon these components, we develop a unified Vision Language Grasping generation model that explicitly incorporates grasp taxonomy, contact structure, and functional affordance semantics, enabling fine grained control over grasp synthesis from natural language instructions. Extensive experiments in simulation and real world object grasping and ablation studies demonstrate that our method substantially outperforms state of the art approaches in terms of grasp diversity, contact semantic diversity, functional affordance diversity, and semantic consistency.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†OmniDexVLGæ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€å’Œè¯­ä¹‰æ„ŸçŸ¥æ–¹æ³•ç”Ÿæˆçµå·§æŠ“å–å§¿åŠ¿ï¼Œç»“åˆè¯­è¨€å’Œè§†è§‰æŒ‡å¯¼å®ç°è¯­ä¹‰æ§åˆ¶ã€‚å…¶ä¸­ï¼ŒOmniDexReasoneræ¨¡å—åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯æ¨æ–­æŠ“å–ç›¸å…³è¯­ä¹‰å¹¶ç”Ÿæˆé«˜è´¨é‡æ³¨é‡Šï¼Œæå‡ä¸ä»»åŠ¡ç‰¹å®šæŠ“å–æ„å›¾çš„è¯­è¨€å¯¹é½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.03737v1">AR-Med: Automated Relevance Enhancement in Medical Search via LLM-Driven Information Augmentation</a></td><td><details><summary>å±•å¼€</summary>Accurate and reliable search on online healthcare platforms is critical for user safety and service efficacy. Traditional methods, however, often fail to comprehend complex and nuanced user queries, limiting their effectiveness. Large language models (LLMs) present a promising solution, offering powerful semantic understanding to bridge this gap. Despite their potential, deploying LLMs in this high-stakes domain is fraught with challenges, including factual hallucinations, specialized knowledge gaps, and high operational costs. To overcome these barriers, we introduce \textbf{AR-Med}, a novel framework for \textbf{A}utomated \textbf{R}elevance assessment for \textbf{Med}ical search that has been successfully deployed at scale on the Online Medical Delivery Platforms. AR-Med grounds LLM reasoning in verified medical knowledge through a retrieval-augmented approach, ensuring high accuracy and reliability. To enable efficient online service, we design a practical knowledge distillation scheme that compresses large teacher models into compact yet powerful student models. We also introduce LocalQSMed, a multi-expert annotated benchmark developed to guide model iteration and ensure strong alignment between offline and online performance. Extensive experiments show AR-Med achieves an offline accuracy of over 93\%, a 24\% absolute improvement over the original online system, and delivers significant gains in online relevance and user satisfaction. Our work presents a practical and scalable blueprint for developing trustworthy, LLM-powered systems in real-world healthcare applications.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†AR-Medæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ£€ç´¢å¢å¼ºçš„æ–¹æ³•å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†åŸºäºç»è¿‡éªŒè¯çš„åŒ»å­¦çŸ¥è¯†ï¼Œä»¥æé«˜åŒ»ç–—æœç´¢çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚æ–‡ç« è¿˜è®¨è®ºäº†çŸ¥è¯†è’¸é¦æ–¹æ¡ˆå’Œå¤šä¸“å®¶æ ‡æ³¨çš„åŸºå‡†æµ‹è¯•ï¼Œå±•ç¤ºäº†AR-Medåœ¨ç¦»çº¿å’Œåœ¨çº¿æ€§èƒ½ä¸Šçš„æ˜¾è‘—æå‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.03454v1">Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles</a></td><td><details><summary>å±•å¼€</summary>Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ThinkDeeperæ¡†æ¶ï¼Œé€šè¿‡ç©ºé—´æ„ŸçŸ¥ä¸–ç•Œæ¨¡å‹ï¼ˆSA-WMï¼‰å’Œè¶…å›¾å¼•å¯¼è§£ç å™¨è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­è§†è§‰å®šä½çš„æ¨¡ç³Šæ€§é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†åŸºäºRAGå’Œæ€ç»´é“¾æç¤ºçš„LLMç®¡é“ç”Ÿæˆçš„DrivePilotæ•°æ®é›†ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.03413v1">BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents</a></td><td><details><summary>å±•å¼€</summary>As an effective method to boost the performance of Large Language Models (LLMs) on the question answering (QA) task, Retrieval-Augmented Generation (RAG), which queries highly relevant information from external complex documents, has attracted tremendous attention from both industry and academia. Existing RAG approaches often focus on general documents, and they overlook the fact that many real-world documents (such as books, booklets, handbooks, etc.) have a hierarchical structure, which organizes their content from different granularity levels, leading to poor performance for the QA task. To address these limitations, we introduce BookRAG, a novel RAG approach targeted for documents with a hierarchical structure, which exploits logical hierarchies and traces entity relations to query the highly relevant information. Specifically, we build a novel index structure, called BookIndex, by extracting a hierarchical tree from the document, which serves as the role of its table of contents, using a graph to capture the intricate relationships between entities, and mapping entities to tree nodes. Leveraging the BookIndex, we then propose an agent-based query method inspired by the Information Foraging Theory, which dynamically classifies queries and employs a tailored retrieval workflow. Extensive experiments on three widely adopted benchmarks demonstrate that BookRAG achieves state-of-the-art performance, significantly outperforming baselines in both retrieval recall and QA accuracy while maintaining competitive efficiency.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºBookRAGçš„æ–°å‹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ï¼Œä¸“é—¨é’ˆå¯¹å…·æœ‰å±‚æ¬¡ç»“æ„çš„æ–‡æ¡£ï¼ˆå¦‚ä¹¦ç±ã€æ‰‹å†Œç­‰ï¼‰ã€‚é€šè¿‡æ„å»ºBookIndexç´¢å¼•ç»“æ„ï¼ˆæå–æ–‡æ¡£çš„å±‚æ¬¡æ ‘å’Œå®ä½“å…³ç³»å›¾ï¼‰å¹¶ç»“åˆåŸºäºä¿¡æ¯è§…é£Ÿç†è®ºçš„ä»£ç†æŸ¥è¯¢æ–¹æ³•ï¼ŒBookRAGæ˜¾è‘—æå‡äº†å±‚æ¬¡åŒ–æ–‡æ¡£é—®ç­”ä»»åŠ¡ä¸­çš„æ£€ç´¢å¬å›ç‡å’Œå›ç­”å‡†ç¡®æ€§ï¼Œå®éªŒè¯æ˜å…¶æ€§èƒ½ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-02
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.02906v1">MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding</a></td><td><details><summary>å±•å¼€</summary>Understanding high-resolution images remains a significant challenge for multimodal large language models (MLLMs). Recent study address this issue by dividing the image into smaller crops and computing the semantic similarity between each crop and a query using a pretrained retrieval-augmented generation (RAG) model. The most relevant crops are then selected to localize the target object and suppress irrelevant information. However, such crop-based processing can fragment complete objects across multiple crops, thereby disrupting the computation of semantic similarity. In our experiments, we find that image crops of objects with different sizes are better handled at different resolutions. Based on this observation, we propose Multi-resolution Retrieval-Detection (MRD), a training-free framework for high-resolution image understanding. To address the issue of semantic similarity bias caused by objects being split across different image crops, we propose a multi-resolution semantic fusion method, which integrates semantic similarity maps obtained at different resolutions to produce more accurate semantic information and preserve the integrity of target objects. Furthermore, to achieve direct localization of target objects at a global scale, we introduce an open-vocalbulary object detection (OVD) model that identifies object regions using a sliding-window approach.Experiments on high-resolution image understanding benchmarks using different MLLMs demonstrate the effectiveness of our approach.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºå¤šåˆ†è¾¨ç‡æ£€ç´¢æ£€æµ‹ï¼ˆMRDï¼‰çš„è®­ç»ƒå…è´¹æ¡†æ¶ï¼Œç”¨äºè§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶é‡åˆ°çš„æŒ‘æˆ˜ã€‚é€šè¿‡ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¨¡å‹å’Œå¤šåˆ†è¾¨ç‡è¯­ä¹‰èåˆæ–¹æ³•ï¼ŒMRDèƒ½å¤Ÿæ›´å‡†ç¡®åœ°å®šä½ç›®æ ‡å¯¹è±¡å¹¶ä¿æŒå…¶å®Œæ•´æ€§ï¼ŒåŒæ—¶ä½¿ç”¨å¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹ï¼ˆOVDï¼‰æ¨¡å‹ç›´æ¥åœ¨å…¨å±€èŒƒå›´å†…è¯†åˆ«å¯¹è±¡åŒºåŸŸï¼Œä»è€Œæé«˜é«˜åˆ†è¾¨ç‡å›¾åƒçš„ç†è§£èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.02799v1">TriLex: A Framework for Multilingual Sentiment Analysis in Low-Resource South African Languages</a></td><td><details><summary>å±•å¼€</summary>Low-resource African languages remain underrepresented in sentiment analysis, limiting both lexical coverage and the performance of multilingual Natural Language Processing (NLP) systems. This study proposes TriLex, a three-stage retrieval augmented framework that unifies corpus-based extraction, cross lingual mapping, and retrieval augmented generation (RAG) driven lexical refinement to systematically expand sentiment lexicons for low-resource languages. Using the enriched lexicon, the performance of two prominent African pretrained language models (AfroXLMR and AfriBERTa) is evaluated across multiple case studies. Results demonstrate that AfroXLMR delivers superior performance, achieving F1-scores above 80% for isiXhosa and isiZulu and exhibiting strong cross-lingual stability. Although AfriBERTa lacks pre-training on these target languages, it still achieves reliable F1-scores around 64%, validating its utility in computationally constrained settings. Both models outperform traditional machine learning baselines, and ensemble analyses further enhance precision and robustness. The findings establish TriLex as a scalable and effective framework for multilingual sentiment lexicon expansion and sentiment modeling in low-resource South African languages.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºTriLexçš„ä¸‰é˜¶æ®µæ£€ç´¢å¢å¼ºæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç»“åˆåŸºäºè¯­æ–™åº“çš„æå–ã€è·¨è¯­è¨€æ˜ å°„å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é©±åŠ¨çš„è¯æ±‡ä¼˜åŒ–ï¼Œç³»ç»Ÿåœ°æ‰©å±•ä½èµ„æºéæ´²è¯­è¨€çš„æƒ…æ„Ÿè¯å…¸ã€‚ç ”ç©¶åˆ©ç”¨å¢å¼ºåçš„è¯å…¸è¯„ä¼°äº†ä¸¤ç§éæ´²é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆAfroXLMRå’ŒAfriBERTaï¼‰çš„æ€§èƒ½ï¼Œç»“æœæ˜¾ç¤ºAfroXLMRè¡¨ç°ä¼˜å¼‚ï¼Œè€ŒTriLexæ¡†æ¶ä¸ºä½èµ„æºè¯­è¨€çš„è·¨è¯­è¨€æƒ…æ„Ÿåˆ†ææä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.02720v1">StockMem: An Event-Reflection Memory Framework for Stock Forecasting</a></td><td><details><summary>å±•å¼€</summary>Stock price prediction is challenging due to market volatility and its sensitivity to real-time events. While large language models (LLMs) offer new avenues for text-based forecasting, their application in finance is hindered by noisy news data and the lack of explicit answers in text. General-purpose memory architectures struggle to identify the key drivers of price movements. To address this, we propose StockMem, an event-reflection dual-layer memory framework. It structures news into events and mines them along two dimensions: horizontal consolidation integrates daily events, while longitudinal tracking captures event evolution to extract incremental information reflecting market expectation discrepancies. This builds a temporal event knowledge base. By analyzing event-price dynamics, the framework further forms a reflection knowledge base of causal experiences. For prediction, it retrieves analogous historical scenarios and reasons with current events, incremental data, and past experiences. Experiments show StockMem outperforms existing memory architectures and provides superior, explainable reasoning by tracing the information chain affecting prices, enhancing decision transparency in financial forecasting.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºStockMemçš„åŒå±‚è®°å¿†æ¡†æ¶ï¼Œç”¨äºè‚¡ç¥¨ä»·æ ¼é¢„æµ‹ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“æ„åŒ–æ–°é—»äº‹ä»¶å¹¶è¿›è¡Œæ¨ªå‘æ•´åˆå’Œçºµå‘è¿½è¸ªï¼Œæ„å»ºäº†ä¸€ä¸ªæ—¶åºäº‹ä»¶çŸ¥è¯†åº“ï¼Œå¹¶é€šè¿‡åˆ†æäº‹ä»¶ä¸ä»·æ ¼çš„åŠ¨æ€å…³ç³»å½¢æˆå› æœç»éªŒçŸ¥è¯†åº“ã€‚åœ¨é¢„æµ‹æ—¶ï¼Œå®ƒæ£€ç´¢ç±»ä¼¼çš„å†å²åœºæ™¯ï¼Œå¹¶ç»“åˆå½“å‰äº‹ä»¶ã€å¢é‡æ•°æ®å’Œè¿‡å»ç»éªŒè¿›è¡Œæ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒStockMemä¼˜äºç°æœ‰çš„è®°å¿†æ¶æ„ï¼Œå¹¶é€šè¿‡è¿½è¸ªå½±å“ä»·æ ¼çš„ä¿¡æ¯é“¾æä¾›äº†å¯è§£é‡Šçš„æ¨ç†ï¼Œå¢å¼ºäº†é‡‘èé¢„æµ‹çš„å†³ç­–é€æ˜åº¦ã€‚è¿™ä¸€æ–¹æ³•ç»“åˆäº†æ£€ç´¢ï¼ˆç±»ä¼¼å†å²åœºæ™¯ï¼‰ä¸ç”Ÿæˆï¼ˆæ¨ç†é¢„æµ‹ï¼‰ï¼Œå±äºRAGæŠ€æœ¯çš„åº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.02660v1">Spatially-Grounded Document Retrieval via Patch-to-Region Relevance Propagation</a></td><td><details><summary>å±•å¼€</summary>Vision-language models (VLMs) like ColPali achieve state-of-the-art document retrieval by embedding pages as images and computing fine-grained similarity between query tokens and visual patches. However, they return entire pages rather than specific regions, limiting utility for retrieval-augmented generation (RAG) where precise context is paramount. Conversely, OCR-based systems extract structured text with bounding box coordinates but lack semantic grounding for relevance assessment. We propose a hybrid architecture that unifies these paradigms: using ColPali's patch-level similarity scores as spatial relevance filters over OCR-extracted regions. We formalize the coordinate mapping between vision transformer patch grids and OCR bounding boxes, introduce intersection metrics for relevance propagation, and establish theoretical bounds on retrieval precision. Our approach operates at inference time without additional training. We release Snappy, an open-source implementation demonstrating practical applicability, with empirical evaluation ongoing.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’ŒOCRæŠ€æœ¯çš„æ··åˆæ¶æ„ï¼Œé€šè¿‡å°†ColPaliçš„è§†è§‰å—çº§ç›¸ä¼¼åº¦è¯„åˆ†ä¸OCRæå–çš„æ–‡æœ¬åŒºåŸŸç©ºé—´åæ ‡å¯¹é½ï¼Œå®ç°æ›´ç²¾å‡†çš„æ–‡æ¡£ç‰‡æ®µæ£€ç´¢ï¼Œä»¥ä¼˜åŒ–æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦ä¸è¶³çš„é—®é¢˜ï¼Œå¹¶å¼€å‘äº†å¼€æºå·¥å…·Snappyè¿›è¡Œå®è·µéªŒè¯ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.02561v1">EZYer: A simulacrum of high school with generative agent</a></td><td><details><summary>å±•å¼€</summary>With the rapid development of the online education and large language model, the existing educational tools still suffer from incomplete service, insufficient performance and weak interactivity in terms of courseware generation, interactive notes and quality assurance of content. In particular, the proposed generative agent EZYer : 1) Teacher Module: Integrating the Text Corpus retrieval and in-depth generation technologies, it automatically generates structured teaching materials and LaTeX Beamer courseware in line with the high school mathematics syllabus and supports user-defined image insertion. 2) Student Module: Throughout the collaborative interaction of the four roles of Teacher, Assistant, Top Student and Struggling Student, Note Taker summarizes and generates academic notes to enhance the depth and interest of learning. 3) Controller: set up keyword filtering system, content scoring system, role co-validation system, and dynamic content correction system. This ensure academic strictness and pedagogical propriety of EZYer inputs and outputs. In order to evaluate EZYer, this paper designs five-dimensional evaluation indexes of content accuracy, knowledge coverage, usability, formatting correctness and visual design and appeal, and scores 100 Beamer and Notes generated by EZYer by five large language models, separately, and the results show that the quality of EZYer-generated content is excellent and has a good application prospect.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºEZYerçš„ç”Ÿæˆå¼ä»£ç†ï¼Œå®ƒç»“åˆäº†æ–‡æœ¬æ£€ç´¢å’Œæ·±åº¦ç”ŸæˆæŠ€æœ¯ï¼Œä¸ºåœ¨çº¿æ•™è‚²æä¾›è‡ªåŠ¨åŒ–çš„æ•™å­¦ææ–™ç”Ÿæˆï¼ˆåŒ…æ‹¬LaTeX Beamerè¯¾ä»¶ï¼‰å’Œäº’åŠ¨å­¦ä¹ ç¬”è®°åŠŸèƒ½ï¼Œå¹¶é€šè¿‡å¤šè§’è‰²åä½œå’Œå¤šä¸ªè´¨é‡æ§åˆ¶æœºåˆ¶ç¡®ä¿å†…å®¹çš„å­¦æœ¯ä¸¥è°¨æ€§å’Œæ•™å­¦é€‚ç”¨æ€§ã€‚è®ºæ–‡è¿˜é€šè¿‡å¤šç»´åº¦è¯„ä¼°è¯æ˜äº†EZYerç”Ÿæˆå†…å®¹çš„é«˜è´¨é‡å’Œè‰¯å¥½åº”ç”¨å‰æ™¯ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.02530v1">Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration</a></td><td><details><summary>å±•å¼€</summary>The exponential growth of digital content presents significant challenges for content safety. Current moderation systems, often based on single models or fixed pipelines, exhibit limitations in identifying implicit risks and providing interpretable judgment processes. To address these issues, we propose Aetheria, a multimodal interpretable content safety framework based on multi-agent debate and collaboration.Employing a collaborative architecture of five core agents, Aetheria conducts in-depth analysis and adjudication of multimodal content through a dynamic, mutually persuasive debate mechanism, which is grounded by RAG-based knowledge retrieval.Comprehensive experiments on our proposed benchmark (AIR-Bench) validate that Aetheria not only generates detailed and traceable audit reports but also demonstrates significant advantages over baselines in overall content safety accuracy, especially in the identification of implicit risks. This framework establishes a transparent and interpretable paradigm, significantly advancing the field of trustworthy AI content moderation.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†Aetheriaï¼Œä¸€ä¸ªåŸºäºå¤šæ™ºèƒ½ä½“è¾©è®ºä¸åä½œçš„å¤šæ¨¡æ€å¯è§£é‡Šå†…å®¹å®‰å…¨æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è¾©è®ºæœºåˆ¶å’ŒRAGçŸ¥è¯†æ£€ç´¢å¯¹å¤šæ¨¡æ€å†…å®¹è¿›è¡Œæ·±åº¦åˆ†æä¸è£å†³ï¼Œå®éªŒè¯æ˜å…¶åœ¨å†…å®¹å®‰å…¨å‡†ç¡®æ€§å’Œéšæ€§é£é™©è¯†åˆ«æ–¹é¢ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶ç”Ÿæˆå¯è¿½æº¯çš„å®¡è®¡æŠ¥å‘Šã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.02502v1">AskNearby: An LLM-Based Application for Neighborhood Information Retrieval and Personalized Cognitive-Map Recommendations</a></td><td><details><summary>å±•å¼€</summary>The "15-minute city" envisions neighborhoods where residents can meet daily needs via a short walk or bike ride. Realizing this vision requires not only physical proximity but also efficient and reliable access to information about nearby places, services, and events. Existing location-based systems, however, focus mainly on city-level tasks and neglect the spatial, temporal, and cognitive factors that shape localized decision-making. We conceptualize this gap as the Local Life Information Accessibility (LLIA) problem and introduce AskNearby, an AI-driven community application that unifies retrieval and recommendation within the 15-minute life circle. AskNearby integrates (i) a three-layer Retrieval-Augmented Generation (RAG) pipeline that synergizes graph-based, semantic-vector, and geographic retrieval with (ii) a cognitive-map model that encodes each user's neighborhood familiarity and preferences. Experiments on real-world community datasets demonstrate that AskNearby significantly outperforms LLM-based and map-based baselines in retrieval accuracy and recommendation quality, achieving robust performance in spatiotemporal grounding and cognitive-aware ranking. Real-world deployments further validate its effectiveness. By addressing the LLIA challenge, AskNearby empowers residents to more effectively discover local resources, plan daily activities, and engage in community life.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºAskNearbyçš„AIç¤¾åŒºåº”ç”¨ï¼Œé€šè¿‡ç»“åˆä¸‰å±‚RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç®¡é“å’Œè®¤çŸ¥åœ°å›¾æ¨¡å‹ï¼Œè§£å†³15åˆ†é’Ÿç”Ÿæ´»åœˆä¸­çš„æœ¬åœ°ç”Ÿæ´»ä¿¡æ¯å¯åŠæ€§é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†æ£€ç´¢å‡†ç¡®æ€§å’Œæ¨èè´¨é‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.02425v1">WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning</a></td><td><details><summary>å±•å¼€</summary>Recent advances in video large language models have demonstrated strong capabilities in understanding short clips. However, scaling them to hours- or days-long videos remains highly challenging due to limited context capacity and the loss of critical visual details during abstraction. Existing memory-augmented methods mitigate this by leveraging textual summaries of video segments, yet they heavily rely on text and fail to utilize visual evidence when reasoning over complex scenes. Moreover, retrieving from fixed temporal scales further limits their flexibility in capturing events that span variable durations. To address this, we introduce WorldMM, a novel multimodal memory agent that constructs and retrieves from multiple complementary memories, encompassing both textual and visual representations. WorldMM comprises three types of memory: episodic memory indexes factual events across multiple temporal scales, semantic memory continuously updates high-level conceptual knowledge, and visual memory preserves detailed information about scenes. During inference, an adaptive retrieval agent iteratively selects the most relevant memory source and leverages multiple temporal granularities based on the query, continuing until it determines that sufficient information has been gathered. WorldMM significantly outperforms existing baselines across five long video question-answering benchmarks, achieving an average 8.4% performance gain over previous state-of-the-art methods, showing its effectiveness on long video reasoning.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºWorldMMçš„å¤šæ¨¡æ€è®°å¿†ä»£ç†ï¼Œé€šè¿‡æ„å»ºå’Œæ£€ç´¢åŒ…å«æ–‡æœ¬ä¸è§†è§‰è¡¨å¾çš„äº’è¡¥è®°å¿†ï¼ˆæƒ…æ™¯è®°å¿†ã€è¯­ä¹‰è®°å¿†å’Œè§†è§‰è®°å¿†ï¼‰ï¼Œè§£å†³é•¿è§†é¢‘ç†è§£ä¸­çš„ä¸Šä¸‹æ–‡é™åˆ¶å’Œç»†èŠ‚ä¸¢å¤±é—®é¢˜ã€‚å…¶è‡ªé€‚åº”æ£€ç´¢æœºåˆ¶ä¸RAGçš„æ ¸å¿ƒæ€æƒ³ï¼ˆæ£€ç´¢å¤–éƒ¨ä¿¡æ¯è¾…åŠ©ç”Ÿæˆï¼‰é«˜åº¦ä¸€è‡´ï¼Œä½†æ‰©å±•åˆ°äº†å¤šæ¨¡æ€é¢†åŸŸï¼Œå¹¶åœ¨é•¿è§†é¢‘é—®ç­”ä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.02333v1">Retrieval-Augmented Memory for Online Learning</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented models couple parametric predictors with non-parametric memories, but their use in streaming supervised learning with concept drift is not well understood. We study online classification in non-stationary environments and propose Retrieval-Augmented Memory for Online Learning (RAM-OL), a simple extension of stochastic gradient descent that maintains a small buffer of past examples. At each time step, RAM-OL retrieves a few nearest neighbours of the current input in the hidden representation space and updates the model jointly on the current example and the retrieved neighbours. We compare a naive replay variant with a gated replay variant that constrains neighbours using a time window, similarity thresholds, and gradient reweighting, in order to balance fast reuse of relevant past data against robustness to outdated regimes. From a theoretical perspective, we interpret RAM-OL under a bounded drift model and discuss how retrieval can reduce adaptation cost and improve regret constants when patterns recur over time. Empirically, we instantiate RAM-OL on a simple online multilayer perceptron and evaluate it on three real-world data streams derived from electricity pricing, electricity load, and airline delay data. On strongly and periodically drifting streams, RAM-OL improves prequential accuracy by up to about seven percentage points and greatly reduces variance across random seeds, while on a noisy airline stream the gated variant closely matches the purely online baseline. These results show that retrieval-augmented memory is a practical and robust tool for online learning under concept drift.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRAM-OLï¼ˆRetrieval-Augmented Memory for Online Learningï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆæ£€ç´¢å¢å¼ºè®°å¿†ä¸åœ¨çº¿å­¦ä¹ æ¥å¤„ç†éå¹³ç¨³ç¯å¢ƒä¸­çš„æ¦‚å¿µæ¼‚ç§»é—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨éšè¡¨ç¤ºç©ºé—´ä¸­æ£€ç´¢å½“å‰è¾“å…¥çš„æœ€è¿‘é‚»æ ·æœ¬ï¼Œå¹¶è”åˆæ›´æ–°æ¨¡å‹ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨å‘¨æœŸæ€§æ¼‚ç§»æ•°æ®æµä¸­èƒ½æ˜¾è‘—æå‡å‡†ç¡®ç‡å¹¶é™ä½æ–¹å·®ï¼Œç†è®ºåˆ†æåˆ™æ¢è®¨äº†æ£€ç´¢å¦‚ä½•é™ä½é€‚åº”æˆæœ¬ã€‚å°½ç®¡èšç„¦äºåœ¨çº¿å­¦ä¹ è€Œéå…¸å‹RAGçš„ç”Ÿæˆä»»åŠ¡ï¼Œä½†å…¶æ ¸å¿ƒæœºåˆ¶ï¼ˆæ£€ç´¢å¤–éƒ¨è®°å¿†å¢å¼ºæ¨¡å‹å†³ç­–ï¼‰ä¸RAGæŠ€æœ¯åŸç†é«˜åº¦ä¸€è‡´ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-12-01
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.01659v1">HalluGraph: Auditable Hallucination Detection for Legal RAG Systems via Knowledge Graph Alignment</a></td><td><details><summary>å±•å¼€</summary>Legal AI systems powered by retrieval-augmented generation (RAG) face a critical accountability challenge: when an AI assistant cites case law, statutes, or contractual clauses, practitioners need verifiable guarantees that generated text faithfully represents source documents. Existing hallucination detectors rely on semantic similarity metrics that tolerate entity substitutions, a dangerous failure mode when confusing parties, dates, or legal provisions can have material consequences. We introduce HalluGraph, a graph-theoretic framework that quantifies hallucinations through structural alignment between knowledge graphs extracted from context, query, and response. Our approach produces bounded, interpretable metrics decomposed into \textit{Entity Grounding} (EG), measuring whether entities in the response appear in source documents, and \textit{Relation Preservation} (RP), verifying that asserted relationships are supported by context. On structured control documents, HalluGraph achieves near-perfect discrimination ($>$400 words, $>$20 entities), HalluGraph achieves $AUC = 0.979$, while maintaining robust performance ($AUC \approx 0.89$) on challenging generative legal task, consistently outperforming semantic similarity baselines. The framework provides the transparency and traceability required for high-stakes legal applications, enabling full audit trails from generated assertions back to source passages.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºHalluGraphçš„å›¾è®ºæ¡†æ¶ï¼Œç”¨äºè§£å†³æ³•å¾‹AIç³»ç»Ÿä¸­åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„å¹»è§‰é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡é‡åŒ–ä¸Šä¸‹æ–‡ã€æŸ¥è¯¢å’Œå“åº”ä¹‹é—´çŸ¥è¯†å›¾çš„ç»“æ„å¯¹é½ï¼Œæä¾›å¯è§£é‡Šçš„æŒ‡æ ‡ï¼ˆå®ä½“æ¥åœ°å’Œå…³ç³»ä¿æŒï¼‰ï¼Œä»¥éªŒè¯ç”Ÿæˆæ–‡æœ¬æ˜¯å¦å¿ å®äºæºæ–‡æ¡£ï¼Œä»è€Œåœ¨é«˜é£é™©æ³•å¾‹åº”ç”¨ä¸­å®ç°é€æ˜æ€§å’Œå¯è¿½æº¯æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒHalluGraphåœ¨ç»“æ„åŒ–æ§åˆ¶æ–‡æ¡£å’Œç”Ÿæˆæ€§æ³•å¾‹ä»»åŠ¡ä¸­å‡ä¼˜äºè¯­ä¹‰ç›¸ä¼¼æ€§åŸºçº¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.01335v1">EmoRAG: Evaluating RAG Robustness to Symbolic Perturbations</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) systems are increasingly central to robust AI, enhancing large language model (LLM) faithfulness by incorporating external knowledge. However, our study unveils a critical, overlooked vulnerability: their profound susceptibility to subtle symbolic perturbations, particularly through near-imperceptible emoticon tokens such as "(@_@)" that can catastrophically mislead retrieval, termed EmoRAG. We demonstrate that injecting a single emoticon into a query makes it nearly 100% likely to retrieve semantically unrelated texts that contain a matching emoticon. Our extensive experiment across general question-answering and code domains, using a range of state-of-the-art retrievers and generators, reveals three key findings: (I) Single-Emoticon Disaster: Minimal emoticon injections cause maximal disruptions, with a single emoticon almost 100% dominating RAG output. (II) Positional Sensitivity: Placing an emoticon at the beginning of a query can cause severe perturbation, with F1-Scores exceeding 0.92 across all datasets. (III) Parameter-Scale Vulnerability: Counterintuitively, models with larger parameters exhibit greater vulnerability to the interference. We provide an in-depth analysis to uncover the underlying mechanisms of these phenomena. Furthermore, we raise a critical concern regarding the robustness assumption of current RAG systems, envisioning a threat scenario where an adversary exploits this vulnerability to manipulate the RAG system. We evaluate standard defenses and find them insufficient against EmoRAG. To address this, we propose targeted defenses, analyzing their strengths and limitations in mitigating emoticon-based perturbations. Finally, we outline future directions for building robust RAG systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ­ç¤ºäº†RAGç³»ç»Ÿçš„ä¸€ä¸ªå…³é”®æ¼æ´â€”â€”EmoRAGï¼Œå³é€šè¿‡è¿‘ä¹ä¸å¯å¯Ÿè§‰çš„è¡¨æƒ…ç¬¦å·ï¼ˆå¦‚"(@_@)"ï¼‰å¯¹æŸ¥è¯¢è¿›è¡Œç»†å¾®æ‰°åŠ¨ï¼Œä¼šå¯¼è‡´æ£€ç´¢ç»“æœä¸¥é‡åç¦»è¯­ä¹‰ç›¸å…³æ€§ï¼ˆå•è¡¨æƒ…æ³¨å…¥å³å¯è¿‘100%è¯¯å¯¼è¾“å‡ºï¼‰ã€‚ç ”ç©¶å‘ç°ï¼šï¼ˆ1ï¼‰è¡¨æƒ…ç¬¦å·çš„ä½ç½®ï¼ˆå¦‚ç½®äºæŸ¥è¯¢å¼€å¤´ï¼‰å’Œæ¨¡å‹å‚æ•°é‡çº§ä¼šåŠ å‰§å¹²æ‰°ï¼›ï¼ˆ2ï¼‰ç°æœ‰é˜²å¾¡æªæ–½å¯¹æ­¤æ— æ•ˆã€‚ä½œè€…æå‡ºäº†é’ˆå¯¹æ€§é˜²å¾¡æ–¹æ¡ˆï¼Œå¹¶å‘¼åæœªæ¥ç ”ç©¶éœ€å…³æ³¨RAGç³»ç»Ÿçš„é²æ£’æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.01183v1">TempPerturb-Eval: On the Joint Effects of Internal Temperature and External Perturbations in RAG Robustness</a></td><td><details><summary>å±•å¼€</summary>The evaluation of Retrieval-Augmented Generation (RAG) systems typically examines retrieval quality and generation parameters like temperature in isolation, overlooking their interaction. This work presents a systematic investigation of how text perturbations (simulating noisy retrieval) interact with temperature settings across multiple LLM runs. We propose a comprehensive RAG Perturbation-Temperature Analysis Framework that subjects retrieved documents to three distinct perturbation types across varying temperature settings. Through extensive experiments on HotpotQA with both open-source and proprietary LLMs, we demonstrate that performance degradation follows distinct patterns: high-temperature settings consistently amplify vulnerability to perturbations, while certain perturbation types exhibit non-linear sensitivity across the temperature range. Our work yields three key contributions: (1) a diagnostic benchmark for assessing RAG robustness, (2) an analytical framework for quantifying perturbation-temperature interactions, and (3) practical guidelines for model selection and parameter tuning under noisy retrieval conditions.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç³»ç»Ÿåœ°ç ”ç©¶äº†RAGç³»ç»Ÿä¸­æ–‡æœ¬æ‰°åŠ¨ï¼ˆæ¨¡æ‹Ÿå™ªå£°æ£€ç´¢ï¼‰ä¸ç”Ÿæˆæ¸©åº¦å‚æ•°çš„äº¤äº’å½±å“ï¼Œæå‡ºäº†ä¸€ä¸ªåˆ†ææ¡†æ¶ï¼Œé€šè¿‡å®éªŒæ­ç¤ºäº†é«˜æ¸©åº¦è®¾ç½®ä¼šæ”¾å¤§æ‰°åŠ¨æ•æ„Ÿæ€§ï¼Œå¹¶æä¾›äº†è¯„ä¼°RAGé²æ£’æ€§çš„åŸºå‡†ã€åˆ†æå·¥å…·åŠå™ªå£°æ£€ç´¢ä¸‹çš„è°ƒå‚æŒ‡å¯¼ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-30
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.02213v1">InstructLR: A Scalable Approach to Create Instruction Dataset for Under-Resourced Languages</a></td><td><details><summary>å±•å¼€</summary>Effective text generation and chat interfaces for low-resource languages (LRLs) remain a challenge for state-of-the-art large language models (LLMs) to support. This is mainly due to the difficulty of curating high-quality instruction datasets for LRLs, a limitation prevalent in the languages spoken across the African continent and other regions. Current approaches, such as automated translation and synthetic data generation, frequently yield outputs that lack fluency or even orthographic consistency. In this paper, we introduce InstructLR, a novel framework designed to generate high-quality instruction datasets for LRLs. Our approach integrates LLM-driven text generation with a dual-layer quality filtering mechanism: an automated filtering layer based on retrieval-augmented-generation (RAG)-based n-shot prompting, and a human-in-the-loop validation layer. Drawing inspiration from benchmarks such as MMLU in task definition, InstructLR has facilitated the creation of three multi-domain instruction benchmarks: ZarmaInstruct-50k, BambaraInstruct-50k, and FulfuldeInstruct-50k.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºInstructLRçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºä½èµ„æºè¯­è¨€ç”Ÿæˆé«˜è´¨é‡çš„æŒ‡ä»¤æ•°æ®é›†ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¤§è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›å’ŒåŒå±‚æ¬¡çš„è´¨é‡è¿‡æ»¤æœºåˆ¶ï¼Œå…¶ä¸­åŒ…æ‹¬åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„è‡ªåŠ¨åŒ–è¿‡æ»¤å±‚å’Œäººå·¥éªŒè¯å±‚ï¼Œå¹¶æˆåŠŸåˆ›å»ºäº†ä¸‰ä¸ªå¤šé¢†åŸŸçš„æŒ‡ä»¤æ•°æ®é›†åŸºå‡†ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.03100v1">Ensemble Privacy Defense for Knowledge-Intensive LLMs against Membership Inference Attacks</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) and Supervised Finetuning (SFT) have become the predominant paradigms for equipping Large Language Models (LLMs) with external knowledge for diverse, knowledge-intensive tasks. However, while such knowledge injection improves performance, it also exposes new attack surfaces. Membership Inference Attacks (MIAs), which aim to determine whether a given data sample was included in a model's training set, pose serious threats to privacy and trust in sensitive domains. To this end, we first systematically evaluate the vulnerability of RAG- and SFT-based LLMs to various MIAs. Then, to address the privacy risk, we further introduce a novel, model-agnostic defense framework, Ensemble Privacy Defense (EPD), which aggregates and evaluates the outputs of a knowledge-injected LLM, a base LLM, and a dedicated judge model to enhance resistance against MIAs. Comprehensive experiments show that, on average, EPD reduces MIA success by up to 27.8\% for SFT and 526.3\% for RAG compared to inference-time baseline, while maintaining answer quality.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ç ”ç©¶äº†RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰å’ŒSFTï¼ˆç›‘ç£å¾®è°ƒï¼‰æ¨¡å‹çš„éšç§è„†å¼±æ€§ï¼Œç‰¹åˆ«æ˜¯æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAsï¼‰çš„é£é™©ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºEPDï¼ˆé›†æˆéšç§é˜²å¾¡ï¼‰çš„æ–°å‹é˜²å¾¡æ¡†æ¶ï¼Œæœ‰æ•ˆé™ä½äº†æ”»å‡»æˆåŠŸç‡ï¼ˆRAGæœ€é«˜å‡å°‘526.3%ï¼‰ï¼ŒåŒæ—¶ä¿æŒäº†å›ç­”è´¨é‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.01659v1">HalluGraph: Auditable Hallucination Detection for Legal RAG Systems via Knowledge Graph Alignment</a></td><td><details><summary>å±•å¼€</summary>Legal AI systems powered by retrieval-augmented generation (RAG) face a critical accountability challenge: when an AI assistant cites case law, statutes, or contractual clauses, practitioners need verifiable guarantees that generated text faithfully represents source documents. Existing hallucination detectors rely on semantic similarity metrics that tolerate entity substitutions, a dangerous failure mode when confusing parties, dates, or legal provisions can have material consequences. We introduce HalluGraph, a graph-theoretic framework that quantifies hallucinations through structural alignment between knowledge graphs extracted from context, query, and response. Our approach produces bounded, interpretable metrics decomposed into \textit{Entity Grounding} (EG), measuring whether entities in the response appear in source documents, and \textit{Relation Preservation} (RP), verifying that asserted relationships are supported by context. On structured control documents, HalluGraph achieves near-perfect discrimination ($>$400 words, $>$20 entities), HalluGraph achieves $AUC = 0.979$, while maintaining robust performance ($AUC \approx 0.89$) on challenging generative legal task, consistently outperforming semantic similarity baselines. The framework provides the transparency and traceability required for high-stakes legal applications, enabling full audit trails from generated assertions back to source passages.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºHalluGraphçš„å›¾è®ºæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³RAGç³»ç»Ÿä¸­çš„å¹»è§‰é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ³•å¾‹AIé¢†åŸŸã€‚é€šè¿‡æ„å»ºçŸ¥è¯†å›¾å¹¶é‡åŒ–å“åº”ä¸æºæ–‡æ¡£ä¹‹é—´çš„ç»“æ„å¯¹é½ï¼ŒHalluGraphæä¾›äº†å¯è§£é‡Šçš„æŒ‡æ ‡ï¼ˆå®ä½“åŸºç¡€å’Œå…³ç³»ä¿ç•™ï¼‰æ¥æ£€æµ‹ç”Ÿæˆæ–‡æœ¬æ˜¯å¦å¿ å®äºåŸå§‹æ³•å¾‹æ–‡æ¡£ï¼Œä»è€Œåœ¨é«˜é£é™©æ³•å¾‹åº”ç”¨ä¸­æé«˜é€æ˜åº¦å’Œå¯è¿½æº¯æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.01335v1">EmoRAG: Evaluating RAG Robustness to Symbolic Perturbations</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) systems are increasingly central to robust AI, enhancing large language model (LLM) faithfulness by incorporating external knowledge. However, our study unveils a critical, overlooked vulnerability: their profound susceptibility to subtle symbolic perturbations, particularly through near-imperceptible emoticon tokens such as "(@_@)" that can catastrophically mislead retrieval, termed EmoRAG. We demonstrate that injecting a single emoticon into a query makes it nearly 100% likely to retrieve semantically unrelated texts that contain a matching emoticon. Our extensive experiment across general question-answering and code domains, using a range of state-of-the-art retrievers and generators, reveals three key findings: (I) Single-Emoticon Disaster: Minimal emoticon injections cause maximal disruptions, with a single emoticon almost 100% dominating RAG output. (II) Positional Sensitivity: Placing an emoticon at the beginning of a query can cause severe perturbation, with F1-Scores exceeding 0.92 across all datasets. (III) Parameter-Scale Vulnerability: Counterintuitively, models with larger parameters exhibit greater vulnerability to the interference. We provide an in-depth analysis to uncover the underlying mechanisms of these phenomena. Furthermore, we raise a critical concern regarding the robustness assumption of current RAG systems, envisioning a threat scenario where an adversary exploits this vulnerability to manipulate the RAG system. We evaluate standard defenses and find them insufficient against EmoRAG. To address this, we propose targeted defenses, analyzing their strengths and limitations in mitigating emoticon-based perturbations. Finally, we outline future directions for building robust RAG systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ­ç¤ºäº†RAGç³»ç»Ÿå­˜åœ¨çš„ä¸€ä¸ªå…³é”®æ¼æ´â€”â€”EmoRAGï¼Œå³é€šè¿‡å¾®å¦™çš„ç¬¦å·æ‰°åŠ¨ï¼ˆå¦‚éš¾ä»¥å¯Ÿè§‰çš„è¡¨æƒ…ç¬¦å·ï¼‰å¯ä»¥ä¸¥é‡è¯¯å¯¼æ£€ç´¢è¿‡ç¨‹ï¼Œå¯¼è‡´æ£€ç´¢åˆ°è¯­ä¹‰æ— å…³çš„æ–‡æœ¬ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨æŸ¥è¯¢ä¸­æ’å…¥ä¸€ä¸ªè¡¨æƒ…ç¬¦å·å‡ ä¹100%ä¼šå½±å“RAGçš„è¾“å‡ºï¼Œä¸”æ¨¡å‹å‚æ•°è¶Šå¤§è¶Šå®¹æ˜“å—åˆ°å¹²æ‰°ã€‚ä½œè€…åˆ†æäº†è¿™ä¸€ç°è±¡çš„æœºåˆ¶ï¼Œè¯„ä¼°äº†ç°æœ‰é˜²å¾¡æªæ–½çš„ä¸è¶³ï¼Œå¹¶æå‡ºäº†é’ˆå¯¹æ€§çš„é˜²å¾¡æ–¹æ¡ˆï¼Œæœ€åå±•æœ›äº†æ„å»ºæ›´é²æ£’çš„RAGç³»ç»Ÿçš„æœªæ¥æ–¹å‘ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.01183v1">TempPerturb-Eval: On the Joint Effects of Internal Temperature and External Perturbations in RAG Robustness</a></td><td><details><summary>å±•å¼€</summary>The evaluation of Retrieval-Augmented Generation (RAG) systems typically examines retrieval quality and generation parameters like temperature in isolation, overlooking their interaction. This work presents a systematic investigation of how text perturbations (simulating noisy retrieval) interact with temperature settings across multiple LLM runs. We propose a comprehensive RAG Perturbation-Temperature Analysis Framework that subjects retrieved documents to three distinct perturbation types across varying temperature settings. Through extensive experiments on HotpotQA with both open-source and proprietary LLMs, we demonstrate that performance degradation follows distinct patterns: high-temperature settings consistently amplify vulnerability to perturbations, while certain perturbation types exhibit non-linear sensitivity across the temperature range. Our work yields three key contributions: (1) a diagnostic benchmark for assessing RAG robustness, (2) an analytical framework for quantifying perturbation-temperature interactions, and (3) practical guidelines for model selection and parameter tuning under noisy retrieval conditions.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç³»ç»Ÿåœ°ç ”ç©¶äº†RAGç³»ç»Ÿä¸­æ–‡æœ¬æ‰°åŠ¨ï¼ˆæ¨¡æ‹Ÿå™ªå£°æ£€ç´¢ï¼‰ä¸æ¸©åº¦è®¾ç½®çš„äº¤äº’ä½œç”¨ï¼Œæå‡ºäº†ä¸€ä¸ªåˆ†ææ¡†æ¶æ¥è¯„ä¼°ä¸åŒæ‰°åŠ¨ç±»å‹å’Œæ¸©åº¦è®¾ç½®å¯¹æ€§èƒ½çš„å½±å“ï¼Œå¹¶æä¾›äº†æ¨¡å‹é€‰æ‹©å’Œå‚æ•°è°ƒä¼˜çš„å®ç”¨æŒ‡å—ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.01020v1">Evaluating Legal Reasoning Traces with Legal Issue Tree Rubrics</a></td><td><details><summary>å±•å¼€</summary>Evaluating the quality of LLM-generated reasoning traces in expert domains (e.g., law) is essential for ensuring credibility and explainability, yet remains challenging due to the inherent complexity of such reasoning tasks. We introduce LEGIT (LEGal Issue Trees), a novel large-scale (24K instances) expert-level legal reasoning dataset with an emphasis on reasoning trace evaluation. We convert court judgments into hierarchical trees of opposing parties' arguments and the court's conclusions, which serve as rubrics for evaluating the issue coverage and correctness of the reasoning traces. We verify the reliability of these rubrics via human expert annotations and comparison with coarse, less informative rubrics. Using the LEGIT dataset, we show that (1) LLMs' legal reasoning ability is seriously affected by both legal issue coverage and correctness, and that (2) retrieval-augmented generation (RAG) and RL with rubrics bring complementary benefits for legal reasoning abilities, where RAG improves overall reasoning capability, whereas RL improves correctness albeit with reduced coverage.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†LEGITæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨æ³•å¾‹é¢†åŸŸç”Ÿæˆçš„æ¨ç†è½¨è¿¹çš„è´¨é‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’ŒåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹æå‡æ³•å¾‹æ¨ç†èƒ½åŠ›å…·æœ‰äº’è¡¥ä½œç”¨ï¼Œå…¶ä¸­RAGèƒ½æé«˜æ•´ä½“æ¨ç†èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.00991v1">Advancing Academic Chatbots: Evaluation of Non Traditional Outputs</a></td><td><details><summary>å±•å¼€</summary>Most evaluations of large language models focus on standard tasks such as factual question answering or short summarization. This research expands that scope in two directions: first, by comparing two retrieval strategies, Graph RAG, structured knowledge-graph based, and Advanced RAG, hybrid keyword-semantic search, for QA; and second, by evaluating whether LLMs can generate high quality non-traditional academic outputs, specifically slide decks and podcast scripts. We implemented a prototype combining Meta's LLaMA 3 70B open weight and OpenAI's GPT 4o mini API based. QA performance was evaluated using both human ratings across eleven quality dimensions and large language model judges for scalable cross validation. GPT 4o mini with Advanced RAG produced the most accurate responses. Graph RAG offered limited improvements and led to more hallucinations, partly due to its structural complexity and manual setup. Slide and podcast generation was tested with document grounded retrieval. GPT 4o mini again performed best, though LLaMA 3 showed promise in narrative coherence. Human reviewers were crucial for detecting layout and stylistic flaws, highlighting the need for combined human LLM evaluation in assessing emerging academic outputs.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æ¯”è¾ƒäº†ä¸¤ç§RAGæ£€ç´¢ç­–ç•¥ï¼ˆåŸºäºçŸ¥è¯†å›¾è°±çš„Graph RAGå’Œæ··åˆå…³é”®è¯-è¯­ä¹‰æœç´¢çš„Advanced RAGï¼‰åœ¨é—®ç­”ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¹¶è¯„ä¼°äº†LLMsç”Ÿæˆéä¼ ç»Ÿå­¦æœ¯è¾“å‡ºï¼ˆå¹»ç¯ç‰‡å’Œæ’­å®¢è„šæœ¬ï¼‰çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°GPT 4o miniç»“åˆAdvanced RAGå‡†ç¡®æ€§æœ€é«˜ï¼ŒGraph RAGå› ç»“æ„å¤æ‚å¯¼è‡´æ›´å¤šå¹»è§‰ï¼ŒåŒæ—¶å¼ºè°ƒäº†äººæœºååŒè¯„ä¼°å¯¹æ–°å…´å­¦æœ¯è¾“å‡ºçš„é‡è¦æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.00881v1">Hybrid-DMKG: A Hybrid Reasoning Framework over Dynamic Multimodal Knowledge Graphs for Multimodal Multihop QA with Knowledge Editing</a></td><td><details><summary>å±•å¼€</summary>Multimodal Knowledge Editing (MKE) extends traditional knowledge editing to settings involving both textual and visual modalities. However, existing MKE benchmarks primarily assess final answer correctness while neglecting the quality of intermediate reasoning and robustness to visually rephrased inputs. To address this limitation, we introduce MMQAKE, the first benchmark for multimodal multihop question answering with knowledge editing. MMQAKE evaluates (1) a model's ability to reason over 2-5-hop factual chains that span both text and images, including performance at each intermediate step, and (2) robustness to visually rephrased inputs in multihop questions. Our evaluation shows that current MKE methods often struggle to consistently update and reason over multimodal reasoning chains after knowledge edits. To overcome these challenges, we propose Hybrid-DMKG, a hybrid reasoning framework built on a dynamic multimodal knowledge graph (DMKG) to enable accurate multihop reasoning over updated multimodal knowledge. Hybrid-DMKG first uses a large language model to decompose multimodal multihop questions into sequential sub-questions, then applies a multimodal retrieval model to locate updated facts by jointly encoding each sub-question with candidate entities and their associated images. For answer inference, a hybrid reasoning module operates over the DMKG via two parallel paths: (1) relation linking prediction, and (2) RAG reasoning with large vision-language models. A decision module aggregates evidence from both paths to select the most credible answer. Experimental results on MMQAKE show that Hybrid-DMKG significantly outperforms existing MKE approaches, achieving higher accuracy and improved robustness to knowledge updates.</details></td><td><details><summary>å±•å¼€</summary>  </details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.00804v1">Bias Injection Attacks on RAG Databases and Sanitization Defenses</a></td><td><details><summary>å±•å¼€</summary>This paper explores attacks and defenses on vector databases in retrieval-augmented generation (RAG) systems. Prior work on knowledge poisoning attacks primarily inject false or toxic content, which fact-checking or linguistic analysis easily detects. We reveal a new and subtle threat: bias injection attacks, which insert factually correct yet semantically biased passages into the knowledge base to covertly influence the ideological framing of answers generated by large language models (LLMs). We demonstrate that these adversarial passages, though linguistically coherent and truthful, can systematically crowd out opposing views from the retrieved context and steer LLM answers toward the attacker's intended perspective.
  We precisely characterize this class of attacks and then develop a post-retrieval filtering defense, BiasDef. We construct a comprehensive benchmark based on public question answering datasets to evaluate them. Our results show that: (1) the proposed attack induces significant perspective shifts in LLM answers, effectively evading existing retrieval-based sanitization defenses; and (2) BiasDef outperforms existing methods by reducing adversarial passages retrieved by 15\% which mitigates perspective shift by 6.2\times in answers, while enabling the retrieval of 62\% more benign passages.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­å‘é‡æ•°æ®åº“çš„åè§æ³¨å…¥æ”»å‡»ä¸é˜²å¾¡æ–¹æ³•ï¼Œæ­ç¤ºäº†æ”»å‡»è€…å¦‚ä½•é€šè¿‡æ¤å…¥äº‹å®æ­£ç¡®ä½†è¯­ä¹‰åè§çš„æ–‡æœ¬æ¥éšè”½æ“æ§å¤§å‹è¯­è¨€æ¨¡å‹çš„è¾“å‡ºè§‚ç‚¹ï¼Œå¹¶æå‡ºäº†ä¸€ç§åæ£€ç´¢è¿‡æ»¤é˜²å¾¡æ–¹æ³•BiasDefä»¥æœ‰æ•ˆå‡å°‘æ”»å‡»å½±å“å¹¶æå‡è‰¯æ€§å†…å®¹æ£€ç´¢æ•ˆç‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.00772v1">SHRAG: AFrameworkfor Combining Human-Inspired Search with RAG</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) is gaining recognition as one of the key technological axes for next generation information retrieval, owing to its ability to mitigate the hallucination phenomenon in Large Language
  Models (LLMs)and effectively incorporate up-to-date information. However, specialized expertise is necessary to
  construct ahigh-quality retrieval system independently; moreover, RAGdemonstratesrelativelyslowerprocessing
  speeds compared to conventional pure retrieval systems because it involves both retrieval and generation stages.
  Accordingly, this study proposes SHRAG, a novel framework designed to facilitate the seamless integration of
  Information Retrieval and RAG while simultaneously securing precise retrieval performance. SHRAG utilizes a
  Large Language Model as a Query Strategist to automatically transform unstructured natural language queries
  into logically structured search queries, subsequently performing Boolean retrieval to emulate the search process
  of an expert human searcher. Furthermore, it incorporates multilingual query expansion and a multilingual
  embedding model, enabling it to perform efficient cross-lingual question answering within the multilingual
  dataset environment of the ScienceON Challenge. Experimental results demonstrate that the proposed method,
  combining logical retrieval capabilities and generative reasoning, can significantly enhance the accuracy and
  reliability of RAG systems. Furthermore, SHRAG movesbeyondconventionaldocument-centric retrieval methods,
  presenting the potential for a new search paradigm capable of providing direct and reliable responses to queries.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSHRAGçš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–RAGç³»ç»Ÿçš„æ£€ç´¢ä¸ç”Ÿæˆæ•´åˆè¿‡ç¨‹ã€‚å®ƒåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºæŸ¥è¯¢ç­–ç•¥ç”Ÿæˆå™¨ï¼Œå°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºç»“æ„åŒ–æœç´¢æŸ¥è¯¢ï¼Œå¹¶é€šè¿‡å¸ƒå°”æ£€ç´¢æ¨¡æ‹Ÿä¸“å®¶çº§æœç´¢ï¼Œç»“åˆå¤šè¯­è¨€æŸ¥è¯¢æ‰©å±•å’ŒåµŒå…¥æ¨¡å‹æå‡è·¨è¯­è¨€é—®ç­”æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—æé«˜RAGç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œå¹¶æ¢ç´¢äº†è¶…è¶Šä¼ ç»Ÿæ–‡æ¡£æ£€ç´¢çš„æ–°æœç´¢èŒƒå¼ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-29
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.01020v1">Evaluating Legal Reasoning Traces with Legal Issue Tree Rubrics</a></td><td><details><summary>å±•å¼€</summary>Evaluating the quality of LLM-generated reasoning traces in expert domains (e.g., law) is essential for ensuring credibility and explainability, yet remains challenging due to the inherent complexity of such reasoning tasks. We introduce LEGIT (LEGal Issue Trees), a novel large-scale (24K instances) expert-level legal reasoning dataset with an emphasis on reasoning trace evaluation. We convert court judgments into hierarchical trees of opposing parties' arguments and the court's conclusions, which serve as rubrics for evaluating the issue coverage and correctness of the reasoning traces. We verify the reliability of these rubrics via human expert annotations and comparison with coarse, less informative rubrics. Using the LEGIT dataset, we show that (1) LLMs' legal reasoning ability is seriously affected by both legal issue coverage and correctness, and that (2) retrieval-augmented generation (RAG) and RL with rubrics bring complementary benefits for legal reasoning abilities, where RAG improves overall reasoning capability, whereas RL improves correctness albeit with reduced coverage.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ä»‹ç»äº†LEGITæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°æ³•å¾‹é¢†åŸŸä¸“å®¶çº§æ¨ç†ä»»åŠ¡ä¸­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆæ¨ç†è½¨è¿¹çš„è´¨é‡ï¼Œå¹¶æ¢è®¨äº†RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰å’ŒåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹æå‡æ³•å¾‹æ¨ç†èƒ½åŠ›çš„äº’è¡¥ä½œç”¨ï¼Œå…¶ä¸­RAGæ˜¾è‘—æ”¹å–„äº†æ•´ä½“æ¨ç†èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.00991v1">Advancing Academic Chatbots: Evaluation of Non Traditional Outputs</a></td><td><details><summary>å±•å¼€</summary>Most evaluations of large language models focus on standard tasks such as factual question answering or short summarization. This research expands that scope in two directions: first, by comparing two retrieval strategies, Graph RAG, structured knowledge-graph based, and Advanced RAG, hybrid keyword-semantic search, for QA; and second, by evaluating whether LLMs can generate high quality non-traditional academic outputs, specifically slide decks and podcast scripts. We implemented a prototype combining Meta's LLaMA 3 70B open weight and OpenAI's GPT 4o mini API based. QA performance was evaluated using both human ratings across eleven quality dimensions and large language model judges for scalable cross validation. GPT 4o mini with Advanced RAG produced the most accurate responses. Graph RAG offered limited improvements and led to more hallucinations, partly due to its structural complexity and manual setup. Slide and podcast generation was tested with document grounded retrieval. GPT 4o mini again performed best, though LLaMA 3 showed promise in narrative coherence. Human reviewers were crucial for detecting layout and stylistic flaws, highlighting the need for combined human LLM evaluation in assessing emerging academic outputs.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æ¯”è¾ƒäº†åŸºäºçŸ¥è¯†å›¾è°±çš„Graph RAGå’Œæ··åˆå…³é”®è¯-è¯­ä¹‰æœç´¢çš„Advanced RAGä¸¤ç§æ£€ç´¢ç­–ç•¥åœ¨é—®ç­”ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¹¶è¯„ä¼°äº†LLMsç”Ÿæˆéä¼ ç»Ÿå­¦æœ¯è¾“å‡ºï¼ˆå¦‚å¹»ç¯ç‰‡å’Œæ’­å®¢è„šæœ¬ï¼‰çš„èƒ½åŠ›ï¼Œå‘ç°GPT 4o minié…åˆAdvanced RAGæ•ˆæœæœ€ä½³ï¼ŒåŒæ—¶å¼ºè°ƒäº†äººå·¥è¯„ä¼°åœ¨æ£€æµ‹å¸ƒå±€å’Œé£æ ¼é—®é¢˜ä¸­çš„é‡è¦æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.00881v1">Hybrid-DMKG: A Hybrid Reasoning Framework over Dynamic Multimodal Knowledge Graphs for Multimodal Multihop QA with Knowledge Editing</a></td><td><details><summary>å±•å¼€</summary>Multimodal Knowledge Editing (MKE) extends traditional knowledge editing to settings involving both textual and visual modalities. However, existing MKE benchmarks primarily assess final answer correctness while neglecting the quality of intermediate reasoning and robustness to visually rephrased inputs. To address this limitation, we introduce MMQAKE, the first benchmark for multimodal multihop question answering with knowledge editing. MMQAKE evaluates (1) a model's ability to reason over 2-5-hop factual chains that span both text and images, including performance at each intermediate step, and (2) robustness to visually rephrased inputs in multihop questions. Our evaluation shows that current MKE methods often struggle to consistently update and reason over multimodal reasoning chains after knowledge edits. To overcome these challenges, we propose Hybrid-DMKG, a hybrid reasoning framework built on a dynamic multimodal knowledge graph (DMKG) to enable accurate multihop reasoning over updated multimodal knowledge. Hybrid-DMKG first uses a large language model to decompose multimodal multihop questions into sequential sub-questions, then applies a multimodal retrieval model to locate updated facts by jointly encoding each sub-question with candidate entities and their associated images. For answer inference, a hybrid reasoning module operates over the DMKG via two parallel paths: (1) relation linking prediction, and (2) RAG reasoning with large vision-language models. A decision module aggregates evidence from both paths to select the most credible answer. Experimental results on MMQAKE show that Hybrid-DMKG significantly outperforms existing MKE approaches, achieving higher accuracy and improved robustness to knowledge updates.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†MMQAKEåŸºå‡†æµ‹è¯•å’ŒHybrid-DMKGæ··åˆæ¨ç†æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å’Œæ”¹è¿›å¤šæ¨¡æ€çŸ¥è¯†ç¼–è¾‘ï¼ˆMKEï¼‰ä¸­çš„å¤šè·³æ¨ç†èƒ½åŠ›ã€‚Hybrid-DMKGç»“åˆäº†å¤§è¯­è¨€æ¨¡å‹åˆ†è§£é—®é¢˜ã€å¤šæ¨¡æ€æ£€ç´¢æ¨¡å‹å®šä½æ›´æ–°äº‹å®ï¼Œä»¥åŠé€šè¿‡åŠ¨æ€å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±ï¼ˆDMKGï¼‰è¿›è¡Œå…³ç³»é“¾æ¥é¢„æµ‹å’ŒRAGæ¨ç†çš„åŒè·¯å¾„æ··åˆæ¨ç†ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€çŸ¥è¯†æ›´æ–°åçš„ç­”æ¡ˆå‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.00804v1">Bias Injection Attacks on RAG Databases and Sanitization Defenses</a></td><td><details><summary>å±•å¼€</summary>This paper explores attacks and defenses on vector databases in retrieval-augmented generation (RAG) systems. Prior work on knowledge poisoning attacks primarily inject false or toxic content, which fact-checking or linguistic analysis easily detects. We reveal a new and subtle threat: bias injection attacks, which insert factually correct yet semantically biased passages into the knowledge base to covertly influence the ideological framing of answers generated by large language models (LLMs). We demonstrate that these adversarial passages, though linguistically coherent and truthful, can systematically crowd out opposing views from the retrieved context and steer LLM answers toward the attacker's intended perspective.
  We precisely characterize this class of attacks and then develop a post-retrieval filtering defense, BiasDef. We construct a comprehensive benchmark based on public question answering datasets to evaluate them. Our results show that: (1) the proposed attack induces significant perspective shifts in LLM answers, effectively evading existing retrieval-based sanitization defenses; and (2) BiasDef outperforms existing methods by reducing adversarial passages retrieved by 15\% which mitigates perspective shift by 6.2\times in answers, while enabling the retrieval of 62\% more benign passages.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­å‘é‡æ•°æ®åº“çš„æ–°å‹æ”»å‡»ä¸é˜²å¾¡æ–¹æ³•ï¼Œæå‡ºäº†ä¸€ç§éšè”½çš„åè§æ³¨å…¥æ”»å‡»æ–¹å¼ï¼ˆé€šè¿‡æ’å…¥äº‹å®æ­£ç¡®ä½†è¯­ä¹‰åé¢‡çš„æ–‡æœ¬æ¥å½±å“å¤§è¯­è¨€æ¨¡å‹è¾“å‡ºçš„æ„è¯†å½¢æ€å€¾å‘ï¼‰ï¼Œå¹¶å¼€å‘äº†ä¸€ç§åæ£€ç´¢è¿‡æ»¤é˜²å¾¡æœºåˆ¶BiasDefï¼Œå®éªŒè¡¨æ˜è¯¥é˜²å¾¡èƒ½æœ‰æ•ˆå‡å°‘æ”»å‡»å½±å“å¹¶æå‡è‰¯æ€§æ–‡æœ¬çš„æ£€ç´¢ç‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.00772v1">SHRAG: AFrameworkfor Combining Human-Inspired Search with RAG</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) is gaining recognition as one of the key technological axes for next generation information retrieval, owing to its ability to mitigate the hallucination phenomenon in Large Language
  Models (LLMs)and effectively incorporate up-to-date information. However, specialized expertise is necessary to
  construct ahigh-quality retrieval system independently; moreover, RAGdemonstratesrelativelyslowerprocessing
  speeds compared to conventional pure retrieval systems because it involves both retrieval and generation stages.
  Accordingly, this study proposes SHRAG, a novel framework designed to facilitate the seamless integration of
  Information Retrieval and RAG while simultaneously securing precise retrieval performance. SHRAG utilizes a
  Large Language Model as a Query Strategist to automatically transform unstructured natural language queries
  into logically structured search queries, subsequently performing Boolean retrieval to emulate the search process
  of an expert human searcher. Furthermore, it incorporates multilingual query expansion and a multilingual
  embedding model, enabling it to perform efficient cross-lingual question answering within the multilingual
  dataset environment of the ScienceON Challenge. Experimental results demonstrate that the proposed method,
  combining logical retrieval capabilities and generative reasoning, can significantly enhance the accuracy and
  reliability of RAG systems. Furthermore, SHRAG movesbeyondconventionaldocument-centric retrieval methods,
  presenting the potential for a new search paradigm capable of providing direct and reliable responses to queries.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSHRAGçš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹è¿›RAGæŠ€æœ¯ï¼Œé€šè¿‡åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºæŸ¥è¯¢ç­–ç•¥å™¨ï¼Œå°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºç»“æ„åŒ–æœç´¢æŸ¥è¯¢ï¼Œå¹¶é‡‡ç”¨å¸ƒå°”æ£€ç´¢å’Œå¤šè¯­è¨€æŸ¥è¯¢æ‰©å±•ç­‰æŠ€æœ¯æ¥æé«˜æ£€ç´¢ç²¾åº¦å’Œæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSHRAGåœ¨å‡†ç¡®æ€§ã€å¯é æ€§å’Œè·¨è¯­è¨€é—®ç­”æ€§èƒ½æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»ŸRAGç³»ç»Ÿï¼ŒåŒæ—¶çªç ´äº†ä»¥æ–‡æ¡£ä¸ºä¸­å¿ƒçš„æ£€ç´¢æ¨¡å¼ï¼Œæä¾›æ›´ç›´æ¥çš„æŸ¥è¯¢å“åº”ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.00590v1">Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models</a></td><td><details><summary>å±•å¼€</summary>Knowledge graphs (KGs) provide structured, verifiable grounding for large language models (LLMs), but current LLM-based systems commonly use KGs as auxiliary structures for text retrieval, leaving their intrinsic quality underexplored. In this work, we propose Wikontic, a multi-stage pipeline that constructs KGs from open-domain text by extracting candidate triplets with qualifiers, enforcing Wikidata-based type and relation constraints, and normalizing entities to reduce duplication. The resulting KGs are compact, ontology-consistent, and well-connected; on MuSiQue, the correct answer entity appears in 96% of generated triplets. On HotpotQA, our triplets-only setup achieves 76.0 F1, and on MuSiQue 59.8 F1, matching or surpassing several retrieval-augmented generation baselines that still require textual context. In addition, Wikontic attains state-of-the-art information-retention performance on the MINE-1 benchmark (86%), outperforming prior KG construction methods. Wikontic is also efficient at build time: KG construction uses less than 1,000 output tokens, about 3$\times$ fewer than AriGraph and $<$1/20 of GraphRAG. The proposed pipeline enhances the quality of the generated KG and offers a scalable solution for leveraging structured knowledge in LLMs.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºWikonticçš„å¤šé˜¶æ®µæµç¨‹ï¼Œé€šè¿‡ä»å¼€æ”¾åŸŸæ–‡æœ¬æ„å»ºçŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ï¼Œæå‡KGçš„è´¨é‡å’Œä¸€è‡´æ€§ï¼Œå¹¶å°†å…¶ç”¨äºå¢å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç”šè‡³è¶…è¶Šäº†ä¸€äº›éœ€è¦æ–‡æœ¬ä¸Šä¸‹æ–‡çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åŸºçº¿ï¼Œå±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨ç»“æ„åŒ–çŸ¥è¯†æå‡LLMsçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.00367v1">Breaking It Down: Domain-Aware Semantic Segmentation for Retrieval Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Document chunking is a crucial component of Retrieval-Augmented Generation (RAG), as it directly affects the retrieval of relevant and precise context. Conventional fixed-length and recursive splitters often produce arbitrary, incoherent segments that fail to preserve semantic structure. Although semantic chunking has gained traction, its influence on generation quality remains underexplored. This paper introduces two efficient semantic chunking methods, Projected Similarity Chunking (PSC) and Metric Fusion Chunking (MFC), trained on PubMed data using three different embedding models. We further present an evaluation framework that measures the effect of chunking on both retrieval and generation by augmenting PubMedQA with full-text PubMed Central articles. Our results show substantial retrieval improvements (24x with PSC) in MRR and higher Hits@k on PubMedQA. We provide a comprehensive analysis, including statistical significance and response-time comparisons with common chunking libraries. Despite being trained on a single domain, PSC and MFC also generalize well, achieving strong out-of-domain generation performance across multiple datasets. Overall, our findings confirm that our semantic chunkers, especially PSC, consistently deliver superior performance.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ç ”ç©¶RAGä¸­å…³é”®çš„æ–‡ä»¶åˆ†å—æŠ€æœ¯ï¼Œæå‡ºä¸¤ç§åŸºäºè¯­ä¹‰çš„åˆ’åˆ†æ–¹æ³•ï¼ˆPSCå’ŒMFCï¼‰ï¼Œé€šè¿‡åœ¨PubMedæ•°æ®ä¸Šçš„å®éªŒè¯æ˜å…¶èƒ½æ˜¾è‘—æå‡æ£€ç´¢æ•ˆæœï¼ˆå¦‚PSCä½¿MRRæå‡24å€ï¼‰å¹¶å¢å¼ºç”Ÿæˆè´¨é‡ï¼ŒåŒæ—¶éªŒè¯äº†æ–¹æ³•çš„è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.00360v1">CourseTimeQA: A Lecture-Video Benchmark and a Latency-Constrained Cross-Modal Fusion Method for Timestamped QA</a></td><td><details><summary>å±•å¼€</summary>We study timestamped question answering over educational lecture videos under a single-GPU latency/memory budget. Given a natural-language query, the system retrieves relevant timestamped segments and synthesizes a grounded answer. We present CourseTimeQA (52.3 h, 902 queries across six courses) and a lightweight, latency-constrained cross-modal retriever (CrossFusion-RAG) that combines frozen encoders, a learned 512->768 vision projection, shallow query-agnostic cross-attention over ASR and frames with a temporal-consistency regularizer, and a small cross-attentive reranker. On CourseTimeQA, CrossFusion-RAG improves nDCG@10 by 0.10 and MRR by 0.08 over a strong BLIP-2 retriever while achieving approximately 1.55 s median end-to-end latency on a single A100. Closest comparators (zero-shot CLIP multi-frame pooling; CLIP + cross-encoder reranker + MMR; learned late-fusion gating; text-only hybrid with cross-encoder reranking and its MMR variant; caption-augmented text retrieval; non-learned temporal smoothing) are evaluated under matched hardware and indexing. We report robustness across ASR noise (WER quartiles), diagnostics for temporal localization, and full training/tuning details to support reproducible comparison.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºCrossFusion-RAGçš„è½»é‡çº§è·¨æ¨¡æ€æ£€ç´¢ç³»ç»Ÿï¼Œç”¨äºæ•™è‚²è®²åº§è§†é¢‘çš„æ—¶é—´æ ‡å®šé—®ç­”ä»»åŠ¡ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†å†»ç»“ç¼–ç å™¨ã€è§†è§‰æŠ•å½±ã€è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶å’Œæ—¶é—´ä¸€è‡´æ€§æ­£åˆ™åŒ–å™¨ï¼Œèƒ½å¤Ÿåœ¨å•GPUèµ„æºé™åˆ¶ä¸‹é«˜æ•ˆæ£€ç´¢ç›¸å…³è§†é¢‘ç‰‡æ®µå¹¶ç”Ÿæˆç­”æ¡ˆã€‚è®ºæ–‡è¿˜ä»‹ç»äº†æ–°çš„æ•°æ®é›†CourseTimeQAï¼Œå¹¶åœ¨å…¶ä¸­éªŒè¯äº†CrossFusion-RAGçš„æ€§èƒ½æå‡å’Œä½å»¶è¿Ÿä¼˜åŠ¿ï¼ŒåŒæ—¶å¯¹æ¯”äº†å¤šç§åŸºçº¿æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.00331v1">CogEvo-Edu: Cognitive Evolution Educational Multi-Agent Collaborative System</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) are increasingly deployed as conversational tutors in STEM education, yet most systems still rely on a single LLM with a static retrieval-augmented generation (RAG) pipeline over course materials. This design struggles in complex domains such as digital signal processing (DSP), where tutors must maintain coherent long-term student models, manage heterogeneous knowledge bases, and adapt teaching strategies over extended interactions. We argue that retrieval, memory, and control should be treated as a coupled cognitive evolution process. We instantiate this view in CogEvo-Edu, a hierarchical educational multi-agent system comprising a Cognitive Perception Layer (CPL), a Knowledge Evolution Layer (KEL), and a Meta-Control Layer (MCL). CPL maintains dual memories and performs confidence-weighted consolidation to build structured, self-correcting student profiles under limited context. KEL assigns each knowledge chunk a spatiotemporal value that drives activation, semantic compression, and forgetting. MCL formulates tutoring as hierarchical sequential decision making, orchestrating specialized agents and jointly adapting CPL/KEL hyperparameters via a dual inner--outer loop. To evaluate CogEvo-Edu, we construct DSP-EduBench, a vertical benchmark for DSP tutoring with heterogeneous resources, simulated student profiles, and long-horizon interaction scripts. Using a three-model LLM-as-a-Judge ensemble, CogEvo-Edu raises the overall score from 5.32 to 9.23 and improves all six indicators over static RAG, simple memory, and a single-agent variant, demonstrating the value of jointly evolving student profiles, knowledge bases, and teaching policies.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æ¢è®¨äº†åœ¨STEMæ•™è‚²ï¼ˆå¦‚æ•°å­—ä¿¡å·å¤„ç†é¢†åŸŸï¼‰ä¸­ä½¿ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆCogEvo-Eduï¼‰æ”¹è¿›ä¼ ç»Ÿé™æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„å±€é™æ€§ã€‚æå‡ºäº†åˆ†å±‚æ¶æ„ï¼Œé€šè¿‡è®¤çŸ¥æ¼”åŒ–è¿‡ç¨‹åŠ¨æ€æ•´åˆæ£€ç´¢ã€è®°å¿†å’Œæ§åˆ¶ï¼Œä¼˜åŒ–å­¦ç”Ÿå»ºæ¨¡ã€çŸ¥è¯†åº“ç®¡ç†å’Œæ•™å­¦ç­–ç•¥ï¼Œå¹¶åœ¨DSP-EduBenchåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šé™æ€RAGæ–¹æ¡ˆã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-28
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2512.00590v1">Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models</a></td><td><details><summary>å±•å¼€</summary>Knowledge graphs (KGs) provide structured, verifiable grounding for large language models (LLMs), but current LLM-based systems commonly use KGs as auxiliary structures for text retrieval, leaving their intrinsic quality underexplored. In this work, we propose Wikontic, a multi-stage pipeline that constructs KGs from open-domain text by extracting candidate triplets with qualifiers, enforcing Wikidata-based type and relation constraints, and normalizing entities to reduce duplication. The resulting KGs are compact, ontology-consistent, and well-connected; on MuSiQue, the correct answer entity appears in 96% of generated triplets. On HotpotQA, our triplets-only setup achieves 76.0 F1, and on MuSiQue 59.8 F1, matching or surpassing several retrieval-augmented generation baselines that still require textual context. In addition, Wikontic attains state-of-the-art information-retention performance on the MINE-1 benchmark (86%), outperforming prior KG construction methods. Wikontic is also efficient at build time: KG construction uses less than 1,000 output tokens, about 3$\times$ fewer than AriGraph and $<$1/20 of GraphRAG. The proposed pipeline enhances the quality of the generated KG and offers a scalable solution for leveraging structured knowledge in LLMs.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºWikonticçš„å¤šé˜¶æ®µæµç¨‹ï¼Œç”¨äºä»å¼€æ”¾åŸŸæ–‡æœ¬ä¸­æ„å»ºé«˜è´¨é‡çš„çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ï¼Œå¹¶å°†å…¶ä½œä¸ºç»“æ„åŒ–çŸ¥è¯†æºåº”ç”¨äºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚é€šè¿‡æå–å¸¦é™å®šè¯çš„ä¸‰å…ƒç»„ã€åº”ç”¨åŸºäºWikidataçš„ç±»å‹å’Œå…³ç³»çº¦æŸä»¥åŠå®ä½“å½’ä¸€åŒ–ï¼Œç”Ÿæˆçš„KGå…·æœ‰ç´§å‡‘æ€§ã€æœ¬ä½“ä¸€è‡´æ€§å’Œè‰¯å¥½è¿é€šæ€§ã€‚å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨HotpotQAå’ŒMuSiQueç­‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºæˆ–åŒ¹é…éœ€è¦æ–‡æœ¬ä¸Šä¸‹æ–‡çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åŸºçº¿æ¨¡å‹ï¼Œå¹¶åœ¨MINE-1åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ä¿¡æ¯ä¿ç•™æ€§èƒ½ã€‚æ–‡ç« é‡ç‚¹æ¢è®¨äº†KGçš„æ„å»ºè´¨é‡åŠå…¶å¯¹LLMsçš„å¢å¼ºä½œç”¨ï¼Œå±äºRAGä¸­åˆ©ç”¨ç»“æ„åŒ–çŸ¥è¯†æå‡æ¨¡å‹èƒ½åŠ›çš„ç›¸å…³ç ”ç©¶ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.00367v1">Breaking It Down: Domain-Aware Semantic Segmentation for Retrieval Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Document chunking is a crucial component of Retrieval-Augmented Generation (RAG), as it directly affects the retrieval of relevant and precise context. Conventional fixed-length and recursive splitters often produce arbitrary, incoherent segments that fail to preserve semantic structure. Although semantic chunking has gained traction, its influence on generation quality remains underexplored. This paper introduces two efficient semantic chunking methods, Projected Similarity Chunking (PSC) and Metric Fusion Chunking (MFC), trained on PubMed data using three different embedding models. We further present an evaluation framework that measures the effect of chunking on both retrieval and generation by augmenting PubMedQA with full-text PubMed Central articles. Our results show substantial retrieval improvements (24x with PSC) in MRR and higher Hits@k on PubMedQA. We provide a comprehensive analysis, including statistical significance and response-time comparisons with common chunking libraries. Despite being trained on a single domain, PSC and MFC also generalize well, achieving strong out-of-domain generation performance across multiple datasets. Overall, our findings confirm that our semantic chunkers, especially PSC, consistently deliver superior performance.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æ¢è®¨äº†æ–‡æ¡£åˆ†å—ï¼ˆchunkingï¼‰å¯¹RAGç³»ç»Ÿæ€§èƒ½çš„å½±å“ï¼Œæå‡ºäº†ä¸¤ç§é«˜æ•ˆçš„è¯­ä¹‰åˆ†å—æ–¹æ³•ï¼ˆPSCå’ŒMFCï¼‰ï¼Œé€šè¿‡åœ¨PubMedæ•°æ®ä¸Šè®­ç»ƒå¹¶ç»“åˆä¸åŒåµŒå…¥æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†æ£€ç´¢æ•ˆæœï¼ˆå¦‚MRRæå‡24å€ï¼‰å’Œç”Ÿæˆè´¨é‡ã€‚ç ”ç©¶è¿˜è®¾è®¡äº†è¯„ä¼°æ¡†æ¶ï¼ŒéªŒè¯äº†æ–¹æ³•åœ¨è·¨åŸŸä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.00360v1">CourseTimeQA: A Lecture-Video Benchmark and a Latency-Constrained Cross-Modal Fusion Method for Timestamped QA</a></td><td><details><summary>å±•å¼€</summary>We study timestamped question answering over educational lecture videos under a single-GPU latency/memory budget. Given a natural-language query, the system retrieves relevant timestamped segments and synthesizes a grounded answer. We present CourseTimeQA (52.3 h, 902 queries across six courses) and a lightweight, latency-constrained cross-modal retriever (CrossFusion-RAG) that combines frozen encoders, a learned 512->768 vision projection, shallow query-agnostic cross-attention over ASR and frames with a temporal-consistency regularizer, and a small cross-attentive reranker. On CourseTimeQA, CrossFusion-RAG improves nDCG@10 by 0.10 and MRR by 0.08 over a strong BLIP-2 retriever while achieving approximately 1.55 s median end-to-end latency on a single A100. Closest comparators (zero-shot CLIP multi-frame pooling; CLIP + cross-encoder reranker + MMR; learned late-fusion gating; text-only hybrid with cross-encoder reranking and its MMR variant; caption-augmented text retrieval; non-learned temporal smoothing) are evaluated under matched hardware and indexing. We report robustness across ASR noise (WER quartiles), diagnostics for temporal localization, and full training/tuning details to support reproducible comparison.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºCrossFusion-RAGçš„è½»é‡çº§è·¨æ¨¡æ€æ£€ç´¢æ¨¡å‹ï¼Œç”¨äºæ•™è‚²è®²åº§è§†é¢‘çš„æ—¶é—´æˆ³é—®ç­”ä»»åŠ¡ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ç»“åˆå†»ç»“ç¼–ç å™¨ã€è§†è§‰æŠ•å½±ã€æµ…å±‚è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶å’Œæ—¶é—´ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œé«˜æ•ˆæ£€ç´¢ç›¸å…³è§†é¢‘ç‰‡æ®µå¹¶ç”Ÿæˆç­”æ¡ˆï¼Œåœ¨ä½å»¶è¿Ÿå•GPUç¯å¢ƒä¸‹æ˜¾è‘—æå‡äº†æ£€ç´¢æ€§èƒ½ï¼ˆå¦‚nDCG@10å’ŒMRRæŒ‡æ ‡ï¼‰ï¼Œå¹¶åœ¨CourseTimeQAæ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶é²æ£’æ€§å’Œæ—¶æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.00331v1">CogEvo-Edu: Cognitive Evolution Educational Multi-Agent Collaborative System</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) are increasingly deployed as conversational tutors in STEM education, yet most systems still rely on a single LLM with a static retrieval-augmented generation (RAG) pipeline over course materials. This design struggles in complex domains such as digital signal processing (DSP), where tutors must maintain coherent long-term student models, manage heterogeneous knowledge bases, and adapt teaching strategies over extended interactions. We argue that retrieval, memory, and control should be treated as a coupled cognitive evolution process. We instantiate this view in CogEvo-Edu, a hierarchical educational multi-agent system comprising a Cognitive Perception Layer (CPL), a Knowledge Evolution Layer (KEL), and a Meta-Control Layer (MCL). CPL maintains dual memories and performs confidence-weighted consolidation to build structured, self-correcting student profiles under limited context. KEL assigns each knowledge chunk a spatiotemporal value that drives activation, semantic compression, and forgetting. MCL formulates tutoring as hierarchical sequential decision making, orchestrating specialized agents and jointly adapting CPL/KEL hyperparameters via a dual inner--outer loop. To evaluate CogEvo-Edu, we construct DSP-EduBench, a vertical benchmark for DSP tutoring with heterogeneous resources, simulated student profiles, and long-horizon interaction scripts. Using a three-model LLM-as-a-Judge ensemble, CogEvo-Edu raises the overall score from 5.32 to 9.23 and improves all six indicators over static RAG, simple memory, and a single-agent variant, demonstrating the value of jointly evolving student profiles, knowledge bases, and teaching policies.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºåˆ†å±‚å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ•™è‚²æ¡†æ¶CogEvo-Eduï¼Œç”¨äºæ”¹è¿›STEMæ•™è‚²ä¸­çš„å¯¹è¯å¼è¾…å¯¼ã€‚æ–‡ç« æ‰¹è¯„äº†ç°æœ‰ä¾èµ–é™æ€RAGæµç¨‹çš„å•ä¸€å¤§è¯­è¨€æ¨¡å‹ç³»ç»Ÿçš„å±€é™æ€§ï¼ˆå°¤å…¶åœ¨æ•°å­—ä¿¡å·å¤„ç†ç­‰å¤æ‚é¢†åŸŸï¼‰ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªæ•´åˆè®¤çŸ¥æ¼”åŒ–è¿‡ç¨‹çš„ç³»ç»Ÿï¼Œé€šè¿‡ä¸‰å±‚æ¶æ„ï¼ˆè®¤çŸ¥æ„ŸçŸ¥å±‚ã€çŸ¥è¯†æ¼”åŒ–å±‚ã€å…ƒæ§åˆ¶å±‚ï¼‰å®ç°åŠ¨æ€çš„å­¦ç”Ÿå»ºæ¨¡ã€çŸ¥è¯†åº“ç®¡ç†å’Œæ•™å­¦ç­–ç•¥è°ƒæ•´ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨DSP-EduBenchåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºé™æ€RAGç­‰åŸºçº¿æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.23397v1">MegaChat: A Synthetic Persian Q&A Dataset for High-Quality Sales Chatbot Evaluation</a></td><td><details><summary>å±•å¼€</summary>Small and medium-sized enterprises (SMEs) in Iran increasingly leverage Telegram for sales, where real-time engagement is essential for conversion. However, developing AI-driven chatbots for this purpose requires large, high-quality question-and-answer (Q&A) datasets, which are typically expensive and resource-intensive to produce, especially for low-resource languages like Persian. In this paper, we introduce MegaChat, the first fully synthetic Persian Q&A dataset designed to evaluate intelligent sales chatbots in Telegram-based e-commerce. We propose a novel, automated multi-agent architecture that generates persona-aware Q&A pairs by collecting data from active Telegram shopping channels. The system employs specialized agents for question generation, validation, and refinement, ensuring the production of realistic and diverse conversational data. To evaluate answer generation, we compare three classic retrieval-augmented generation (RAG) models with our advanced agentic system, which features multi-query retrieval, reranking, and persona-aligned response synthesis. Using GPT-5.1 for evaluation across six quality dimensions, our results show that the agentic architecture outperformed traditional RAG models in 4 out of 5 diverse channels, demonstrating its ability to generate scalable, high-quality datasets without relying on expensive human annotation or complex fine-tuning. MegaChat provides SMEs with an efficient, cost-effective solution for building intelligent customer engagement systems in specialized commercial domains, enabling advancements in multilingual conversational AI for low-resource languages. Download: https://github.com/MegaChat-Tech/MegaChat-DataSet</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†MegaChatï¼Œä¸€ä¸ªé’ˆå¯¹æ³¢æ–¯è¯­çš„å…¨åˆæˆé—®ç­”æ•°æ®é›†ï¼Œç”¨äºTelegramç”µå•†ä¸­çš„æ™ºèƒ½é”€å”®èŠå¤©æœºå™¨äººè¯„ä¼°ã€‚ä½œè€…æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“æ¶æ„ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–ç”Ÿæˆä¸ªæ€§åŒ–çš„é—®ç­”å¯¹ï¼Œå¹¶ä¸ä¼ ç»ŸRAGæ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ™ºèƒ½ä½“æ¶æ„åœ¨å¤šæŸ¥è¯¢æ£€ç´¢ã€é‡æ’åºå’Œä¸ªæ€§åŒ–å“åº”åˆæˆæ–¹é¢ä¼˜äºä¼ ç»ŸRAGæ¨¡å‹ï¼Œèƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡æ•°æ®é›†ï¼Œæ— éœ€æ˜‚è´µçš„äººå·¥æ ‡æ³¨æˆ–å¤æ‚å¾®è°ƒã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.23281v1">MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web (Technical Report)</a></td><td><details><summary>å±•å¼€</summary>Large language model agents are increasingly used to automate web tasks such as product search, offer comparison, and checkout. Current research explores different interfaces through which these agents interact with websites, including traditional HTML browsing, retrieval-augmented generation (RAG) over pre-crawled content, communication via Web APIs using the Model Context Protocol (MCP), and natural-language querying through the NLWeb interface. However, no prior work has compared these four architectures within a single controlled environment using identical tasks.
  To address this gap, we introduce a testbed consisting of four simulated e-shops, each offering its products via HTML, MCP, and NLWeb interfaces. For each interface (HTML, RAG, MCP, and NLWeb) we develop specialized agents that perform the same sets of tasks, ranging from simple product searches and price comparisons to complex queries for complementary or substitute products and checkout processes. We evaluate the agents using GPT 4.1, GPT 5, GPT 5 mini, and Claude Sonnet 4 as underlying LLM. Our evaluation shows that the RAG, MCP and NLWeb agents outperform HTML on both effectiveness and efficiency. Averaged over all tasks, F1 rises from 0.67 for HTML to between 0.75 and 0.77 for the other agents. Token usage falls from about 241k for HTML to between 47k and 140k per task. The runtime per task drops from 291 seconds to between 50 and 62 seconds. The best overall configuration is RAG with GPT 5 achieving an F1 score of 0.87 and a completion rate of 0.79. Also taking cost into consideration, RAG with GPT 5 mini offers a good compromise between API usage fees and performance. Our experiments show the choice of the interaction interface has a substantial impact on both the effectiveness and efficiency of LLM-based web agents.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¯”è¾ƒäº†å››ç§LLMä»£ç†ä¸ç½‘ç«™äº¤äº’çš„æ¶æ„ï¼ˆHTMLæµè§ˆã€RAGã€MCPåè®®å’ŒNLWebæ¥å£ï¼‰ï¼Œåœ¨æ¨¡æ‹Ÿç”µå•†ç¯å¢ƒä¸­æ‰§è¡Œç›¸åŒä»»åŠ¡çš„æ€§èƒ½è¡¨ç°ï¼Œç»“æœè¡¨æ˜RAGç»“åˆGPT 5åœ¨æ•ˆæœå’Œæ•ˆç‡ä¸Šæœ€ä¼˜ï¼Œå¹¶åœ¨æˆæœ¬ä¸æ€§èƒ½é—´è¾¾åˆ°å¹³è¡¡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.04106v1">Retrieval-Augmented Few-Shot Prompting Versus Fine-Tuning for Code Vulnerability Detection</a></td><td><details><summary>å±•å¼€</summary>Few-shot prompting has emerged as a practical alternative to fine-tuning for leveraging the capabilities of large language models (LLMs) in specialized tasks. However, its effectiveness depends heavily on the selection and quality of in-context examples, particularly in complex domains. In this work, we examine retrieval-augmented prompting as a strategy to improve few-shot performance in code vulnerability detection, where the goal is to identify one or more security-relevant weaknesses present in a given code snippet from a predefined set of vulnerability categories. We perform a systematic evaluation using the Gemini-1.5-Flash model across three approaches: (1) standard few-shot prompting with randomly selected examples, (2) retrieval-augmented prompting using semantically similar examples, and (3) retrieval-based labeling, which assigns labels based on retrieved examples without model inference. Our results show that retrieval-augmented prompting consistently outperforms the other prompting strategies. At 20 shots, it achieves an F1 score of 74.05% and a partial match accuracy of 83.90%. We further compare this approach against zero-shot prompting and several fine-tuned models, including Gemini-1.5-Flash and smaller open-source models such as DistilBERT, DistilGPT2, and CodeBERT. Retrieval-augmented prompting outperforms both zero-shot (F1 score: 36.35%, partial match accuracy: 20.30%) and fine-tuned Gemini (F1 score: 59.31%, partial match accuracy: 53.10%), while avoiding the training time and cost associated with model fine-tuning. On the other hand, fine-tuning CodeBERT yields higher performance (F1 score: 91.22%, partial match accuracy: 91.30%) but requires additional training, maintenance effort, and resources.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æ£€ç´¢å¢å¼ºæç¤ºï¼ˆretrieval-augmented promptingï¼‰åœ¨ä»£ç æ¼æ´æ£€æµ‹ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œé€šè¿‡æ£€ç´¢è¯­ä¹‰ç›¸ä¼¼çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Gemini-1.5-Flashï¼‰çš„å°‘æ ·æœ¬å­¦ä¹ æ€§èƒ½ï¼Œè¯æ˜å…¶ä¼˜äºéšæœºç¤ºä¾‹é€‰æ‹©ã€é›¶æ ·æœ¬æç¤ºåŠéƒ¨åˆ†å¾®è°ƒæ¨¡å‹ï¼ŒåŒæ—¶é¿å¼€äº†å¾®è°ƒçš„æˆæœ¬å’Œèµ„æºæ¶ˆè€—ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.00126v1">RadDiff: Retrieval-Augmented Denoising Diffusion for Protein Inverse Folding</a></td><td><details><summary>å±•å¼€</summary>Protein inverse folding, the design of an amino acid sequence based on a target 3D structure, is a fundamental problem of computational protein engineering. Existing methods either generate sequences without leveraging external knowledge or relying on protein language models (PLMs). The former omits the evolutionary information stored in protein databases, while the latter is parameter-inefficient and inflexible to adapt to ever-growing protein data. To overcome the above drawbacks, in this paper we propose a novel method, called retrieval-augmented denoising diffusion (RadDiff), for protein inverse folding. Given the target protein backbone, RadDiff uses a hierarchical search strategy to efficiently retrieve structurally similar proteins from large protein databases. The retrieved structures are then aligned residue-by-residue to the target to construct a position-specific amino acid profile, which serves as an evolutionary-informed prior that conditions the denoising process. A lightweight integration module is further designed to incorporate this prior effectively. Experimental results on the CATH, PDB, and TS50 datasets show that RadDiff consistently outperforms existing methods, improving sequence recovery rate by up to 19%. Experimental results also demonstrate that RadDiff generates highly foldable sequences and scales effectively with database size.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRadDiffçš„æ£€ç´¢å¢å¼ºçš„è›‹ç™½è´¨é€†æŠ˜å æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä»å¤§å‹è›‹ç™½è´¨æ•°æ®åº“ä¸­æ£€ç´¢ç»“æ„ç›¸ä¼¼çš„è›‹ç™½è´¨ï¼Œå¹¶å°†æ£€ç´¢ç»“æœä½œä¸ºè¿›åŒ–ä¿¡æ¯å…ˆéªŒï¼Œç»“åˆå»å™ªæ‰©æ•£æ¨¡å‹ç”Ÿæˆæ°¨åŸºé…¸åºåˆ—ï¼Œæ˜¾è‘—æé«˜äº†åºåˆ—æ¢å¤ç‡å’Œå¯æŠ˜å æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.22858v1">RAG System for Supporting Japanese Litigation Procedures: Faithful Response Generation Complying with Legal Norms</a></td><td><details><summary>å±•å¼€</summary>This study discusses the essential components that a Retrieval-Augmented Generation (RAG)-based LLM system should possess in order to support Japanese medical litigation procedures complying with legal norms. In litigation, expert commissioners, such as physicians, architects, accountants, and engineers, provide specialized knowledge to help judges clarify points of dispute. When considering the substitution of these expert roles with a RAG-based LLM system, the constraint of strict adherence to legal norms is imposed. Specifically, three requirements arise: (1) the retrieval module must retrieve appropriate external knowledge relevant to the disputed issues in accordance with the principle prohibiting the use of private knowledge, (2) the responses generated must originate from the context provided by the RAG and remain faithful to that context, and (3) the retrieval module must reference external knowledge with appropriate timestamps corresponding to the issues at hand. This paper discusses the design of a RAG-based LLM system that satisfies these requirements.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æ¢è®¨äº†åŸºäºRAGçš„LLMç³»ç»Ÿåœ¨æ—¥æœ¬åŒ»ç–—è¯‰è®¼ç¨‹åºä¸­éœ€æ»¡è¶³çš„ä¸‰å¤§æ³•å¾‹è§„èŒƒè¦æ±‚ï¼ˆç²¾å‡†æ£€ç´¢äº‰è®®ç›¸å…³å¤–éƒ¨çŸ¥è¯†ã€ç”Ÿæˆå†…å®¹ä¸¥æ ¼ä¾èµ–æ£€ç´¢ä¸Šä¸‹æ–‡ã€ç¡®ä¿å¼•ç”¨çŸ¥è¯†çš„æ—¶æ•ˆæ€§ï¼‰ï¼Œå¹¶è®¾è®¡äº†ç¬¦åˆè¿™äº›çº¦æŸæ¡ä»¶çš„ç³»ç»Ÿæ¡†æ¶ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-27
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.23397v1">MegaChat: A Synthetic Persian Q&A Dataset for High-Quality Sales Chatbot Evaluation</a></td><td><details><summary>å±•å¼€</summary>Small and medium-sized enterprises (SMEs) in Iran increasingly leverage Telegram for sales, where real-time engagement is essential for conversion. However, developing AI-driven chatbots for this purpose requires large, high-quality question-and-answer (Q&A) datasets, which are typically expensive and resource-intensive to produce, especially for low-resource languages like Persian. In this paper, we introduce MegaChat, the first fully synthetic Persian Q&A dataset designed to evaluate intelligent sales chatbots in Telegram-based e-commerce. We propose a novel, automated multi-agent architecture that generates persona-aware Q&A pairs by collecting data from active Telegram shopping channels. The system employs specialized agents for question generation, validation, and refinement, ensuring the production of realistic and diverse conversational data. To evaluate answer generation, we compare three classic retrieval-augmented generation (RAG) models with our advanced agentic system, which features multi-query retrieval, reranking, and persona-aligned response synthesis. Using GPT-5.1 for evaluation across six quality dimensions, our results show that the agentic architecture outperformed traditional RAG models in 4 out of 5 diverse channels, demonstrating its ability to generate scalable, high-quality datasets without relying on expensive human annotation or complex fine-tuning. MegaChat provides SMEs with an efficient, cost-effective solution for building intelligent customer engagement systems in specialized commercial domains, enabling advancements in multilingual conversational AI for low-resource languages. Download: https://github.com/MegaChat-Tech/MegaChat-DataSet</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†MegaChatï¼Œä¸€ä¸ªä¸ºä¼Šæœ—ä¸­å°ä¼ä¸šè®¾è®¡çš„å…¨åˆæˆæ³¢æ–¯è¯­é—®ç­”æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°Telegramç”µå•†ä¸­çš„æ™ºèƒ½é”€å”®èŠå¤©æœºå™¨äººã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šæ™ºèƒ½ä½“æ¶æ„ï¼Œè‡ªåŠ¨ç”Ÿæˆè§’è‰²æ„ŸçŸ¥çš„é—®ç­”å¯¹ï¼Œå¹¶é€šè¿‡å¯¹æ¯”ä¸‰ç§ç»å…¸çš„RAGæ¨¡å‹ä¸é«˜çº§æ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆå…·æœ‰å¤šæŸ¥è¯¢æ£€ç´¢ã€é‡æ’åºå’Œè§’è‰²å¯¹é½å“åº”åˆæˆåŠŸèƒ½ï¼‰ï¼Œè¯æ˜äº†æ™ºèƒ½ä½“æ¶æ„åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä¼˜äºä¼ ç»ŸRAGæ¨¡å‹ã€‚è¯¥ç³»ç»Ÿä¸ºä½èµ„æºè¯­è¨€çš„å¤šè¯­è¨€å¯¹è¯AIæä¾›äº†é«˜æ•ˆã€ä½æˆæœ¬çš„è§£å†³æ–¹æ¡ˆã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.23281v1">MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web (Technical Report)</a></td><td><details><summary>å±•å¼€</summary>Large language model agents are increasingly used to automate web tasks such as product search, offer comparison, and checkout. Current research explores different interfaces through which these agents interact with websites, including traditional HTML browsing, retrieval-augmented generation (RAG) over pre-crawled content, communication via Web APIs using the Model Context Protocol (MCP), and natural-language querying through the NLWeb interface. However, no prior work has compared these four architectures within a single controlled environment using identical tasks.
  To address this gap, we introduce a testbed consisting of four simulated e-shops, each offering its products via HTML, MCP, and NLWeb interfaces. For each interface (HTML, RAG, MCP, and NLWeb) we develop specialized agents that perform the same sets of tasks, ranging from simple product searches and price comparisons to complex queries for complementary or substitute products and checkout processes. We evaluate the agents using GPT 4.1, GPT 5, GPT 5 mini, and Claude Sonnet 4 as underlying LLM. Our evaluation shows that the RAG, MCP and NLWeb agents outperform HTML on both effectiveness and efficiency. Averaged over all tasks, F1 rises from 0.67 for HTML to between 0.75 and 0.77 for the other agents. Token usage falls from about 241k for HTML to between 47k and 140k per task. The runtime per task drops from 291 seconds to between 50 and 62 seconds. The best overall configuration is RAG with GPT 5 achieving an F1 score of 0.87 and a completion rate of 0.79. Also taking cost into consideration, RAG with GPT 5 mini offers a good compromise between API usage fees and performance. Our experiments show the choice of the interaction interface has a substantial impact on both the effectiveness and efficiency of LLM-based web agents.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¤§è¯­è¨€æ¨¡å‹ä»£ç†åœ¨è‡ªåŠ¨åŒ–ç½‘ç»œä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ¯”è¾ƒäº†å››ç§äº¤äº’æ¶æ„ï¼ˆHTMLã€RAGã€MCPå’ŒNLWebï¼‰åœ¨ç›¸åŒä»»åŠ¡ä¸‹çš„æ•ˆæœä¸æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºRAGçš„ä»£ç†åœ¨å‡†ç¡®æ€§ã€è¿è¡Œæ—¶é—´å’Œæˆæœ¬æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶æ˜¯ç»“åˆGPT-5æ—¶ç»¼åˆæ€§èƒ½æœ€ä½³ï¼Œå¼ºè°ƒæ¥å£é€‰æ‹©å¯¹ç½‘ç»œä»£ç†æ•ˆèƒ½çš„é‡è¦å½±å“ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.04106v1">Retrieval-Augmented Few-Shot Prompting Versus Fine-Tuning for Code Vulnerability Detection</a></td><td><details><summary>å±•å¼€</summary>Few-shot prompting has emerged as a practical alternative to fine-tuning for leveraging the capabilities of large language models (LLMs) in specialized tasks. However, its effectiveness depends heavily on the selection and quality of in-context examples, particularly in complex domains. In this work, we examine retrieval-augmented prompting as a strategy to improve few-shot performance in code vulnerability detection, where the goal is to identify one or more security-relevant weaknesses present in a given code snippet from a predefined set of vulnerability categories. We perform a systematic evaluation using the Gemini-1.5-Flash model across three approaches: (1) standard few-shot prompting with randomly selected examples, (2) retrieval-augmented prompting using semantically similar examples, and (3) retrieval-based labeling, which assigns labels based on retrieved examples without model inference. Our results show that retrieval-augmented prompting consistently outperforms the other prompting strategies. At 20 shots, it achieves an F1 score of 74.05% and a partial match accuracy of 83.90%. We further compare this approach against zero-shot prompting and several fine-tuned models, including Gemini-1.5-Flash and smaller open-source models such as DistilBERT, DistilGPT2, and CodeBERT. Retrieval-augmented prompting outperforms both zero-shot (F1 score: 36.35%, partial match accuracy: 20.30%) and fine-tuned Gemini (F1 score: 59.31%, partial match accuracy: 53.10%), while avoiding the training time and cost associated with model fine-tuning. On the other hand, fine-tuning CodeBERT yields higher performance (F1 score: 91.22%, partial match accuracy: 91.30%) but requires additional training, maintenance effort, and resources.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†æ£€ç´¢å¢å¼ºæç¤ºï¼ˆretrieval-augmented promptingï¼‰åœ¨ä»£ç æ¼æ´æ£€æµ‹ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œé€šè¿‡ä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢è¯­ä¹‰ç›¸ä¼¼çš„ç¤ºä¾‹æ¥å¢å¼ºå°‘æ ·æœ¬æç¤ºï¼ˆfew-shot promptingï¼‰çš„æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºéšæœºé€‰æ‹©ç¤ºä¾‹çš„æ ‡å‡†å°‘æ ·æœ¬æç¤ºå’ŒåŸºäºæ£€ç´¢çš„æ ‡ç­¾åˆ†é…ï¼Œä¸”åœ¨é¿å…å¾®è°ƒæˆæœ¬çš„åŒæ—¶ä¼˜äºé›¶æ ·æœ¬æç¤ºå’Œéƒ¨åˆ†å¾®è°ƒæ¨¡å‹ã€‚å°½ç®¡å¾®è°ƒæ¨¡å‹ï¼ˆå¦‚CodeBERTï¼‰è¡¨ç°æ›´ä¼˜ï¼Œä½†æ£€ç´¢å¢å¼ºæç¤ºæä¾›äº†é«˜æ•ˆä¸”ä½èµ„æºæ¶ˆè€—çš„æ›¿ä»£æ–¹æ¡ˆã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2512.00126v1">RadDiff: Retrieval-Augmented Denoising Diffusion for Protein Inverse Folding</a></td><td><details><summary>å±•å¼€</summary>Protein inverse folding, the design of an amino acid sequence based on a target 3D structure, is a fundamental problem of computational protein engineering. Existing methods either generate sequences without leveraging external knowledge or relying on protein language models (PLMs). The former omits the evolutionary information stored in protein databases, while the latter is parameter-inefficient and inflexible to adapt to ever-growing protein data. To overcome the above drawbacks, in this paper we propose a novel method, called retrieval-augmented denoising diffusion (RadDiff), for protein inverse folding. Given the target protein backbone, RadDiff uses a hierarchical search strategy to efficiently retrieve structurally similar proteins from large protein databases. The retrieved structures are then aligned residue-by-residue to the target to construct a position-specific amino acid profile, which serves as an evolutionary-informed prior that conditions the denoising process. A lightweight integration module is further designed to incorporate this prior effectively. Experimental results on the CATH, PDB, and TS50 datasets show that RadDiff consistently outperforms existing methods, improving sequence recovery rate by up to 19%. Experimental results also demonstrate that RadDiff generates highly foldable sequences and scales effectively with database size.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRadDiffçš„è›‹ç™½è´¨é€†æŠ˜å æ–°æ–¹æ³•ï¼Œç»“åˆäº†æ£€ç´¢å’Œç”ŸæˆæŠ€æœ¯ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»å¤§å‹è›‹ç™½è´¨æ•°æ®åº“ä¸­é«˜æ•ˆæ£€ç´¢ç»“æ„ç›¸ä¼¼çš„è›‹ç™½è´¨ï¼Œæ„å»ºè¿›åŒ–ä¿¡æ¯æŒ‡å¯¼çš„æ°¨åŸºé…¸åºåˆ—åˆ†å¸ƒï¼Œå¹¶åˆ©ç”¨è½»é‡çº§æ¨¡å—å°†è¿™äº›ä¿¡æ¯æ•´åˆåˆ°å»å™ªæ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»è€Œæå‡åºåˆ—æ¢å¤ç‡å¹¶å¢å¼ºå¯æ‰©å±•æ€§ã€‚å®éªŒè¡¨æ˜RadDiffåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.22858v1">RAG System for Supporting Japanese Litigation Procedures: Faithful Response Generation Complying with Legal Norms</a></td><td><details><summary>å±•å¼€</summary>This study discusses the essential components that a Retrieval-Augmented Generation (RAG)-based LLM system should possess in order to support Japanese medical litigation procedures complying with legal norms. In litigation, expert commissioners, such as physicians, architects, accountants, and engineers, provide specialized knowledge to help judges clarify points of dispute. When considering the substitution of these expert roles with a RAG-based LLM system, the constraint of strict adherence to legal norms is imposed. Specifically, three requirements arise: (1) the retrieval module must retrieve appropriate external knowledge relevant to the disputed issues in accordance with the principle prohibiting the use of private knowledge, (2) the responses generated must originate from the context provided by the RAG and remain faithful to that context, and (3) the retrieval module must reference external knowledge with appropriate timestamps corresponding to the issues at hand. This paper discusses the design of a RAG-based LLM system that satisfies these requirements.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åŸºäºRAGçš„å¤§å‹è¯­è¨€æ¨¡å‹ç³»ç»Ÿåœ¨æ”¯æŒæ—¥æœ¬åŒ»ç–—è¯‰è®¼ç¨‹åºæ—¶éœ€æ»¡è¶³çš„æ³•å¾‹è§„èŒƒè¦æ±‚ï¼Œæå‡ºäº†æ£€ç´¢æ¨¡å—éœ€éµå¾ªç¦æ­¢ä½¿ç”¨ç§äººçŸ¥è¯†ã€ç”Ÿæˆå†…å®¹å¿…é¡»å¿ å®äºæ£€ç´¢ä¸Šä¸‹æ–‡ä»¥åŠå¼•ç”¨å…·æœ‰æ—¶æ•ˆæ€§çŸ¥è¯†ç­‰ä¸‰å¤§è®¾è®¡åŸåˆ™ï¼Œæ—¨åœ¨æ›¿ä»£ä¼ ç»Ÿä¸“å®¶å§”å‘˜è§’è‰²å¹¶ç¡®ä¿ç³»ç»Ÿç¬¦åˆæ³•å¾‹çº¦æŸã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.22715v1">ReAG: Reasoning-Augmented Generation for Knowledge-based Visual Question Answering</a></td><td><details><summary>å±•å¼€</summary>Multimodal Large Language Models (MLLMs) have shown impressive capabilities in jointly understanding text, images, and videos, often evaluated via Visual Question Answering (VQA). However, even state-of-the-art MLLMs struggle with domain-specific or knowledge-intensive queries, where relevant information is underrepresented in pre-training data. Knowledge-based VQA (KB-VQA) addresses this by retrieving external documents to condition answer generation, but current retrieval-augmented approaches suffer from low precision, noisy passages, and limited reasoning. To address this, we propose ReAG, a novel Reasoning-Augmented Multimodal RAG approach that combines coarse- and fine-grained retrieval with a critic model that filters irrelevant passages, ensuring high-quality additional context. The model follows a multi-stage training strategy leveraging reinforcement learning to enhance reasoning over retrieved content, while supervised fine-tuning serves only as a cold start. Extensive experiments on Encyclopedic-VQA and InfoSeek demonstrate that ReAG significantly outperforms prior methods, improving answer accuracy and providing interpretable reasoning grounded in retrieved evidence. Our source code is publicly available at: https://github.com/aimagelab/ReAG.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºReAGçš„æ–°å‹æ¨ç†å¢å¼ºå¤šæ¨¡æ€RAGæ–¹æ³•ï¼Œç»“åˆç²—ç²’åº¦ä¸ç»†ç²’åº¦æ£€ç´¢ï¼Œå¹¶å¼•å…¥æ‰¹åˆ¤æ¨¡å‹è¿‡æ»¤æ— å…³æ®µè½ä»¥æå‡å¤–éƒ¨çŸ¥è¯†è´¨é‡ã€‚é€šè¿‡å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ˆåŒ…æ‹¬å¼ºåŒ–å­¦ä¹ å¢å¼ºæ¨ç†èƒ½åŠ›ï¼‰ï¼Œè¯¥æ–¹æ³•åœ¨çŸ¥è¯†å¯†é›†å‹è§†è§‰é—®ç­”ä»»åŠ¡ï¼ˆå¦‚Encyclopedic-VQAï¼‰ä¸­æ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œæé«˜äº†ç­”æ¡ˆå‡†ç¡®æ€§å¹¶å®ç°åŸºäºæ£€ç´¢è¯æ®çš„å¯è§£é‡Šæ¨ç†ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.22584v1">Smarter, not Bigger: Fine-Tuned RAG-Enhanced LLMs for Automotive HIL Testing</a></td><td><details><summary>å±•å¼€</summary>Hardware-in-the-Loop (HIL) testing is essential for automotive validation but suffers from fragmented and underutilized test artifacts. This paper presents HIL-GPT, a retrieval-augmented generation (RAG) system integrating domain-adapted large language models (LLMs) with semantic retrieval. HIL-GPT leverages embedding fine-tuning using a domain-specific dataset constructed via heuristic mining and LLM-assisted synthesis, combined with vector indexing for scalable, traceable test case and requirement retrieval. Experiments show that fine-tuned compact models, such as \texttt{bge-base-en-v1.5}, achieve a superior trade-off between accuracy, latency, and cost compared to larger models, challenging the notion that bigger is always better. An A/B user study further confirms that RAG-enhanced assistants improve perceived helpfulness, truthfulness, and satisfaction over general-purpose LLMs. These findings provide insights for deploying efficient, domain-aligned LLM-based assistants in industrial HIL environments.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†HIL-GPTç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯çš„è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨é€šè¿‡å¾®è°ƒé¢†åŸŸé€‚åº”çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè¯­ä¹‰æ£€ç´¢æ¥æé«˜æ±½è½¦ç¡¬ä»¶åœ¨ç¯ï¼ˆHILï¼‰æµ‹è¯•çš„æ•ˆç‡ã€‚ç ”ç©¶å‘ç°ï¼Œç»è¿‡å¾®è°ƒçš„ç´§å‡‘æ¨¡å‹åœ¨å‡†ç¡®æ€§ã€å»¶è¿Ÿå’Œæˆæœ¬ä¹‹é—´å®ç°äº†è¾ƒå¥½çš„å¹³è¡¡ï¼Œå¹¶é€šè¿‡A/Bç”¨æˆ·ç ”ç©¶éªŒè¯äº†RAGå¢å¼ºåŠ©æ‰‹åœ¨å·¥ä¸šHILç¯å¢ƒä¸­çš„å®ç”¨æ€§å’Œç”¨æˆ·æ»¡æ„åº¦æå‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.22490v1">SciPostGen: Bridging the Gap between Scientific Papers and Poster Layouts</a></td><td><details><summary>å±•å¼€</summary>As the number of scientific papers continues to grow, there is a demand for approaches that can effectively convey research findings, with posters serving as a key medium for presenting paper contents. Poster layouts determine how effectively research is communicated and understood, highlighting their growing importance. In particular, a gap remains in understanding how papers correspond to the layouts that present them, which calls for datasets with paired annotations at scale. To bridge this gap, we introduce SciPostGen, a large-scale dataset for understanding and generating poster layouts from scientific papers. Our analyses based on SciPostGen show that paper structures are associated with the number of layout elements in posters. Based on this insight, we explore a framework, Retrieval-Augmented Poster Layout Generation, which retrieves layouts consistent with a given paper and uses them as guidance for layout generation. We conducted experiments under two conditions: with and without layout constraints typically specified by poster creators. The results show that the retriever estimates layouts aligned with paper structures, and our framework generates layouts that also satisfy given constraints.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºSciPostGençš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œç”¨äºåˆ†æå’Œç”Ÿæˆç§‘å­¦æµ·æŠ¥å¸ƒå±€ï¼Œå¹¶æ¢ç´¢äº†ä¸€ç§åŸºäºæ£€ç´¢å¢å¼ºçš„æµ·æŠ¥å¸ƒå±€ç”Ÿæˆæ¡†æ¶ï¼ˆRetrieval-Augmented Poster Layout Generationï¼‰ï¼Œé€šè¿‡æ£€ç´¢ä¸è®ºæ–‡ç»“æ„ä¸€è‡´çš„å¸ƒå±€ä½œä¸ºç”Ÿæˆå¼•å¯¼ï¼Œåœ¨æœ‰æ— å¸ƒå±€çº¦æŸæ¡ä»¶ä¸‹éªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.22333v1">PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel</a></td><td><details><summary>å±•å¼€</summary>LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: *one-query-per-CTA* execution repeatedly loads shared prefix KV cache, while *one-size-fits-all* tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.
  This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 67.4% on average and TPOT by 13.6-83.4% under the same configurations against state-of-the-art attention kernels.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºPATçš„å‰ç¼€æ„ŸçŸ¥æ³¨æ„åŠ›å†…æ ¸å®ç°ï¼Œé’ˆå¯¹LLMè§£ç ä¸­çš„å†…å­˜ç“¶é¢ˆé—®é¢˜è¿›è¡Œä¼˜åŒ–ã€‚å®ƒé€šè¿‡åˆ©ç”¨RAGç­‰åœºæ™¯ä¸­è¯·æ±‚é—´çš„å…±äº«å‰ç¼€ï¼ˆå¦‚ç³»ç»Ÿæç¤ºã€å·¥å…·æ¨¡æ¿å’ŒRAGä¸Šä¸‹æ–‡ï¼‰ï¼Œé‡‡ç”¨"pack-forward-merge"èŒƒå¼å‡å°‘KVç¼“å­˜é‡å¤åŠ è½½ï¼Œæœ€ç»ˆåœ¨vLLMä¸­å®ç°æ³¨æ„åŠ›å»¶è¿Ÿé™ä½67.4%ï¼Œæ˜¾è‘—æå‡è§£ç æ•ˆç‡ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-26
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.21121v1">Beyond Patch Aggregation: 3-Pass Pyramid Indexing for Vision-Enhanced Document Retrieval</a></td><td><details><summary>å±•å¼€</summary>Document centric RAG pipelines usually begin with OCR, followed by brittle heuristics for chunking, table parsing, and layout reconstruction. These text first workflows are costly to maintain, sensitive to small layout shifts, and often lose the spatial cues that contain the answer. Vision first retrieval has emerged as a strong alternative. By operating directly on page images, systems like ColPali and ColQwen preserve structure and reduce pipeline complexity while achieving strong benchmark performance. However, these late interaction models tie retrieval to a specific vision backbone and require storing hundreds of patch embeddings per page, creating high memory overhead and complicating large scale deployment.
  We introduce VisionRAG, a multimodal retrieval system that is OCR free and model agnostic. VisionRAG indexes documents directly as images, preserving layout, tables, and spatial cues, and builds semantic vectors without committing to a specific extraction. Our three pass pyramid indexing framework creates vectors using global page summaries, section headers, visual hotspots, and fact level cues. These summaries act as lightweight retrieval surrogates. At query time, VisionRAG retrieves the most relevant pages using the pyramid index, then forwards the raw page image encoded as base64 to a multimodal LLM for final question answering. During retrieval, reciprocal rank fusion integrates signals across the pyramid to produce robust ranking.
  VisionRAG stores only 17 to 27 vectors per page, matching the efficiency of patch based methods while staying flexible across multimodal encoders. On financial document benchmarks, it achieves 0.8051 accuracy at 10 on FinanceBench and 0.9629 recall at 100 on TAT DQA. These results show that OCR free, summary guided multimodal retrieval is a practical and scalable alternative to traditional text extraction pipelines.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºVisionRAGçš„å¤šæ¨¡æ€æ£€ç´¢ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ–‡æ¡£ä¸­å¿ƒRAGæµç¨‹ä¸­å› OCRå’Œæ–‡æœ¬ä¼˜å…ˆå¤„ç†å¯¼è‡´çš„å¸ƒå±€æ•æ„Ÿæ€§å’Œç»“æ„ä¸¢å¤±é—®é¢˜ã€‚VisionRAGé€šè¿‡ç›´æ¥ç´¢å¼•æ–‡æ¡£å›¾åƒï¼ˆè·³è¿‡OCRï¼‰ï¼Œä¿ç•™å¸ƒå±€å’Œç©ºé—´çº¿ç´¢ï¼Œå¹¶é‡‡ç”¨ä¸‰é˜¶æ®µé‡‘å­—å¡”ç´¢å¼•æ¡†æ¶ç”Ÿæˆè½»é‡çº§è¯­ä¹‰å‘é‡è¿›è¡Œæ£€ç´¢ã€‚ç³»ç»Ÿåœ¨æŸ¥è¯¢æ—¶èåˆå¤šå±‚çº§ä¿¡å·ï¼Œæœ€ç»ˆå°†åŸå§‹å›¾åƒä¼ é€’ç»™å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆç­”æ¡ˆï¼Œæ˜¾è‘—é™ä½äº†å†…å­˜å¼€é”€å¹¶æå‡äº†é‡‘èæ–‡æ¡£åŸºå‡†ä»»åŠ¡çš„æ€§èƒ½ï¼ˆå¦‚FinanceBenchå’ŒTAT DQAï¼‰ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.21066v1">Context-Aware Pragmatic Metacognitive Prompting for Sarcasm Detection</a></td><td><details><summary>å±•å¼€</summary>Detecting sarcasm remains a challenging task in the areas of Natural Language Processing (NLP) despite recent advances in neural network approaches. Currently, Pre-trained Language Models (PLMs) and Large Language Models (LLMs) are the preferred approach for sarcasm detection. However, the complexity of sarcastic text, combined with linguistic diversity and cultural variation across communities, has made the task more difficult even for PLMs and LLMs. Beyond that, those models also exhibit unreliable detection of words or tokens that require extra grounding for analysis. Building on a state-of-the-art prompting method in LLMs for sarcasm detection called Pragmatic Metacognitive Prompting (PMP), we introduce a retrieval-aware approach that incorporates retrieved contextual information for each target text. Our pipeline explores two complementary ways to provide context: adding non-parametric knowledge using web-based retrieval when the model lacks necessary background, and eliciting the model's own internal knowledge for a self-knowledge awareness strategy. We evaluated our approach with three datasets, such as Twitter Indonesia Sarcastic, SemEval-2018 Task 3, and MUStARD. Non-parametric retrieval resulted in a significant 9.87% macro-F1 improvement on Twitter Indonesia Sarcastic compared to the original PMP method. Self-knowledge retrieval improves macro-F1 by 3.29% on Semeval and by 4.08% on MUStARD. These findings highlight the importance of context in enhancing LLMs performance in sarcasm detection task, particularly the involvement of culturally specific slang, references, or unknown terms to the LLMs. Future work will focus on optimizing the retrieval of relevant contextual information and examining how retrieval quality affects performance. The experiment code is available at: https://github.com/wllchrst/sarcasm-detection_pmp_knowledge-base.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯çš„æ”¹è¿›æ–¹æ³•ï¼Œç”¨äºæå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®½åˆºæ£€æµ‹ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚ä½œè€…åœ¨ç°æœ‰çš„Pragmatic Metacognitive Promptingï¼ˆPMPï¼‰æ–¹æ³•åŸºç¡€ä¸Šï¼Œå¼•å…¥äº†ä¸¤ç§æ£€ç´¢ç­–ç•¥ï¼šåŸºäºå¤–éƒ¨ç½‘ç»œæ£€ç´¢çš„éå‚æ•°åŒ–çŸ¥è¯†è¡¥å……ï¼ˆè§£å†³æ¨¡å‹èƒŒæ™¯çŸ¥è¯†ä¸è¶³ï¼‰å’Œæ¨¡å‹å†…éƒ¨çŸ¥è¯†è‡ªæ£€ç´¢ï¼ˆè‡ªæˆ‘è®¤çŸ¥ç­–ç•¥ï¼‰ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Twitter Indonesia Sarcasticã€SemEval-2018å’ŒMUStARDæ•°æ®é›†ä¸Šåˆ†åˆ«å–å¾—äº†9.87%ã€3.29%å’Œ4.08%çš„å®F1å€¼æå‡ï¼Œå¼ºè°ƒäº†ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå°¤å…¶æ˜¯æ–‡åŒ–ç‰¹å®šè¯æ±‡å’ŒæœªçŸ¥æœ¯è¯­ï¼‰å¯¹è®½åˆºæ£€æµ‹çš„é‡è¦æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.21016v1">Gated KalmaNet: A Fading Memory Layer Through Test-Time Ridge Regression</a></td><td><details><summary>å±•å¼€</summary>As efficient alternatives to softmax Attention, linear state-space models (SSMs) achieve constant memory and linear compute, but maintain only a lossy, fading summary of the past, often leading to inferior performance in recall oriented tasks. We propose Gated KalmaNet (GKA), a layer that reduces this gap by accounting for the full past when predicting the next token, while maintaining SSM-style efficiency. GKA achieves this by solving an online ridge regression problem at test time, with constant memory and linear compute cost in the sequence length. Drawing inspiration from the Kalman Filter, we iteratively solve the online ridge regression problem. However, a critical insight is that standard Kalman filter equations are numerically unstable in low-precision environments (like bfloat16) and difficult to parallelize in modern hardware. We address both challenges through two key innovations: (1) an adaptive regularization strategy with input-dependent gating that controls the condition number of the ridge regression, ensuring numerical stability while balancing memory retention. And (2) the use of Chebyshev Iteration instead of other conventional iterative solvers, which we demonstrate to be more stable in low-precision settings. To further improve scalability, we develop a hardware-aware chunk-wise implementation of Chebyshev Iteration along with custom kernels for backpropagating through our adaptive regularization and gating mechanisms. Empirically, GKA shows strong language understanding capabilites on short-context tasks outperforming existing SSM layers (like Mamba2, GLA and Gated DeltaNet). On long-context, GKA excels at real-world RAG and LongQA tasks up to 128k tokens, achieving more than $10$% relative improvement over other fading memory baselines.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†Gated KalmaNet (GKA)ï¼Œä¸€ç§æ”¹è¿›çš„çº¿æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰å±‚ï¼Œé€šè¿‡åœ¨çº¿è§£å†³å²­å›å½’é—®é¢˜æ¥å¢å¼ºå¯¹è¿‡å»ä¿¡æ¯çš„ä¿ç•™èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒé«˜æ•ˆè®¡ç®—ã€‚GKAåœ¨ä½ç²¾åº¦ç¯å¢ƒä¸‹é€šè¿‡è‡ªé€‚åº”æ­£åˆ™åŒ–å’Œåˆ‡æ¯”é›ªå¤«è¿­ä»£ç¡®ä¿æ•°å€¼ç¨³å®šæ€§ï¼Œå¹¶åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ï¼ˆå¦‚RAGå’ŒLongQAï¼‰ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–åŸºçº¿æ¨¡å‹ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.21002v1">Knowledge Completes the Vision: A Multimodal Entity-aware Retrieval-Augmented Generation Framework for News Image Captioning</a></td><td><details><summary>å±•å¼€</summary>News image captioning aims to produce journalistically informative descriptions by combining visual content with contextual cues from associated articles. Despite recent advances, existing methods struggle with three key challenges: (1) incomplete information coverage, (2) weak cross-modal alignment, and (3) suboptimal visual-entity grounding. To address these issues, we introduce MERGE, the first Multimodal Entity-aware Retrieval-augmented GEneration framework for news image captioning. MERGE constructs an entity-centric multimodal knowledge base (EMKB) that integrates textual, visual, and structured knowledge, enabling enriched background retrieval. It improves cross-modal alignment through a multistage hypothesis-caption strategy and enhances visual-entity matching via dynamic retrieval guided by image content. Extensive experiments on GoodNews and NYTimes800k show that MERGE significantly outperforms state-of-the-art baselines, with CIDEr gains of +6.84 and +1.16 in caption quality, and F1-score improvements of +4.14 and +2.64 in named entity recognition. Notably, MERGE also generalizes well to the unseen Visual News dataset, achieving +20.17 in CIDEr and +6.22 in F1-score, demonstrating strong robustness and domain adaptability.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMERGEçš„å¤šæ¨¡æ€å®ä½“æ„ŸçŸ¥æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºæ–°é—»å›¾åƒæè¿°ç”Ÿæˆã€‚MERGEé€šè¿‡æ„å»ºå®ä½“ä¸­å¿ƒçš„å¤šæ¨¡æ€çŸ¥è¯†åº“ï¼ˆEMKBï¼‰æ•´åˆæ–‡æœ¬ã€è§†è§‰å’Œç»“æ„åŒ–çŸ¥è¯†ï¼Œå®ç°èƒŒæ™¯ä¿¡æ¯å¢å¼ºæ£€ç´¢ï¼Œå¹¶é‡‡ç”¨å¤šé˜¶æ®µå‡è®¾-æè¿°ç­–ç•¥æ”¹å–„è·¨æ¨¡æ€å¯¹é½ï¼Œä»¥åŠé€šè¿‡å›¾åƒå†…å®¹å¼•å¯¼çš„åŠ¨æ€æ£€ç´¢ä¼˜åŒ–è§†è§‰-å®ä½“åŒ¹é…ã€‚å®éªŒè¡¨æ˜ï¼ŒMERGEåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æè¿°è´¨é‡å’Œå®ä½“è¯†åˆ«æ€§èƒ½ï¼Œå±•ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§å’Œé¢†åŸŸé€‚åº”æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.20965v1">TrafficLens: Multi-Camera Traffic Video Analysis Using LLMs</a></td><td><details><summary>å±•å¼€</summary>Traffic cameras are essential in urban areas, playing a crucial role in intelligent transportation systems. Multiple cameras at intersections enhance law enforcement capabilities, traffic management, and pedestrian safety. However, efficiently managing and analyzing multi-camera feeds poses challenges due to the vast amount of data. Analyzing such huge video data requires advanced analytical tools. While Large Language Models (LLMs) like ChatGPT, equipped with retrieval-augmented generation (RAG) systems, excel in text-based tasks, integrating them into traffic video analysis demands converting video data into text using a Vision-Language Model (VLM), which is time-consuming and delays the timely utilization of traffic videos for generating insights and investigating incidents. To address these challenges, we propose TrafficLens, a tailored algorithm for multi-camera traffic intersections. TrafficLens employs a sequential approach, utilizing overlapping coverage areas of cameras. It iteratively applies VLMs with varying token limits, using previous outputs as prompts for subsequent cameras, enabling rapid generation of detailed textual descriptions while reducing processing time. Additionally, TrafficLens intelligently bypasses redundant VLM invocations through an object-level similarity detector. Experimental results with real-world datasets demonstrate that TrafficLens reduces video-to-text conversion time by up to $4\times$ while maintaining information accuracy.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºTrafficLensç®—æ³•ï¼Œé€šè¿‡ç»“åˆVision-Language Modelï¼ˆVLMï¼‰å’Œç±»RAGçš„è¿­ä»£æç¤ºæŠ€æœ¯ï¼Œä¼˜åŒ–å¤šæ‘„åƒå¤´äº¤é€šè§†é¢‘çš„æ–‡æœ¬è½¬æ¢æ•ˆç‡ï¼Œå‡å°‘å†—ä½™å¤„ç†å¹¶æå‡åˆ†æé€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒä¿¡æ¯å‡†ç¡®æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.20940v1">Chatty-KG: A Multi-Agent AI System for On-Demand Conversational Question Answering over Knowledge Graphs</a></td><td><details><summary>å±•å¼€</summary>Conversational Question Answering over Knowledge Graphs (KGs) combines the factual grounding of KG-based QA with the interactive nature of dialogue systems. KGs are widely used in enterprise and domain applications to provide structured, evolving, and reliable knowledge. Large language models (LLMs) enable natural and context-aware conversations, but lack direct access to private and dynamic KGs. Retrieval-augmented generation (RAG) systems can retrieve graph content but often serialize structure, struggle with multi-turn context, and require heavy indexing. Traditional KGQA systems preserve structure but typically support only single-turn QA, incur high latency, and struggle with coreference and context tracking. To address these limitations, we propose Chatty-KG, a modular multi-agent system for conversational QA over KGs. Chatty-KG combines RAG-style retrieval with structured execution by generating SPARQL queries through task-specialized LLM agents. These agents collaborate for contextual interpretation, dialogue tracking, entity and relation linking, and efficient query planning, enabling accurate and low-latency translation of natural questions into executable queries. Experiments on large and diverse KGs show that Chatty-KG significantly outperforms state-of-the-art baselines in both single-turn and multi-turn settings, achieving higher F1 and P@1 scores. Its modular design preserves dialogue coherence and supports evolving KGs without fine-tuning or pre-processing. Evaluations with commercial (e.g., GPT-4o, Gemini-2.0) and open-weight (e.g., Phi-4, Gemma 3) LLMs confirm broad compatibility and stable performance. Overall, Chatty-KG unifies conversational flexibility with structured KG grounding, offering a scalable and extensible approach for reliable multi-turn KGQA.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºChatty-KGçš„æ¨¡å—åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œç”¨äºåœ¨çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ä¸Šè¿›è¡Œå¯¹è¯å¼é—®ç­”ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é£æ ¼çš„æ£€ç´¢ä¸ç»“æ„åŒ–æ‰§è¡Œï¼Œé€šè¿‡ä»»åŠ¡ä¸“ç”¨çš„LLMæ™ºèƒ½ä½“ç”ŸæˆSPARQLæŸ¥è¯¢ï¼Œä»¥è§£å†³ä¼ ç»ŸKGQAç³»ç»Ÿåœ¨ä¸Šä¸‹æ–‡è·Ÿè¸ªã€å»¶è¿Ÿå’Œæ ¸å¿ƒferenceæ–¹é¢çš„å±€é™æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒChatty-KGåœ¨å•è½®å’Œå¤šè½®é—®ç­”ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå¹¶æ”¯æŒåŠ¨æ€æ›´æ–°çš„çŸ¥è¯†å›¾è°±ï¼Œæ— éœ€å¾®è°ƒæˆ–é¢„å¤„ç†ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-25
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.20513v1">DesignPref: Capturing Personal Preferences in Visual Design Generation</a></td><td><details><summary>å±•å¼€</summary>Generative models, such as large language models and text-to-image diffusion models, are increasingly used to create visual designs like user interfaces (UIs) and presentation slides. Finetuning and benchmarking these generative models have often relied on datasets of human-annotated design preferences. Yet, due to the subjective and highly personalized nature of visual design, preference varies widely among individuals. In this paper, we study this problem by introducing DesignPref, a dataset of 12k pairwise comparisons of UI design generation annotated by 20 professional designers with multi-level preference ratings. We found that among trained designers, substantial levels of disagreement exist (Krippendorff's alpha = 0.25 for binary preferences). Natural language rationales provided by these designers indicate that disagreements stem from differing perceptions of various design aspect importance and individual preferences. With DesignPref, we demonstrate that traditional majority-voting methods for training aggregated judge models often do not accurately reflect individual preferences. To address this challenge, we investigate multiple personalization strategies, particularly fine-tuning or incorporating designer-specific annotations into RAG pipelines. Our results show that personalized models consistently outperform aggregated baseline models in predicting individual designers' preferences, even when using 20 times fewer examples. Our work provides the first dataset to study personalized visual design evaluation and support future research into modeling individual design taste.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†è§†è§‰è®¾è®¡ç”Ÿæˆä¸­ä¸ªæ€§åŒ–åå¥½çš„é—®é¢˜ï¼Œé€šè¿‡æ„å»ºDesignPrefæ•°æ®é›†ï¼ˆåŒ…å«ä¸“ä¸šè®¾è®¡å¸ˆçš„å¤šçº§åå¥½è¯„åˆ†ï¼‰ï¼Œå‘ç°ä¼ ç»Ÿå¤šæ•°æŠ•ç¥¨æ–¹æ³•éš¾ä»¥åæ˜ ä¸ªä½“å·®å¼‚ï¼Œå¹¶æå‡ºç»“åˆè®¾è®¡å¸ˆç‰¹å®šæ³¨é‡Šçš„RAGæµç¨‹ç­‰ä¸ªæ€§åŒ–ç­–ç•¥ï¼Œå®éªŒè¯æ˜ä¸ªæ€§åŒ–æ¨¡å‹èƒ½æ›´å‡†ç¡®é¢„æµ‹ä¸ªä½“åå¥½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.20333v1">NNGPT: Rethinking AutoML with Large Language Models</a></td><td><details><summary>å±•å¼€</summary>Building self-improving AI systems remains a fundamental challenge in the AI domain. We present NNGPT, an open-source framework that turns a large language model (LLM) into a self-improving AutoML engine for neural network development, primarily for computer vision. Unlike previous frameworks, NNGPT extends the dataset of neural networks by generating new models, enabling continuous fine-tuning of LLMs based on closed-loop system of generation, assessment, and self-improvement. It integrates within one unified workflow five synergistic LLM-based pipelines: zero-shot architecture synthesis, hyperparameter optimization (HPO), code-aware accuracy/early-stop prediction, retrieval-augmented synthesis of scope-closed PyTorch blocks (NN-RAG), and reinforcement learning. Built on the LEMUR dataset as an audited corpus with reproducible metrics, NNGPT emits from a single prompt and validates network architecture, preprocessing code, and hyperparameters, executes them end-to-end, and learns from result. The PyTorch adapter makes NNGPT framework-agnostic, enabling strong performance: NN-RAG achieves 73% executability on 1,289 targets, 3-shot prompting boosts accuracy on common datasets, and hash-based deduplication saves hundreds of runs. One-shot prediction matches search-based AutoML, reducing the need for numerous trials. HPO on LEMUR achieves RMSE 0.60, outperforming Optuna (0.64), while the code-aware predictor reaches RMSE 0.14 with Pearson r=0.78. The system has already generated over 5K validated models, proving NNGPT as an autonomous AutoML engine. Upon acceptance, the code, prompts, and checkpoints will be released for public access to enable reproducibility and facilitate community usage.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†NNGPTæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè‡ªæˆ‘æ”¹è¿›çš„AutoMLå¼•æ“ï¼Œä¸“æ³¨äºç¥ç»ç½‘ç»œå¼€å‘ï¼ˆå°¤å…¶æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸï¼‰ã€‚å…¶ä¸­åŒ…å«ä¸€ä¸ªåä¸ºNN-RAGçš„æ¨¡å—ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºåˆæˆæŠ€æœ¯ç”Ÿæˆç¬¦åˆç‰¹å®šèŒƒå›´çš„PyTorchä»£ç å—ï¼Œå¹¶ç»“åˆç”Ÿæˆã€è¯„ä¼°å’Œè‡ªæˆ‘æ”¹è¿›çš„é—­ç¯ç³»ç»Ÿæ¥æŒç»­ä¼˜åŒ–LLMã€‚æ­¤å¤–ï¼Œæ¡†æ¶è¿˜æ•´åˆäº†é›¶æ ·æœ¬æ¶æ„åˆæˆã€è¶…å‚æ•°ä¼˜åŒ–ã€ä»£ç æ„ŸçŸ¥é¢„æµ‹å’Œå¼ºåŒ–å­¦ä¹ ç­‰å¤šç§LLMé©±åŠ¨çš„æµç¨‹ï¼ŒåŸºäºLEMURæ•°æ®é›†å®ç°äº†ç«¯åˆ°ç«¯çš„æ¨¡å‹ç”Ÿæˆä¸éªŒè¯ï¼Œæ˜¾è‘—æå‡äº†AutoMLçš„æ•ˆç‡ä¸æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.20227v1">HKRAG: Holistic Knowledge Retrieval-Augmented Generation Over Visually-Rich Documents</a></td><td><details><summary>å±•å¼€</summary>Existing multimodal Retrieval-Augmented Generation (RAG) methods for visually rich documents (VRD) are often biased towards retrieving salient knowledge(e.g., prominent text and visual elements), while largely neglecting the critical fine-print knowledge(e.g., small text, contextual details). This limitation leads to incomplete retrieval and compromises the generator's ability to produce accurate and comprehensive answers. To bridge this gap, we propose HKRAG, a new holistic RAG framework designed to explicitly capture and integrate both knowledge types. Our framework features two key components: (1) a Hybrid Masking-based Holistic Retriever that employs explicit masking strategies to separately model salient and fine-print knowledge, ensuring a query-relevant holistic information retrieval; and (2) an Uncertainty-guided Agentic Generator that dynamically assesses the uncertainty of initial answers and actively decides how to integrate the two distinct knowledge streams for optimal response generation. Extensive experiments on open-domain visual question answering benchmarks show that HKRAG consistently outperforms existing methods in both zero-shot and supervised settings, demonstrating the critical importance of holistic knowledge retrieval for VRD understanding.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºHKRAGçš„æ–°å‹æ•´ä½“RAGæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€RAGæ–¹æ³•åœ¨è§†è§‰ä¸°å¯Œæ–‡æ¡£ï¼ˆVRDï¼‰ä¸­åå‘æ£€ç´¢æ˜¾è‘—çŸ¥è¯†è€Œå¿½ç•¥ç»†ç²’åº¦çŸ¥è¯†çš„é—®é¢˜ã€‚HKRAGé€šè¿‡æ··åˆæ©ç ç­–ç•¥çš„æ£€ç´¢å™¨åˆ†åˆ«å»ºæ¨¡æ˜¾è‘—å’Œç»†ç²’åº¦çŸ¥è¯†ï¼Œå¹¶ç»“åˆä¸ç¡®å®šæ€§å¼•å¯¼çš„ç”Ÿæˆå™¨åŠ¨æ€æ•´åˆä¸¤ç±»çŸ¥è¯†ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.20177v1">Enhancing Sequential Recommendation with World Knowledge from Large Language Models</a></td><td><details><summary>å±•å¼€</summary>Sequential Recommendation System~(SRS) has become pivotal in modern society, which predicts subsequent actions based on the user's historical behavior. However, traditional collaborative filtering-based sequential recommendation models often lead to suboptimal performance due to the limited information of their collaborative signals. With the rapid development of LLMs, an increasing number of works have incorporated LLMs' world knowledge into sequential recommendation. Although they achieve considerable gains, these approaches typically assume the correctness of LLM-generated results and remain susceptible to noise induced by LLM hallucinations. To overcome these limitations, we propose GRASP (Generation Augmented Retrieval with Holistic Attention for Sequential Prediction), a flexible framework that integrates generation augmented retrieval for descriptive synthesis and similarity retrieval, and holistic attention enhancement which employs multi-level attention to effectively employ LLM's world knowledge even with hallucinations and better capture users' dynamic interests. The retrieved similar users/items serve as auxiliary contextual information for the later holistic attention enhancement module, effectively mitigating the noisy guidance of supervision-based methods. Comprehensive evaluations on two public benchmarks and one industrial dataset reveal that GRASP consistently achieves state-of-the-art performance when integrated with diverse backbones. The code is available at: https://anonymous.4open.science/r/GRASP-SRS.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†GRASPæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆç”Ÿæˆå¢å¼ºæ£€ç´¢ï¼ˆGeneration Augmented Retrievalï¼‰å’Œå…¨å±€æ³¨æ„åŠ›å¢å¼ºï¼ˆHolistic Attention Enhancementï¼‰æŠ€æœ¯ï¼Œå°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸–ç•ŒçŸ¥è¯†æ•´åˆåˆ°åºåˆ—æ¨èç³»ç»Ÿä¸­ã€‚GRASPåˆ©ç”¨æ£€ç´¢åˆ°çš„ç›¸ä¼¼ç”¨æˆ·/é¡¹ç›®ä½œä¸ºè¾…åŠ©ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæœ‰æ•ˆç¼“è§£äº†åŸºäºç›‘ç£æ–¹æ³•çš„å™ªå£°å¹²æ‰°ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.19987v1">$\text{R}^2\text{R}$: A Route-to-Rerank Post-Training Framework for Multi-Domain Decoder-Only Rerankers</a></td><td><details><summary>å±•å¼€</summary>Decoder-only rerankers are central to Retrieval-Augmented Generation (RAG). However, generalist models miss domain-specific nuances in high-stakes fields like finance and law, and naive fine-tuning causes surface-form overfitting and catastrophic forgetting. To address this challenge, we introduce R2R, a domain-aware framework that combines dynamic expert routing with a two-stage training strategy, Entity Abstraction for Generalization (EAG). EAG introduces a counter-shortcut mechanism by masking the most predictive surface cues, forcing the reranker to learn domain-invariant relevance patterns rather than memorizing dataset-specific entities. To efficiently activate domain experts, R2R employs a lightweight Latent Semantic Router that probes internal representations from the frozen backbone decoder to select the optimal LoRA expert per query. Extensive experiments across different reranker backbones and diverse domains (legal, medical, and financial) demonstrate that R2R consistently surpasses generalist and single-domain fine-tuned baselines. Our results confirm that R2R is a model-agnostic and modular approach to domain specialization with strong cross-domain robustness.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºR2Rçš„é¢†åŸŸæ„ŸçŸ¥æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€ä¸“å®¶è·¯ç”±å’Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ˆEAGï¼‰è§£å†³RAGä¸­è§£ç å™¨é‡æ’å™¨åœ¨ä¸“ä¸šé¢†åŸŸï¼ˆå¦‚é‡‘èã€æ³•å¾‹ï¼‰çš„å±€é™æ€§ã€‚EAGé€šè¿‡æ©è”½æœ€å…·é¢„æµ‹æ€§çš„è¡¨é¢çº¿ç´¢ï¼Œè¿«ä½¿é‡æ’å™¨å­¦ä¹ é¢†åŸŸæ— å…³çš„ç›¸å…³æ€§æ¨¡å¼ï¼Œè€Œéè®°å¿†ç‰¹å®šæ•°æ®é›†çš„å®ä½“ã€‚R2Ré‡‡ç”¨è½»é‡çº§æ½œåœ¨è¯­ä¹‰è·¯ç”±å™¨é€‰æ‹©æœ€ä¼˜LoRAä¸“å®¶ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨å¤šä¸ªé¢†åŸŸï¼ˆæ³•å¾‹ã€åŒ»ç–—ã€é‡‘èï¼‰ä¸­ä¼˜äºé€šç”¨å’Œå•é¢†åŸŸå¾®è°ƒåŸºçº¿ï¼Œå…·æœ‰è·¨é¢†åŸŸé²æ£’æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.19969v1">M$^3$Prune: Hierarchical Communication Graph Pruning for Efficient Multi-Modal Multi-Agent Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Recent advancements in multi-modal retrieval-augmented generation (mRAG), which enhance multi-modal large language models (MLLMs) with external knowledge, have demonstrated that the collective intelligence of multiple agents can significantly outperform a single model through effective communication. Despite impressive performance, existing multi-agent systems inherently incur substantial token overhead and increased computational costs, posing challenges for large-scale deployment. To address these issues, we propose a novel Multi-Modal Multi-agent hierarchical communication graph PRUNING framework, termed M$^3$Prune. Our framework eliminates redundant edges across different modalities, achieving an optimal balance between task performance and token overhead. Specifically, M$^3$Prune first applies intra-modal graph sparsification to textual and visual modalities, identifying the edges most critical for solving the task. Subsequently, we construct a dynamic communication topology using these key edges for inter-modal graph sparsification. Finally, we progressively prune redundant edges to obtain a more efficient and hierarchical topology. Extensive experiments on both general and domain-specific mRAG benchmarks demonstrate that our method consistently outperforms both single-agent and robust multi-agent mRAG systems while significantly reducing token consumption.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºM$^3$Pruneçš„å¤šæ¨¡æ€å¤šä»£ç†å±‚æ¬¡é€šä¿¡å›¾å‰ªææ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆmRAGï¼‰ç³»ç»Ÿä¸­å› å¤šä»£ç†é€šä¿¡å¯¼è‡´çš„ä»¤ç‰Œå¼€é”€å’Œè®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚é€šè¿‡è·¨æ¨¡æ€å†—ä½™è¾¹å‰ªæå’ŒåŠ¨æ€é€šä¿¡æ‹“æ‰‘ä¼˜åŒ–ï¼Œè¯¥æ¡†æ¶åœ¨ä¿æŒä»»åŠ¡æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½äº†èµ„æºæ¶ˆè€—ï¼Œå¹¶åœ¨é€šç”¨å’Œé¢†åŸŸç‰¹å®šçš„mRAGåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºå•ä»£ç†å’Œç°æœ‰å¤šä»£ç†ç³»ç»Ÿã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.19895v1">RPM-MCTS: Knowledge-Retrieval as Process Reward Model with Monte Carlo Tree Search for Code Generation</a></td><td><details><summary>å±•å¼€</summary>Tree search-based methods have made significant progress in enhancing the code generation capabilities of large language models. However, due to the difficulty in effectively evaluating intermediate algorithmic steps and the inability to locate and timely correct erroneous steps, these methods often generate incorrect code and incur increased computational costs. To tackle these problems, we propose RPM-MCTS, an effective method that utilizes Knowledge-Retrieval as Process Reward Model based on Monte Carlo Tree Search to evaluate intermediate algorithmic steps. By utilizing knowledge base retrieval, RPM-MCTS avoids the complex training of process reward models. During the expansion phase, similarity filtering is employed to remove redundant nodes, ensuring diversity in reasoning paths. Furthermore, our method utilizes sandbox execution feedback to locate erroneous algorithmic steps during generation, enabling timely and targeted corrections. Extensive experiments on four public code generation benchmarks demonstrate that RPM-MCTS outperforms current state-of-the-art methods while achieving an approximately 15% reduction in token consumption. Furthermore, full fine-tuning of the base model using the data constructed by RPM-MCTS significantly enhances its code capabilities.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºRPM-MCTSçš„æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆçŸ¥è¯†æ£€ç´¢ä¸è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¥è¯„ä¼°ä»£ç ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä¸­é—´ç®—æ³•æ­¥éª¤ï¼Œä»è€Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚è¯¥æ–¹æ³•åˆ©ç”¨çŸ¥è¯†åº“æ£€ç´¢é¿å…å¤æ‚çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹è®­ç»ƒï¼Œé€šè¿‡ç›¸ä¼¼æ€§è¿‡æ»¤å‡å°‘å†—ä½™èŠ‚ç‚¹ï¼Œå¹¶å€ŸåŠ©æ²™ç®±æ‰§è¡Œåé¦ˆå®šä½å’Œçº æ­£é”™è¯¯æ­¥éª¤ã€‚å®éªŒè¡¨æ˜ï¼ŒRPM-MCTSåœ¨å¤šä¸ªä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œä¸”å…¶æ„é€ çš„æ•°æ®èƒ½æ˜¾è‘—å¢å¼ºåŸºç¡€æ¨¡å‹çš„ä»£ç èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.19858v1">A Systematic Analysis of Large Language Models with RAG-enabled Dynamic Prompting for Medical Error Detection and Correction</a></td><td><details><summary>å±•å¼€</summary>Objective: Clinical documentation contains factual, diagnostic, and management errors that can compromise patient safety. Large language models (LLMs) may help detect and correct such errors, but their behavior under different prompting strategies remains unclear. We evaluate zero-shot prompting, static prompting with random exemplars (SPR), and retrieval-augmented dynamic prompting (RDP) for three subtasks of medical error processing: error flag detection, error sentence detection, and error correction.
  Methods: Using the MEDEC dataset, we evaluated nine instruction-tuned LLMs (GPT, Claude, Gemini, and OpenAI o-series models). We measured performance using accuracy, recall, false-positive rate (FPR), and an aggregate score of ROUGE-1, BLEURT, and BERTScore for error correction. We also analyzed example outputs to identify failure modes and differences between LLM and clinician reasoning.
  Results: Zero-shot prompting showed low recall in both detection tasks, often missing abbreviation-heavy or atypical errors. SPR improved recall but increased FPR. Across all nine LLMs, RDP reduced FPR by about 15 percent, improved recall by 5 to 10 percent in error sentence detection, and generated more contextually accurate corrections.
  Conclusion: Across diverse LLMs, RDP outperforms zero-shot and SPR prompting. Using retrieved exemplars improves detection accuracy, reduces false positives, and enhances the reliability of medical error correction.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†ä¸åŒæç¤ºç­–ç•¥ï¼ˆåŒ…æ‹¬æ£€ç´¢å¢å¼ºåŠ¨æ€æç¤ºRDPï¼‰åœ¨åŒ»ç–—é”™è¯¯å¤„ç†ä»»åŠ¡ä¸­çš„æ•ˆæœï¼Œå‘ç°RDPé€šè¿‡æ£€ç´¢ç›¸å…³ç¤ºä¾‹æ˜¾è‘—æé«˜äº†é”™è¯¯æ£€æµ‹çš„å‡†ç¡®æ€§å’Œçº æ­£çš„å¯é æ€§ï¼Œä¼˜äºé›¶æ ·æœ¬æç¤ºå’Œé™æ€éšæœºç¤ºä¾‹æç¤ºã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-24
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.19423v1">Beyond Protein Language Models: An Agentic LLM Framework for Mechanistic Enzyme Design</a></td><td><details><summary>å±•å¼€</summary>We present Genie-CAT, a tool-augmented large-language-model (LLM) system designed to accelerate scientific hypothesis generation in protein design. Using metalloproteins (e.g., ferredoxins) as a case study, Genie-CAT integrates four capabilities -- literature-grounded reasoning through retrieval-augmented generation (RAG), structural parsing of Protein Data Bank files, electrostatic potential calculations, and machine-learning prediction of redox properties -- into a unified agentic workflow. By coupling natural-language reasoning with data-driven and physics-based computation, the system generates mechanistically interpretable, testable hypotheses linking sequence, structure, and function. In proof-of-concept demonstrations, Genie-CAT autonomously identifies residue-level modifications near [Fe--S] clusters that affect redox tuning, reproducing expert-derived hypotheses in a fraction of the time. The framework highlights how AI agents combining language models with domain-specific tools can bridge symbolic reasoning and numerical simulation, transforming LLMs from conversational assistants into partners for computational discovery.</details></td><td><details><summary>å±•å¼€</summary>Genie-CATæ˜¯ä¸€ä¸ªç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€ç»“æ„è§£æã€é™ç”µåŠ¿è®¡ç®—å’Œæœºå™¨å­¦ä¹ é¢„æµ‹çš„å·¥å…·å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹ç³»ç»Ÿï¼Œæ—¨åœ¨åŠ é€Ÿè›‹ç™½è´¨è®¾è®¡ä¸­çš„ç§‘å­¦å‡è®¾ç”Ÿæˆï¼Œé€šè¿‡å¤šæ¨¡æ€å·¥ä½œæµç”Ÿæˆå¯æµ‹è¯•çš„æœºåˆ¶æ€§å‡è®¾ï¼Œå¹¶ä»¥é‡‘å±è›‹ç™½ä¸ºä¾‹éªŒè¯å…¶é«˜æ•ˆå¤ç°ä¸“å®¶æˆæœçš„èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.19349v1">Revisiting Feedback Models for HyDE</a></td><td><details><summary>å±•å¼€</summary>Recent approaches that leverage large language models (LLMs) for pseudo-relevance feedback (PRF) have generally not utilized well-established feedback models like Rocchio and RM3 when expanding queries for sparse retrievers like BM25. Instead, they often opt for a simple string concatenation of the query and LLM-generated expansion content. But is this optimal? To answer this question, we revisit and systematically evaluate traditional feedback models in the context of HyDE, a popular method that enriches query representations with LLM-generated hypothetical answer documents. Our experiments show that HyDE's effectiveness can be substantially improved when leveraging feedback algorithms such as Rocchio to extract and weight expansion terms, providing a simple way to further enhance the accuracy of LLM-based PRF methods.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æ¢è®¨äº†åœ¨åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œä¼ªç›¸å…³åé¦ˆï¼ˆPRFï¼‰æ—¶ï¼Œç»“åˆä¼ ç»Ÿåé¦ˆæ¨¡å‹ï¼ˆå¦‚Rocchioå’ŒRM3ï¼‰æ¥ä¼˜åŒ–æŸ¥è¯¢æ‰©å±•çš„æ–¹æ³•ï¼Œç‰¹åˆ«é’ˆå¯¹HyDEï¼ˆä¸€ç§é€šè¿‡LLMç”Ÿæˆå‡è®¾ç­”æ¡ˆæ–‡æ¡£æ¥ä¸°å¯ŒæŸ¥è¯¢è¡¨ç¤ºçš„æŠ€æœ¯ï¼‰ï¼Œç ”ç©¶è¡¨æ˜ä¼ ç»Ÿåé¦ˆç®—æ³•èƒ½æ˜¾è‘—æå‡HyDEçš„æ•ˆæœï¼Œä»è€Œå¢å¼ºåŸºäºLLMçš„PRFæ–¹æ³•çš„å‡†ç¡®æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.19257v1">Medusa: Cross-Modal Transferable Adversarial Attacks on Multimodal Medical Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>With the rapid advancement of retrieval-augmented vision-language models, multimodal medical retrieval-augmented generation (MMed-RAG) systems are increasingly adopted in clinical decision support. These systems enhance medical applications by performing cross-modal retrieval to integrate relevant visual and textual evidence for tasks, e.g., report generation and disease diagnosis. However, their complex architecture also introduces underexplored adversarial vulnerabilities, particularly via visual input perturbations. In this paper, we propose Medusa, a novel framework for crafting cross-modal transferable adversarial attacks on MMed-RAG systems under a black-box setting. Specifically, Medusa formulates the attack as a perturbation optimization problem, leveraging a multi-positive InfoNCE loss (MPIL) to align adversarial visual embeddings with medically plausible but malicious textual targets, thereby hijacking the retrieval process. To enhance transferability, we adopt a surrogate model ensemble and design a dual-loop optimization strategy augmented with invariant risk minimization (IRM). Extensive experiments on two real-world medical tasks, including medical report generation and disease diagnosis, demonstrate that Medusa achieves over 90% average attack success rate across various generation models and retrievers under appropriate parameter configuration, while remaining robust against four mainstream defenses, outperforming state-of-the-art baselines. Our results reveal critical vulnerabilities in the MMed-RAG systems and highlight the necessity of robustness benchmarking in safety-critical medical applications. The code and data are available at https://anonymous.4open.science/r/MMed-RAG-Attack-F05A.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤šæ¨¡æ€åŒ»å­¦æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆMMed-RAGï¼‰ç³»ç»Ÿåœ¨ä¸´åºŠå†³ç­–æ”¯æŒä¸­çš„åº”ç”¨åŠå…¶å¯¹æŠ—æ€§æ¼æ´ã€‚ä½œè€…æå‡ºäº†Medusaæ¡†æ¶ï¼Œé€šè¿‡è·¨æ¨¡æ€å¯è½¬ç§»å¯¹æŠ—æ”»å‡»ï¼ˆåˆ©ç”¨å¤šæ­£ä¾‹InfoNCEæŸå¤±å’ŒåŒé‡å¾ªç¯ä¼˜åŒ–ç­–ç•¥ï¼‰åœ¨åŒ»ç–—æŠ¥å‘Šç”Ÿæˆå’Œç–¾ç—…è¯Šæ–­ä»»åŠ¡ä¸­æˆåŠŸæ”»å‡»MMed-RAGç³»ç»Ÿï¼Œæ­ç¤ºäº†å…¶å®‰å…¨é£é™©å¹¶å¼ºè°ƒé²æ£’æ€§è¯„ä¼°çš„å¿…è¦æ€§ã€‚å®éªŒæ˜¾ç¤ºæ”»å‡»æˆåŠŸç‡è¶…90%ï¼Œä¸”èƒ½æŠµæŠ—ä¸»æµé˜²å¾¡æªæ–½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.19149v1">From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation</a></td><td><details><summary>å±•å¼€</summary>This paper introduces the retrieval-augmented framework for automatic fashion caption and hashtag generation, combining multi-garment detection, attribute reasoning, and Large Language Model (LLM) prompting. The system aims to produce visually grounded, descriptive, and stylistically interesting text for fashion imagery, overcoming the limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization. The pipeline combines a YOLO-based detector for multi-garment localization, k-means clustering for dominant color extraction, and a CLIP-FAISS retrieval module for fabric and gender attribute inference based on a structured product index. These attributes, together with retrieved style examples, create a factual evidence pack that is used to guide an LLM to generate human-like captions and contextually rich hashtags. A fine-tuned BLIP model is used as a supervised baseline model for comparison. Experimental results show that the YOLO detector is able to obtain a mean Average Precision (mAP@0.5) of 0.71 for nine categories of garments. The RAG-LLM pipeline generates expressive attribute-aligned captions and achieves mean attribute coverage of 0.80 with full coverage at the 50% threshold in hashtag generation, whereas BLIP gives higher lexical overlap and lower generalization. The retrieval-augmented approach exhibits better factual grounding, less hallucination, and great potential for scalable deployment in various clothing domains. These results demonstrate the use of retrieval-augmented generation as an effective and interpretable paradigm for automated and visually grounded fashion content generation.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„è‡ªåŠ¨æ—¶å°šæè¿°ä¸æ ‡ç­¾ç”Ÿæˆæ¡†æ¶ï¼Œç»“åˆå¤šæœè£…æ£€æµ‹ã€å±æ€§æ¨ç†å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œé€šè¿‡æ£€ç´¢å¤–éƒ¨æ—¶å°šçŸ¥è¯†åº“æ„å»ºäº‹å®è¯æ®åŒ…æ¥æŒ‡å¯¼LLMç”Ÿæˆé«˜ä¿çœŸã€é£æ ¼åŒ–çš„æ–‡æœ¬ï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨å‡å°‘å¹»è§‰å’Œæå‡é¢†åŸŸæ³›åŒ–æ€§ä¸Šä¼˜äºä¼ ç»Ÿç«¯åˆ°ç«¯æ¨¡å‹ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.19083v1">A Multi-Agent LLM Framework for Multi-Domain Low-Resource In-Context NER via Knowledge Retrieval, Disambiguation and Reflective Analysis</a></td><td><details><summary>å±•å¼€</summary>In-context learning (ICL) with large language models (LLMs) has emerged as a promising paradigm for named entity recognition (NER) in low-resource scenarios. However, existing ICL-based NER methods suffer from three key limitations: (1) reliance on dynamic retrieval of annotated examples, which is problematic when annotated data is scarce; (2) limited generalization to unseen domains due to the LLM's insufficient internal domain knowledge; and (3) failure to incorporate external knowledge or resolve entity ambiguities. To address these challenges, we propose KDR-Agent, a novel multi-agent framework for multi-domain low-resource in-context NER that integrates Knowledge retrieval, Disambiguation, and Reflective analysis. KDR-Agent leverages natural-language type definitions and a static set of entity-level contrastive demonstrations to reduce dependency on large annotated corpora. A central planner coordinates specialized agents to (i) retrieve factual knowledge from Wikipedia for domain-specific mentions, (ii) resolve ambiguous entities via contextualized reasoning, and (iii) reflect on and correct model predictions through structured self-assessment. Experiments across ten datasets from five domains demonstrate that KDR-Agent significantly outperforms existing zero-shot and few-shot ICL baselines across multiple LLM backbones. The code and data can be found at https://github.com/MWXGOD/KDR-Agent.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºKDR-Agentçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºä½èµ„æºåœºæ™¯ä¸‹çš„å¤šé¢†åŸŸå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆçŸ¥è¯†æ£€ç´¢ï¼ˆä»Wikipediaè·å–é¢†åŸŸç‰¹å®šçŸ¥è¯†ï¼‰ã€æ¶ˆæ­§ï¼ˆè§£å†³å®ä½“æ­§ä¹‰ï¼‰å’Œåæ€åˆ†æï¼ˆè‡ªæˆ‘è¯„ä¼°ä¿®æ­£é¢„æµ‹ï¼‰ï¼Œå‡å°‘äº†ä¼ ç»ŸåŸºäºä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ–¹æ³•å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œå¹¶æå‡äº†æ¨¡å‹åœ¨æœªè§é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚å…¶æ ¸å¿ƒæ€æƒ³ä¸RAGä¸€è‡´ï¼Œå³é€šè¿‡æ£€ç´¢å¤–éƒ¨çŸ¥è¯†å¢å¼ºç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.18832v1">Concept than Document: Context Compression via AMR-based Conceptual Entropy</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) face information overload when handling long contexts, particularly in Retrieval-Augmented Generation (RAG) where extensive supporting documents often introduce redundant content. This issue not only weakens reasoning accuracy but also increases computational overhead. We propose an unsupervised context compression framework that exploits Abstract Meaning Representation (AMR) graphs to preserve semantically essential information while filtering out irrelevant text. By quantifying node-level entropy within AMR graphs, our method estimates the conceptual importance of each node, enabling the retention of core semantics. Specifically, we construct AMR graphs from raw contexts, compute the conceptual entropy of each node, and screen significant informative nodes to form a condensed and semantically focused context than raw documents. Experiments on the PopQA and EntityQuestions datasets show that our method outperforms vanilla and other baselines, achieving higher accuracy while substantially reducing context length. To the best of our knowledge, this is the first work introducing AMR-based conceptual entropy for context compression, demonstrating the potential of stable linguistic features in context engineering.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡é’ˆå¯¹RAGä¸­é•¿ä¸Šä¸‹æ–‡å¯¼è‡´çš„ä¿¡æ¯å†—ä½™é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæŠ½è±¡æ„ä¹‰è¡¨ç¤ºï¼ˆAMRï¼‰å›¾çš„æ— ç›‘ç£ä¸Šä¸‹æ–‡å‹ç¼©æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡è®¡ç®—AMRèŠ‚ç‚¹çš„æ¦‚å¿µç†µæ¥é‡åŒ–è¯­ä¹‰é‡è¦æ€§ï¼Œç­›é€‰æ ¸å¿ƒèŠ‚ç‚¹ä»¥ç”Ÿæˆæ›´ç´§å‡‘ä¸”è¯­ä¹‰é›†ä¸­çš„ä¸Šä¸‹æ–‡ï¼Œå®éªŒè¯æ˜å…¶åœ¨å‹ç¼©æ•ˆæœå’Œæ¨ç†å‡†ç¡®æ€§ä¸Šä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.18808v1">HyperbolicRAG: Enhancing Retrieval-Augmented Generation with Hyperbolic Representations</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) enables large language models (LLMs) to access external knowledge, helping mitigate hallucinations and enhance domain-specific expertise. Graph-based RAG enhances structural reasoning by introducing explicit relational organization that enables information propagation across semantically connected text units. However, these methods typically rely on Euclidean embeddings that capture semantic similarity but lack a geometric notion of hierarchical depth, limiting their ability to represent abstraction relationships inherent in complex knowledge graphs. To capture both fine-grained semantics and global hierarchy, we propose HyperbolicRAG, a retrieval framework that integrates hyperbolic geometry into graph-based RAG. HyperbolicRAG introduces three key designs: (1) a depth-aware representation learner that embeds nodes within a shared Poincare manifold to align semantic similarity with hierarchical containment, (2) an unsupervised contrastive regularization that enforces geometric consistency across abstraction levels, and (3) a mutual-ranking fusion mechanism that jointly exploits retrieval signals from Euclidean and hyperbolic spaces, emphasizing cross-space agreement during inference. Extensive experiments across multiple QA benchmarks demonstrate that HyperbolicRAG outperforms competitive baselines, including both standard RAG and graph-augmented baselines.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºHyperbolicRAGçš„æ–°å‹æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡å°†åŒæ›²å‡ ä½•èå…¥åŸºäºå›¾çš„RAGæ–¹æ³•ï¼Œè§£å†³äº†ä¼ ç»Ÿæ¬§å‡ é‡Œå¾—åµŒå…¥åœ¨æ•æ‰å±‚æ¬¡ç»“æ„å…³ç³»ä¸Šçš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶åŒ…å«æ·±åº¦æ„ŸçŸ¥è¡¨ç¤ºå­¦ä¹ ã€å¯¹æ¯”æ­£åˆ™åŒ–å’Œå¤šç©ºé—´äº’æ’åèåˆæœºåˆ¶ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨å¤šé¡¹é—®ç­”ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.18749v1">Large Language Models Require Curated Context for Reliable Political Fact-Checking -- Even with Reasoning and Web Search</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) have raised hopes for automated end-to-end fact-checking, but prior studies report mixed results. As mainstream chatbots increasingly ship with reasoning capabilities and web search tools -- and millions of users already rely on them for verification -- rigorous evaluation is urgent. We evaluate 15 recent LLMs from OpenAI, Google, Meta, and DeepSeek on more than 6,000 claims fact-checked by PolitiFact, comparing standard models with reasoning- and web-search variants. Standard models perform poorly, reasoning offers minimal benefits, and web search provides only moderate gains, despite fact-checks being available on the web. In contrast, a curated RAG system using PolitiFact summaries improved macro F1 by 233% on average across model variants. These findings suggest that giving models access to curated high-quality context is a promising path for automated fact-checking.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡è¯„ä¼°äº†15ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äº‹å®æ ¸æŸ¥ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå‘ç°æ ‡å‡†æ¨¡å‹å’Œå…·å¤‡æ¨ç†æˆ–ç½‘ç»œæœç´¢åŠŸèƒ½çš„å˜ä½“æ•ˆæœæœ‰é™ï¼Œè€Œé‡‡ç”¨PolitiFactæ‘˜è¦æ„å»ºçš„RAGç³»ç»Ÿæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼ˆå®è§‚F1å¹³å‡æé«˜233%ï¼‰ï¼Œè¡¨æ˜åŸºäºé«˜è´¨é‡æ£€ç´¢çš„ä¸Šä¸‹æ–‡å¢å¼ºæ˜¯è‡ªåŠ¨åŒ–äº‹å®æ ¸æŸ¥çš„æœ‰æ•ˆè·¯å¾„ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.18659v1">CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but still suffers from long contexts and disjoint retrieval-generation optimization. In this work, we propose CLaRa (Continuous Latent Reasoning), a unified framework that performs embedding-based compression and joint optimization in a shared continuous space. To obtain semantically rich and retrievable compressed vectors, we introduce SCP, a key-preserving data synthesis framework using QA and paraphrase supervision. CLaRa then trains the reranker and generator end-to-end via a single language modeling loss, with gradients flowing through both modules using a differentiable top-k estimator. Theoretically, this unified optimization aligns retrieval relevance with answer quality. Experiments across multiple QA benchmarks show that CLaRa achieves state-of-the-art compression and reranking performance, often surpassing text-based fine-tuned baselines.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºCLaRaçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ”¹è¿›æ¡†æ¶ï¼Œé€šè¿‡åµŒå…¥å‹ç¼©å’Œè”åˆä¼˜åŒ–è§£å†³é•¿ä¸Šä¸‹æ–‡ä¸æ£€ç´¢-ç”Ÿæˆå‰²è£‚é—®é¢˜ï¼Œå¼•å…¥SCPæ•°æ®åˆæˆæ–¹æ³•å¢å¼ºè¯­ä¹‰å‹ç¼©ï¼Œå¹¶åˆ©ç”¨ç«¯åˆ°ç«¯è®­ç»ƒå®ç°æ£€ç´¢ä¸ç”Ÿæˆçš„ç»Ÿä¸€ä¼˜åŒ–ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨å¤šé¡¹QAä»»åŠ¡ä¸­è¾¾åˆ°æœ€ä¼˜æ€§èƒ½ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-23
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.19780v1">NOEM$^{3}$A: A Neuro-Symbolic Ontology-Enhanced Method for Multi-Intent Understanding in Mobile Agents</a></td><td><details><summary>å±•å¼€</summary>We introduce a neuro-symbolic framework for multi-intent understanding in mobile AI agents by integrating a structured intent ontology with compact language models. Our method leverages retrieval-augmented prompting, logit biasing and optional classification heads to inject symbolic intent structure into both input and output representations. We formalize a new evaluation metric-Semantic Intent Similarity (SIS)-based on hierarchical ontology depth, capturing semantic proximity even when predicted intents differ lexically. Experiments on a subset of ambiguous/demanding dialogues of MultiWOZ 2.3 (with oracle labels from GPT-o3) demonstrate that a 3B Llama model with ontology augmentation approaches GPT-4 accuracy (85% vs 90%) at a tiny fraction of the energy and memory footprint. Qualitative comparisons show that ontology-augmented models produce more grounded, disambiguated multi-intent interpretations. Our results validate symbolic alignment as an effective strategy for enabling accurate and efficient on-device NLU.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç¥ç»ç¬¦å·æ¡†æ¶ï¼Œç”¨äºç§»åŠ¨AIä»£ç†ä¸­çš„å¤šæ„å›¾ç†è§£ï¼Œé€šè¿‡ç»“åˆç»“æ„åŒ–æ„å›¾æœ¬ä½“å’Œç´§å‡‘è¯­è¨€æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨æ£€ç´¢å¢å¼ºæç¤ºï¼ˆretrieval-augmented promptingï¼‰ç­‰æŠ€æœ¯å°†ç¬¦å·æ„å›¾ç»“æ„æ³¨å…¥è¾“å…¥å’Œè¾“å‡ºè¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨èƒ½æ•ˆå’Œå†…å­˜å ç”¨æä½çš„æƒ…å†µä¸‹ï¼Œä½¿3Bå‚æ•°çš„Llamaæ¨¡å‹æ¥è¿‘GPT-4çš„æ„å›¾ç†è§£å‡†ç¡®ç‡ï¼ˆ85% vs 90%ï¼‰ï¼Œä¸”ç”Ÿæˆçš„è§£é‡Šæ›´æ¸…æ™°å¯é ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.19423v1">Beyond Protein Language Models: An Agentic LLM Framework for Mechanistic Enzyme Design</a></td><td><details><summary>å±•å¼€</summary>We present Genie-CAT, a tool-augmented large-language-model (LLM) system designed to accelerate scientific hypothesis generation in protein design. Using metalloproteins (e.g., ferredoxins) as a case study, Genie-CAT integrates four capabilities -- literature-grounded reasoning through retrieval-augmented generation (RAG), structural parsing of Protein Data Bank files, electrostatic potential calculations, and machine-learning prediction of redox properties -- into a unified agentic workflow. By coupling natural-language reasoning with data-driven and physics-based computation, the system generates mechanistically interpretable, testable hypotheses linking sequence, structure, and function. In proof-of-concept demonstrations, Genie-CAT autonomously identifies residue-level modifications near [Fe--S] clusters that affect redox tuning, reproducing expert-derived hypotheses in a fraction of the time. The framework highlights how AI agents combining language models with domain-specific tools can bridge symbolic reasoning and numerical simulation, transforming LLMs from conversational assistants into partners for computational discovery.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ä»‹ç»äº†Genie-CATï¼Œä¸€ä¸ªåŸºäºå·¥å…·å¢å¼ºçš„å¤§è¯­è¨€æ¨¡å‹ç³»ç»Ÿï¼Œæ—¨åœ¨åŠ é€Ÿè›‹ç™½è´¨è®¾è®¡é¢†åŸŸçš„ç§‘å­¦å‡è®¾ç”Ÿæˆã€‚é€šè¿‡ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ã€è›‹ç™½è´¨ç»“æ„è§£æã€é™ç”µåŠ¿è®¡ç®—å’Œæœºå™¨å­¦ä¹ é¢„æµ‹ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿç”Ÿæˆæœºåˆ¶ä¸Šå¯è§£é‡Šä¸”å¯æµ‹è¯•çš„å‡è®¾ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒGenie-CATèƒ½å¤Ÿå¿«é€Ÿè‡ªä¸»è¯†åˆ«å½±å“æ°§åŒ–è¿˜åŸè°ƒèŠ‚çš„æ®‹åŸºä¿®é¥°ï¼Œå±•ç¤ºäº†å…¶åœ¨è®¡ç®—å‘ç°ä¸­çš„æ½œåŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.19349v1">Revisiting Feedback Models for HyDE</a></td><td><details><summary>å±•å¼€</summary>Recent approaches that leverage large language models (LLMs) for pseudo-relevance feedback (PRF) have generally not utilized well-established feedback models like Rocchio and RM3 when expanding queries for sparse retrievers like BM25. Instead, they often opt for a simple string concatenation of the query and LLM-generated expansion content. But is this optimal? To answer this question, we revisit and systematically evaluate traditional feedback models in the context of HyDE, a popular method that enriches query representations with LLM-generated hypothetical answer documents. Our experiments show that HyDE's effectiveness can be substantially improved when leveraging feedback algorithms such as Rocchio to extract and weight expansion terms, providing a simple way to further enhance the accuracy of LLM-based PRF methods.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åœ¨åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¼ªç›¸å…³åé¦ˆï¼ˆPRFï¼‰æ–¹æ³•ä¸­ï¼Œå¦‚ä½•é€šè¿‡æ”¹è¿›æŸ¥è¯¢æ‰©å±•æŠ€æœ¯ï¼ˆå¦‚ç»“åˆRocchioå’ŒRM3ç­‰ä¼ ç»Ÿåé¦ˆæ¨¡å‹ï¼‰æ¥ä¼˜åŒ–HyDEï¼ˆä¸€ç§åˆ©ç”¨LLMç”Ÿæˆå‡è®¾ç­”æ¡ˆæ–‡æ¡£å¢å¼ºæŸ¥è¯¢è¡¨ç¤ºçš„æ–¹æ³•ï¼‰ï¼Œä»è€Œæå‡ç¨€ç–æ£€ç´¢å™¨ï¼ˆå¦‚BM25ï¼‰çš„æ€§èƒ½ï¼Œå±äºRAGæŠ€æœ¯ä¸­æ£€ç´¢ä¸ç”Ÿæˆç»“åˆçš„ä¼˜åŒ–ç ”ç©¶ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.19325v1">Generative Query Expansion with Multilingual LLMs for Cross-Lingual Information Retrieval</a></td><td><details><summary>å±•å¼€</summary>Query expansion is the reformulation of a user query by adding semantically related information, and is an essential component of monolingual and cross-lingual information retrieval used to ensure that relevant documents are not missed. Recently, multilingual large language models (mLLMs) have shifted query expansion from semantic augmentation with synonyms and related words to pseudo-document generation. Pseudo-documents both introduce additional relevant terms and bridge the gap between short queries and long documents, which is particularly beneficial in dense retrieval. This study evaluates recent mLLMs and fine-tuned variants across several generative expansion strategies to identify factors that drive cross-lingual retrieval performance. Results show that query length largely determines which prompting technique is effective, and that more elaborate prompts often do not yield further gains. Substantial linguistic disparities persist: cross-lingual query expansion can produce the largest improvements for languages with the weakest baselines, yet retrieval is especially poor between languages written in different scripts. Fine-tuning is found to lead to performance gains only when the training and test data are of similar format. These outcomes underline the need for more balanced multilingual and cross-lingual training and evaluation resources.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä½¿ç”¨å¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹ï¼ˆmLLMsï¼‰è¿›è¡ŒæŸ¥è¯¢æ‰©å±•ï¼ˆquery expansionï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆä¼ªæ–‡æ¡£ï¼ˆpseudo-documentï¼‰æ¥å¢å¼ºä¿¡æ¯æ£€ç´¢çš„æ•ˆæœã€‚ç ”ç©¶è¯„ä¼°äº†ä¸åŒç”Ÿæˆç­–ç•¥å¯¹è·¨è¯­è¨€æ£€ç´¢æ€§èƒ½çš„å½±å“ï¼Œå‘ç°æŸ¥è¯¢é•¿åº¦ã€æç¤ºæŠ€æœ¯å’Œè¯­è¨€å·®å¼‚æ˜¯å…³é”®å› ç´ ã€‚è™½ç„¶ä¸RAGçš„æ ¸å¿ƒæµç¨‹ï¼ˆæ£€ç´¢+ç”Ÿæˆï¼‰ä¸å®Œå…¨ç›¸åŒï¼Œä½†æ¶‰åŠåˆ©ç”¨ç”Ÿæˆæ¨¡å‹ä¼˜åŒ–æ£€ç´¢ä¸Šä¸‹æ–‡ï¼Œå› æ­¤å±äºRAGç›¸å…³ç ”ç©¶ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.19257v1">Medusa: Cross-Modal Transferable Adversarial Attacks on Multimodal Medical Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>With the rapid advancement of retrieval-augmented vision-language models, multimodal medical retrieval-augmented generation (MMed-RAG) systems are increasingly adopted in clinical decision support. These systems enhance medical applications by performing cross-modal retrieval to integrate relevant visual and textual evidence for tasks, e.g., report generation and disease diagnosis. However, their complex architecture also introduces underexplored adversarial vulnerabilities, particularly via visual input perturbations. In this paper, we propose Medusa, a novel framework for crafting cross-modal transferable adversarial attacks on MMed-RAG systems under a black-box setting. Specifically, Medusa formulates the attack as a perturbation optimization problem, leveraging a multi-positive InfoNCE loss (MPIL) to align adversarial visual embeddings with medically plausible but malicious textual targets, thereby hijacking the retrieval process. To enhance transferability, we adopt a surrogate model ensemble and design a dual-loop optimization strategy augmented with invariant risk minimization (IRM). Extensive experiments on two real-world medical tasks, including medical report generation and disease diagnosis, demonstrate that Medusa achieves over 90% average attack success rate across various generation models and retrievers under appropriate parameter configuration, while remaining robust against four mainstream defenses, outperforming state-of-the-art baselines. Our results reveal critical vulnerabilities in the MMed-RAG systems and highlight the necessity of robustness benchmarking in safety-critical medical applications. The code and data are available at https://anonymous.4open.science/r/MMed-RAG-Attack-F05A.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤šæ¨¡æ€åŒ»ç–—æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆMMed-RAGï¼‰ç³»ç»Ÿçš„å¯¹æŠ—æ€§æ¼æ´ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMedusaçš„æ–°å‹é»‘ç›’æ”»å‡»æ¡†æ¶ï¼Œé€šè¿‡è·¨æ¨¡æ€å¯è½¬ç§»å¯¹æŠ—æ”»å‡»ï¼ˆåˆ©ç”¨è§†è§‰è¾“å…¥æ‰°åŠ¨å’Œæ–‡æœ¬ç›®æ ‡å¯¹é½ï¼‰æ¥åŠ«æŒæ£€ç´¢è¿‡ç¨‹ï¼Œå¹¶åœ¨åŒ»ç–—æŠ¥å‘Šç”Ÿæˆå’Œç–¾ç—…è¯Šæ–­ä»»åŠ¡ä¸­éªŒè¯äº†å…¶æ”»å‡»æœ‰æ•ˆæ€§ï¼ˆæˆåŠŸç‡è¶…90%ï¼‰ï¼Œæ­ç¤ºäº†å®‰å…¨å…³é”®åŒ»ç–—åº”ç”¨ä¸­RAGç³»ç»Ÿçš„è„†å¼±æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.19149v1">From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation</a></td><td><details><summary>å±•å¼€</summary>This paper introduces the retrieval-augmented framework for automatic fashion caption and hashtag generation, combining multi-garment detection, attribute reasoning, and Large Language Model (LLM) prompting. The system aims to produce visually grounded, descriptive, and stylistically interesting text for fashion imagery, overcoming the limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization. The pipeline combines a YOLO-based detector for multi-garment localization, k-means clustering for dominant color extraction, and a CLIP-FAISS retrieval module for fabric and gender attribute inference based on a structured product index. These attributes, together with retrieved style examples, create a factual evidence pack that is used to guide an LLM to generate human-like captions and contextually rich hashtags. A fine-tuned BLIP model is used as a supervised baseline model for comparison. Experimental results show that the YOLO detector is able to obtain a mean Average Precision (mAP@0.5) of 0.71 for nine categories of garments. The RAG-LLM pipeline generates expressive attribute-aligned captions and achieves mean attribute coverage of 0.80 with full coverage at the 50% threshold in hashtag generation, whereas BLIP gives higher lexical overlap and lower generalization. The retrieval-augmented approach exhibits better factual grounding, less hallucination, and great potential for scalable deployment in various clothing domains. These results demonstrate the use of retrieval-augmented generation as an effective and interpretable paradigm for automated and visually grounded fashion content generation.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ¡†æ¶ï¼Œç”¨äºæ—¶å°šé¢†åŸŸçš„å›¾åƒæè¿°å’Œæ ‡ç­¾ç”Ÿæˆã€‚é€šè¿‡ç»“åˆå¤šæœè£…æ£€æµ‹ã€å±æ€§æ¨ç†å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œåˆ©ç”¨YOLOæ£€æµ‹å™¨ã€é¢œè‰²èšç±»å’ŒCLIP-FAISSæ£€ç´¢æ¨¡å—æ„å»ºäº‹å®è¯æ®åŒ…ï¼ŒæŒ‡å¯¼LLMç”Ÿæˆé«˜ä¿çœŸã€é£æ ¼åŒ–çš„æè¿°å’Œæ ‡ç­¾ã€‚å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨å±æ€§è¦†ç›–ç‡å’Œå‡å°‘å¹»è§‰æ–¹é¢ä¼˜äºä¼ ç»ŸåŸºçº¿æ¨¡å‹ï¼ˆå¦‚BLIPï¼‰ï¼Œä½“ç°äº†RAGåœ¨æ—¶å°šå†…å®¹ç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.19083v1">A Multi-Agent LLM Framework for Multi-Domain Low-Resource In-Context NER via Knowledge Retrieval, Disambiguation and Reflective Analysis</a></td><td><details><summary>å±•å¼€</summary>In-context learning (ICL) with large language models (LLMs) has emerged as a promising paradigm for named entity recognition (NER) in low-resource scenarios. However, existing ICL-based NER methods suffer from three key limitations: (1) reliance on dynamic retrieval of annotated examples, which is problematic when annotated data is scarce; (2) limited generalization to unseen domains due to the LLM's insufficient internal domain knowledge; and (3) failure to incorporate external knowledge or resolve entity ambiguities. To address these challenges, we propose KDR-Agent, a novel multi-agent framework for multi-domain low-resource in-context NER that integrates Knowledge retrieval, Disambiguation, and Reflective analysis. KDR-Agent leverages natural-language type definitions and a static set of entity-level contrastive demonstrations to reduce dependency on large annotated corpora. A central planner coordinates specialized agents to (i) retrieve factual knowledge from Wikipedia for domain-specific mentions, (ii) resolve ambiguous entities via contextualized reasoning, and (iii) reflect on and correct model predictions through structured self-assessment. Experiments across ten datasets from five domains demonstrate that KDR-Agent significantly outperforms existing zero-shot and few-shot ICL baselines across multiple LLM backbones. The code and data can be found at https://github.com/MWXGOD/KDR-Agent.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºKDR-Agentï¼Œä¸€ç§å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡é›†æˆçŸ¥è¯†æ£€ç´¢ã€æ¶ˆæ­§å’Œåæ€åˆ†æï¼Œè§£å†³ä½èµ„æºåœºæ™¯ä¸‹åŸºäºä¸Šä¸‹æ–‡å­¦ä¹ çš„å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç»´åŸºç™¾ç§‘æ£€ç´¢é¢†åŸŸçŸ¥è¯†ã€ä¸Šä¸‹æ–‡æ¨ç†æ¶ˆæ­§å®ä½“ï¼Œå¹¶é€šè¿‡è‡ªè¯„ä¼°ä¼˜åŒ–é¢„æµ‹ï¼Œæ˜¾è‘—æå‡äº†è·¨é¢†åŸŸNERæ€§èƒ½ï¼Œå±äºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯çš„åº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.18832v1">Concept than Document: Context Compression via AMR-based Conceptual Entropy</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) face information overload when handling long contexts, particularly in Retrieval-Augmented Generation (RAG) where extensive supporting documents often introduce redundant content. This issue not only weakens reasoning accuracy but also increases computational overhead. We propose an unsupervised context compression framework that exploits Abstract Meaning Representation (AMR) graphs to preserve semantically essential information while filtering out irrelevant text. By quantifying node-level entropy within AMR graphs, our method estimates the conceptual importance of each node, enabling the retention of core semantics. Specifically, we construct AMR graphs from raw contexts, compute the conceptual entropy of each node, and screen significant informative nodes to form a condensed and semantically focused context than raw documents. Experiments on the PopQA and EntityQuestions datasets show that our method outperforms vanilla and other baselines, achieving higher accuracy while substantially reducing context length. To the best of our knowledge, this is the first work introducing AMR-based conceptual entropy for context compression, demonstrating the potential of stable linguistic features in context engineering.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæŠ½è±¡æ„ä¹‰è¡¨ç¤ºï¼ˆAMRï¼‰å›¾çš„æ— ç›‘ç£ä¸Šä¸‹æ–‡å‹ç¼©æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³RAGä¸­å› é•¿ä¸Šä¸‹æ–‡å¯¼è‡´çš„ä¿¡æ¯å†—ä½™é—®é¢˜ã€‚é€šè¿‡é‡åŒ–AMRå›¾ä¸­èŠ‚ç‚¹çš„æ¦‚å¿µç†µï¼Œè¯¥æ–¹æ³•ç­›é€‰æ ¸å¿ƒè¯­ä¹‰èŠ‚ç‚¹ä»¥å‹ç¼©ä¸Šä¸‹æ–‡ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨PopQAå’ŒEntityQuestionsæ•°æ®é›†ä¸Šä¼˜äºåŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦å¹¶æé«˜å‡†ç¡®æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.18808v2">HyperbolicRAG: Enhancing Retrieval-Augmented Generation with Hyperbolic Representations</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) enables large language models (LLMs) to access external knowledge, helping mitigate hallucinations and enhance domain-specific expertise. Graph-based RAG enhances structural reasoning by introducing explicit relational organization that enables information propagation across semantically connected text units. However, these methods typically rely on Euclidean embeddings that capture semantic similarity but lack a geometric notion of hierarchical depth, limiting their ability to represent abstraction relationships inherent in complex knowledge graphs. To capture both fine-grained semantics and global hierarchy, we propose HyperbolicRAG, a retrieval framework that integrates hyperbolic geometry into graph-based RAG. HyperbolicRAG introduces three key designs: (1) a depth-aware representation learner that embeds nodes within a shared Poincare manifold to align semantic similarity with hierarchical containment, (2) an unsupervised contrastive regularization that enforces geometric consistency across abstraction levels, and (3) a mutual-ranking fusion mechanism that jointly exploits retrieval signals from Euclidean and hyperbolic spaces, emphasizing cross-space agreement during inference. Extensive experiments across multiple QA benchmarks demonstrate that HyperbolicRAG outperforms competitive baselines, including both standard RAG and graph-augmented baselines.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºHyperbolicRAGçš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡å°†åŒæ›²å‡ ä½•èå…¥åŸºäºå›¾çš„RAGç³»ç»Ÿï¼Œè§£å†³äº†ä¼ ç»Ÿæ¬§å‡ é‡Œå¾—åµŒå…¥æ–¹æ³•åœ¨æ•æ‰å¤æ‚çŸ¥è¯†å›¾è°±å±‚æ¬¡ç»“æ„ä¸Šçš„ä¸è¶³ã€‚è¯¥æ–¹æ³•ç»“åˆæ·±åº¦æ„ŸçŸ¥è¡¨ç¤ºå­¦ä¹ ã€å¯¹æ¯”æ­£åˆ™åŒ–å’Œå¤šç©ºé—´äº’æ’åèåˆæœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†é—®ç­”ä»»åŠ¡ä¸­çš„æ£€ç´¢å’Œç”Ÿæˆæ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.18749v1">Large Language Models Require Curated Context for Reliable Political Fact-Checking -- Even with Reasoning and Web Search</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) have raised hopes for automated end-to-end fact-checking, but prior studies report mixed results. As mainstream chatbots increasingly ship with reasoning capabilities and web search tools -- and millions of users already rely on them for verification -- rigorous evaluation is urgent. We evaluate 15 recent LLMs from OpenAI, Google, Meta, and DeepSeek on more than 6,000 claims fact-checked by PolitiFact, comparing standard models with reasoning- and web-search variants. Standard models perform poorly, reasoning offers minimal benefits, and web search provides only moderate gains, despite fact-checks being available on the web. In contrast, a curated RAG system using PolitiFact summaries improved macro F1 by 233% on average across model variants. These findings suggest that giving models access to curated high-quality context is a promising path for automated fact-checking.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡è¯„ä¼°äº†15ç§å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äº‹å®æ ¸æŸ¥ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå‘ç°æ ‡å‡†æ¨¡å‹å’Œå…·å¤‡æ¨ç†æˆ–ç½‘ç»œæœç´¢åŠŸèƒ½çš„å˜ä½“æ•ˆæœæœ‰é™ï¼Œè€Œé‡‡ç”¨PolitiFactæ‘˜è¦çš„RAGç³»ç»Ÿæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼ˆå®è§‚F1å¹³å‡æé«˜233%ï¼‰ï¼Œè¡¨æ˜åŸºäºé«˜è´¨é‡æ£€ç´¢å†…å®¹çš„RAGæ˜¯è‡ªåŠ¨åŒ–äº‹å®æ ¸æŸ¥çš„æœ‰æ•ˆè·¯å¾„ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.18659v2">CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but still suffers from long contexts and disjoint retrieval-generation optimization. In this work, we propose CLaRa (Continuous Latent Reasoning), a unified framework that performs embedding-based compression and joint optimization in a shared continuous space. To obtain semantically rich and retrievable compressed vectors, we introduce SCP, a key-preserving data synthesis framework using QA and paraphrase supervision. CLaRa then trains the reranker and generator end-to-end via a single language modeling loss, with gradients flowing through both modules using a differentiable top-k estimator. Theoretically, this unified optimization aligns retrieval relevance with answer quality. Experiments across multiple QA benchmarks show that CLaRa achieves state-of-the-art compression and reranking performance, often surpassing text-based fine-tuned baselines.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºCLaRaæ¡†æ¶ï¼Œé€šè¿‡åµŒå…¥å‹ç¼©å’Œè”åˆä¼˜åŒ–è§£å†³RAGä¸­é•¿ä¸Šä¸‹æ–‡ä¸æ£€ç´¢-ç”Ÿæˆå‰²è£‚çš„é—®é¢˜ï¼Œå¼•å…¥SCPæ•°æ®åˆæˆæ–¹æ³•å¢å¼ºè¯­ä¹‰å‹ç¼©ï¼Œå¹¶é‡‡ç”¨ç«¯åˆ°ç«¯è®­ç»ƒç»Ÿä¸€ä¼˜åŒ–æ£€ç´¢ä¸ç”Ÿæˆï¼Œå®éªŒè¡¨æ˜å…¶åœ¨å¤šé¡¹QAä»»åŠ¡ä¸­è¾¾åˆ°æœ€ä¼˜æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.18423v1">General Agentic Memory Via Deep Research</a></td><td><details><summary>å±•å¼€</summary>Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called \textbf{general agentic memory (GAM)}. GAM follows the principle of "\textbf{just-in time (JIT) compilation}" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) \textbf{Memorizer}, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) \textbf{Researcher}, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œé€šç”¨ä»£ç†è®°å¿†ï¼ˆGAMï¼‰â€çš„æ–°æ¡†æ¶ï¼Œé€šè¿‡â€œå³æ—¶ç¼–è¯‘â€åŸåˆ™ä¼˜åŒ–è¿è¡Œæ—¶ä¸Šä¸‹æ–‡ç”Ÿæˆï¼Œç»“åˆè½»é‡çº§è®°å¿†ï¼ˆMemorizerï¼‰å’ŒåŠ¨æ€æ£€ç´¢æ•´åˆæ¨¡å—ï¼ˆResearcherï¼‰ï¼Œåˆ©ç”¨LLMçš„ä»£ç†èƒ½åŠ›ä¸å¼ºåŒ–å­¦ä¹ æå‡ä»»åŠ¡å®Œæˆæ€§èƒ½ã€‚å…¶æ ¸å¿ƒæœºåˆ¶ï¼ˆæ£€ç´¢ä¸ç”ŸæˆååŒï¼‰ä¸RAGæŠ€æœ¯é«˜åº¦ç›¸å…³ï¼Œä½†æ›´ä¾§é‡äºä»£ç†ç³»ç»Ÿçš„åŠ¨æ€è®°å¿†ç®¡ç†ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.18354v1">Toward an AI-Native Internet: Rethinking the Web Architecture for Semantic Retrieval</a></td><td><details><summary>å±•å¼€</summary>The rise of Generative AI Search is fundamentally transforming how users and intelligent systems interact with the Internet. LLMs increasingly act as intermediaries between humans and web information. Yet the web remains optimized for human browsing rather than AI-driven semantic retrieval, resulting in wasted network bandwidth, lower information quality, and unnecessary complexity for developers. We introduce the concept of an AI-Native Internet, a web architecture in which servers expose semantically relevant information chunks rather than full documents, supported by a Web-native semantic resolver that allows AI applications to discover relevant information sources before retrieving fine-grained chunks. Through motivational experiments, we quantify the inefficiencies of current HTML-based retrieval, and outline architectural directions and open challenges for evolving today's document-centric web into an AI-oriented substrate that better supports semantic access to web content.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§ç§°ä¸ºâ€œAI-Native Internetâ€çš„æ–°å‹ç½‘ç»œæ¶æ„ï¼Œæ—¨åœ¨ä¼˜åŒ–AIç³»ç»Ÿï¼ˆå¦‚å¤§è¯­è¨€æ¨¡å‹ï¼‰å¯¹ç½‘ç»œä¿¡æ¯çš„è¯­ä¹‰æ£€ç´¢ã€‚é€šè¿‡å°†æœåŠ¡å™¨æä¾›çš„ä¿¡æ¯ä»å®Œæ•´æ–‡æ¡£è½¬å˜ä¸ºè¯­ä¹‰ç›¸å…³çš„ä¿¡æ¯å—ï¼Œå¹¶å¼•å…¥Web-nativeè¯­ä¹‰è§£æå™¨ï¼Œè¯¥æ¶æ„èƒ½å¤Ÿæ›´é«˜æ•ˆåœ°æ”¯æŒAIåº”ç”¨çš„æ£€ç´¢éœ€æ±‚ï¼Œä»è€Œå‡å°‘å¸¦å®½æµªè´¹ã€æé«˜ä¿¡æ¯è´¨é‡å¹¶ç®€åŒ–å¼€å‘æµç¨‹ã€‚è¿™ä¸RAGæŠ€æœ¯ä¸­æ£€ç´¢å¤–éƒ¨çŸ¥è¯†åº“ä»¥å¢å¼ºç”Ÿæˆæ¨¡å‹æ€§èƒ½çš„æ ¸å¿ƒæ€æƒ³å¯†åˆ‡ç›¸å…³ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.18306v1">Table Comprehension in Building Codes using Vision Language Models and Domain-Specific Fine-Tuning</a></td><td><details><summary>å±•å¼€</summary>Building codes contain critical information for ensuring safety, regulatory compliance, and informed decision-making in construction and engineering. Automated question answering systems over such codes enable quick and accurate access to specific regulatory clauses, improving efficiency and reducing errors. Retrieval-Augmented Generation (RAG) systems are essential for this task as they combine the precision of information retrieval with the generative capabilities of language models. However, tabular data are challenging to extract as they often involve complex layouts, merged cells, multi-row headers, and embedded semantic relationships that are not easily captured by traditional natural language processing techniques and Vision Language Models (VLMs). This paper explores and compares two methods for extracting information from tabular data in building codes using several pre-trained VLMs. First, a direct input method is used, where the image of the page is input directly into the VLMs, which are then tasked with answering questions based on the image. Second, an indirect input method is introduced, which involves converting an image of a page containing tables into the LaTeX code and then answering inquires based on the LaTeX-based input. The experiments find that the direct input method generally resulted in higher accuracy than the indirect input method. To further improve the performance, we fine-tuned each VLM using Low Rank Adaptation (LoRA) on a domain-specific tabular dataset. The fine-tuned models exhibited substantial improvements, with Qwen2.5-VL-3B-Instruct achieving relative accuracy gains exceeding 100%. Our results highlight the potential of parameter-efficient fine-tuning methods to adapt powerful VLMs for understanding complex structured data in specialized fields, such as building code interpretation and regulatory compliance.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ç ”ç©¶äº†åœ¨å»ºç­‘è§„èŒƒè‡ªåŠ¨åŒ–é—®ç­”ç³»ç»Ÿä¸­åº”ç”¨RAGæŠ€æœ¯æ—¶å¤„ç†å¤æ‚è¡¨æ ¼æ•°æ®çš„æŒ‘æˆ˜ï¼Œæå‡ºå¹¶æ¯”è¾ƒäº†åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ç›´æ¥ä¸é—´æ¥è¡¨æ ¼æå–æ–¹æ³•ï¼Œå‘ç°ç›´æ¥è¾“å…¥æ³•æ›´ä¼˜ï¼Œå¹¶é€šè¿‡LoRAå¾®è°ƒæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.18303v1">Hierarchical Deep Research with Local-Web RAG: Toward Automated System-Level Materials Discovery</a></td><td><details><summary>å±•å¼€</summary>We present a long-horizon, hierarchical deep research (DR) agent designed for complex materials and device discovery problems that exceed the scope of existing Machine Learning (ML) surrogates and closed-source commercial agents. Our framework instantiates a locally deployable DR instance that integrates local retrieval-augmented generation with large language model reasoners, enhanced by a Deep Tree of Research (DToR) mechanism that adaptively expands and prunes research branches to maximize coverage, depth, and coherence. We systematically evaluate across 27 nanomaterials/device topics using a large language model (LLM)-as-judge rubric with five web-enabled state-of-the-art models as jurors. In addition, we conduct dry-lab validations on five representative tasks, where human experts use domain simulations (e.g., density functional theory, DFT) to verify whether DR-agent proposals are actionable. Results show that our DR agent produces reports with quality comparable to--and often exceeding--those of commercial systems (ChatGPT-5-thinking/o3/o4-mini-high Deep Research) at a substantially lower cost, while enabling on-prem integration with local data and tools.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§ç”¨äºå¤æ‚ææ–™å’Œè®¾å¤‡å‘ç°é—®é¢˜çš„åˆ†å±‚æ·±åº¦ç ”ç©¶ï¼ˆDRï¼‰æ™ºèƒ½ä½“ï¼Œè¯¥æ™ºèƒ½ä½“é›†æˆäº†æœ¬åœ°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸å¤§è¯­è¨€æ¨¡å‹æ¨ç†å™¨ï¼Œå¹¶é€šè¿‡è‡ªé€‚åº”æ‰©å±•å’Œå‰ªæç ”ç©¶åˆ†æ”¯çš„æ·±åº¦ç ”ç©¶æ ‘ï¼ˆDToRï¼‰æœºåˆ¶è¿›è¡Œå¢å¼ºã€‚ç ”ç©¶åœ¨å¤šä»»åŠ¡è¯„ä¼°å’Œä¸“å®¶éªŒè¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…¶æŠ¥å‘Šè´¨é‡ä¸å•†ä¸šç³»ç»Ÿç›¸å½“ç”šè‡³æ›´é«˜ï¼ŒåŒæ—¶æˆæœ¬æ›´ä½ä¸”æ”¯æŒæœ¬åœ°æ•°æ®ä¸å·¥å…·çš„é›†æˆã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.18298v1">Cross-Disciplinary Knowledge Retrieval and Synthesis: A Compound AI Architecture for Scientific Discovery</a></td><td><details><summary>å±•å¼€</summary>The exponential growth of scientific knowledge has created significant barriers to cross-disciplinary knowledge discovery, synthesis and research collaboration. In response to this challenge, we present BioSage, a novel compound AI architecture that integrates LLMs with RAG, orchestrated specialized agents and tools to enable discoveries across AI, data science, biomedical, and biosecurity domains. Our system features several specialized agents including the retrieval agent with query planning and response synthesis that enable knowledge retrieval across domains with citation-backed responses, cross-disciplinary translation agents that align specialized terminology and methodologies, and reasoning agents that synthesize domain-specific insights with transparency, traceability and usability. We demonstrate the effectiveness of our BioSage system through a rigorous evaluation on scientific benchmarks (LitQA2, GPQA, WMDP, HLE-Bio) and introduce a new cross-modal benchmark for biology and AI, showing that our BioSage agents outperform vanilla and RAG approaches by 13\%-21\% powered by Llama 3.1. 70B and GPT-4o models. We perform causal investigations into compound AI system behavior and report significant performance improvements by adding RAG and agents over the vanilla models. Unlike other systems, our solution is driven by user-centric design principles and orchestrates specialized user-agent interaction workflows supporting scientific activities including but not limited to summarization, research debate and brainstorming. Our ongoing work focuses on multimodal retrieval and reasoning over charts, tables, and structured scientific data, along with developing comprehensive multimodal benchmarks for cross-disciplinary discovery. Our compound AI solution demonstrates significant potential for accelerating scientific advancement by reducing barriers between traditionally siloed domains.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†BioSageï¼Œä¸€ç§ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€ä¸“ä¸šä»£ç†å’Œå·¥å…·çš„å¤åˆAIæ¶æ„ï¼Œæ—¨åœ¨ä¿ƒè¿›è·¨å­¦ç§‘çŸ¥è¯†å‘ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨AIã€æ•°æ®ç§‘å­¦ã€ç”Ÿç‰©åŒ»å­¦å’Œç”Ÿç‰©å®‰å…¨é¢†åŸŸã€‚BioSageé€šè¿‡æ£€ç´¢ä»£ç†ã€è·¨å­¦ç§‘ç¿»è¯‘ä»£ç†å’Œæ¨ç†ä»£ç†ç­‰åŠŸèƒ½ï¼Œå®ç°äº†è·¨é¢†åŸŸçŸ¥è¯†æ£€ç´¢ã€æœ¯è¯­å¯¹é½å’Œé€æ˜æ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨ç§‘å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•å’Œçº¯RAGæ–¹æ³•ï¼Œæ€§èƒ½æå‡13%-21%ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢è®¨äº†ç”¨æˆ·ä¸­å¿ƒè®¾è®¡åŸåˆ™å’Œå¤šæ¨¡æ€æ£€ç´¢çš„æœªæ¥å‘å±•æ–¹å‘ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-22
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.18423v1">General Agentic Memory Via Deep Research</a></td><td><details><summary>å±•å¼€</summary>Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called \textbf{general agentic memory (GAM)}. GAM follows the principle of "\textbf{just-in time (JIT) compilation}" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) \textbf{Memorizer}, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) \textbf{Researcher}, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§åä¸º**é€šç”¨ä»£ç†è®°å¿†ï¼ˆGAMï¼‰**çš„æ–°æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è¿è¡Œæ—¶ç”Ÿæˆä¼˜åŒ–ä¸Šä¸‹æ–‡ï¼ˆç±»ä¼¼JITç¼–è¯‘åŸåˆ™ï¼‰æ¥å¢å¼ºAIä»£ç†çš„è®°å¿†èƒ½åŠ›ã€‚å…¶æ ¸å¿ƒè®¾è®¡åŒ…æ‹¬**Memorizer**ï¼ˆè½»é‡çº§è®°å¿†æå–å…³é”®å†å²ä¿¡æ¯ï¼‰å’Œ**Researcher**ï¼ˆåŸºäºè®°å¿†æ£€ç´¢å¹¶æ•´åˆå­˜å‚¨ä¸­çš„ä¿¡æ¯ï¼‰ï¼Œç»“åˆLLMçš„ä»£ç†èƒ½åŠ›ä¸å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼Œæ˜¾è‘—æå‡äº†ä¾èµ–è®°å¿†çš„ä»»åŠ¡æ€§èƒ½ã€‚è¯¥æ¡†æ¶ä¸RAGå…±äº«â€œæ£€ç´¢-å¢å¼ºâ€çš„æ ¸å¿ƒæ€æƒ³ï¼Œä½†æ›´ä¾§é‡äºä»£ç†è®°å¿†çš„åŠ¨æ€æ„å»ºä¸ä¼˜åŒ–ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.18354v1">Toward an AI-Native Internet: Rethinking the Web Architecture for Semantic Retrieval</a></td><td><details><summary>å±•å¼€</summary>The rise of Generative AI Search is fundamentally transforming how users and intelligent systems interact with the Internet. LLMs increasingly act as intermediaries between humans and web information. Yet the web remains optimized for human browsing rather than AI-driven semantic retrieval, resulting in wasted network bandwidth, lower information quality, and unnecessary complexity for developers. We introduce the concept of an AI-Native Internet, a web architecture in which servers expose semantically relevant information chunks rather than full documents, supported by a Web-native semantic resolver that allows AI applications to discover relevant information sources before retrieving fine-grained chunks. Through motivational experiments, we quantify the inefficiencies of current HTML-based retrieval, and outline architectural directions and open challenges for evolving today's document-centric web into an AI-oriented substrate that better supports semantic access to web content.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºâ€œAI-Native Internetâ€æ¦‚å¿µï¼Œæ—¨åœ¨ä¼˜åŒ–ç½‘ç»œæ¶æ„ä»¥é€‚åº”AIé©±åŠ¨çš„è¯­ä¹‰æ£€ç´¢ï¼ˆå¦‚RAGåœºæ™¯ï¼‰ã€‚é€šè¿‡æš´éœ²è¯­ä¹‰ä¿¡æ¯å—è€Œéå®Œæ•´æ–‡æ¡£ï¼Œå¹¶å¼•å…¥WebåŸç”Ÿè¯­ä¹‰è§£æå™¨ï¼Œè§£å†³å½“å‰HTMLæ£€ç´¢çš„ä½æ•ˆé—®é¢˜ï¼Œæå‡AIåº”ç”¨è·å–ç²¾å‡†ä¿¡æ¯çš„èƒ½åŠ›ï¼Œä¸RAGä¸­â€œæ£€ç´¢-ç”Ÿæˆâ€ååŒçš„æ ¸å¿ƒæ€æƒ³é«˜åº¦ç›¸å…³ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.18306v1">Table Comprehension in Building Codes using Vision Language Models and Domain-Specific Fine-Tuning</a></td><td><details><summary>å±•å¼€</summary>Building codes contain critical information for ensuring safety, regulatory compliance, and informed decision-making in construction and engineering. Automated question answering systems over such codes enable quick and accurate access to specific regulatory clauses, improving efficiency and reducing errors. Retrieval-Augmented Generation (RAG) systems are essential for this task as they combine the precision of information retrieval with the generative capabilities of language models. However, tabular data are challenging to extract as they often involve complex layouts, merged cells, multi-row headers, and embedded semantic relationships that are not easily captured by traditional natural language processing techniques and Vision Language Models (VLMs). This paper explores and compares two methods for extracting information from tabular data in building codes using several pre-trained VLMs. First, a direct input method is used, where the image of the page is input directly into the VLMs, which are then tasked with answering questions based on the image. Second, an indirect input method is introduced, which involves converting an image of a page containing tables into the LaTeX code and then answering inquires based on the LaTeX-based input. The experiments find that the direct input method generally resulted in higher accuracy than the indirect input method. To further improve the performance, we fine-tuned each VLM using Low Rank Adaptation (LoRA) on a domain-specific tabular dataset. The fine-tuned models exhibited substantial improvements, with Qwen2.5-VL-3B-Instruct achieving relative accuracy gains exceeding 100%. Our results highlight the potential of parameter-efficient fine-tuning methods to adapt powerful VLMs for understanding complex structured data in specialized fields, such as building code interpretation and regulatory compliance.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åœ¨å»ºç­‘è§„èŒƒä¸­å¤„ç†å¤æ‚è¡¨æ ¼æ•°æ®çš„ä¸¤ç§æ–¹æ³•ï¼Œç»“åˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œé€šè¿‡é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç›´æ¥æˆ–é—´æ¥ï¼ˆè½¬æ¢ä¸ºLaTeXï¼‰è¾“å…¥è¡¨æ ¼å›¾åƒè¿›è¡Œé—®ç­”ï¼Œå¹¶åˆ©ç”¨LoRAå¾®è°ƒæå‡æ¨¡å‹æ€§èƒ½ï¼Œæœ€ç»ˆå±•ç¤ºäº†åœ¨ä¸“ä¸šé¢†åŸŸç†è§£ç»“æ„åŒ–æ•°æ®çš„æ½œåŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.18303v1">Hierarchical Deep Research with Local-Web RAG: Toward Automated System-Level Materials Discovery</a></td><td><details><summary>å±•å¼€</summary>We present a long-horizon, hierarchical deep research (DR) agent designed for complex materials and device discovery problems that exceed the scope of existing Machine Learning (ML) surrogates and closed-source commercial agents. Our framework instantiates a locally deployable DR instance that integrates local retrieval-augmented generation with large language model reasoners, enhanced by a Deep Tree of Research (DToR) mechanism that adaptively expands and prunes research branches to maximize coverage, depth, and coherence. We systematically evaluate across 27 nanomaterials/device topics using a large language model (LLM)-as-judge rubric with five web-enabled state-of-the-art models as jurors. In addition, we conduct dry-lab validations on five representative tasks, where human experts use domain simulations (e.g., density functional theory, DFT) to verify whether DR-agent proposals are actionable. Results show that our DR agent produces reports with quality comparable to--and often exceeding--those of commercial systems (ChatGPT-5-thinking/o3/o4-mini-high Deep Research) at a substantially lower cost, while enabling on-prem integration with local data and tools.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§ç”¨äºå¤æ‚ææ–™å’Œè®¾å¤‡å‘ç°é—®é¢˜çš„åˆ†å±‚æ·±åº¦ç ”ç©¶ï¼ˆDRï¼‰æ™ºèƒ½ä½“ï¼Œè¯¥æ™ºèƒ½ä½“ç»“åˆäº†æœ¬åœ°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œå¤§è¯­è¨€æ¨¡å‹æ¨ç†å™¨ï¼Œå¹¶é€šè¿‡æ·±åº¦ç ”ç©¶æ ‘ï¼ˆDToRï¼‰æœºåˆ¶åŠ¨æ€è°ƒæ•´ç ”ç©¶åˆ†æ”¯ä»¥ä¼˜åŒ–è¦†ç›–èŒƒå›´å’Œæ·±åº¦ã€‚ç ”ç©¶é€šè¿‡LLMè¯„ä¼°å’Œä¸“å®¶éªŒè¯è¡¨æ˜ï¼Œè¯¥DRæ™ºèƒ½ä½“ç”Ÿæˆçš„æŠ¥å‘Šè´¨é‡ä¸å•†ä¸šç³»ç»Ÿç›¸å½“ç”šè‡³æ›´ä¼˜ï¼ŒåŒæ—¶æˆæœ¬æ›´ä½ä¸”æ”¯æŒæœ¬åœ°æ•°æ®ä¸å·¥å…·é›†æˆã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.18298v1">Cross-Disciplinary Knowledge Retrieval and Synthesis: A Compound AI Architecture for Scientific Discovery</a></td><td><details><summary>å±•å¼€</summary>The exponential growth of scientific knowledge has created significant barriers to cross-disciplinary knowledge discovery, synthesis and research collaboration. In response to this challenge, we present BioSage, a novel compound AI architecture that integrates LLMs with RAG, orchestrated specialized agents and tools to enable discoveries across AI, data science, biomedical, and biosecurity domains. Our system features several specialized agents including the retrieval agent with query planning and response synthesis that enable knowledge retrieval across domains with citation-backed responses, cross-disciplinary translation agents that align specialized terminology and methodologies, and reasoning agents that synthesize domain-specific insights with transparency, traceability and usability. We demonstrate the effectiveness of our BioSage system through a rigorous evaluation on scientific benchmarks (LitQA2, GPQA, WMDP, HLE-Bio) and introduce a new cross-modal benchmark for biology and AI, showing that our BioSage agents outperform vanilla and RAG approaches by 13\%-21\% powered by Llama 3.1. 70B and GPT-4o models. We perform causal investigations into compound AI system behavior and report significant performance improvements by adding RAG and agents over the vanilla models. Unlike other systems, our solution is driven by user-centric design principles and orchestrates specialized user-agent interaction workflows supporting scientific activities including but not limited to summarization, research debate and brainstorming. Our ongoing work focuses on multimodal retrieval and reasoning over charts, tables, and structured scientific data, along with developing comprehensive multimodal benchmarks for cross-disciplinary discovery. Our compound AI solution demonstrates significant potential for accelerating scientific advancement by reducing barriers between traditionally siloed domains.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†BioSageï¼Œä¸€ç§ç»“åˆäº†LLMsä¸RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰çš„å¤åˆAIæ¶æ„ï¼Œé€šè¿‡ä¸“ä¸šåŒ–æ™ºèƒ½ä»£ç†ï¼ˆå¦‚æ£€ç´¢ä»£ç†ã€è·¨å­¦ç§‘ç¿»è¯‘ä»£ç†å’Œæ¨ç†ä»£ç†ï¼‰å®ç°è·¨å­¦ç§‘çŸ¥è¯†å‘ç°ï¼Œå°¤å…¶åœ¨ç”Ÿç‰©åŒ»å­¦å’Œç”Ÿç‰©å®‰å…¨é¢†åŸŸã€‚ç³»ç»Ÿåœ¨ç§‘å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»ŸåŠçº¯RAGæ–¹æ³•ï¼ˆæå‡13%-21%ï¼‰ï¼Œå¹¶å¼ºè°ƒç”¨æˆ·ä¸­å¿ƒè®¾è®¡ã€å¯è¿½æº¯æ€§åŠå¤šæ¨¡æ€æ£€ç´¢çš„æœªæ¥å‘å±•æ–¹å‘ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.19481v1">Quality analysis and evaluation prediction of RAG retrieval based on machine learning algorithms</a></td><td><details><summary>å±•å¼€</summary>With the rapid evolution of large language models, retrieval enhanced generation technology has been widely used due to its ability to integrate external knowledge to improve output accuracy. However, the performance of the system is highly dependent on the quality of the retrieval module. If the retrieval results have low relevance to user needs or contain noisy information, it will directly lead to distortion of the generated content. In response to the performance bottleneck of existing models in processing tabular features, this paper proposes an XGBoost machine learning regression model based on feature engineering and particle swarm optimization. Correlation analysis shows that answer_quality is positively correlated with doc_delevance by 0.66, indicating that document relevance has a significant positive effect on answer quality, and improving document relevance may enhance answer quality; The strong negative correlations between semantic similarity, redundancy, and diversity were -0.89 and -0.88, respectively, indicating a tradeoff between semantic similarity, redundancy, and diversity. In other words, as the former two increased, diversity significantly decreased. The experimental results comparing decision trees, AdaBoost, etc. show that the VMD PSO BiLSTM model is superior in all evaluation indicators, with significantly lower MSE, RMSE, MAE, and MAPE compared to the comparison model. The R2 value is higher, indicating that its prediction accuracy, stability, and data interpretation ability are more outstanding. This achievement provides an effective path for optimizing the retrieval quality and improving the generation effect of RAG system, and has important value in promoting the implementation and application of related technologies.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡é’ˆå¯¹RAGç³»ç»Ÿä¸­æ£€ç´¢æ¨¡å—è´¨é‡å¯¹ç”Ÿæˆæ•ˆæœçš„å½±å“é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç‰¹å¾å·¥ç¨‹å’Œç²’å­ç¾¤ä¼˜åŒ–çš„XGBoostå›å½’æ¨¡å‹ï¼Œé€šè¿‡åˆ†ææ–‡æ¡£ç›¸å…³æ€§ç­‰ç‰¹å¾å¯¹ç­”æ¡ˆè´¨é‡çš„å½±å“ï¼ˆå¦‚æ–‡æ¡£ç›¸å…³æ€§ä¸ç­”æ¡ˆè´¨é‡å‘ˆ0.66æ­£ç›¸å…³ï¼‰ï¼Œå¹¶éªŒè¯æ¨¡å‹åœ¨é™ä½é¢„æµ‹è¯¯å·®ï¼ˆMSEç­‰æŒ‡æ ‡ï¼‰å’Œæå‡è§£é‡ŠåŠ›ï¼ˆR2å€¼ï¼‰ä¸Šçš„ä¼˜è¶Šæ€§ï¼Œä¸ºä¼˜åŒ–RAGç³»ç»Ÿçš„æ£€ç´¢è´¨é‡åŠç”Ÿæˆæ•ˆæœæä¾›äº†æœ‰æ•ˆæ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.18194v1">Agent-as-a-Graph: Knowledge Graph-Based Tool and Agent Retrieval for LLM Multi-Agent Systems</a></td><td><details><summary>å±•å¼€</summary>Recent advances in Large Language Model Multi-Agent Systems enable scalable orchestration and retrieval of specialized, parallelized subagents, each equipped with hundreds or thousands of Model Context Protocol (MCP) servers and tools. However, existing agent, MCP, and retrieval methods typically match queries against a single agent description, obscuring fine-grained tool capabilities of each agent, resulting in suboptimal agent selection. We introduce Agent-as-a-Graph retrieval, a knowledge graph retrieval augmented generation approach that represents both tools and their parent agents as nodes and edges in a knowledge graph. During retrieval, i) relevant agents and tool nodes are first retrieved through vector search, ii) we apply a type-specific weighted reciprocal rank fusion (wRRF) for reranking tools and agents, and iii) parent agents are traversed in the knowledge graph for the final set of agents. We evaluate Agent-as-a-Graph on the LiveMCPBenchmark, achieving 14.9% and 14.6% improvements in Recall@5 and nDCG@5 over prior state-of-the-art retrievers, and 2.4% improvements in wRRF optimizations.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºçŸ¥è¯†å›¾è°±æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ–¹æ³•â€”â€”Agent-as-a-Graphï¼Œé€šè¿‡å°†å·¥å…·å’Œå…¶çˆ¶ä»£ç†è¡¨ç¤ºä¸ºçŸ¥è¯†å›¾è°±ä¸­çš„èŠ‚ç‚¹å’Œè¾¹ï¼Œç»“åˆå‘é‡æœç´¢ã€åŠ æƒäº’é€†æ’åºèåˆï¼ˆwRRFï¼‰é‡æ’åºä»¥åŠå›¾è°±éå†æŠ€æœ¯ï¼Œæ˜¾è‘—ä¼˜åŒ–äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„ä»£ç†å’Œå·¥å…·æ£€ç´¢æ•ˆæœï¼Œå¹¶åœ¨å®éªŒä¸­å–å¾—äº†å¬å›ç‡å’Œæ’åºæŒ‡æ ‡ä¸Šçš„æ˜¾è‘—æå‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.18192v1">ARIAL: An Agentic Framework for Document VQA with Precise Answer Localization</a></td><td><details><summary>å±•å¼€</summary>Document Visual Question Answering (VQA) requires models to not only extract accurate textual answers but also precisely localize them within document images, a capability critical for interpretability in high-stakes applications. However, existing systems achieve strong textual accuracy while producing unreliable spatial grounding, or sacrifice performance for interpretability. We present ARIAL (Agentic Reasoning for Interpretable Answer Localization), a modular framework that orchestrates specialized tools through an LLM-based planning agent to achieve both precise answer extraction and reliable spatial grounding. ARIAL decomposes Document VQA into structured subtasks: OCR-based text extraction with TrOCR, retrieval-augmented context selection using semantic search, answer generation via a fine-tuned Gemma 3-27B model, and explicit bounding-box localization through text-to-region alignment. This modular architecture produces transparent reasoning traces, enabling tool-level auditability and independent component optimization. We evaluate ARIAL on four benchmarks (DocVQA, FUNSD, CORD, and SROIE) using both textual accuracy (ANLS) and spatial precision (mAP at IoU 0.50 to 0.95). ARIAL achieves state-of-the-art results across all datasets: 88.7 ANLS and 50.1 mAP on DocVQA, 90.0 ANLS and 50.3 mAP on FUNSD, 85.5 ANLS and 60.2 mAP on CORD, and 93.1 ANLS on SROIE, surpassing the previous best method (DLaVA) by +2.8 ANLS and +3.9 mAP on DocVQA. Our work demonstrates how agentic orchestration of specialized tools can simultaneously improve performance and interpretability, providing a pathway toward trustworthy, explainable document AI systems.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºARIALçš„æ¨¡å—åŒ–æ¡†æ¶ï¼Œé€šè¿‡åŸºäºLLMçš„è§„åˆ’ä»£ç†åè°ƒä¸“ç”¨å·¥å…·ï¼ˆåŒ…æ‹¬åŸºäºOCRçš„æ–‡æœ¬æå–ã€åŸºäºè¯­ä¹‰æœç´¢çš„æ£€ç´¢å¢å¼ºä¸Šä¸‹æ–‡é€‰æ‹©ç­‰ï¼‰ï¼Œä»¥è§£å†³æ–‡æ¡£è§†è§‰é—®ç­”ä»»åŠ¡ä¸­ç­”æ¡ˆæå–å’Œç©ºé—´å®šä½çš„æŒ‘æˆ˜ã€‚å…¶ä¸­æ˜ç¡®æåˆ°ä½¿ç”¨"æ£€ç´¢å¢å¼ºï¼ˆretrieval-augmentedï¼‰"æŠ€æœ¯è¿›è¡Œä¸Šä¸‹æ–‡é€‰æ‹©ï¼Œç»“åˆç”Ÿæˆæ¨¡å‹ï¼ˆGemma 3-27Bï¼‰ç”Ÿæˆç­”æ¡ˆï¼Œç¬¦åˆRAGçš„æ ¸å¿ƒæŠ€æœ¯ç‰¹å¾ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶æé«˜äº†ç³»ç»Ÿçš„å¯è§£é‡Šæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.18177v1">Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models</a></td><td><details><summary>å±•å¼€</summary>Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models to answer financial questions using external knowledge bases of U.S. SEC filings, earnings reports, and regulatory documents. However, existing work lacks systematic comparison of vector-based and non-vector RAG architectures for financial documents, and the empirical impact of advanced RAG techniques on retrieval accuracy, answer quality, latency, and cost remain unclear. We present the first systematic evaluation comparing vector-based agentic RAG using hybrid search and metadata filtering against hierarchical node-based systems that traverse document structure without embeddings. We evaluate two enhancement techniques applied to the vector-based architecture, i) cross-encoder reranking for retrieval precision, and ii) small-to-big chunk retrieval for context completeness. Across 1,200 SEC 10-K, 10-Q, and 8-K filings on a 150-question benchmark, we measure retrieval metrics (MRR, Recall@5), answer quality through LLM-as-a-judge pairwise comparisons, latency, and preprocessing costs. Vector-based agentic RAG achieves a 68% win rate over hierarchical node-based systems with comparable latency (5.2 compared to 5.98 seconds). Cross-encoder reranking achieves a 59% absolute improvement at optimal parameters (10, 5) for MRR@5. Small-to-big retrieval achieves a 65% win rate over baseline chunking with only 0.2 seconds additional latency. Our findings reveal that applying advanced RAG techniques to financial Q&A systems improves retrieval accuracy, answer quality, and has cost-performance tradeoffs to be considered in production.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç³»ç»Ÿåœ°æ¯”è¾ƒäº†åŸºäºå‘é‡å’Œéå‘é‡çš„RAGæ¶æ„åœ¨é‡‘èæ–‡æ¡£å¤„ç†ä¸­çš„è¡¨ç°ï¼Œè¯„ä¼°äº†é«˜çº§RAGæŠ€æœ¯ï¼ˆå¦‚äº¤å‰ç¼–ç å™¨é‡æ’åºå’Œå°åˆ°å¤§å—æ£€ç´¢ï¼‰å¯¹æ£€ç´¢å‡†ç¡®æ€§ã€å›ç­”è´¨é‡ã€å»¶è¿Ÿå’Œæˆæœ¬çš„å½±å“ï¼Œå¹¶å‘ç°åŸºäºå‘é‡çš„RAGåœ¨é‡‘èé—®ç­”ç³»ç»Ÿä¸­è¡¨ç°æ›´ä¼˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.17908v1">Principled Context Engineering for RAG: Statistical Guarantees via Conformal Prediction</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) enhances factual grounding in large language models (LLMs) by incorporating retrieved evidence, but LLM accuracy declines when long or noisy contexts exceed the model's effective attention span. Existing pre-generation filters rely on heuristics or uncalibrated LLM confidence scores, offering no statistical control over retained evidence. We evaluate and demonstrate context engineering through conformal prediction, a coverage-controlled filtering framework that removes irrelevant content while preserving recall of supporting evidence. Using both embedding- and LLM-based scoring functions, we test this approach on the NeuCLIR and RAGTIME collections. Conformal filtering consistently meets its target coverage, ensuring that a specified fraction of relevant snippets are retained, and reduces retained context by 2-3x relative to unfiltered retrieval. On NeuCLIR, downstream factual accuracy measured by ARGUE F1 improves under strict filtering and remains stable at moderate coverage, indicating that most discarded material is redundant or irrelevant. These results demonstrate that conformal prediction enables reliable, coverage-controlled context reduction in RAG, offering a model-agnostic and principled approach to context engineering.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡åŸºäºç¬¦åˆé¢„æµ‹çš„è¦†ç›–æ§åˆ¶è¿‡æ»¤æ¡†æ¶ï¼ˆconformal filteringï¼‰æ¥è§£å†³RAGä¸­é•¿æˆ–å˜ˆæ‚ä¸Šä¸‹æ–‡å¯¼è‡´å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‡†ç¡®æ€§ä¸‹é™çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨ä¿ç•™æ”¯æŒæ€§è¯æ®çš„åŒæ—¶ç§»é™¤æ— å…³å†…å®¹ï¼Œå®éªŒè¡¨æ˜è¯¥æ¡†æ¶èƒ½æœ‰æ•ˆå‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦2-3å€ï¼Œå¹¶åœ¨NeuCLIRæ•°æ®é›†ä¸Šæå‡ä¸‹æ¸¸äº‹å®å‡†ç¡®æ€§ï¼ˆARGUE F1ï¼‰ï¼Œè¯æ˜å…¶æ˜¯ä¸€ç§æ¨¡å‹æ— å…³ä¸”åŸåˆ™æ€§çš„RAGä¸Šä¸‹æ–‡ä¼˜åŒ–æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.17854v1">A superpersuasive autonomous policy debating system</a></td><td><details><summary>å±•å¼€</summary>The capacity for highly complex, evidence-based, and strategically adaptive persuasion remains a formidable great challenge for artificial intelligence. Previous work, like IBM Project Debater, focused on generating persuasive speeches in simplified and shortened debate formats intended for relatively lay audiences. We introduce DeepDebater, a novel autonomous system capable of participating in and winning a full, unmodified, two-team competitive policy debate. Our system employs a hierarchical architecture of specialized multi-agent workflows, where teams of LLM-powered agents collaborate and critique one another to perform discrete argumentative tasks. Each workflow utilizes iterative retrieval, synthesis, and self-correction using a massive corpus of policy debate evidence (OpenDebateEvidence) and produces complete speech transcripts, cross-examinations, and rebuttals. We introduce a live, interactive end-to-end presentation pipeline that renders debates with AI speech and animation: transcripts are surface-realized and synthesized to audio with OpenAI TTS, and then displayed as talking-head portrait videos with EchoMimic V1. Beyond fully autonomous matches (AI vs AI), DeepDebater supports hybrid human-AI operation: human debaters can intervene at any stage, and humans can optionally serve as opponents against AI in any speech, allowing AI-human and AI-AI rounds. In preliminary evaluations against human-authored cases, DeepDebater produces qualitatively superior argumentative components and consistently wins simulated rounds as adjudicated by an independent autonomous judge. Expert human debate coaches also prefer the arguments, evidence, and cases constructed by DeepDebater. We open source all code, generated speech transcripts, audio and talking head video here: https://github.com/Hellisotherpeople/DeepDebater/tree/main</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†DeepDebaterç³»ç»Ÿï¼Œä¸€ä¸ªèƒ½å¤Ÿåœ¨å®Œæ•´æ”¿ç­–è¾©è®ºä¸­è‡ªä¸»å‚ä¸å¹¶è·èƒœçš„å¤šæ™ºèƒ½ä½“æ¶æ„ã€‚è¯¥ç³»ç»Ÿé€šè¿‡åˆ†å±‚å·¥ä½œæµç¨‹ï¼Œåˆ©ç”¨å¤§è§„æ¨¡è¾©è®ºè¯æ®åº“ï¼ˆOpenDebateEvidenceï¼‰è¿›è¡Œè¿­ä»£æ£€ç´¢ã€ç»¼åˆä¸è‡ªæˆ‘ä¿®æ­£ï¼Œç”Ÿæˆè¾©è®ºå‘è¨€ã€è´¨è¯¢å’Œåé©³å†…å®¹ï¼Œå¹¶ç»“åˆè¯­éŸ³åˆæˆä¸åŠ¨ç”»å‘ˆç°ã€‚å…¶æ ¸å¿ƒæ£€ç´¢å¢å¼ºæœºåˆ¶ï¼ˆå¦‚è¯æ®åº“æ£€ç´¢ä¸ä¸Šä¸‹æ–‡æ•´åˆï¼‰ä¸RAGæŠ€æœ¯é«˜åº¦ç›¸å…³ï¼ŒåŒæ—¶æ”¯æŒäººæœºåä½œï¼Œå¹¶åœ¨è¯„ä¼°ä¸­å±•ç°å‡ºä¼˜äºäººç±»çš„è¡¨ç°ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-21
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.19481v1">Quality analysis and evaluation prediction of RAG retrieval based on machine learning algorithms</a></td><td><details><summary>å±•å¼€</summary>With the rapid evolution of large language models, retrieval enhanced generation technology has been widely used due to its ability to integrate external knowledge to improve output accuracy. However, the performance of the system is highly dependent on the quality of the retrieval module. If the retrieval results have low relevance to user needs or contain noisy information, it will directly lead to distortion of the generated content. In response to the performance bottleneck of existing models in processing tabular features, this paper proposes an XGBoost machine learning regression model based on feature engineering and particle swarm optimization. Correlation analysis shows that answer_quality is positively correlated with doc_delevance by 0.66, indicating that document relevance has a significant positive effect on answer quality, and improving document relevance may enhance answer quality; The strong negative correlations between semantic similarity, redundancy, and diversity were -0.89 and -0.88, respectively, indicating a tradeoff between semantic similarity, redundancy, and diversity. In other words, as the former two increased, diversity significantly decreased. The experimental results comparing decision trees, AdaBoost, etc. show that the VMD PSO BiLSTM model is superior in all evaluation indicators, with significantly lower MSE, RMSE, MAE, and MAPE compared to the comparison model. The R2 value is higher, indicating that its prediction accuracy, stability, and data interpretation ability are more outstanding. This achievement provides an effective path for optimizing the retrieval quality and improving the generation effect of RAG system, and has important value in promoting the implementation and application of related technologies.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­æ£€ç´¢æ¨¡å—è´¨é‡å¯¹ç”Ÿæˆæ•ˆæœçš„å½±å“é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç‰¹å¾å·¥ç¨‹å’Œç²’å­ç¾¤ä¼˜åŒ–çš„XGBoostå›å½’æ¨¡å‹ï¼Œé€šè¿‡åˆ†ææ–‡æ¡£ç›¸å…³æ€§ã€è¯­ä¹‰ç›¸ä¼¼æ€§ç­‰ç‰¹å¾ä¸ç­”æ¡ˆè´¨é‡çš„å…³ç³»ï¼Œä¼˜åŒ–æ£€ç´¢ç»“æœè´¨é‡ã€‚å®éªŒè¡¨æ˜è¯¥æ¨¡å‹åœ¨é¢„æµ‹ç²¾åº¦å’Œç¨³å®šæ€§ä¸Šä¼˜äºå¯¹æ¯”æ¨¡å‹ï¼Œä¸ºæå‡RAGç³»ç»Ÿçš„ç”Ÿæˆæ•ˆæœæä¾›äº†æœ‰æ•ˆæ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.18194v1">Agent-as-a-Graph: Knowledge Graph-Based Tool and Agent Retrieval for LLM Multi-Agent Systems</a></td><td><details><summary>å±•å¼€</summary>Recent advances in Large Language Model Multi-Agent Systems enable scalable orchestration and retrieval of specialized, parallelized subagents, each equipped with hundreds or thousands of Model Context Protocol (MCP) servers and tools. However, existing agent, MCP, and retrieval methods typically match queries against a single agent description, obscuring fine-grained tool capabilities of each agent, resulting in suboptimal agent selection. We introduce Agent-as-a-Graph retrieval, a knowledge graph retrieval augmented generation approach that represents both tools and their parent agents as nodes and edges in a knowledge graph. During retrieval, i) relevant agents and tool nodes are first retrieved through vector search, ii) we apply a type-specific weighted reciprocal rank fusion (wRRF) for reranking tools and agents, and iii) parent agents are traversed in the knowledge graph for the final set of agents. We evaluate Agent-as-a-Graph on the LiveMCPBenchmark, achieving 14.9% and 14.6% improvements in Recall@5 and nDCG@5 over prior state-of-the-art retrievers, and 2.4% improvements in wRRF optimizations.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºAgent-as-a-Graphçš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡çŸ¥è¯†å›¾è°±è¡¨ç¤ºå·¥å…·åŠå…¶çˆ¶ä»£ç†èŠ‚ç‚¹ï¼Œç»“åˆå‘é‡æœç´¢ã€åŠ æƒæ’åºèåˆå’ŒçŸ¥è¯†å›¾è°±éå†æŠ€æœ¯ï¼Œä¼˜åŒ–å¤šä»£ç†ç³»ç»Ÿä¸­çš„ä»£ç†å’Œå·¥å…·æ£€ç´¢æ€§èƒ½ï¼Œåœ¨LiveMCPBenchmarkä¸Šæ˜¾è‘—æå‡äº†å¬å›ç‡å’Œæ’åºè´¨é‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.18192v1">ARIAL: An Agentic Framework for Document VQA with Precise Answer Localization</a></td><td><details><summary>å±•å¼€</summary>Document Visual Question Answering (VQA) requires models to not only extract accurate textual answers but also precisely localize them within document images, a capability critical for interpretability in high-stakes applications. However, existing systems achieve strong textual accuracy while producing unreliable spatial grounding, or sacrifice performance for interpretability. We present ARIAL (Agentic Reasoning for Interpretable Answer Localization), a modular framework that orchestrates specialized tools through an LLM-based planning agent to achieve both precise answer extraction and reliable spatial grounding. ARIAL decomposes Document VQA into structured subtasks: OCR-based text extraction with TrOCR, retrieval-augmented context selection using semantic search, answer generation via a fine-tuned Gemma 3-27B model, and explicit bounding-box localization through text-to-region alignment. This modular architecture produces transparent reasoning traces, enabling tool-level auditability and independent component optimization. We evaluate ARIAL on four benchmarks (DocVQA, FUNSD, CORD, and SROIE) using both textual accuracy (ANLS) and spatial precision (mAP at IoU 0.50 to 0.95). ARIAL achieves state-of-the-art results across all datasets: 88.7 ANLS and 50.1 mAP on DocVQA, 90.0 ANLS and 50.3 mAP on FUNSD, 85.5 ANLS and 60.2 mAP on CORD, and 93.1 ANLS on SROIE, surpassing the previous best method (DLaVA) by +2.8 ANLS and +3.9 mAP on DocVQA. Our work demonstrates how agentic orchestration of specialized tools can simultaneously improve performance and interpretability, providing a pathway toward trustworthy, explainable document AI systems.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ARIALæ¡†æ¶ï¼Œé€šè¿‡åŸºäºLLMçš„è§„åˆ’ä»£ç†åè°ƒä¸“ç”¨å·¥å…·ï¼ˆåŒ…æ‹¬æ£€ç´¢å¢å¼ºçš„ä¸Šä¸‹æ–‡é€‰æ‹©æ¨¡å—ï¼‰ï¼Œåœ¨æ–‡æ¡£è§†è§‰é—®ç­”ä»»åŠ¡ä¸­å®ç°ç²¾ç¡®ç­”æ¡ˆæå–å’Œå¯é ç©ºé—´å®šä½ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.18177v1">Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models</a></td><td><details><summary>å±•å¼€</summary>Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models to answer financial questions using external knowledge bases of U.S. SEC filings, earnings reports, and regulatory documents. However, existing work lacks systematic comparison of vector-based and non-vector RAG architectures for financial documents, and the empirical impact of advanced RAG techniques on retrieval accuracy, answer quality, latency, and cost remain unclear. We present the first systematic evaluation comparing vector-based agentic RAG using hybrid search and metadata filtering against hierarchical node-based systems that traverse document structure without embeddings. We evaluate two enhancement techniques applied to the vector-based architecture, i) cross-encoder reranking for retrieval precision, and ii) small-to-big chunk retrieval for context completeness. Across 1,200 SEC 10-K, 10-Q, and 8-K filings on a 150-question benchmark, we measure retrieval metrics (MRR, Recall@5), answer quality through LLM-as-a-judge pairwise comparisons, latency, and preprocessing costs. Vector-based agentic RAG achieves a 68% win rate over hierarchical node-based systems with comparable latency (5.2 compared to 5.98 seconds). Cross-encoder reranking achieves a 59% absolute improvement at optimal parameters (10, 5) for MRR@5. Small-to-big retrieval achieves a 65% win rate over baseline chunking with only 0.2 seconds additional latency. Our findings reveal that applying advanced RAG techniques to financial Q&A systems improves retrieval accuracy, answer quality, and has cost-performance tradeoffs to be considered in production.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç³»ç»Ÿåœ°è¯„ä¼°äº†åŸºäºå‘é‡å’Œéå‘é‡ï¼ˆåˆ†å±‚èŠ‚ç‚¹ï¼‰çš„RAGæ¶æ„åœ¨é‡‘èæ–‡æ¡£é—®ç­”ä¸­çš„æ€§èƒ½ï¼Œæ¯”è¾ƒäº†æ··åˆæœç´¢ã€å…ƒæ•°æ®è¿‡æ»¤ã€äº¤å‰ç¼–ç å™¨é‡æ’åºå’Œå°åˆ°å¤§å—æ£€ç´¢ç­‰æŠ€æœ¯å¯¹æ£€ç´¢å‡†ç¡®æ€§ã€å›ç­”è´¨é‡ã€å»¶è¿Ÿå’Œæˆæœ¬çš„å½±å“ï¼Œå‘ç°åŸºäºå‘é‡çš„æ™ºèƒ½RAGåœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šä¼˜äºéå‘é‡æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.17908v1">Principled Context Engineering for RAG: Statistical Guarantees via Conformal Prediction</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) enhances factual grounding in large language models (LLMs) by incorporating retrieved evidence, but LLM accuracy declines when long or noisy contexts exceed the model's effective attention span. Existing pre-generation filters rely on heuristics or uncalibrated LLM confidence scores, offering no statistical control over retained evidence. We evaluate and demonstrate context engineering through conformal prediction, a coverage-controlled filtering framework that removes irrelevant content while preserving recall of supporting evidence. Using both embedding- and LLM-based scoring functions, we test this approach on the NeuCLIR and RAGTIME collections. Conformal filtering consistently meets its target coverage, ensuring that a specified fraction of relevant snippets are retained, and reduces retained context by 2-3x relative to unfiltered retrieval. On NeuCLIR, downstream factual accuracy measured by ARGUE F1 improves under strict filtering and remains stable at moderate coverage, indicating that most discarded material is redundant or irrelevant. These results demonstrate that conformal prediction enables reliable, coverage-controlled context reduction in RAG, offering a model-agnostic and principled approach to context engineering.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ç ”ç©¶åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­é•¿æˆ–å™ªå£°ä¸Šä¸‹æ–‡å¯¼è‡´å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‡†ç¡®ç‡ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºä¸€ç§åˆ©ç”¨ä¿å½¢é¢„æµ‹ï¼ˆconformal predictionï¼‰çš„è¦†ç›–æ§åˆ¶è¿‡æ»¤æ¡†æ¶ï¼Œé€šè¿‡ç»Ÿè®¡æ–¹æ³•å»é™¤æ— å…³å†…å®¹å¹¶ä¿ç•™å…³é”®è¯æ®ã€‚å®éªŒè¡¨æ˜è¯¥æ–¹æ³•èƒ½æ˜¾è‘—å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦2-3å€ï¼Œåœ¨NeuCLIRæ•°æ®é›†ä¸Šæå‡ä¸‹æ¸¸äº‹å®å‡†ç¡®æ€§ï¼ˆARGUE F1ï¼‰ï¼Œè¯å®å…¶æä¾›äº†ä¸€ç§æ¨¡å‹æ— å…³ä¸”ç†è®ºå¯é çš„RAGä¸Šä¸‹æ–‡ä¼˜åŒ–æ–¹æ¡ˆã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.17854v1">A superpersuasive autonomous policy debating system</a></td><td><details><summary>å±•å¼€</summary>The capacity for highly complex, evidence-based, and strategically adaptive persuasion remains a formidable great challenge for artificial intelligence. Previous work, like IBM Project Debater, focused on generating persuasive speeches in simplified and shortened debate formats intended for relatively lay audiences. We introduce DeepDebater, a novel autonomous system capable of participating in and winning a full, unmodified, two-team competitive policy debate. Our system employs a hierarchical architecture of specialized multi-agent workflows, where teams of LLM-powered agents collaborate and critique one another to perform discrete argumentative tasks. Each workflow utilizes iterative retrieval, synthesis, and self-correction using a massive corpus of policy debate evidence (OpenDebateEvidence) and produces complete speech transcripts, cross-examinations, and rebuttals. We introduce a live, interactive end-to-end presentation pipeline that renders debates with AI speech and animation: transcripts are surface-realized and synthesized to audio with OpenAI TTS, and then displayed as talking-head portrait videos with EchoMimic V1. Beyond fully autonomous matches (AI vs AI), DeepDebater supports hybrid human-AI operation: human debaters can intervene at any stage, and humans can optionally serve as opponents against AI in any speech, allowing AI-human and AI-AI rounds. In preliminary evaluations against human-authored cases, DeepDebater produces qualitatively superior argumentative components and consistently wins simulated rounds as adjudicated by an independent autonomous judge. Expert human debate coaches also prefer the arguments, evidence, and cases constructed by DeepDebater. We open source all code, generated speech transcripts, audio and talking head video here: https://github.com/Hellisotherpeople/DeepDebater/tree/main</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†DeepDebaterç³»ç»Ÿï¼Œä¸€ä¸ªèƒ½å¤Ÿå‚ä¸å¹¶èµ¢å¾—å®Œæ•´æ”¿ç­–è¾©è®ºçš„è‡ªä¸»ç³»ç»Ÿã€‚å®ƒé‡‡ç”¨åˆ†å±‚å¤šæ™ºèƒ½ä½“æ¶æ„ï¼Œç»“åˆå¤§è§„æ¨¡æ”¿ç­–è¾©è®ºè¯æ®åº“ï¼ˆOpenDebateEvidenceï¼‰è¿›è¡Œè¿­ä»£æ£€ç´¢ã€ç»¼åˆå’Œè‡ªæ ¡æ­£ï¼Œç”Ÿæˆè¾©è®ºå†…å®¹ï¼ˆå¦‚æ¼”è®²ã€è´¨è¯¢å’Œåé©³ï¼‰ï¼Œå¹¶é€šè¿‡AIè¯­éŸ³å’ŒåŠ¨ç”»å‘ˆç°ã€‚ç³»ç»Ÿæ”¯æŒå…¨è‡ªä¸»ï¼ˆAIå¯¹AIï¼‰å’Œæ··åˆäººæœºåä½œæ¨¡å¼ï¼Œåˆæ­¥è¯„ä¼°æ˜¾ç¤ºå…¶ç”Ÿæˆçš„è®ºæ®ä¼˜äºäººç±»ï¼Œå¹¶å¾—åˆ°ä¸“å®¶è®¤å¯ã€‚ç ”ç©¶å¼€æºäº†ç›¸å…³ä»£ç å’Œæ•°æ®ï¼Œä½“ç°äº†RAGæŠ€æœ¯ä¸­æ£€ç´¢å¤–éƒ¨çŸ¥è¯†å¢å¼ºç”Ÿæˆçš„ç‰¹ç‚¹ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.17467v1">PersonaAgent with GraphRAG: Community-Aware Knowledge Graphs for Personalized LLM</a></td><td><details><summary>å±•å¼€</summary>We propose a novel framework for persona-based language model system, motivated by the need for personalized AI agents that adapt to individual user preferences. In our approach, the agent embodies the user's "persona" (e.g. user profile or taste) and is powered by a large language model (LLM). To enable the agent to leverage rich contextual information, we introduce a Knowledge-Graph-enhanced Retrieval-Augmented Generation (Graph RAG) mechanism that constructs an LLM-derived graph index of relevant documents and summarizes communities of related information. Our framework generates personalized prompts by combining: (1) a summary of the user's historical behaviors and preferences extracted from the knowledge graph, and (2) relevant global interaction patterns identified through graph-based community detection. This dynamic prompt engineering approach allows the agent to maintain consistent persona-aligned behaviors while benefiting from collective knowledge. On the LaMP benchmark, our method improves news categorization F1 by 11.1%, movie tagging F1 by 56.1%, and reduces product rating MAE by 10.4% over prior methods. Our code is available at https://anonymous.4open.science/r/PersonaAgentwGraphRAG-DE6F</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºçŸ¥è¯†å›¾è°±å¢å¼ºçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆGraph RAGï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æ„å»ºä¸ªæ€§åŒ–çš„è¯­è¨€æ¨¡å‹ä»£ç†ã€‚é€šè¿‡ç»“åˆç”¨æˆ·å†å²è¡Œä¸ºåå¥½å’ŒåŸºäºå›¾è°±ç¤¾åŒºæ£€æµ‹çš„å…¨å±€äº¤äº’æ¨¡å¼ï¼ŒåŠ¨æ€ç”Ÿæˆä¸ªæ€§åŒ–æç¤ºï¼Œæ˜¾è‘—æå‡äº†æ–°é—»åˆ†ç±»ã€ç”µå½±æ ‡ç­¾å’Œäº§å“è¯„åˆ†ç­‰ä»»åŠ¡çš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.17068v2">ReBrain: Brain MRI Reconstruction from Sparse CT Slice via Retrieval-Augmented Diffusion</a></td><td><details><summary>å±•å¼€</summary>Magnetic Resonance Imaging (MRI) plays a crucial role in brain disease diagnosis, but it is not always feasible for certain patients due to physical or clinical constraints. Recent studies attempt to synthesize MRI from Computed Tomography (CT) scans; however, low-dose protocols often result in highly sparse CT volumes with poor through-plane resolution, making accurate reconstruction of the full brain MRI volume particularly challenging. To address this, we propose ReBrain, a retrieval-augmented diffusion framework for brain MRI reconstruction. Given any 3D CT scan with limited slices, we first employ a Brownian Bridge Diffusion Model (BBDM) to synthesize MRI slices along the 2D dimension. Simultaneously, we retrieve structurally and pathologically similar CT slices from a comprehensive prior database via a fine-tuned retrieval model. These retrieved slices are used as references, incorporated through a ControlNet branch to guide the generation of intermediate MRI slices and ensure structural continuity. We further account for rare retrieval failures when the database lacks suitable references and apply spherical linear interpolation to provide supplementary guidance. Extensive experiments on SynthRAD2023 and BraTS demonstrate that ReBrain achieves state-of-the-art performance in cross-modal reconstruction under sparse conditions.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºReBrainçš„æ£€ç´¢å¢å¼ºæ‰©æ•£æ¡†æ¶ï¼Œç”¨äºä»ç¨€ç–CTæ‰«æä¸­é‡å»ºè„‘éƒ¨MRIå›¾åƒã€‚å®ƒç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œæ£€ç´¢æœºåˆ¶ï¼Œé€šè¿‡ä»å…ˆéªŒæ•°æ®åº“ä¸­æ£€ç´¢ç›¸ä¼¼CTåˆ‡ç‰‡ä½œä¸ºå‚è€ƒï¼Œå¹¶åˆ©ç”¨ControlNetåˆ†æ”¯å¼•å¯¼ç”Ÿæˆä¸­é—´MRIåˆ‡ç‰‡ï¼Œä»¥è§£å†³ä½å‰‚é‡CTåˆ†è¾¨ç‡ä¸è¶³çš„é—®é¢˜ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨è·¨æ¨¡æ€é‡å»ºä¸­æ€§èƒ½ä¼˜è¶Šã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.17044v1">Parametric Retrieval-Augmented Generation using Latent Routing of LoRA Adapters</a></td><td><details><summary>å±•å¼€</summary>Parametric Retrieval-Augmented Generation (PRAG) is a novel RAG paradigm that integrates external knowledge directly into a Large Language Model (LLM) by parameterizing documents using LoRA adapters, demonstrating reduced inference costs compared to traditional RAG approaches. However, current PRAG approaches adopt a \textbf{one-to-one} document encoding scheme, using a dedicated LoRA adapter for each individual document. This scheme introduces two major limitations: First, it leads to data scarcity, as the training datasets for individual LoRA adapters are limited. Second, it incurs high overhead during inference, requiring the merging of LLM weights with a new LoRA adapter for every candidate passage, which is computationally inefficient. To overcome these challenges, we propose a novel paradigm for encoding passages in PRAG that utilizes a latent routing encoding process (Poly-PRAG). During offline encoding, we treat the encoding of a set of documents as a multi-task learning process, where each passage is assigned a unique task identifier. By employing a routing function, we use a small set of latent LoRA adapters to encode the entire passage space. During online inference, this routing function selectively activates a subset of latent experts based on the input query. We conduct comprehensive evaluations of Poly-PRAG across multiple knowledge-intensive NLP tasks. Our extensive experiments demonstrate the effectiveness of the proposed method, achieving state-of-the-art results on four distinct datasets.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºPoly-PRAGçš„æ–°å‹å‚æ•°åŒ–æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆPRAGï¼‰æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨æ½œåœ¨è·¯ç”±ç¼–ç è¿‡ç¨‹å’Œå°‘é‡å…±äº«çš„LoRAé€‚é…å™¨æ¥é«˜æ•ˆç¼–ç æ•´ä¸ªæ–‡æ¡£é›†ï¼Œè§£å†³äº†ä¼ ç»ŸPRAGæ–¹æ³•ä¸­ä¸€å¯¹ä¸€æ–‡æ¡£ç¼–ç å¯¼è‡´çš„æ•°æ®ç¨€ç¼ºå’Œé«˜è®¡ç®—å¼€é”€é—®é¢˜ï¼Œå¹¶åœ¨å¤šä¸ªçŸ¥è¯†å¯†é›†å‹NLPä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.17676v1">LLM and Agent-Driven Data Analysis: A Systematic Approach for Enterprise Applications and System-level Deployment</a></td><td><details><summary>å±•å¼€</summary>The rapid progress in Generative AI and Agent technologies is profoundly transforming enterprise data management and analytics. Traditional database applications and system deployment are fundamentally impacted by AI-driven tools, such as Retrieval-Augmented Generation (RAG) and vector database technologies, which provide new pathways for semantic querying over enterprise knowledge bases. In the meantime, data security and compliance are top priorities for organizations adopting AI technologies. For enterprise data analysis, SQL generations powered by large language models (LLMs) and AI agents, has emerged as a key bridge connecting natural language with structured data, effectively lowering the barrier to enterprise data access and improving analytical efficiency. This paper focuses on enterprise data analysis applications and system deployment, covering a range of innovative frameworks, enabling complex query understanding, multi-agent collaboration, security verification, and computational efficiency. Through representative use cases, key challenges related to distributed deployment, data security, and inherent difficulties in SQL generation tasks are discussed.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æ¢è®¨äº†ç”Ÿæˆå¼AIå’Œæ™ºèƒ½ä½“æŠ€æœ¯å¦‚ä½•æ”¹å˜ä¼ä¸šæ•°æ®ç®¡ç†ä¸åˆ†æï¼Œé‡ç‚¹ä»‹ç»äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œå‘é‡æ•°æ®åº“æŠ€æœ¯åœ¨è¯­ä¹‰æŸ¥è¯¢ä¸­çš„ä½œç”¨ï¼Œå¹¶è®¨è®ºäº†åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„SQLç”Ÿæˆã€å¤šæ™ºèƒ½ä½“åä½œã€æ•°æ®å®‰å…¨ç­‰æŒ‘æˆ˜ä¸åˆ›æ–°æ¡†æ¶åœ¨ä¼ä¸šçš„åº”ç”¨éƒ¨ç½²ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-20
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.16654v1">Comparison of Text-Based and Image-Based Retrieval in Multimodal Retrieval Augmented Generation Large Language Model Systems</a></td><td><details><summary>å±•å¼€</summary>Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models (LLMs) to access multimodal knowledge bases containing both text and visual information such as charts, diagrams, and tables in financial documents. However, existing multimodal RAG systems rely on LLM-based summarization to convert images into text during preprocessing, storing only text representations in vector databases, which causes loss of contextual information and visual details critical for downstream retrieval and question answering. To address this limitation, we present a comprehensive comparative analysis of two retrieval approaches for multimodal RAG systems, including text-based chunk retrieval (where images are summarized into text before embedding) and direct multimodal embedding retrieval (where images are stored natively in the vector space). We evaluate all three approaches across 6 LLM models and a two multi-modal embedding models on a newly created financial earnings call benchmark comprising 40 question-answer pairs, each paired with 2 documents (1 image and 1 text chunk). Experimental results demonstrate that direct multimodal embedding retrieval significantly outperforms LLM-summary-based approaches, achieving absolute improvements of 13% in mean average precision (mAP@5) and 11% in normalized discounted cumulative gain. These gains correspond to relative improvements of 32% in mAP@5 and 20% in nDCG@5, providing stronger evidence of their practical impact. We additionally find that direct multimodal retrieval produces more accurate and factually consistent answers as measured by LLM-as-a-judge pairwise comparisons. We demonstrate that LLM summarization introduces information loss during preprocessing, whereas direct multimodal embeddings preserve visual context for retrieval and inference.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æ¢è®¨äº†å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­ä¸¤ç§æ£€ç´¢æ–¹æ³•çš„æ€§èƒ½å¯¹æ¯”ï¼ˆåŸºäºæ–‡æœ¬åˆ†å—æ£€ç´¢å’Œç›´æ¥å¤šæ¨¡æ€åµŒå…¥æ£€ç´¢ï¼‰ï¼Œæå‡ºç›´æ¥å­˜å‚¨åŸç”Ÿå›¾åƒåµŒå…¥èƒ½æ˜¾è‘—æå‡é‡‘èæ–‡æ¡£é—®ç­”ä»»åŠ¡çš„æ•ˆæœï¼ˆå¹³å‡ç»å¯¹æå‡13% mAP@5ï¼‰ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†ä¼ ç»ŸLLMæ‘˜è¦é¢„å¤„ç†ä¼šå¯¼è‡´ä¿¡æ¯æŸå¤±ï¼Œè€Œç›´æ¥å¤šæ¨¡æ€åµŒå…¥èƒ½æ›´å¥½åœ°ä¿ç•™è§†è§‰ä¸Šä¸‹æ–‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.16635v1">SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction</a></td><td><details><summary>å±•å¼€</summary>Survival analysis is critical for cancer prognosis and treatment planning, yet existing methods lack the transparency essential for clinical adoption. While recent pathology agents have demonstrated explainability in diagnostic tasks, they face three limitations for survival prediction: inability to integrate multimodal data, ineffective region-of-interest exploration, and failure to leverage experiential learning from historical cases. We introduce SurvAgent, the first hierarchical chain-of-thought (CoT)-enhanced multi-agent system for multimodal survival prediction. SurvAgent consists of two stages: (1) WSI-Gene CoT-Enhanced Case Bank Construction employs hierarchical analysis through Low-Magnification Screening, Cross-Modal Similarity-Aware Patch Mining, and Confidence-Aware Patch Mining for pathology images, while Gene-Stratified analysis processes six functional gene categories. Both generate structured reports with CoT reasoning, storing complete analytical processes for experiential learning. (2) Dichotomy-Based Multi-Expert Agent Inference retrieves similar cases via RAG and integrates multimodal reports with expert predictions through progressive interval refinement. Extensive experiments on five TCGA cohorts demonstrate SurvAgent's superority over conventional methods, proprietary MLLMs, and medical agents, establishing a new paradigm for explainable AI-driven survival prediction in precision oncology.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†SurvAgentï¼Œä¸€ä¸ªåŸºäºåˆ†å±‚æ€ç»´é“¾ï¼ˆCoTï¼‰å¢å¼ºçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œç”¨äºå¤šæ¨¡æ€ç”Ÿå­˜é¢„æµ‹ã€‚å®ƒé€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ä»å†å²ç—…ä¾‹ä¸­æ£€ç´¢ç›¸ä¼¼æ¡ˆä¾‹ï¼Œå¹¶ç»“åˆå¤šæ¨¡æ€æ•°æ®å’Œä¸“å®¶é¢„æµ‹è¿›è¡Œæ¸è¿›å¼åŒºé—´ä¼˜åŒ–ï¼Œä»è€Œæé«˜äº†ç™Œç—‡ç”Ÿå­˜é¢„æµ‹çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.16566v1">NutriScreener: Retrieval-Augmented Multi-Pose Graph Attention Network for Malnourishment Screening</a></td><td><details><summary>å±•å¼€</summary>Child malnutrition remains a global crisis, yet existing screening methods are laborious and poorly scalable, hindering early intervention. In this work, we present NutriScreener, a retrieval-augmented, multi-pose graph attention network that combines CLIP-based visual embeddings, class-boosted knowledge retrieval, and context awareness to enable robust malnutrition detection and anthropometric prediction from children's images, simultaneously addressing generalizability and class imbalance. In a clinical study, doctors rated it 4.3/5 for accuracy and 4.6/5 for efficiency, confirming its deployment readiness in low-resource settings. Trained and tested on 2,141 children from AnthroVision and additionally evaluated on diverse cross-continent populations, including ARAN and an in-house collected CampusPose dataset, it achieves 0.79 recall, 0.82 AUC, and significantly lower anthropometric RMSEs, demonstrating reliable measurement in unconstrained pediatric settings. Cross-dataset results show up to 25% recall gain and up to 3.5 cm RMSE reduction using demographically matched knowledge bases. NutriScreener offers a scalable and accurate solution for early malnutrition detection in low-resource environments.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ä»‹ç»äº†NutriScreenerç³»ç»Ÿï¼Œä¸€ç§åŸºäºæ£€ç´¢å¢å¼ºçš„å›¾æ³¨æ„åŠ›ç½‘ç»œï¼Œç»“åˆCLIPè§†è§‰åµŒå…¥ã€çŸ¥è¯†æ£€ç´¢å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æŠ€æœ¯ï¼Œç”¨äºå„¿ç«¥è¥å…»ä¸è‰¯æ£€æµ‹å’Œäººä½“æµ‹é‡é¢„æµ‹ã€‚ç³»ç»Ÿé€šè¿‡ä¸´åºŠéªŒè¯æ˜¾ç¤ºå‡ºé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå¹¶åœ¨è·¨æ•°æ®é›†æµ‹è¯•ä¸­é€šè¿‡åŒ¹é…äººå£ç»Ÿè®¡å­¦çŸ¥è¯†åº“æ˜¾è‘—æå‡æ€§èƒ½ï¼ˆå¦‚25%å¬å›ç‡æå‡ï¼‰ï¼Œä¸ºèµ„æºåŒ®ä¹åœ°åŒºæä¾›äº†å¯æ‰©å±•çš„æ—©æœŸè¥å…»ä¸è‰¯ç­›æŸ¥æ–¹æ¡ˆã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.16395v1">CorrectHDL: Agentic HDL Design with LLMs Leveraging High-Level Synthesis as Reference</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) have demonstrated remarkable potential in hardware front-end design using hardware description languages (HDLs). However, their inherent tendency toward hallucination often introduces functional errors into the generated HDL designs. To address this issue, we propose the framework CorrectHDL that leverages high-level synthesis (HLS) results as functional references to correct potential errors in LLM-generated HDL designs.The input to the proposed framework is a C/C++ program that specifies the target circuit's functionality. The program is provided to an LLM to directly generate an HDL design, whose syntax errors are repaired using a Retrieval-Augmented Generation (RAG) mechanism. The functional correctness of the LLM-generated circuit is iteratively improved by comparing its simulated behavior with an HLS reference design produced by conventional HLS tools, which ensures the functional correctness of the result but can lead to suboptimal area and power efficiency. Experimental results demonstrate that circuits generated by the proposed framework achieve significantly better area and power efficiency than conventional HLS designs and approach the quality of human-engineered circuits. Meanwhile, the correctness of the resulting HDL implementation is maintained, highlighting the effectiveness and potential of agentic HDL design leveraging the generative capabilities of LLMs and the rigor of traditional correctness-driven IC design flows.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºæ¡†æ¶CorrectHDLï¼Œåˆ©ç”¨é«˜çº§ç»¼åˆï¼ˆHLSï¼‰ç»“æœä½œä¸ºåŠŸèƒ½å‚è€ƒï¼Œé€šè¿‡RAGæœºåˆ¶ä¿®æ­£LLMç”Ÿæˆçš„HDLè®¾è®¡ä¸­çš„è¯­æ³•é”™è¯¯ï¼Œå¹¶è¿­ä»£æ¯”è¾ƒæ¨¡æ‹Ÿè¡Œä¸ºä¸HLSå‚è€ƒè®¾è®¡ï¼Œåœ¨ä¿è¯åŠŸèƒ½æ­£ç¡®æ€§çš„åŒæ—¶æå‡é¢ç§¯å’ŒåŠŸè€—æ•ˆç‡ï¼Œæœ€ç»ˆç”Ÿæˆæ¥è¿‘äººå·¥è®¾è®¡è´¨é‡çš„ç”µè·¯ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.16326v1">ARK: Answer-Centric Retriever Tuning via KG-augmented Curriculum Learning</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has emerged as a powerful framework for knowledge-intensive tasks, yet its effectiveness in long-context scenarios is often bottlenecked by the retriever's inability to distinguish sparse yet crucial evidence. Standard retrievers, optimized for query-document similarity, frequently fail to align with the downstream goal of generating a precise answer. To bridge this gap, we propose a novel fine-tuning framework that optimizes the retriever for Answer Alignment. Specifically, we first identify high-quality positive chunks by evaluating their sufficiency to generate the correct answer. We then employ a curriculum-based contrastive learning scheme to fine-tune the retriever. This curriculum leverages LLM-constructed Knowledge Graphs (KGs) to generate augmented queries, which in turn mine progressively challenging hard negatives. This process trains the retriever to distinguish the answer-sufficient positive chunks from these nuanced distractors, enhancing its generalization. Extensive experiments on 10 datasets from the Ultradomain and LongBench benchmarks demonstrate that our fine-tuned retriever achieves state-of-the-art performance, improving 14.5% over the base model without substantial architectural modifications and maintaining strong efficiency for long-context RAG. Our work presents a robust and effective methodology for building truly answer-centric retrievers.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡é’ˆå¯¹RAGåœ¨é•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸­æ£€ç´¢å™¨éš¾ä»¥è¯†åˆ«å…³é”®è¯æ®çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç­”æ¡ˆå¯¹é½çš„å¾®è°ƒæ¡†æ¶ã€‚é€šè¿‡è¯„ä¼°æ–‡æœ¬å—ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆçš„å……åˆ†æ€§ç­›é€‰é«˜è´¨é‡æ­£æ ·æœ¬ï¼Œå¹¶åˆ©ç”¨è¯¾ç¨‹å¼å¯¹æ¯”å­¦ä¹ å’ŒLLMæ„å»ºçš„çŸ¥è¯†å›¾è°±ç”Ÿæˆå¢å¼ºæŸ¥è¯¢ä»¥æŒ–æ˜å›°éš¾è´Ÿæ ·æœ¬ï¼Œä»è€Œä¼˜åŒ–æ£€ç´¢å™¨æ€§èƒ½ã€‚å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨10ä¸ªæ•°æ®é›†ä¸Šæå‡14.5%ä¸”ä¿æŒé«˜æ•ˆï¼Œå®ç°äº†é•¿ä¸Šä¸‹æ–‡RAGçš„å…ˆè¿›æ£€ç´¢æ•ˆæœã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.16283v1">MuISQA: Multi-Intent Retrieval-Augmented Generation for Scientific Question Answering</a></td><td><details><summary>å±•å¼€</summary>Complex scientific questions often entail multiple intents, such as identifying gene mutations and linking them to related diseases. These tasks require evidence from diverse sources and multi-hop reasoning, while conventional retrieval-augmented generation (RAG) systems are usually single-intent oriented, leading to incomplete evidence coverage. To assess this limitation, we introduce the Multi-Intent Scientific Question Answering (MuISQA) benchmark, which is designed to evaluate RAG systems on heterogeneous evidence coverage across sub-questions. In addition, we propose an intent-aware retrieval framework that leverages large language models (LLMs) to hypothesize potential answers, decompose them into intent-specific queries, and retrieve supporting passages for each underlying intent. The retrieved fragments are then aggregated and re-ranked via Reciprocal Rank Fusion (RRF) to balance coverage across diverse intents while reducing redundancy. Experiments on both MuISQA benchmark and other general RAG datasets demonstrate that our method consistently outperforms conventional approaches, particularly in retrieval accuracy and evidence coverage.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡é’ˆå¯¹ä¼ ç»Ÿæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿåœ¨å¤„ç†å¤šæ„å›¾ç§‘å­¦é—®é¢˜æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§æ„å›¾æ„ŸçŸ¥çš„æ£€ç´¢æ¡†æ¶ã€‚ä½œè€…é¦–å…ˆæ„å»ºäº†å¤šæ„å›¾ç§‘å­¦é—®ç­”åŸºå‡†ï¼ˆMuISQAï¼‰ä»¥è¯„ä¼°RAGç³»ç»Ÿåœ¨å­é—®é¢˜å¼‚æ„è¯æ®è¦†ç›–æ–¹é¢çš„è¡¨ç°ï¼Œéšååˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå‡è®¾ç­”æ¡ˆã€åˆ†è§£æ„å›¾ï¼Œå¹¶é€šè¿‡èåˆæ’åºç®—æ³•ï¼ˆRRFï¼‰å®ç°å¤šæ„å›¾è¯æ®çš„å‡è¡¡è¦†ç›–å’Œå»å†—ä½™ã€‚å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨æ£€ç´¢å‡†ç¡®æ€§å’Œè¯æ®è¦†ç›–èŒƒå›´ä¸Šå‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.16198v1">SemanticCite: Citation Verification with AI-Powered Full-Text Analysis and Evidence-Based Reasoning</a></td><td><details><summary>å±•å¼€</summary>Effective scientific communication depends on accurate citations that validate sources and guide readers to supporting evidence. Yet academic literature faces mounting challenges: semantic citation errors that misrepresent sources, AI-generated hallucinated references, and traditional citation formats that point to entire papers without indicating which sections substantiate specific claims. We introduce SemanticCite, an AI-powered system that verifies citation accuracy through full-text source analysis while providing rich contextual information via detailed reasoning and relevant text snippets. Our approach combines multiple retrieval methods with a four-class classification system (Supported, Partially Supported, Unsupported, Uncertain) that captures nuanced claim-source relationships and enables appropriate remedial actions for different error types. Our experiments show that fine-tuned lightweight language models achieve performance comparable to large commercial systems with significantly lower computational requirements, making large-scale citation verification practically feasible. The system provides transparent, evidence-based explanations that support user understanding and trust. We contribute a comprehensive dataset of over 1,000 citations with detailed alignments, functional classifications, semantic annotations, and bibliometric metadata across eight disciplines, alongside fine-tuned models and the complete verification framework as open-source software. SemanticCite addresses critical challenges in research integrity through scalable citation verification, streamlined peer review, and quality control for AI-generated content, providing an open-source foundation for maintaining citation accuracy at scale.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†SemanticCiteç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨AIæŠ€æœ¯ç»“åˆå¤šç§æ£€ç´¢æ–¹æ³•å’Œç»†ç²’åº¦åˆ†ç±»ä½“ç³»ï¼ˆæ”¯æŒ/éƒ¨åˆ†æ”¯æŒ/ä¸æ”¯æŒ/ä¸ç¡®å®šï¼‰æ¥éªŒè¯å­¦æœ¯å¼•ç”¨å‡†ç¡®æ€§ï¼Œèƒ½åˆ†æå…¨æ–‡å¹¶æä¾›ä¸Šä¸‹æ–‡ç‰‡æ®µã€‚å…¶ç‰¹ç‚¹åŒ…æ‹¬ï¼šåˆ©ç”¨è½»é‡çº§è¯­è¨€æ¨¡å‹å®ç°é«˜æ•ˆæ£€ç´¢-ç”ŸæˆéªŒè¯ã€æ„å»ºå¤šå­¦ç§‘æ ‡æ³¨æ•°æ®é›†ã€ç”Ÿæˆé€æ˜è§£é‡Šï¼Œå±•ç¤ºäº†RAGæŠ€æœ¯åœ¨è§£å†³å­¦æœ¯å¼•ç”¨å®Œæ•´æ€§ï¼ˆå¦‚è¯­ä¹‰é”™è¯¯æ£€æµ‹ã€AIå¹»è§‰å¼•ç”¨è¯†åˆ«ï¼‰æ–¹é¢çš„åˆ›æ–°åº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.15997v1">Sensorium Arc: AI Agent System for Oceanic Data Exploration and Interactive Eco-Art</a></td><td><details><summary>å±•å¼€</summary>Sensorium Arc (AI reflects on climate) is a real-time multimodal interactive AI agent system that personifies the ocean as a poetic speaker and guides users through immersive explorations of complex marine data. Built on a modular multi-agent system and retrieval-augmented large language model (LLM) framework, Sensorium enables natural spoken conversations with AI agents that embodies the ocean's perspective, generating responses that blend scientific insight with ecological poetics. Through keyword detection and semantic parsing, the system dynamically triggers data visualizations and audiovisual playback based on time, location, and thematic cues drawn from the dialogue. Developed in collaboration with the Center for the Study of the Force Majeure and inspired by the eco-aesthetic philosophy of Newton Harrison, Sensorium Arc reimagines ocean data not as an abstract dataset but as a living narrative. The project demonstrates the potential of conversational AI agents to mediate affective, intuitive access to high-dimensional environmental data and proposes a new paradigm for human-machine-ecosystem.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸º"Sensorium Arc"çš„å¤šæ¨¡æ€äº¤äº’å¼AIä»£ç†ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé€šè¿‡æ£€ç´¢å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œå°†æµ·æ´‹æ•°æ®è½¬åŒ–ä¸ºæ‹ŸäººåŒ–çš„è¯—æ„è¡¨è¾¾ï¼Œå®ç°ç”¨æˆ·ä¸"æµ·æ´‹è§†è§’"çš„è‡ªç„¶å¯¹è¯ï¼Œå¹¶åŠ¨æ€è§¦å‘æ•°æ®å¯è§†åŒ–ä¸è§†å¬æ•ˆæœï¼Œå°†ç§‘å­¦æ•°æ®è½¬åŒ–ä¸ºç”Ÿæ€å™äº‹ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.15994v1">CARE-RAG - Clinical Assessment and Reasoning in RAG</a></td><td><details><summary>å±•å¼€</summary>Access to the right evidence does not guarantee that large language models (LLMs) will reason with it correctly. This gap between retrieval and reasoning is especially concerning in clinical settings, where outputs must align with structured protocols. We study this gap using Written Exposure Therapy (WET) guidelines as a testbed. In evaluating model responses to curated clinician-vetted questions, we find that errors persist even when authoritative passages are provided. To address this, we propose an evaluation framework that measures accuracy, consistency, and fidelity of reasoning. Our results highlight both the potential and the risks: retrieval-augmented generation (RAG) can constrain outputs, but safe deployment requires assessing reasoning as rigorously as retrieval.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ç ”ç©¶åœ¨ä¸´åºŠç¯å¢ƒä¸­æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯é¢ä¸´çš„æ£€ç´¢ä¸æ¨ç†è„±èŠ‚é—®é¢˜ï¼Œä»¥ä¹¦é¢æš´éœ²ç–—æ³•ï¼ˆWETï¼‰æŒ‡å—ä¸ºæµ‹è¯•æ¡ˆä¾‹ï¼Œå‘ç°å³ä½¿æä¾›æƒå¨æ–‡æœ¬ç‰‡æ®µï¼Œå¤§è¯­è¨€æ¨¡å‹ä»å­˜åœ¨æ¨ç†é”™è¯¯ã€‚ä½œè€…æå‡ºé€šè¿‡è¯„ä¼°æ¡†æ¶ï¼ˆå‡†ç¡®æ€§ã€ä¸€è‡´æ€§å’Œæ¨ç†ä¿çœŸåº¦ï¼‰æ¥æå‡RAGåœ¨ä¸´åºŠåè®®å¯¹é½ä¸­çš„åº”ç”¨å®‰å…¨æ€§ï¼Œå¼ºè°ƒéœ€å¯¹æ¨ç†è¿‡ç¨‹è¿›è¡Œä¸¥æ ¼è¯„ä¼°ä»¥å®ç°å®‰å…¨éƒ¨ç½²ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.15974v1">KRAL: Knowledge and Reasoning Augmented Learning for LLM-assisted Clinical Antimicrobial Therapy</a></td><td><details><summary>å±•å¼€</summary>Clinical antimicrobial therapy requires the dynamic integration of pathogen profiles, host factors, pharmacological properties of antimicrobials, and the severity of infection.This complexity imposes fundamental limitations on the applicability of Large Language Models (LLMs) in high-stakes clinical decision-making including knowledge gaps, data privacy concerns, high deployment costs, and limited reasoning capabilities. To address these challenges, we propose KRAL (Knowledge and Reasoning Augmented Learning), a low-cost, scalable, privacy-preserving paradigm that leverages teacher-model reasoning to automatically distill knowledge and reasoning trajectories via answer-to-question reverse generation, employs heuristic learning for semi-supervised data augmentation (reducing manual annotation requirements by approximately 80%), and utilizes agentic reinforcement learning to jointly enhance medical knowledge and reasoning while optimizing computational and memory efficiency. A hierarchical evaluation employing diverse teacher-model proxies reduces assessment costs, while modular interface design facilitates seamless system updates. Experimental results demonstrate that KRAL significantly outperforms traditional Retrieval-Augmented Generation (RAG) and Supervised Fine-Tuning (SFT) methods. It improves knowledge question-answering capability (Accuracy@1 on the external open-source benchmark MEDQA increased by 1.8% vs. SFT and 3.6% vs. RAG) and reasoning capability (Pass@1 on the external benchmark PUMCH Antimicrobial increased by 27% vs. SFT and 27.2% vs. RAG), achieved at ~20% of SFT's long-term training costs. This establishes KRAL as an effective solution for enhancing local LLMs' clinical diagnostic capabilities, enabling low-cost, high-safety deployment in complex medical decision support.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºKRALï¼ˆKnowledge and Reasoning Augmented Learningï¼‰çš„æ–°èŒƒå¼ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸´åºŠå†³ç­–ä¸­çš„å±€é™æ€§ï¼Œå¦‚çŸ¥è¯†é¸¿æ²Ÿã€éšç§é—®é¢˜å’Œé«˜æˆæœ¬ã€‚KRALé€šè¿‡é€†å‘ç”Ÿæˆã€åŠç›‘ç£æ•°æ®å¢å¼ºå’Œå¼ºåŒ–å­¦ä¹ ç­‰æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†åŒ»å­¦çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨å®éªŒä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨çŸ¥è¯†é—®ç­”å’Œæ¨ç†èƒ½åŠ›ä¸Šå–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼ŒåŒæ—¶é™ä½äº†é•¿æœŸè®­ç»ƒæˆæœ¬ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-19
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.15435v1">HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Advanced multimodal Retrieval-Augmented Generation (MRAG) techniques have been widely applied to enhance the capabilities of Large Multimodal Models (LMMs), but they also bring along novel safety issues. Existing adversarial research has revealed the vulnerability of MRAG systems to knowledge poisoning attacks, which fool the retriever into recalling injected poisoned contents. However, our work considers a different setting: visual attack of MRAG by solely adding imperceptible perturbations at the image inputs of users, without manipulating any other components. This is challenging due to the robustness of fine-tuned retrievers and large-scale generators, and the effect of visual perturbation may be further weakened by propagation through the RAG chain. We propose a novel Hierarchical Visual Attack that misaligns and disrupts the two inputs (the multimodal query and the augmented knowledge) of MRAG's generator to confuse its generation. We further design a hierarchical two-stage strategy to obtain misaligned augmented knowledge. We disrupt the image input of the retriever to make it recall irrelevant knowledge from the original database, by optimizing the perturbation which first breaks the cross-modal alignment and then disrupts the multimodal semantic alignment. We conduct extensive experiments on two widely-used MRAG datasets: OK-VQA and InfoSeek. We use CLIP-based retrievers and two LMMs BLIP-2 and LLaVA as generators. Results demonstrate the effectiveness of our visual attack on MRAG through the significant decrease in both retrieval and generation performance.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ç ”ç©¶å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆMRAGï¼‰ç³»ç»Ÿä¸­çš„è§†è§‰æ”»å‡»å®‰å…¨é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä»…é€šè¿‡å›¾åƒè¾“å…¥æ·»åŠ ä¸å¯å¯Ÿè§‰æ‰°åŠ¨çš„åˆ†å±‚æ”»å‡»æ–¹æ³•ï¼ˆHierarchical Visual Attackï¼‰ï¼Œé€šè¿‡ç ´åæ£€ç´¢å™¨å’Œç”Ÿæˆå™¨çš„è¾“å…¥å¯¹é½æ¥å¹²æ‰°ç³»ç»Ÿæ€§èƒ½ï¼Œå¹¶åœ¨OK-VQAå’ŒInfoSeekæ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ”»å‡»å¯¹CLIPæ£€ç´¢å™¨å’ŒBLIP-2/LLaVAç”Ÿæˆå™¨çš„æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.15370v1">The Empowerment of Science of Science by Large Language Models: New Tools and Methods</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) have exhibited exceptional capabilities in natural language understanding and generation, image recognition, and multimodal tasks, charting a course towards AGI and emerging as a central issue in the global technological race. This manuscript conducts a comprehensive review of the core technologies that support LLMs from a user standpoint, including prompt engineering, knowledge-enhanced retrieval augmented generation, fine tuning, pretraining, and tool learning. Additionally, it traces the historical development of Science of Science (SciSci) and presents a forward looking perspective on the potential applications of LLMs within the scientometric domain. Furthermore, it discusses the prospect of an AI agent based model for scientific evaluation, and presents new research fronts detection and knowledge graph building methods with LLMs.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç»¼è¿°äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ ¸å¿ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬æç¤ºå·¥ç¨‹ã€çŸ¥è¯†å¢å¼ºçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€å¾®è°ƒã€é¢„è®­ç»ƒå’Œå·¥å…·å­¦ä¹ ï¼Œå¹¶æ¢è®¨äº†LLMsåœ¨ç§‘å­¦è®¡é‡å­¦é¢†åŸŸçš„æ½œåœ¨åº”ç”¨ï¼Œä»¥åŠåŸºäºAIä»£ç†çš„ç§‘å­¦è¯„ä¼°æ¨¡å‹ã€æ–°ç ”ç©¶å‰æ²¿æ£€æµ‹å’ŒçŸ¥è¯†å›¾è°±æ„å»ºæ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.15355v1">HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning</a></td><td><details><summary>å±•å¼€</summary>We introduce HEAD-QA v2, an expanded and updated version of a Spanish/English healthcare multiple-choice reasoning dataset originally released by Vilares and GÃ³mez-RodrÃ­guez (2019). The update responds to the growing need for high-quality datasets that capture the linguistic and conceptual complexity of healthcare reasoning. We extend the dataset to over 12,000 questions from ten years of Spanish professional exams, benchmark several open-source LLMs using prompting, RAG, and probability-based answer selection, and provide additional multilingual versions to support future work. Results indicate that performance is mainly driven by model scale and intrinsic reasoning ability, with complex inference strategies obtaining limited gains. Together, these results establish HEAD-QA v2 as a reliable resource for advancing research on biomedical reasoning and model improvement.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†HEAD-QA v2æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªæ‰©å±•ç‰ˆçš„è¥¿ç­ç‰™è¯­/è‹±è¯­åŒ»ç–—å¥åº·å¤šé¡¹é€‰æ‹©é¢˜æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶å¯¹ä¸åŒå¼€æºå¤§è¯­è¨€æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ä½¿ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­‰ç­–ç•¥ï¼Œç»“æœè¡¨æ˜æ¨¡å‹æ€§èƒ½ä¸»è¦å—æ¨¡å‹è§„æ¨¡å’Œå†…åœ¨æ¨ç†èƒ½åŠ›é©±åŠ¨ï¼Œè€Œå¤æ‚æ¨ç†ç­–ç•¥å¸¦æ¥çš„æå‡æœ‰é™ã€‚è¯¥æ•°æ®é›†æ—¨åœ¨æ”¯æŒç”Ÿç‰©åŒ»å­¦æ¨ç†å’Œæ¨¡å‹æ”¹è¿›çš„ç ”ç©¶ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.15141v1">ItemRAG: Item-Based Retrieval-Augmented Generation for LLM-Based Recommendation</a></td><td><details><summary>å±•å¼€</summary>Recently, large language models (LLMs) have been widely used as recommender systems, owing to their strong reasoning capability and their effectiveness in handling cold-start items. To better adapt LLMs for recommendation, retrieval-augmented generation (RAG) has been incorporated. Most existing RAG methods are user-based, retrieving purchase patterns of users similar to the target user and providing them to the LLM. In this work, we propose ItemRAG, an item-based RAG method for LLM-based recommendation that retrieves relevant items (rather than users) from item-item co-purchase histories. ItemRAG helps LLMs capture co-purchase patterns among items, which are beneficial for recommendations. Especially, our retrieval strategy incorporates semantically similar items to better handle cold-start items and uses co-purchase frequencies to improve the relevance of the retrieved items. Through extensive experiments, we demonstrate that ItemRAG consistently (1) improves the zero-shot LLM-based recommender by up to 43% in Hit-Ratio-1 and (2) outperforms user-based RAG baselines under both standard and cold-start item recommendation settings.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºé¡¹ç›®çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ï¼ˆItemRAGï¼‰ï¼Œç”¨äºæ”¹è¿›åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨èç³»ç»Ÿã€‚ä¸ç°æœ‰åŸºäºç”¨æˆ·çš„RAGæ–¹æ³•ä¸åŒï¼ŒItemRAGé€šè¿‡æ£€ç´¢é¡¹ç›®é—´çš„å…±åŒè´­ä¹°å†å²å’Œè¯­ä¹‰ç›¸ä¼¼æ€§æ¥æ•æ‰é¡¹ç›®é—´çš„å…³è”æ¨¡å¼ï¼Œä»è€Œæé«˜æ¨èæ•ˆæœï¼Œå°¤å…¶åœ¨å†·å¯åŠ¨é¡¹ç›®æ¨èåœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå®éªŒè¡¨æ˜å…¶æ˜¾è‘—æå‡äº†æ¨èæ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.15074v1">Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents</a></td><td><details><summary>å±•å¼€</summary>The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a "flooding-pruning" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRogue Oneçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨åŒ–ç‰¹å¾æå–ï¼ˆAutoFEï¼‰ï¼Œé€šè¿‡ç»“åˆæ£€ç´¢å¢å¼ºï¼ˆRAGï¼‰ç³»ç»ŸåŠ¨æ€æ•´åˆå¤–éƒ¨çŸ¥è¯†ï¼Œä»¥ç”Ÿæˆç»Ÿè®¡æ€§èƒ½å¼ºä¸”å…·æœ‰è¯­ä¹‰æ„ä¹‰çš„ç‰¹å¾ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸‰ä¸ªååŒå·¥ä½œçš„æ™ºèƒ½ä½“ï¼ˆScientistã€Extractorã€Testerï¼‰å®ç°äº†ç‰¹å¾æ¢ç´¢ä¸éªŒè¯çš„å¹³è¡¡ï¼Œå¹¶å¼•å…¥å®šæ€§åé¦ˆæœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†19ä¸ªåˆ†ç±»å’Œ9ä¸ªå›å½’æ•°æ®é›†çš„æ€§èƒ½ï¼ŒåŒæ—¶å±•ç¤ºäº†ç”Ÿæˆå¯è§£é‡Šæ€§ç‰¹å¾å’Œç§‘å­¦å‘ç°çš„èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.15005v1">Mathematical Analysis of Hallucination Dynamics in Large Language Models: Uncertainty Quantification, Advanced Decoding, and Principled Mitigation</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) are powerful linguistic engines but remain susceptible to hallucinations: plausible-sounding outputs that are factually incorrect or unsupported. In this work, we present a mathematically grounded framework to understand, measure, and mitigate these hallucinations. Drawing on probabilistic modeling, information theory, trigonometric signal analysis, and Bayesian uncertainty estimation, we analyze how errors compound autoregressively, propose refined uncertainty metrics, including semantic and phase-aware variants, and develop principled mitigation strategies such as contrastive decoding, retrieval-augmented grounding, factual alignment, and abstention. This unified lens connects recent advances in calibration, retrieval, and alignment to support safer and more reliable LLMs.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºä¸€ä¸ªåŸºäºæ•°å­¦ç†è®ºçš„æ¡†æ¶æ¥åˆ†æã€é‡åŒ–å’Œç¼“è§£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¹»è§‰é—®é¢˜ï¼Œå…¶ä¸­ç»“åˆæ¦‚ç‡å»ºæ¨¡ã€ä¿¡æ¯è®ºç­‰æ–¹æ³•ï¼Œå¹¶ç‰¹åˆ«æåŠäº†æ£€ç´¢å¢å¼ºä¿®æ­£ï¼ˆretrieval-augmented groundingï¼‰ä½œä¸ºç¼“è§£ç­–ç•¥ä¹‹ä¸€ï¼Œæ—¨åœ¨é€šè¿‡æ£€ç´¢å¤–éƒ¨ä¿¡æ¯æå‡LLMsçš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-18
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.14671v1">Streamlining Industrial Contract Management with Retrieval-Augmented LLMs</a></td><td><details><summary>å±•å¼€</summary>Contract management involves reviewing and negotiating provisions, individual clauses that define rights, obligations, and terms of agreement. During this process, revisions to provisions are proposed and iteratively refined, some of which may be problematic or unacceptable. Automating this workflow is challenging due to the scarcity of labeled data and the abundance of unstructured legacy contracts. In this paper, we present a modular framework designed to streamline contract management through a retrieval-augmented generation (RAG) pipeline. Our system integrates synthetic data generation, semantic clause retrieval, acceptability classification, and reward-based alignment to flag problematic revisions and generate improved alternatives. Developed and evaluated in collaboration with an industry partner, our system achieves over 80% accuracy in both identifying and optimizing problematic revisions, demonstrating strong performance under real-world, low-resource conditions and offering a practical means of accelerating contract revision workflows.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ¨¡å—åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åˆæˆæ•°æ®ç”Ÿæˆã€è¯­ä¹‰æ¡æ¬¾æ£€ç´¢ã€å¯æ¥å—æ€§åˆ†ç±»å’ŒåŸºäºå¥–åŠ±çš„å¯¹é½æŠ€æœ¯ï¼Œè‡ªåŠ¨åŒ–åˆåŒç®¡ç†æµç¨‹ä¸­çš„é—®é¢˜ä¿®è®¢è¯†åˆ«ä¸ä¼˜åŒ–ï¼Œå®ç°åœ¨ä½èµ„æºæ¡ä»¶ä¸‹é«˜æ•ˆå¤„ç†éç»“æ„åŒ–åˆåŒï¼Œç³»ç»Ÿå‡†ç¡®ç‡è¶…è¿‡80%ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.14638v1">A Specialized Large Language Model for Clinical Reasoning and Diagnosis in Rare Diseases</a></td><td><details><summary>å±•å¼€</summary>Rare diseases affect hundreds of millions worldwide, yet diagnosis often spans years. Convectional pipelines decouple noisy evidence extraction from downstream inferential diagnosis, and general/medical large language models (LLMs) face scarce real world electronic health records (EHRs), stale domain knowledge, and hallucinations. We assemble a large, domain specialized clinical corpus and a clinician validated reasoning set, and develop RareSeek R1 via staged instruction tuning, chain of thought learning, and graph grounded retrieval. Across multicenter EHR narratives and public benchmarks, RareSeek R1 attains state of the art accuracy, robust generalization, and stability under noisy or overlapping phenotypes. Augmented retrieval yields the largest gains when narratives pair with prioritized variants by resolving ambiguity and aligning candidates to mechanisms. Human studies show performance on par with experienced physicians and consistent gains in assistive use. Notably, transparent reasoning highlights decisive non phenotypic evidence (median 23.1%, such as imaging, interventions, functional tests) underpinning many correct diagnoses. This work advances a narrative first, knowledge integrated reasoning paradigm that shortens the diagnostic odyssey and enables auditable, clinically translatable decision support.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºRareSeek R1ç³»ç»Ÿï¼Œé€šè¿‡æ•´åˆé¢†åŸŸä¸“ç”¨ä¸´åºŠè¯­æ–™åº“ã€å›¾æ£€ç´¢å¢å¼ºå’Œåˆ†é˜¶æ®µæŒ‡ä»¤å¾®è°ƒæŠ€æœ¯ï¼Œè§£å†³ç½•è§ç—…è¯Šæ–­ä¸­æ•°æ®ç¨€ç¼ºå’Œæ¨¡å‹å¹»è§‰é—®é¢˜ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆæ£€ç´¢ä¸´åºŠè¯æ®å¹¶åŸºäºå›¾ç»“æ„å¯¹é½å˜å¼‚ä¼˜å…ˆçº§ï¼‰ä¸é“¾å¼æ¨ç†å­¦ä¹ ï¼Œæ˜¾è‘—æå‡è¯Šæ–­å‡†ç¡®æ€§ï¼Œå°¤å…¶åœ¨å¤„ç†å™ªå£°æˆ–é‡å è¡¨å‹æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œæœ€ç»ˆå®ç°ä¸èµ„æ·±åŒ»ç”Ÿç›¸å½“çš„è¯Šæ–­æ€§èƒ½ï¼Œå¹¶å¼ºè°ƒéè¡¨å‹è¯æ®å¯¹é€æ˜æ¨ç†çš„å…³é”®ä½œç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.14531v1">LiveRAG: A diverse Q&A dataset with varying difficulty level for RAG evaluation</a></td><td><details><summary>å±•å¼€</summary>With Retrieval Augmented Generation (RAG) becoming more and more prominent in generative AI solutions, there is an emerging need for systematically evaluating their effectiveness. We introduce the LiveRAG benchmark, a publicly available dataset of 895 synthetic questions and answers designed to support systematic evaluation of RAG-based Q&A systems. This synthetic benchmark is derived from the one used during the SIGIR'2025 LiveRAG Challenge, where competitors were evaluated under strict time constraints. It is augmented with information that was not made available to competitors during the Challenge, such as the ground-truth answers, together with their associated supporting claims which were used for evaluating competitors' answers. In addition, each question is associated with estimated difficulty and discriminability scores, derived from applying an Item Response Theory model to competitors' responses. Our analysis highlights the benchmark's questions diversity, the wide range of their difficulty levels, and their usefulness in differentiating between system capabilities. The LiveRAG benchmark will hopefully help the community advance RAG research, conduct systematic evaluation, and develop more robust Q&A systems.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ä»‹ç»äº†LiveRAGåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«895ä¸ªåˆæˆé—®é¢˜ä¸ç­”æ¡ˆçš„å…¬å¼€æ•°æ®é›†ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°åŸºäºRAGçš„é—®ç­”ç³»ç»Ÿã€‚è¯¥åŸºå‡†æºè‡ªSIGIR'2025 LiveRAGæŒ‘æˆ˜èµ›ï¼Œè¡¥å……äº†çœŸå®ç­”æ¡ˆã€æ”¯æŒè¯æ®åŠå…¶éš¾åº¦å’ŒåŒºåˆ†åº¦åˆ†æ•°ï¼Œç”¨äºåˆ†æé—®é¢˜å¤šæ ·æ€§å’Œç³»ç»Ÿæ€§èƒ½å·®å¼‚ï¼Œä»¥æ¨åŠ¨RAGç ”ç©¶å’Œé²æ£’æ€§é—®ç­”ç³»ç»Ÿå¼€å‘ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.14445v1">Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue Generation, and Agentic Planning</a></td><td><details><summary>å±•å¼€</summary>We present Tell Me, a mental well-being system that leverages advances in large language models to provide accessible, context-aware support for users and researchers. The system integrates three components: (i) a retrieval-augmented generation (RAG) assistant for personalized, knowledge-grounded dialogue; (ii) a synthetic client-therapist dialogue generator conditioned on client profiles to facilitate research on therapeutic language and data augmentation; and (iii) a Well-being AI crew, implemented with CrewAI, that produces weekly self-care plans and guided meditation audio. The system is designed as a reflective space for emotional processing rather than a substitute for professional therapy. It illustrates how conversational assistants can lower barriers to support, complement existing care, and broaden access to mental health resources. To address the shortage of confidential therapeutic data, we introduce synthetic client-therapist dialogue generation conditioned on client profiles. Finally, the planner demonstrates an innovative agentic workflow for dynamically adaptive, personalized self-care, bridging the limitations of static well-being tools. We describe the architecture, demonstrate its functionalities, and report evaluation of the RAG assistant in curated well-being scenarios using both automatic LLM-based judgments and a human-user study. This work highlights opportunities for interdisciplinary collaboration between NLP researchers and mental health professionals to advance responsible innovation in human-AI interaction for well-being.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªåä¸ºâ€œTell Meâ€çš„å¿ƒç†å¥åº·ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹æŠ€æœ¯æä¾›ä¸ªæ€§åŒ–ã€æƒ…å¢ƒæ„ŸçŸ¥çš„æ”¯æŒã€‚ç³»ç»ŸåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šä¸€ä¸ªåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„å¯¹è¯åŠ©æ‰‹ï¼ˆç”¨äºçŸ¥è¯†é©±åŠ¨çš„ä¸ªæ€§åŒ–å¯¹è¯ï¼‰ã€ä¸€ä¸ªåŸºäºç”¨æˆ·æ¡£æ¡ˆç”Ÿæˆåˆæˆæ²»ç–—å¯¹è¯çš„æ¨¡å—ï¼ˆç”¨äºç ”ç©¶åŠæ•°æ®å¢å¼ºï¼‰ï¼Œä»¥åŠä¸€ä¸ªé€šè¿‡CrewAIå®ç°çš„åŠ¨æ€è‡ªæˆ‘æŠ¤ç†è®¡åˆ’ç”Ÿæˆå™¨ã€‚ç ”ç©¶é‡ç‚¹å±•ç¤ºäº†RAGåŠ©æ‰‹åœ¨å¿ƒç†å¥åº·åœºæ™¯ä¸­çš„åº”ç”¨ï¼Œå¹¶é€šè¿‡è‡ªåŠ¨è¯„ä¼°å’Œç”¨æˆ·å®éªŒéªŒè¯å…¶æ•ˆæœï¼ŒåŒæ—¶æ¢è®¨äº†NLPä¸å¿ƒç†å¥åº·é¢†åŸŸçš„è·¨å­¦ç§‘åˆä½œæ½œåŠ›ã€‚ç³»ç»Ÿå¼ºè°ƒè¾…åŠ©æ€§ï¼ˆéæ›¿ä»£ä¸“ä¸šæ²»ç–—ï¼‰å’Œå¯åŠæ€§ï¼Œå¹¶åˆ›æ–°æ€§åœ°è§£å†³äº†æ²»ç–—æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.14362v1">SciRAG: Adaptive, Citation-Aware, and Outline-Guided Retrieval and Synthesis for Scientific Literature</a></td><td><details><summary>å±•å¼€</summary>The accelerating growth of scientific publications has intensified the need for scalable, trustworthy systems to synthesize knowledge across diverse literature. While recent retrieval-augmented generation (RAG) methods have improved access to scientific information, they often overlook citation graph structure, adapt poorly to complex queries, and yield fragmented, hard-to-verify syntheses. We introduce SciRAG, an open-source framework for scientific literature exploration that addresses these gaps through three key innovations: (1) adaptive retrieval that flexibly alternates between sequential and parallel evidence gathering; (2) citation-aware symbolic reasoning that leverages citation graphs to organize and filter supporting documents; and (3) outline-guided synthesis that plans, critiques, and refines answers to ensure coherence and transparent attribution. Extensive experiments across multiple benchmarks such as QASA and ScholarQA demonstrate that SciRAG outperforms prior systems in factual accuracy and synthesis quality, establishing a new foundation for reliable, large-scale scientific knowledge aggregation.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†SciRAGï¼Œä¸€ä¸ªé’ˆå¯¹ç§‘å­¦æ–‡çŒ®æ¢ç´¢çš„å¼€æºæ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”æ£€ç´¢ã€åŸºäºå¼•ç”¨çš„ç¬¦å·æ¨ç†å’Œå¤§çº²å¼•å¯¼çš„åˆæˆä¸‰é¡¹åˆ›æ–°ï¼Œè§£å†³äº†ç°æœ‰RAGæ–¹æ³•åœ¨ç§‘å­¦é¢†åŸŸä¸­çš„å±€é™æ€§ï¼Œå¦‚å¿½è§†å¼•ç”¨å›¾ç»“æ„ã€å¤æ‚æŸ¥è¯¢é€‚åº”æ€§å·®å’Œåˆæˆç»“æœç¢ç‰‡åŒ–ç­‰é—®é¢˜ï¼Œå®éªŒè¯æ˜å…¶åœ¨äº‹å®å‡†ç¡®æ€§å’Œåˆæˆè´¨é‡ä¸Šä¼˜äºç°æœ‰ç³»ç»Ÿã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.14299v1">DataSage: Multi-agent Collaboration for Insight Discovery with External Knowledge Retrieval, Multi-role Debating, and Multi-path Reasoning</a></td><td><details><summary>å±•å¼€</summary>In today's data-driven era, fully automated end-to-end data analytics, particularly insight discovery, is critical for discovering actionable insights that assist organizations in making effective decisions. With the rapid advancement of large language models (LLMs), LLM-driven agents have emerged as a promising paradigm for automating data analysis and insight discovery. However, existing data insight agents remain limited in several key aspects, often failing to deliver satisfactory results due to: (1) insufficient utilization of domain knowledge, (2) shallow analytical depth, and (3) error-prone code generation during insight generation. To address these issues, we propose DataSage, a novel multi-agent framework that incorporates three innovative features including external knowledge retrieval to enrich the analytical context, a multi-role debating mechanism to simulate diverse analytical perspectives and deepen analytical depth, and multi-path reasoning to improve the accuracy of the generated code and insights. Extensive experiments on InsightBench demonstrate that DataSage consistently outperforms existing data insight agents across all difficulty levels, offering an effective solution for automated data insight discovery.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDataSageçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡å¤–éƒ¨çŸ¥è¯†æ£€ç´¢ã€å¤šè§’è‰²è¾©è®ºæœºåˆ¶å’Œå¤šè·¯å¾„æ¨ç†æ¥è§£å†³ç°æœ‰æ•°æ®åˆ†ææ™ºèƒ½ä½“åœ¨é¢†åŸŸçŸ¥è¯†åˆ©ç”¨ä¸è¶³ã€åˆ†ææ·±åº¦ä¸è¶³å’Œä»£ç ç”Ÿæˆé”™è¯¯ç­‰æ–¹é¢çš„é—®é¢˜ï¼Œä»è€Œæå‡æ•°æ®æ´å¯Ÿå‘ç°çš„è‡ªåŠ¨åŒ–æ•ˆæœã€‚å…¶ä¸­å¤–éƒ¨çŸ¥è¯†æ£€ç´¢éƒ¨åˆ†ä¸RAGæŠ€æœ¯ç›¸å…³ï¼Œç”¨äºä¸°å¯Œåˆ†æä¸Šä¸‹æ–‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.14256v1">PathMind: A Retrieve-Prioritize-Reason Framework for Knowledge Graph Reasoning with Large Language Models</a></td><td><details><summary>å±•å¼€</summary>Knowledge graph reasoning (KGR) is the task of inferring new knowledge by performing logical deductions on knowledge graphs. Recently, large language models (LLMs) have demonstrated remarkable performance in complex reasoning tasks. Despite promising success, current LLM-based KGR methods still face two critical limitations. First, existing methods often extract reasoning paths indiscriminately, without assessing their different importance, which may introduce irrelevant noise that misleads LLMs. Second, while many methods leverage LLMs to dynamically explore potential reasoning paths, they require high retrieval demands and frequent LLM calls. To address these limitations, we propose PathMind, a novel framework designed to enhance faithful and interpretable reasoning by selectively guiding LLMs with important reasoning paths. Specifically, PathMind follows a "Retrieve-Prioritize-Reason" paradigm. First, it retrieves a query subgraph from KG through the retrieval module. Next, it introduces a path prioritization mechanism that identifies important reasoning paths using a semantic-aware path priority function, which simultaneously considers the accumulative cost and the estimated future cost for reaching the target. Finally, PathMind generates accurate and logically consistent responses via a dual-phase training strategy, including task-specific instruction tuning and path-wise preference alignment. Extensive experiments on benchmark datasets demonstrate that PathMind consistently outperforms competitive baselines, particularly on complex reasoning tasks with fewer input tokens, by identifying essential reasoning paths.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºPathMindçš„æ¡†æ¶ï¼Œé€šè¿‡â€œæ£€ç´¢-ä¼˜å…ˆæ’åº-æ¨ç†â€èŒƒå¼å¢å¼ºçŸ¥è¯†å›¾è°±æ¨ç†ï¼ˆKGRï¼‰ã€‚å®ƒé¦–å…ˆä»çŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢ç›¸å…³å­å›¾ï¼Œç„¶åé€šè¿‡è¯­ä¹‰æ„ŸçŸ¥è·¯å¾„ä¼˜å…ˆçº§æœºåˆ¶ç­›é€‰é‡è¦æ¨ç†è·¯å¾„ï¼Œæœ€åç»“åˆåŒé˜¶æ®µè®­ç»ƒç­–ç•¥ç”Ÿæˆå‡†ç¡®ä¸”é€»è¾‘ä¸€è‡´çš„ç­”æ¡ˆã€‚è¯¥æ–¹æ³•å‡å°‘äº†æ— å…³å™ªå£°å’Œé¢‘ç¹è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„éœ€æ±‚ï¼Œåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå±äºRAGæŠ€æœ¯çš„ä¸€ç§åº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.14249v1">Towards Authentic Movie Dubbing with Retrieve-Augmented Director-Actor Interaction Learning</a></td><td><details><summary>å±•å¼€</summary>The automatic movie dubbing model generates vivid speech from given scripts, replicating a speaker's timbre from a brief timbre prompt while ensuring lip-sync with the silent video. Existing approaches simulate a simplified workflow where actors dub directly without preparation, overlooking the critical director-actor interaction. In contrast, authentic workflows involve a dynamic collaboration: directors actively engage with actors, guiding them to internalize the context cues, specifically emotion, before performance. To address this issue, we propose a new Retrieve-Augmented Director-Actor Interaction Learning scheme to achieve authentic movie dubbing, termed Authentic-Dubber, which contains three novel mechanisms: (1) We construct a multimodal Reference Footage library to simulate the learning footage provided by directors. Note that we integrate Large Language Models (LLMs) to achieve deep comprehension of emotional representations across multimodal signals. (2) To emulate how actors efficiently and comprehensively internalize director-provided footage during dubbing, we propose an Emotion-Similarity-based Retrieval-Augmentation strategy. This strategy retrieves the most relevant multimodal information that aligns with the target silent video. (3) We develop a Progressive Graph-based speech generation approach that incrementally incorporates the retrieved multimodal emotional knowledge, thereby simulating the actor's final dubbing process. The above mechanisms enable the Authentic-Dubber to faithfully replicate the authentic dubbing workflow, achieving comprehensive improvements in emotional expressiveness. Both subjective and objective evaluations on the V2C Animation benchmark dataset validate the effectiveness. The code and demos are available at https://github.com/AI-S2-Lab/Authentic-Dubber.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºAuthentic-Dubberçš„æ£€ç´¢å¢å¼ºå¯¼æ¼”-æ¼”å‘˜äº¤äº’å­¦ä¹ æ–¹æ¡ˆï¼Œç”¨äºå®ç°æ›´çœŸå®çš„ç”µå½±é…éŸ³ã€‚é€šè¿‡æ„å»ºå¤šæ¨¡æ€å‚è€ƒç´ æåº“ã€åŸºäºæƒ…æ„Ÿç›¸ä¼¼æ€§çš„æ£€ç´¢å¢å¼ºç­–ç•¥ä»¥åŠæ¸è¿›å¼å›¾ç»“æ„çš„è¯­éŸ³ç”Ÿæˆæ–¹æ³•ï¼Œç³»ç»Ÿèƒ½å¤Ÿæ¨¡æ‹ŸçœŸå®é…éŸ³æµç¨‹ä¸­çš„å¯¼æ¼”-æ¼”å‘˜äº’åŠ¨ï¼Œæ˜¾è‘—æå‡æƒ…æ„Ÿè¡¨ç°åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.14182v1">WebRec: Enhancing LLM-based Recommendations with Attention-guided RAG from Web</a></td><td><details><summary>å±•å¼€</summary>Recommender systems play a vital role in alleviating information overload and enriching users' online experience. In the era of large language models (LLMs), LLM-based recommender systems have emerged as a prevalent paradigm for advancing personalized recommendations. Recently, retrieval-augmented generation (RAG) has drawn growing interest to facilitate the recommendation capability of LLMs, incorporating useful information retrieved from external knowledge bases. However, as a rich source of up-to-date information, the web remains under-explored by existing RAG-based recommendations. In particular, unique challenges are posed from two perspectives: one is to generate effective queries for web retrieval, considering the inherent knowledge gap between web search and recommendations; another challenge lies in harnessing online websites that contain substantial noisy content. To tackle these limitations, we propose WebRec, a novel web-based RAG framework, which takes advantage of the reasoning capability of LLMs to interpret recommendation tasks into queries of user preferences that cater to web retrieval. Moreover, given noisy web-retrieved information, where relevant pieces of evidence are scattered far apart, an insightful MP-Head is designed to enhance LLM attentions between distant tokens of relevant information via message passing. Extensive experiments have been conducted to demonstrate the effectiveness of our proposed web-based RAG methods in recommendation scenarios.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ¨èç³»ç»Ÿï¼Œæå‡ºäº†ä¸€ç§åä¸ºWebRecçš„æ–°æ¡†æ¶ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›å°†æ¨èä»»åŠ¡è½¬åŒ–ä¸ºé€‚åˆç½‘ç»œæ£€ç´¢çš„ç”¨æˆ·åå¥½æŸ¥è¯¢ã€‚è¯¥æ¡†æ¶é€šè¿‡è®¾è®¡MP-Headæœºåˆ¶æ¥å¢å¼ºLLMså¯¹åˆ†æ•£åœ¨ç½‘ç»œæ£€ç´¢ä¿¡æ¯ä¸­çš„ç›¸å…³è¯æ®çš„å…³æ³¨ï¼Œä»è€Œæå‡æ¨èæ•ˆæœã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨èåœºæ™¯ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.14129v1">MalRAG: A Retrieval-Augmented LLM Framework for Open-set Malicious Traffic Identification</a></td><td><details><summary>å±•å¼€</summary>Fine-grained identification of IDS-flagged suspicious traffic is crucial in cybersecurity. In practice, cyber threats evolve continuously, making the discovery of novel malicious traffic a critical necessity as well as the identification of known classes. Recent studies have advanced this goal with deep models, but they often rely on task-specific architectures that limit transferability and require per-dataset tuning. In this paper we introduce MalRAG, the first LLM driven retrieval-augmented framework for open-set malicious traffic identification. MalRAG freezes the LLM and operates via comprehensive traffic knowledge construction, adaptive retrieval, and prompt engineering. Concretely, we construct a multi-view traffic database by mining prior malicious traffic from content, structural, and temporal perspectives. Furthermore, we introduce a Coverage-Enhanced Retrieval Algorithm that queries across these views to assemble the most probable candidates, thereby improving the inclusion of correct evidence. We then employ Traffic-Aware Adaptive Pruning to select a variable subset of these candidates based on traffic-aware similarity scores, suppressing incorrect matches and yielding reliable retrieved evidence. Moreover, we develop a suite of guidance prompts where task instruction, evidence referencing, and decision guidance are integrated with the retrieved evidence to improve LLM performance. Across diverse real-world datasets and settings, MalRAG delivers state-of-the-art results in both fine-grained identification of known classes and novel malicious traffic discovery. Ablation and deep-dive analyses further show that MalRAG effective leverages LLM capabilities yet achieves open-set malicious traffic identification without relying on a specific LLM.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†MalRAGï¼Œä¸€ä¸ªåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯çš„æ¡†æ¶ï¼Œç”¨äºå¼€æ”¾é›†æ¶æ„æµé‡çš„ç»†ç²’åº¦è¯†åˆ«ã€‚é€šè¿‡æ„å»ºå¤šè§†è§’æµé‡çŸ¥è¯†åº“ã€è¦†ç›–å¢å¼ºæ£€ç´¢ç®—æ³•å’Œæµé‡æ„ŸçŸ¥è‡ªé€‚åº”å‰ªæï¼Œç»“åˆæç¤ºå·¥ç¨‹ï¼Œåˆ©ç”¨å†»ç»“çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®ç°å·²çŸ¥å’Œæ–°å‹æ¶æ„æµé‡çš„é«˜ç²¾åº¦æ£€æµ‹ï¼Œæ— éœ€ä¾èµ–ç‰¹å®šLLMæ¶æ„ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.14096v1">NeuroPath: Neurobiology-Inspired Path Tracking and Reflection for Semantically Coherent Retrieval</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) greatly enhances large language models (LLMs) performance in knowledge-intensive tasks. However, naive RAG methods struggle with multi-hop question answering due to their limited capacity to capture complex dependencies across documents. Recent studies employ graph-based RAG to capture document connections. However, these approaches often result in a loss of semantic coherence and introduce irrelevant noise during node matching and subgraph construction. To address these limitations, we propose NeuroPath, an LLM-driven semantic path tracking RAG framework inspired by the path navigational planning of place cells in neurobiology. It consists of two steps: Dynamic Path Tracking and Post-retrieval Completion. Dynamic Path Tracking performs goal-directed semantic path tracking and pruning over the constructed knowledge graph (KG), improving noise reduction and semantic coherence. Post-retrieval Completion further reinforces these benefits by conducting second-stage retrieval using intermediate reasoning and the original query to refine the query goal and complete missing information in the reasoning path. NeuroPath surpasses current state-of-the-art baselines on three multi-hop QA datasets, achieving average improvements of 16.3% on recall@2 and 13.5% on recall@5 over advanced graph-based RAG methods. Moreover, compared to existing iter-based RAG methods, NeuroPath achieves higher accuracy and reduces token consumption by 22.8%. Finally, we demonstrate the robustness of NeuroPath across four smaller LLMs (Llama3.1, GLM4, Mistral0.3, and Gemma3), and further validate its scalability across tasks of varying complexity. Code is available at https://github.com/KennyCaty/NeuroPath.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºNeuroPathï¼Œä¸€ç§å—ç¥ç»ç”Ÿç‰©å­¦å¯å‘çš„LLMé©±åŠ¨è¯­ä¹‰è·¯å¾„è¿½è¸ªRAGæ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è·¯å¾„è¿½è¸ªå’Œæ£€ç´¢åè¡¥å…¨ä¸¤é˜¶æ®µä¼˜åŒ–å¤šè·³é—®ç­”ä»»åŠ¡ï¼Œåœ¨çŸ¥è¯†å›¾è°±ä¸Šæ‰§è¡Œç›®æ ‡å¯¼å‘çš„è¯­ä¹‰è·¯å¾„ä¿®å‰ªï¼Œæ˜¾è‘—æå‡å™ªå£°æŠ‘åˆ¶ä¸è¯­ä¹‰è¿è´¯æ€§ã€‚å®éªŒæ˜¾ç¤ºå…¶åœ¨ä¸‰ä¸ªå¤šè·³QAæ•°æ®é›†ä¸Šå¹³å‡å¬å›ç‡è¶…è¶Šç°æœ‰å›¾åŸºRAGæ–¹æ³•16.3%ï¼ˆrecall@2ï¼‰å’Œ13.5%ï¼ˆrecall@5ï¼‰ï¼ŒåŒæ—¶è¾ƒè¿­ä»£å¼RAGå‡å°‘22.8%çš„tokenæ¶ˆè€—ï¼Œå¹¶åœ¨ä¸åŒè§„æ¨¡LLMä¸ŠéªŒè¯äº†é²æ£’æ€§ä¸ä»»åŠ¡æ‰©å±•æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.14043v1">AISAC: An Integrated multi-agent System for Transparent, Retrieval-Grounded Scientific Assistance</a></td><td><details><summary>å±•å¼€</summary>AI Scientific Assistant Core (AISAC) is an integrated multi-agent system developed at Argonne National Laboratory for scientific and engineering workflows. AISAC builds on established technologies - LangGraph for orchestration, FAISS for vector search, and SQLite for persistence - and integrates them into a unified system prototype focused on transparency, provenance tracking, and scientific adaptability.
  The system implements a Router-Planner-Coordinator workflow and an optional Evaluator role, using prompt-engineered agents coordinated via LangGraph's StateGraph and supported by helper agents such as a Researcher. Each role is defined through custom system prompts that enforce structured JSON outputs. A hybrid memory approach (FAISS + SQLite) enables both semantic retrieval and structured conversation history. An incremental indexing strategy based on file hashing minimizes redundant re-embedding when scientific corpora evolve. A configuration-driven project bootstrap layer allows research teams to customize tools, prompts, and data sources without modifying core code.
  All agent decisions, tool invocations, and retrievals are logged and visualized through a custom Gradio interface, providing step-by-step transparency for each reasoning episode. The authors have applied AISAC to multiple research areas at Argonne, including specialized deployments for waste-to-products research and energy process safety, as well as general-purpose scientific assistance, demonstrating its cross-domain applicability.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ä»‹ç»äº†AI Scientific Assistant Core (AISAC)ï¼Œä¸€ä¸ªé›†æˆå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œç»“åˆäº†FAISSå‘é‡æœç´¢å’ŒSQLiteæŒä¹…åŒ–æŠ€æœ¯ï¼Œé‡‡ç”¨æ··åˆå†…å­˜æ–¹æ³•å®ç°è¯­ä¹‰æ£€ç´¢å’Œç»“æ„åŒ–å¯¹è¯å†å²ï¼Œå¹¶é€šè¿‡åŸºäºæ–‡ä»¶å“ˆå¸Œçš„å¢é‡ç´¢å¼•ç­–ç•¥ä¼˜åŒ–ç§‘å­¦æ–‡çŒ®æ›´æ–°æ—¶çš„åµŒå…¥æ•ˆç‡ï¼Œå…·æœ‰æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ ¸å¿ƒç‰¹å¾ï¼Œåº”ç”¨äºç§‘å­¦ç ”ç©¶å’Œå·¥ç¨‹å·¥ä½œæµã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.14027v1">HiEAG: Evidence-Augmented Generation for Out-of-Context Misinformation Detection</a></td><td><details><summary>å±•å¼€</summary>Recent advancements in multimodal out-of-context (OOC) misinformation detection have made remarkable progress in checking the consistencies between different modalities for supporting or refuting image-text pairs. However, existing OOC misinformation detection methods tend to emphasize the role of internal consistency, ignoring the significant of external consistency between image-text pairs and external evidence. In this paper, we propose HiEAG, a novel Hierarchical Evidence-Augmented Generation framework to refine external consistency checking through leveraging the extensive knowledge of multimodal large language models (MLLMs). Our approach decomposes external consistency checking into a comprehensive engine pipeline, which integrates reranking and rewriting, apart from retrieval. Evidence reranking module utilizes Automatic Evidence Selection Prompting (AESP) that acquires the relevant evidence item from the products of evidence retrieval. Subsequently, evidence rewriting module leverages Automatic Evidence Generation Prompting (AEGP) to improve task adaptation on MLLM-based OOC misinformation detectors. Furthermore, our approach enables explanation for judgment, and achieves impressive performance with instruction tuning. Experimental results on different benchmark datasets demonstrate that our proposed HiEAG surpasses previous state-of-the-art (SOTA) methods in the accuracy over all samples.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºHiEAGæ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚è¯æ®å¢å¼ºç”ŸæˆæŠ€æœ¯æ”¹è¿›å¤šæ¨¡æ€OOCè™šå‡ä¿¡æ¯æ£€æµ‹ï¼Œç»“åˆæ£€ç´¢ã€é‡æ’åºï¼ˆåˆ©ç”¨AESPç­›é€‰ç›¸å…³è¯æ®ï¼‰å’Œé‡å†™ï¼ˆåˆ©ç”¨AEGPé€‚é…ä»»åŠ¡ï¼‰æ¨¡å—ï¼Œåˆ©ç”¨MLLMsçš„å¤–éƒ¨çŸ¥è¯†éªŒè¯å›¾åƒ-æ–‡æœ¬å¯¹ä¸å¤–éƒ¨è¯æ®çš„ä¸€è‡´æ€§ï¼Œå¹¶ç”Ÿæˆè§£é‡Šã€‚å®éªŒè¡¨æ˜å…¶æ€§èƒ½è¶…è¶Šç°æœ‰SOTAæ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.14010v1">Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports</a></td><td><details><summary>å±•å¼€</summary>Post-disaster reconnaissance reports contain critical evidence for understanding multi-hazard interactions, yet their unstructured narratives make systematic knowledge transfer difficult. Large language models (LLMs) offer new potential for analyzing these reports, but often generate unreliable or hallucinated outputs when domain grounding is absent. This study introduces the Mixture-of-Retrieval Agentic RAG (MoRA-RAG), a knowledge-grounded LLM framework that transforms reconnaissance reports into a structured foundation for multi-hazard reasoning. The framework integrates a Mixture-of-Retrieval mechanism that dynamically routes queries across hazard-specific databases while using agentic chunking to preserve contextual coherence during retrieval. It also includes a verification loop that assesses evidence sufficiency, refines queries, and initiates targeted searches when information remains incomplete. We construct HazardRecQA by deriving question-answer pairs from GEER reconnaissance reports, which document 90 global events across seven major hazard types. MoRA-RAG achieves up to 94.5 percent accuracy, outperforming zero-shot LLMs by 30 percent and state-of-the-art RAG systems by 10 percent, while reducing hallucinations across diverse LLM architectures. MoRA-RAG also enables open-weight LLMs to achieve performance comparable to proprietary models. It establishes a new paradigm for transforming post-disaster documentation into actionable, trustworthy intelligence for hazard resilience.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMoRA-RAGçš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ··åˆæ£€ç´¢æœºåˆ¶å’Œä»£ç†å¼åˆ†å—æŠ€æœ¯ï¼Œå°†ç¾å®³å‹˜å¯ŸæŠ¥å‘Šè½¬åŒ–ä¸ºç»“æ„åŒ–çŸ¥è¯†åº“ï¼Œä»¥æå‡å¤šç¾å®³æ¨ç†çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚è¯¥æ–¹æ³•ç»“åˆåŠ¨æ€æŸ¥è¯¢è·¯ç”±ã€ä¸Šä¸‹æ–‡ä¿æŒæ£€ç´¢åŠéªŒè¯å¾ªç¯ï¼Œæ˜¾è‘—å‡å°‘äº†LLMçš„å¹»è§‰ç°è±¡ï¼Œå¹¶åœ¨HazardRecQAæ•°æ®é›†ä¸Šå®ç°äº†94.5%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šé›¶æ ·æœ¬LLMå’Œç°æœ‰RAGç³»ç»Ÿã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-17
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.13526v1">Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models</a></td><td><details><summary>å±•å¼€</summary>Artificial intelligence (AI) is reshaping modern healthcare by advancing disease diagnosis, treatment decision-making, and biomedical research. Among AI technologies, large language models (LLMs) have become especially impactful, enabling deep knowledge extraction and semantic reasoning from complex medical texts. However, effective clinical decision support requires knowledge in structured, interoperable formats. Knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks. Yet, current clinical knowledge graphs still depend heavily on manual curation and rule-based extraction, which is limited by the complexity and contextual ambiguity of medical guidelines and literature. To overcome these challenges, we propose an automated framework that combines retrieval-augmented generation (RAG) with LLMs to construct medical indicator knowledge graphs. The framework incorporates guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability. The resulting knowledge graphs can be integrated into intelligent diagnosis and question-answering systems, accelerating the development of AI-driven healthcare solutions.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œç”¨äºæ„å»ºåŒ»ç–—æŒ‡æ ‡çŸ¥è¯†å›¾è°±ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿä¸´åºŠçŸ¥è¯†å›¾è°±ä¾èµ–äººå·¥å’Œè§„åˆ™æå–çš„å±€é™æ€§ï¼Œå¹¶é€šè¿‡æŒ‡å—é©±åŠ¨æ•°æ®è·å–ã€æœ¬ä½“æ¨¡å¼è®¾è®¡å’Œä¸“å®¶éªŒè¯ç¡®ä¿å…¶å¯æ‰©å±•æ€§ã€å‡†ç¡®æ€§å’Œä¸´åºŠå¯é æ€§ï¼Œæœ€ç»ˆåº”ç”¨äºæ™ºèƒ½è¯Šæ–­å’Œé—®ç­”ç³»ç»Ÿã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.13489v1">PolicyBot - Reliable Question Answering over Policy Documents</a></td><td><details><summary>å±•å¼€</summary>All citizens of a country are affected by the laws and policies introduced by their government. These laws and policies serve essential functions for citizens. Such as granting them certain rights or imposing specific obligations. However, these documents are often lengthy, complex, and difficult to navigate, making it challenging for citizens to locate and understand relevant information. This work presents PolicyBot, a retrieval-augmented generation (RAG) system designed to answer user queries over policy documents with a focus on transparency and reproducibility. The system combines domain-specific semantic chunking, multilingual dense embeddings, multi-stage retrieval with reranking, and source-aware generation to provide responses grounded in the original documents. We implemented citation tracing to reduce hallucinations and improve user trust, and evaluated alternative retrieval and generation configurations to identify effective design choices. The end-to-end pipeline is built entirely with open-source tools, enabling easy adaptation to other domains requiring document-grounded question answering. This work highlights design considerations, practical challenges, and lessons learned in deploying trustworthy RAG systems for governance-related contexts.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†PolicyBotï¼Œä¸€ä¸ªåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„ç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡ç»“åˆé¢†åŸŸç‰¹å®šçš„è¯­ä¹‰åˆ†å—ã€å¤šè¯­è¨€å¯†é›†åµŒå…¥ã€å¤šé˜¶æ®µæ£€ç´¢ä¸é‡æ’åºä»¥åŠåŸºäºæºæ–‡æ¡£çš„ç”ŸæˆæŠ€æœ¯ï¼Œå¸®åŠ©ç”¨æˆ·æŸ¥è¯¢å’Œç†è§£å¤æ‚çš„æ”¿ç­–æ–‡æ¡£ã€‚ç³»ç»Ÿå¼ºè°ƒé€æ˜æ€§å’Œå¯å¤ç°æ€§ï¼Œé‡‡ç”¨å¼•ç”¨è¿½è¸ªå‡å°‘å¹»è§‰å¹¶æå‡ç”¨æˆ·ä¿¡ä»»ï¼ŒåŒæ—¶è¯„ä¼°äº†ä¸åŒæ£€ç´¢ä¸ç”Ÿæˆé…ç½®çš„æ•ˆæœï¼Œå…¨éƒ¨é‡‡ç”¨å¼€æºå·¥å…·æ„å»ºï¼Œä¾¿äºæ‰©å±•åˆ°å…¶ä»–éœ€è¦æ–‡æ¡£åŸºç¡€é—®ç­”çš„é¢†åŸŸã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.13410v1">Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction</a></td><td><details><summary>å±•å¼€</summary>With the rise of smart personal devices, service-oriented human-agent interactions have become increasingly prevalent. This trend highlights the need for personalized dialogue assistants that can understand user-specific traits to accurately interpret requirements and tailor responses to individual preferences. However, existing approaches often overlook the complexities of long-term interactions and fail to capture users' subjective characteristics. To address these gaps, we present PAL-Bench, a new benchmark designed to evaluate the personalization capabilities of service-oriented assistants in long-term user-agent interactions. In the absence of available real-world data, we develop a multi-step LLM-based synthesis pipeline, which is further verified and refined by human annotators. This process yields PAL-Set, the first Chinese dataset comprising multi-session user logs and dialogue histories, which serves as the foundation for PAL-Bench. Furthermore, to improve personalized service-oriented interactions, we propose H$^2$Memory, a hierarchical and heterogeneous memory framework that incorporates retrieval-augmented generation to improve personalized response generation. Comprehensive experiments on both our PAL-Bench and an external dataset demonstrate the effectiveness of the proposed memory framework.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºPAL-BenchåŸºå‡†å’ŒHÂ²Memoryæ¡†æ¶ï¼Œç ”ç©¶æœåŠ¡å¯¼å‘å‹å¯¹è¯åŠ©æ‰‹çš„ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚é€šè¿‡æ„å»ºé¦–ä¸ªä¸­æ–‡å¤šä¼šè¯æ•°æ®é›†PAL-Setï¼Œå¹¶è®¾è®¡ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„å¼‚æ„åˆ†å±‚è®°å¿†æ¶æ„ï¼Œä¼˜åŒ–é•¿æœŸäº¤äº’ä¸­çš„ä¸ªæ€§åŒ–å“åº”ç”Ÿæˆã€‚å®éªŒéªŒè¯äº†è¯¥æ¡†æ¶åœ¨æå‡ä¸ªæ€§åŒ–æœåŠ¡äº¤äº’æ•ˆæœæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.13293v1">Grounded by Experience: Generative Healthcare Prediction Augmented with Hierarchical Agentic Retrieval</a></td><td><details><summary>å±•å¼€</summary>Accurate healthcare prediction is critical for improving patient outcomes and reducing operational costs. Bolstered by growing reasoning capabilities, large language models (LLMs) offer a promising path to enhance healthcare predictions by drawing on their rich parametric knowledge. However, LLMs are prone to factual inaccuracies due to limitations in the reliability and coverage of their embedded knowledge. While retrieval-augmented generation (RAG) frameworks, such as GraphRAG and its variants, have been proposed to mitigate these issues by incorporating external knowledge, they face two key challenges in the healthcare scenario: (1) identifying the clinical necessity to activate the retrieval mechanism, and (2) achieving synergy between the retriever and the generator to craft contextually appropriate retrievals. To address these challenges, we propose GHAR, a \underline{g}enerative \underline{h}ierarchical \underline{a}gentic \underline{R}AG framework that simultaneously resolves when to retrieve and how to optimize the collaboration between submodules in healthcare. Specifically, for the first challenge, we design a dual-agent architecture comprising Agent-Top and Agent-Low. Agent-Top acts as the primary physician, iteratively deciding whether to rely on parametric knowledge or to initiate retrieval, while Agent-Low acts as the consulting service, summarising all task-relevant knowledge once retrieval was triggered. To tackle the second challenge, we innovatively unify the optimization of both agents within a formal Markov Decision Process, designing diverse rewards to align their shared goal of accurate prediction while preserving their distinct roles. Extensive experiments on three benchmark datasets across three popular tasks demonstrate our superiority over state-of-the-art baselines, highlighting the potential of hierarchical agentic RAG in advancing healthcare systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºGHARçš„ç”Ÿæˆå¼åˆ†å±‚ä»£ç†RAGæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŒ»ç–—ä¿å¥é¢„æµ‹ä¸­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å› çŸ¥è¯†å±€é™æ€§å¯¼è‡´çš„äº‹å®é”™è¯¯é—®é¢˜ã€‚GHARé€šè¿‡åŒä»£ç†æ¶æ„ï¼ˆAgent-Topå†³å®šæ˜¯å¦æ£€ç´¢ï¼ŒAgent-Lowæ•´åˆæ£€ç´¢çŸ¥è¯†ï¼‰å’ŒåŸºäºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹çš„ååŒä¼˜åŒ–ï¼Œæ”¹è¿›äº†ä¼ ç»ŸRAGåœ¨åŒ»ç–—åœºæ™¯ä¸‹çš„ä¸¤å¤§æŒ‘æˆ˜ï¼šæ£€ç´¢æ—¶æœºåˆ¤æ–­ä¸æ£€ç´¢-ç”Ÿæˆæ¨¡å—çš„åä½œã€‚å®éªŒè¡¨æ˜å…¶åœ¨å¤šé¡¹åŒ»ç–—ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰åŸºçº¿ï¼Œæ¨åŠ¨äº†RAGåœ¨åŒ»ç–—ç³»ç»Ÿä¸­çš„åº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.13201v1">Cog-RAG: Cognitive-Inspired Dual-Hypergraph with Theme Alignment Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) enhances the response quality and domain-specific performance of large language models (LLMs) by incorporating external knowledge to combat hallucinations. In recent research, graph structures have been integrated into RAG to enhance the capture of semantic relations between entities. However, it primarily focuses on low-order pairwise entity relations, limiting the high-order associations among multiple entities. Hypergraph-enhanced approaches address this limitation by modeling multi-entity interactions via hyperedges, but they are typically constrained to inter-chunk entity-level representations, overlooking the global thematic organization and alignment across chunks. Drawing inspiration from the top-down cognitive process of human reasoning, we propose a theme-aligned dual-hypergraph RAG framework (Cog-RAG) that uses a theme hypergraph to capture inter-chunk thematic structure and an entity hypergraph to model high-order semantic relations. Furthermore, we design a cognitive-inspired two-stage retrieval strategy that first activates query-relevant thematic content from the theme hypergraph, and then guides fine-grained recall and diffusion in the entity hypergraph, achieving semantic alignment and consistent generation from global themes to local details. Our extensive experiments demonstrate that Cog-RAG significantly outperforms existing state-of-the-art baseline approaches.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸»é¢˜å¯¹é½åŒè¶…å›¾çš„RAGæ¡†æ¶ï¼ˆCog-RAGï¼‰ï¼Œé€šè¿‡ä¸»é¢˜è¶…å›¾æ•è·è·¨æ–‡æœ¬å—çš„ä¸»é¢˜ç»“æ„ï¼Œé€šè¿‡å®ä½“è¶…å›¾å»ºæ¨¡é«˜é˜¶è¯­ä¹‰å…³ç³»ï¼Œå¹¶è®¾è®¡äº†ä¸¤é˜¶æ®µæ£€ç´¢ç­–ç•¥ä»¥æå‡è¯­ä¹‰å¯¹é½å’Œç”Ÿæˆä¸€è‡´æ€§ï¼Œå®éªŒè¡¨æ˜å…¶æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.12979v1">RAGPulse: An Open-Source RAG Workload Trace to Optimize RAG Serving Systems</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) is a critical paradigm for building reliable, knowledge-intensive Large Language Model (LLM) applications. However, the multi-stage pipeline (retrieve, generate) and unique workload characteristics (e.g., knowledge dependency) of RAG systems pose significant challenges for serving performance optimization. Existing generic LLM inference traces fail to capture these RAG-specific dynamics, creating a significant performance gap between academic research and real-world deployment. To bridge this gap, this paper introduces RAGPulse, an open-source RAG workload trace dataset. This dataset was collected from an university-wide Q&A system serving that has served more than 40,000 students and faculties since April 2024. We detail RAGPulse's system architecture, its privacy-preserving hash-based data format, and provide an in-depth statistical analysis. Our analysis reveals that real-world RAG workloads exhibit significant temporal locality and a highly skewed hot document access pattern. RAGPulse provides a high-fidelity foundation for researchers to develop and validate novel optimization strategies for RAG systems, such as content-aware batching and retrieval caching, ultimately enhancing the efficiency and reliability of RAG services. The code is available at https://github.com/flashserve/RAGPulse.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†RAGPulseï¼Œä¸€ä¸ªå¼€æºçš„RAGå·¥ä½œè´Ÿè½½è·Ÿè¸ªæ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³RAGç³»ç»Ÿåœ¨æ€§èƒ½ä¼˜åŒ–æ–¹é¢çš„æŒ‘æˆ˜ã€‚è¯¥æ•°æ®é›†æ¥è‡ªä¸€ä¸ªè‡ª2024å¹´4æœˆèµ·ä¸º4ä¸‡å¤šåå¸ˆç”ŸæœåŠ¡çš„å¤§å­¦é—®ç­”ç³»ç»Ÿï¼Œè¯¦ç»†è®°å½•äº†RAGç‰¹æœ‰çš„å·¥ä½œè´Ÿè½½ç‰¹å¾ï¼ˆå¦‚æ—¶é—´å±€éƒ¨æ€§å’Œçƒ­é—¨æ–‡æ¡£è®¿é—®æ¨¡å¼ï¼‰ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›äº†å¼€å‘ä¼˜åŒ–ç­–ç•¥ï¼ˆå¦‚å†…å®¹æ„ŸçŸ¥æ‰¹å¤„ç†å’Œæ£€ç´¢ç¼“å­˜ï¼‰çš„é«˜ä¿çœŸåŸºç¡€ï¼Œä»¥æé«˜RAGæœåŠ¡çš„æ•ˆç‡å’Œå¯é æ€§ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-16
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.13994v1">Hint-Augmented Re-ranking: Efficient Product Search using LLM-Based Query Decomposition</a></td><td><details><summary>å±•å¼€</summary>Search queries with superlatives (e.g., best, most popular) require comparing candidates across multiple dimensions, demanding linguistic understanding and domain knowledge. We show that LLMs can uncover latent intent behind these expressions in e-commerce queries through a framework that extracts structured interpretations or hints. Our approach decomposes queries into attribute-value hints generated concurrently with retrieval, enabling efficient integration into the ranking pipeline. Our method improves search performanc eby 10.9 points in MAP and ranking by 5.9 points in MRR over baselines. Since direct LLM-based reranking faces prohibitive latency, we develop an efficient approach transferring superlative interpretations to lightweight models. Our findings provide insights into how superlative semantics can be represented and transferred between models, advancing linguistic interpretation in retrieval systems while addressing practical deployment constraints.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§£æç”µå•†æŸ¥è¯¢ä¸­éšå«çš„è¶…çº§è¯­ä¹‰ï¼ˆå¦‚â€œæœ€ä½³â€â€œæœ€å—æ¬¢è¿â€ï¼‰çš„æ¡†æ¶ï¼Œé€šè¿‡æå–ç»“æ„åŒ–æç¤ºï¼ˆå±æ€§-å€¼å¯¹ï¼‰å¹¶å°†å…¶ä¸æ£€ç´¢è¿‡ç¨‹ç»“åˆï¼Œä¼˜åŒ–æœç´¢æ’åæ€§èƒ½ã€‚è¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æœç´¢æ•ˆæœï¼ˆMAPæé«˜10.9%ï¼ŒMRRæé«˜5.9%ï¼‰ï¼Œå¹¶é€šè¿‡å°†LLMçš„è¶…çº§è¯­ä¹‰è§£é‡Šè¿ç§»è‡³è½»é‡çº§æ¨¡å‹è§£å†³äº†æ—¶å»¶é—®é¢˜ï¼Œå±äºæ£€ç´¢ä¸ç”ŸæˆååŒä¼˜åŒ–çš„RAGæŠ€æœ¯èŒƒç•´ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.13987v1">Artificial Intelligence Agents in Music Analysis: An Integrative Perspective Based on Two Use Cases</a></td><td><details><summary>å±•å¼€</summary>This paper presents an integrative review and experimental validation of artificial intelligence (AI) agents applied to music analysis and education. We synthesize the historical evolution from rule-based models to contemporary approaches involving deep learning, multi-agent architectures, and retrieval-augmented generation (RAG) frameworks. The pedagogical implications are evaluated through a dual-case methodology: (1) the use of generative AI platforms in secondary education to foster analytical and creative skills; (2) the design of a multiagent system for symbolic music analysis, enabling modular, scalable, and explainable workflows.
  Experimental results demonstrate that AI agents effectively enhance musical pattern recognition, compositional parameterization, and educational feedback, outperforming traditional automated methods in terms of interpretability and adaptability. The findings highlight key challenges concerning transparency, cultural bias, and the definition of hybrid evaluation metrics, emphasizing the need for responsible deployment of AI in educational environments.
  This research contributes to a unified framework that bridges technical, pedagogical, and ethical considerations, offering evidence-based guidance for the design and application of intelligent agents in computational musicology and music education.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç»¼è¿°äº†äººå·¥æ™ºèƒ½åœ¨éŸ³ä¹åˆ†æä¸æ•™è‚²ä¸­çš„åº”ç”¨ï¼Œé‡ç‚¹æ¢è®¨äº†ä»è§„åˆ™æ¨¡å‹åˆ°æ·±åº¦å­¦ä¹ å’ŒRAGæ¡†æ¶çš„æ¼”è¿›ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå’Œç”Ÿæˆå¼AIåœ¨æå‡éŸ³ä¹æ¨¡å¼è¯†åˆ«ã€ä½œæ›²å‚æ•°åŒ–åŠæ•™è‚²åé¦ˆæ–¹é¢çš„æ•ˆæœã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.13526v1">Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models</a></td><td><details><summary>å±•å¼€</summary>Artificial intelligence (AI) is reshaping modern healthcare by advancing disease diagnosis, treatment decision-making, and biomedical research. Among AI technologies, large language models (LLMs) have become especially impactful, enabling deep knowledge extraction and semantic reasoning from complex medical texts. However, effective clinical decision support requires knowledge in structured, interoperable formats. Knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks. Yet, current clinical knowledge graphs still depend heavily on manual curation and rule-based extraction, which is limited by the complexity and contextual ambiguity of medical guidelines and literature. To overcome these challenges, we propose an automated framework that combines retrieval-augmented generation (RAG) with LLMs to construct medical indicator knowledge graphs. The framework incorporates guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability. The resulting knowledge graphs can be integrated into intelligent diagnosis and question-answering systems, accelerating the development of AI-driven healthcare solutions.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œç”¨äºæ„å»ºåŒ»ç–—æŒ‡æ ‡çŸ¥è¯†å›¾è°±ï¼Œä»¥è§£å†³å½“å‰ä¸´åºŠçŸ¥è¯†å›¾è°±ä¾èµ–äººå·¥å’Œè§„åˆ™çš„é™åˆ¶ï¼Œå¹¶æ”¯æŒæ™ºèƒ½è¯Šæ–­å’Œé—®ç­”ç³»ç»Ÿçš„å‘å±•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.13489v1">PolicyBot - Reliable Question Answering over Policy Documents</a></td><td><details><summary>å±•å¼€</summary>All citizens of a country are affected by the laws and policies introduced by their government. These laws and policies serve essential functions for citizens. Such as granting them certain rights or imposing specific obligations. However, these documents are often lengthy, complex, and difficult to navigate, making it challenging for citizens to locate and understand relevant information. This work presents PolicyBot, a retrieval-augmented generation (RAG) system designed to answer user queries over policy documents with a focus on transparency and reproducibility. The system combines domain-specific semantic chunking, multilingual dense embeddings, multi-stage retrieval with reranking, and source-aware generation to provide responses grounded in the original documents. We implemented citation tracing to reduce hallucinations and improve user trust, and evaluated alternative retrieval and generation configurations to identify effective design choices. The end-to-end pipeline is built entirely with open-source tools, enabling easy adaptation to other domains requiring document-grounded question answering. This work highlights design considerations, practical challenges, and lessons learned in deploying trustworthy RAG systems for governance-related contexts.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†PolicyBotç³»ç»Ÿï¼Œä¸€ä¸ªåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯çš„å·¥å…·ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·æŸ¥è¯¢å’Œç†è§£å¤æ‚çš„æ”¿ç­–æ–‡æ¡£ã€‚ç³»ç»Ÿç»“åˆäº†é¢†åŸŸç‰¹å®šçš„è¯­ä¹‰åˆ†å—ã€å¤šè¯­è¨€å¯†é›†åµŒå…¥ã€å¤šé˜¶æ®µæ£€ç´¢ä¸é‡æ’åºä»¥åŠåŸºäºæºæ–‡æ¡£çš„ç”ŸæˆæŠ€æœ¯ï¼Œä»¥æé«˜å›ç­”çš„å‡†ç¡®æ€§å’Œé€æ˜åº¦ï¼Œå¹¶é€šè¿‡å¼•ç”¨è¿½è¸ªå‡å°‘å¹»è§‰å¢å¼ºç”¨æˆ·ä¿¡ä»»ã€‚è®ºæ–‡è¿˜æ¢è®¨äº†ç³»ç»Ÿè®¾è®¡ä¸­çš„æŒ‘æˆ˜å’Œå®é™…éƒ¨ç½²ç»éªŒï¼Œå¼ºè°ƒå…¶å¼€æºç‰¹æ€§ä¾¿äºå…¶ä»–é¢†åŸŸåº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.13415v1">Attention Grounded Enhancement for Visual Document Retrieval</a></td><td><details><summary>å±•å¼€</summary>Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs. Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance. However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match. As a result, retrievers tend to rely on surface-level cues and struggle to capture implicit semantic connections, hindering their ability to handle non-extractive queries. To alleviate this problem, we propose a \textbf{A}ttention-\textbf{G}rounded \textbf{RE}triever \textbf{E}nhancement (AGREE) framework. AGREE leverages cross-modal attention from multimodal large language models as proxy local supervision to guide the identification of relevant document regions. During training, AGREE combines local signals with the global signals to jointly optimize the retriever, enabling it to learn not only whether documents match, but also which content drives relevance. Experiments on the challenging ViDoRe V2 benchmark show that AGREE significantly outperforms the global-supervision-only baseline. Quantitative and qualitative analyses further demonstrate that AGREE promotes deeper alignment between query terms and document regions, moving beyond surface-level matching toward more accurate and interpretable retrieval. Our code is available at: https://anonymous.4open.science/r/AGREE-2025.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºAGREEçš„æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºè§†è§‰æ–‡æ¡£æ£€ç´¢çš„æ€§èƒ½ã€‚å®ƒé€šè¿‡å¼•å…¥å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è·¨æ¨¡æ€æ³¨æ„åŠ›ä½œä¸ºå±€éƒ¨ç›‘ç£ä¿¡å·ï¼Œç»“åˆå…¨å±€ç›‘ç£å…±åŒä¼˜åŒ–æ£€ç´¢æ¨¡å‹ï¼Œä½¿å…¶ä¸ä»…èƒ½åˆ¤æ–­æ–‡æ¡£ç›¸å…³æ€§ï¼Œè¿˜èƒ½è¯†åˆ«æ”¯æŒåŒ¹é…çš„å…·ä½“åŒºåŸŸã€‚è¯¥æ–¹æ³•åœ¨ViDoRe V2åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ£€ç´¢å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ï¼Œå°¤å…¶æ”¹å–„äº†éæŠ½å–å¼æŸ¥è¯¢çš„å¤„ç†èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.13410v2">Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction</a></td><td><details><summary>å±•å¼€</summary>With the rise of smart personal devices, service-oriented human-agent interactions have become increasingly prevalent. This trend highlights the need for personalized dialogue assistants that can understand user-specific traits to accurately interpret requirements and tailor responses to individual preferences. However, existing approaches often overlook the complexities of long-term interactions and fail to capture users' subjective characteristics. To address these gaps, we present PAL-Bench, a new benchmark designed to evaluate the personalization capabilities of service-oriented assistants in long-term user-agent interactions. In the absence of available real-world data, we develop a multi-step LLM-based synthesis pipeline, which is further verified and refined by human annotators. This process yields PAL-Set, the first Chinese dataset comprising multi-session user logs and dialogue histories, which serves as the foundation for PAL-Bench. Furthermore, to improve personalized service-oriented interactions, we propose H$^2$Memory, a hierarchical and heterogeneous memory framework that incorporates retrieval-augmented generation to improve personalized response generation. Comprehensive experiments on both our PAL-Bench and an external dataset demonstrate the effectiveness of the proposed memory framework.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†PAL-BenchåŸºå‡†å’ŒPAL-Setä¸­æ–‡æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°é¢å‘æœåŠ¡çš„ä¸ªæ€§åŒ–å¯¹è¯åŠ©æ‰‹åœ¨é•¿æœŸäº¤äº’ä¸­çš„è¡¨ç°ï¼Œå¹¶å¼•å…¥H$^2$Memoryæ¡†æ¶ï¼Œç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯æå‡ä¸ªæ€§åŒ–å“åº”ç”Ÿæˆèƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.13293v1">Grounded by Experience: Generative Healthcare Prediction Augmented with Hierarchical Agentic Retrieval</a></td><td><details><summary>å±•å¼€</summary>Accurate healthcare prediction is critical for improving patient outcomes and reducing operational costs. Bolstered by growing reasoning capabilities, large language models (LLMs) offer a promising path to enhance healthcare predictions by drawing on their rich parametric knowledge. However, LLMs are prone to factual inaccuracies due to limitations in the reliability and coverage of their embedded knowledge. While retrieval-augmented generation (RAG) frameworks, such as GraphRAG and its variants, have been proposed to mitigate these issues by incorporating external knowledge, they face two key challenges in the healthcare scenario: (1) identifying the clinical necessity to activate the retrieval mechanism, and (2) achieving synergy between the retriever and the generator to craft contextually appropriate retrievals. To address these challenges, we propose GHAR, a \underline{g}enerative \underline{h}ierarchical \underline{a}gentic \underline{R}AG framework that simultaneously resolves when to retrieve and how to optimize the collaboration between submodules in healthcare. Specifically, for the first challenge, we design a dual-agent architecture comprising Agent-Top and Agent-Low. Agent-Top acts as the primary physician, iteratively deciding whether to rely on parametric knowledge or to initiate retrieval, while Agent-Low acts as the consulting service, summarising all task-relevant knowledge once retrieval was triggered. To tackle the second challenge, we innovatively unify the optimization of both agents within a formal Markov Decision Process, designing diverse rewards to align their shared goal of accurate prediction while preserving their distinct roles. Extensive experiments on three benchmark datasets across three popular tasks demonstrate our superiority over state-of-the-art baselines, highlighting the potential of hierarchical agentic RAG in advancing healthcare systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†GHARï¼ˆGenerative Hierarchical Agentric RAGï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŒ»ç–—åœºæ™¯ä¸­æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰ä½•æ—¶æ¿€æ´»æ£€ç´¢æœºåˆ¶ï¼Œä»¥åŠï¼ˆ2ï¼‰å¦‚ä½•ä¼˜åŒ–æ£€ç´¢å™¨ä¸ç”Ÿæˆå™¨çš„ååŒã€‚GHARé‡‡ç”¨åŒä»£ç†æ¶æ„ï¼ˆAgent-Topå’ŒAgent-Lowï¼‰åˆ†åˆ«å†³ç­–æ£€ç´¢éœ€æ±‚å’Œæ±‡æ€»ç›¸å…³çŸ¥è¯†ï¼Œå¹¶é€šè¿‡é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ç»Ÿä¸€ä¼˜åŒ–ä»£ç†ç›®æ ‡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªåŒ»ç–—é¢„æµ‹ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå‡¸æ˜¾äº†å±‚æ¬¡åŒ–ä»£ç†RAGåœ¨åŒ»ç–—ç³»ç»Ÿä¸­çš„æ½œåŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.13201v1">Cog-RAG: Cognitive-Inspired Dual-Hypergraph with Theme Alignment Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) enhances the response quality and domain-specific performance of large language models (LLMs) by incorporating external knowledge to combat hallucinations. In recent research, graph structures have been integrated into RAG to enhance the capture of semantic relations between entities. However, it primarily focuses on low-order pairwise entity relations, limiting the high-order associations among multiple entities. Hypergraph-enhanced approaches address this limitation by modeling multi-entity interactions via hyperedges, but they are typically constrained to inter-chunk entity-level representations, overlooking the global thematic organization and alignment across chunks. Drawing inspiration from the top-down cognitive process of human reasoning, we propose a theme-aligned dual-hypergraph RAG framework (Cog-RAG) that uses a theme hypergraph to capture inter-chunk thematic structure and an entity hypergraph to model high-order semantic relations. Furthermore, we design a cognitive-inspired two-stage retrieval strategy that first activates query-relevant thematic content from the theme hypergraph, and then guides fine-grained recall and diffusion in the entity hypergraph, achieving semantic alignment and consistent generation from global themes to local details. Our extensive experiments demonstrate that Cog-RAG significantly outperforms existing state-of-the-art baseline approaches.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸»é¢˜å¯¹é½çš„åŒè¶…å›¾RAGæ¡†æ¶ï¼ˆCog-RAGï¼‰ï¼Œé€šè¿‡ä¸»é¢˜è¶…å›¾æ•æ‰å—é—´ä¸»é¢˜ç»“æ„å’Œå®ä½“è¶…å›¾å»ºæ¨¡é«˜é˜¶è¯­ä¹‰å…³ç³»ï¼Œç»“åˆä¸¤é˜¶æ®µæ£€ç´¢ç­–ç•¥ï¼ˆå…ˆæ¿€æ´»ä¸»é¢˜å†…å®¹å†ç»†ç²’åº¦å¬å›ï¼‰ï¼Œæ˜¾è‘—æå‡äº†ç°æœ‰RAGæ–¹æ³•çš„æ€§èƒ½ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨å…¨å±€ä¸»é¢˜ç»„ç»‡å’Œé«˜é˜¶å®ä½“å…³è”ä¸Šçš„å±€é™æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.13131v1">MM-Telco: Benchmarks and Multimodal Large Language Models for Telecom Applications</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) have emerged as powerful tools for automating complex reasoning and decision-making tasks. In telecommunications, they hold the potential to transform network optimization, automate troubleshooting, enhance customer support, and ensure regulatory compliance. However, their deployment in telecom is hindered by domain-specific challenges that demand specialized adaptation. To overcome these challenges and to accelerate the adaptation of LLMs for telecom, we propose MM-Telco, a comprehensive suite of multimodal benchmarks and models tailored for the telecom domain. The benchmark introduces various tasks (both text based and image based) that address various practical real-life use cases such as network operations, network management, improving documentation quality, and retrieval of relevant text and images. Further, we perform baseline experiments with various LLMs and VLMs. The models fine-tuned on our dataset exhibit a significant boost in performance. Our experiments also help analyze the weak areas in the working of current state-of-art multimodal LLMs, thus guiding towards further development and research.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºMM-Telcoâ€”â€”ä¸€ä¸ªé’ˆå¯¹ç”µä¿¡é¢†åŸŸå®šåˆ¶çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹å¥—ä»¶ï¼Œå…¶ä»»åŠ¡è®¾è®¡åŒ…å«æ–‡æœ¬å’Œå›¾åƒç›¸å…³ä¿¡æ¯çš„æ£€ç´¢ï¼ˆå¦‚æ–‡æ¡£è´¨é‡æ”¹è¿›ã€å›¾æ–‡æ£€ç´¢ï¼‰ï¼Œå¹¶é€šè¿‡å¾®è°ƒå®éªŒéªŒè¯äº†æ¨¡å‹åœ¨ç”µä¿¡é¢†åŸŸä»»åŠ¡ä¸­çš„æ€§èƒ½æå‡ï¼Œå±äºRAGæŠ€æœ¯åœ¨å®é™…é¢†åŸŸåº”ç”¨çš„ç ”ç©¶èŒƒç•´ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.13118v1">Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction</a></td><td><details><summary>å±•å¼€</summary>Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.</details></td><td><details><summary>å±•å¼€</summary></details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.12979v1">RAGPulse: An Open-Source RAG Workload Trace to Optimize RAG Serving Systems</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) is a critical paradigm for building reliable, knowledge-intensive Large Language Model (LLM) applications. However, the multi-stage pipeline (retrieve, generate) and unique workload characteristics (e.g., knowledge dependency) of RAG systems pose significant challenges for serving performance optimization. Existing generic LLM inference traces fail to capture these RAG-specific dynamics, creating a significant performance gap between academic research and real-world deployment. To bridge this gap, this paper introduces RAGPulse, an open-source RAG workload trace dataset. This dataset was collected from an university-wide Q&A system serving that has served more than 40,000 students and faculties since April 2024. We detail RAGPulse's system architecture, its privacy-preserving hash-based data format, and provide an in-depth statistical analysis. Our analysis reveals that real-world RAG workloads exhibit significant temporal locality and a highly skewed hot document access pattern. RAGPulse provides a high-fidelity foundation for researchers to develop and validate novel optimization strategies for RAG systems, such as content-aware batching and retrieval caching, ultimately enhancing the efficiency and reliability of RAG services. The code is available at https://github.com/flashserve/RAGPulse.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†RAGPulseï¼Œä¸€ä¸ªå¼€æºçš„RAGå·¥ä½œè´Ÿè½½è·Ÿè¸ªæ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³RAGç³»ç»Ÿåœ¨æœåŠ¡æ€§èƒ½ä¼˜åŒ–æ–¹é¢çš„æŒ‘æˆ˜ã€‚æ•°æ®é›†æ¥è‡ªä¸€ä¸ªè‡ª2024å¹´4æœˆèµ·ä¸ºè¶…è¿‡40,000åå­¦ç”Ÿå’Œæ•™èŒå‘˜å·¥æä¾›æœåŠ¡çš„å¤§å­¦é—®ç­”ç³»ç»Ÿï¼Œåˆ†æäº†çœŸå®ä¸–ç•ŒRAGå·¥ä½œè´Ÿè½½çš„æ—¶é—´å±€éƒ¨æ€§å’Œçƒ­é—¨æ–‡æ¡£è®¿é—®æ¨¡å¼ï¼Œä¸ºä¼˜åŒ–RAGç³»ç»Ÿï¼ˆå¦‚å†…å®¹æ„ŸçŸ¥æ‰¹å¤„ç†å’Œæ£€ç´¢ç¼“å­˜ï¼‰æä¾›äº†é«˜ä¿çœŸåŸºç¡€ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.12579v1">Enhancing Conversational Recommender Systems with Tree-Structured Knowledge and Pretrained Language Models</a></td><td><details><summary>å±•å¼€</summary>Recent advances in pretrained language models (PLMs) have significantly improved conversational recommender systems (CRS), enabling more fluent and context-aware interactions. To further enhance accuracy and mitigate hallucination, many methods integrate PLMs with knowledge graphs (KGs), but face key challenges: failing to fully exploit PLM reasoning over graph relationships, indiscriminately incorporating retrieved knowledge without context filtering, and neglecting collaborative preferences in multi-turn dialogues. To this end, we propose PCRS-TKA, a prompt-based framework employing retrieval-augmented generation to integrate PLMs with KGs. PCRS-TKA constructs dialogue-specific knowledge trees from KGs and serializes them into texts, enabling structure-aware reasoning while capturing rich entity semantics. Our approach selectively filters context-relevant knowledge and explicitly models collaborative preferences using specialized supervision signals. A semantic alignment module harmonizes heterogeneous inputs, reducing noise and enhancing accuracy. Extensive experiments demonstrate that PCRS-TKA consistently outperforms all baselines in both recommendation and conversational quality.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºPCRS-TKAçš„æ¡†æ¶ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯å°†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰ä¸çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰ç»“åˆï¼Œä»¥æå‡å¯¹è¯å¼æ¨èç³»ç»Ÿï¼ˆCRSï¼‰çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶ä»çŸ¥è¯†å›¾è°±æ„å»ºå¯¹è¯ç‰¹å®šçš„çŸ¥è¯†æ ‘å¹¶åºåˆ—åŒ–ä¸ºæ–‡æœ¬ï¼Œæ”¯æŒç»“æ„æ„ŸçŸ¥æ¨ç†å’Œå®ä½“è¯­ä¹‰æ•è·ï¼ŒåŒæ—¶é€šè¿‡é€‰æ‹©æ€§çŸ¥è¯†è¿‡æ»¤ã€åä½œåå¥½å»ºæ¨¡åŠè¯­ä¹‰å¯¹é½æ¨¡å—å‡å°‘å™ªå£°å¹¶æé«˜å‡†ç¡®æ€§ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨æ¨èå’Œå¯¹è¯è´¨é‡ä¸Šä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.16691v1">Reproducibility Report: Test-Time Training on Nearest Neighbors for Large Language Models</a></td><td><details><summary>å±•å¼€</summary>We reproduce the central claims of Test-Time Training on Nearest Neighbors for Large Language Models (Hardt and Sun, 2024), which proposes adapting a language model at inference time by fine-tuning on retrieved nearest-neighbor sequences. Using pretrained RoBERTa embeddings indexed with Faiss, we retrieve 20 neighbors per test input and apply one gradient update per neighbor across GPT-2 (117M, 774M), GPT-Neo (1.3B), and R1-Distilled-Qwen2.5-1.5B. Our experiments confirm that test-time training significantly reduces perplexity and bits-per-byte metrics across diverse domains from The Pile, with the largest improvements in structured or specialized datasets such as GitHub and EuroParl. We further validate that models not pretrained on The Pile benefit more from this adaptation than models already trained on similar data, allowing smaller models to approach the performance of larger ones. Due to infrastructure limitations, we introduce a memory-efficient retrieval implementation that loads only required line offsets rather than entire files, reducing RAM requirements from over 128 GB per server to 32 GB. We also extend the original study by evaluating R1-Distilled-Qwen2.5-1.5B, showing that test-time training yields consistent gains even for modern reasoning-optimized architectures. Overall, our results support the robustness and generality of nearest-neighbor test-time training while highlighting practical considerations for reproducing large-scale retrieval-augmented adaptation.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡éªŒè¯äº†é€šè¿‡åœ¨æ¨ç†æ—¶ä½¿ç”¨æ£€ç´¢åˆ°çš„è¿‘é‚»åºåˆ—å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„æ–¹æ³•ï¼ˆTest-Time Training on Nearest Neighborsï¼‰ï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•èƒ½æ˜¾è‘—é™ä½æ¨¡å‹çš„å›°æƒ‘åº¦å’Œæ¯”ç‰¹æ•°æŒ‡æ ‡ï¼Œå°¤å…¶é€‚ç”¨äºç»“æ„åŒ–å’Œä¸“ä¸šæ•°æ®é›†ï¼Œå¹¶æå‡ºäº†å†…å­˜é«˜æ•ˆçš„æ£€ç´¢å®ç°æ–¹å¼ã€‚ç ”ç©¶ç»“æœæ”¯æŒäº†æ£€ç´¢å¢å¼ºé€‚åº”çš„é²æ£’æ€§å’Œæ™®é€‚æ€§ï¼Œå¹¶æ¢è®¨äº†å…¶åœ¨ç°ä»£æ¨ç†ä¼˜åŒ–æ¶æ„ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.12520v1">TAdaRAG: Task Adaptive Retrieval-Augmented Generation via On-the-Fly Knowledge Graph Construction</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) improves large language models by retrieving external knowledge, often truncated into smaller chunks due to the input context window, which leads to information loss, resulting in response hallucinations and broken reasoning chains. Moreover, traditional RAG retrieves unstructured knowledge, introducing irrelevant details that hinder accurate reasoning. To address these issues, we propose TAdaRAG, a novel RAG framework for on-the-fly task-adaptive knowledge graph construction from external sources. Specifically, we design an intent-driven routing mechanism to a domain-specific extraction template, followed by supervised fine-tuning and a reinforcement learning-based implicit extraction mechanism, ensuring concise, coherent, and non-redundant knowledge integration. Evaluations on six public benchmarks and a real-world business benchmark (NowNewsQA) across three backbone models demonstrate that TAdaRAG outperforms existing methods across diverse domains and long-text tasks, highlighting its strong generalization and practical effectiveness.</details></td><td><details><summary>å±•å¼€</summary>  </details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.12495v1">Task-Aware Retrieval Augmentation for Dynamic Recommendation</a></td><td><details><summary>å±•å¼€</summary>Dynamic recommendation systems aim to provide personalized suggestions by modeling temporal user-item interactions across time-series behavioral data. Recent studies have leveraged pre-trained dynamic graph neural networks (GNNs) to learn user-item representations over temporal snapshot graphs. However, fine-tuning GNNs on these graphs often results in generalization issues due to temporal discrepancies between pre-training and fine-tuning stages, limiting the model's ability to capture evolving user preferences. To address this, we propose TarDGR, a task-aware retrieval-augmented framework designed to enhance generalization capability by incorporating task-aware model and retrieval-augmentation. Specifically, TarDGR introduces a Task-Aware Evaluation Mechanism to identify semantically relevant historical subgraphs, enabling the construction of task-specific datasets without manual labeling. It also presents a Graph Transformer-based Task-Aware Model that integrates semantic and structural encodings to assess subgraph relevance. During inference, TarDGR retrieves and fuses task-aware subgraphs with the query subgraph, enriching its representation and mitigating temporal generalization issues. Experiments on multiple large-scale dynamic graph datasets demonstrate that TarDGR consistently outperforms state-of-the-art methods, with extensive empirical evidence underscoring its superior accuracy and generalization capabilities.</details></td><td><details><summary>å±•å¼€</summary></details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.17584v1">LLM-Powered Text-Attributed Graph Anomaly Detection via Retrieval-Augmented Reasoning</a></td><td><details><summary>å±•å¼€</summary>Anomaly detection on attributed graphs plays an essential role in applications such as fraud detection, intrusion monitoring, and misinformation analysis. However, text-attributed graphs (TAGs), in which node information is expressed in natural language, remain underexplored, largely due to the absence of standardized benchmark datasets. In this work, we introduce TAG-AD, a comprehensive benchmark for anomaly node detection on TAGs. TAG-AD leverages large language models (LLMs) to generate realistic anomalous node texts directly in the raw text space, producing anomalies that are semantically coherent yet contextually inconsistent and thus more reflective of real-world irregularities. In addition, TAG-AD incorporates multiple other anomaly types, enabling thorough and reproducible evaluation of graph anomaly detection (GAD) methods. With these datasets, we further benchmark existing unsupervised GNN-based GAD methods as well as zero-shot LLMs for GAD.
  As part of our zero-shot detection setup, we propose a retrieval-augmented generation (RAG)-assisted, LLM-based zero-shot anomaly detection framework. The framework mitigates reliance on brittle, hand-crafted prompts by constructing a global anomaly knowledge base and distilling it into reusable analysis frameworks. Our experimental results reveal a clear division of strengths: LLMs are particularly effective at detecting contextual anomalies, whereas GNN-based methods remain superior for structural anomaly detection. Moreover, RAG-assisted prompting achieves performance comparable to human-designed prompts while eliminating manual prompt engineering, underscoring the practical value of our RAG-assisted zero-shot LLM anomaly detection framework.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†TAG-ADåŸºå‡†æ•°æ®é›†ï¼Œç”¨äºæ–‡æœ¬å±æ€§å›¾ï¼ˆTAGï¼‰çš„å¼‚å¸¸èŠ‚ç‚¹æ£€æµ‹ï¼Œå¹¶å¼€å‘äº†ä¸€ç§åŸºäºRAGçš„é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºå…¨å±€å¼‚å¸¸çŸ¥è¯†åº“æ¥è¾…åŠ©å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ£€æµ‹ä¸Šä¸‹æ–‡å¼‚å¸¸ï¼Œå‡å°‘å¯¹æ‰‹å·¥æç¤ºçš„ä¾èµ–ï¼Œå®éªŒè¡¨æ˜å…¶æ€§èƒ½æ¥è¿‘äººå·¥è®¾è®¡çš„æç¤ºæ–¹æ³•ï¼ŒåŒæ—¶çªæ˜¾äº†RAGåœ¨ç»“åˆLLMä¸å›¾å¼‚å¸¸æ£€æµ‹ä¸­çš„å®ç”¨ä»·å€¼ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-15
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.12579v1">Enhancing Conversational Recommender Systems with Tree-Structured Knowledge and Pretrained Language Models</a></td><td><details><summary>å±•å¼€</summary>Recent advances in pretrained language models (PLMs) have significantly improved conversational recommender systems (CRS), enabling more fluent and context-aware interactions. To further enhance accuracy and mitigate hallucination, many methods integrate PLMs with knowledge graphs (KGs), but face key challenges: failing to fully exploit PLM reasoning over graph relationships, indiscriminately incorporating retrieved knowledge without context filtering, and neglecting collaborative preferences in multi-turn dialogues. To this end, we propose PCRS-TKA, a prompt-based framework employing retrieval-augmented generation to integrate PLMs with KGs. PCRS-TKA constructs dialogue-specific knowledge trees from KGs and serializes them into texts, enabling structure-aware reasoning while capturing rich entity semantics. Our approach selectively filters context-relevant knowledge and explicitly models collaborative preferences using specialized supervision signals. A semantic alignment module harmonizes heterogeneous inputs, reducing noise and enhancing accuracy. Extensive experiments demonstrate that PCRS-TKA consistently outperforms all baselines in both recommendation and conversational quality.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºPCRS-TKAçš„æ¡†æ¶ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯å°†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰ä¸çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰ç»“åˆï¼Œä»¥æå‡å¯¹è¯å¼æ¨èç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œäº¤äº’è´¨é‡ã€‚è¯¥æ–¹æ³•ä»çŸ¥è¯†å›¾è°±æ„å»ºå¯¹è¯ç‰¹å®šçš„çŸ¥è¯†æ ‘å¹¶åºåˆ—åŒ–ä¸ºæ–‡æœ¬ï¼Œæ”¯æŒç»“æ„åŒ–æ¨ç†å’Œå®ä½“è¯­ä¹‰æ•æ‰ï¼ŒåŒæ—¶é€šè¿‡é€‰æ‹©æ€§çŸ¥è¯†è¿‡æ»¤ã€åä½œåå¥½å»ºæ¨¡åŠè¯­ä¹‰å¯¹é½æ¨¡å—ä¼˜åŒ–æ£€ç´¢å†…å®¹ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨æ¨èå’Œå¯¹è¯è´¨é‡ä¸Šå‡ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.16691v1">Reproducibility Report: Test-Time Training on Nearest Neighbors for Large Language Models</a></td><td><details><summary>å±•å¼€</summary>We reproduce the central claims of Test-Time Training on Nearest Neighbors for Large Language Models (Hardt and Sun, 2024), which proposes adapting a language model at inference time by fine-tuning on retrieved nearest-neighbor sequences. Using pretrained RoBERTa embeddings indexed with Faiss, we retrieve 20 neighbors per test input and apply one gradient update per neighbor across GPT-2 (117M, 774M), GPT-Neo (1.3B), and R1-Distilled-Qwen2.5-1.5B. Our experiments confirm that test-time training significantly reduces perplexity and bits-per-byte metrics across diverse domains from The Pile, with the largest improvements in structured or specialized datasets such as GitHub and EuroParl. We further validate that models not pretrained on The Pile benefit more from this adaptation than models already trained on similar data, allowing smaller models to approach the performance of larger ones. Due to infrastructure limitations, we introduce a memory-efficient retrieval implementation that loads only required line offsets rather than entire files, reducing RAM requirements from over 128 GB per server to 32 GB. We also extend the original study by evaluating R1-Distilled-Qwen2.5-1.5B, showing that test-time training yields consistent gains even for modern reasoning-optimized architectures. Overall, our results support the robustness and generality of nearest-neighbor test-time training while highlighting practical considerations for reproducing large-scale retrieval-augmented adaptation.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†åœ¨æ¨ç†æ—¶é€šè¿‡æ£€ç´¢æœ€è¿‘é‚»åºåˆ—å¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„æ–¹æ³•ï¼ˆTest-Time Trainingï¼‰ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„RoBERTaåµŒå…¥å’ŒFaissç´¢å¼•æ£€ç´¢é‚»å±…ï¼Œå¹¶åœ¨å¤šä¸ªæ¨¡å‹ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•èƒ½æ˜¾è‘—é™ä½å›°æƒ‘åº¦ç­‰æŒ‡æ ‡ï¼Œå°¤å…¶å¯¹æœªé¢„è®­ç»ƒæ•°æ®çš„å°æ¨¡å‹æ•ˆæœæ›´æ˜æ˜¾ã€‚ç ”ç©¶è¿˜æ”¹è¿›äº†å†…å­˜é«˜æ•ˆçš„æ£€ç´¢å®ç°ï¼Œå¹¶æ‰©å±•äº†åŸå§‹ç ”ç©¶ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ç°ä»£æ¨ç†ä¼˜åŒ–æ¶æ„ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œæ”¯æŒäº†æ£€ç´¢å¢å¼ºé€‚åº”çš„é²æ£’æ€§å’Œé€šç”¨æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.12520v1">TAdaRAG: Task Adaptive Retrieval-Augmented Generation via On-the-Fly Knowledge Graph Construction</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) improves large language models by retrieving external knowledge, often truncated into smaller chunks due to the input context window, which leads to information loss, resulting in response hallucinations and broken reasoning chains. Moreover, traditional RAG retrieves unstructured knowledge, introducing irrelevant details that hinder accurate reasoning. To address these issues, we propose TAdaRAG, a novel RAG framework for on-the-fly task-adaptive knowledge graph construction from external sources. Specifically, we design an intent-driven routing mechanism to a domain-specific extraction template, followed by supervised fine-tuning and a reinforcement learning-based implicit extraction mechanism, ensuring concise, coherent, and non-redundant knowledge integration. Evaluations on six public benchmarks and a real-world business benchmark (NowNewsQA) across three backbone models demonstrate that TAdaRAG outperforms existing methods across diverse domains and long-text tasks, highlighting its strong generalization and practical effectiveness.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºTAdaRAGçš„æ–°å‹RAGæ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€ä»»åŠ¡è‡ªé€‚åº”çŸ¥è¯†å›¾è°±æ„å»ºè§£å†³ä¼ ç»ŸRAGå› ä¿¡æ¯æˆªæ–­å’Œæ— å…³ç»†èŠ‚å¯¼è‡´çš„é—®é¢˜ï¼Œç»“åˆæ„å›¾é©±åŠ¨è·¯ç”±å’Œå¼ºåŒ–å­¦ä¹ æœºåˆ¶ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.12495v1">Task-Aware Retrieval Augmentation for Dynamic Recommendation</a></td><td><details><summary>å±•å¼€</summary>Dynamic recommendation systems aim to provide personalized suggestions by modeling temporal user-item interactions across time-series behavioral data. Recent studies have leveraged pre-trained dynamic graph neural networks (GNNs) to learn user-item representations over temporal snapshot graphs. However, fine-tuning GNNs on these graphs often results in generalization issues due to temporal discrepancies between pre-training and fine-tuning stages, limiting the model's ability to capture evolving user preferences. To address this, we propose TarDGR, a task-aware retrieval-augmented framework designed to enhance generalization capability by incorporating task-aware model and retrieval-augmentation. Specifically, TarDGR introduces a Task-Aware Evaluation Mechanism to identify semantically relevant historical subgraphs, enabling the construction of task-specific datasets without manual labeling. It also presents a Graph Transformer-based Task-Aware Model that integrates semantic and structural encodings to assess subgraph relevance. During inference, TarDGR retrieves and fuses task-aware subgraphs with the query subgraph, enriching its representation and mitigating temporal generalization issues. Experiments on multiple large-scale dynamic graph datasets demonstrate that TarDGR consistently outperforms state-of-the-art methods, with extensive empirical evidence underscoring its superior accuracy and generalization capabilities.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†TarDGRæ¡†æ¶ï¼Œé€šè¿‡ä»»åŠ¡æ„ŸçŸ¥çš„æ£€ç´¢å¢å¼ºæ–¹æ³•è§£å†³åŠ¨æ€æ¨èç³»ç»Ÿä¸­å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰åœ¨æ—¶åºæ•°æ®ä¸Šçš„æ³›åŒ–é—®é¢˜ï¼Œç»“åˆå†å²å­å›¾æ£€ç´¢ä¸å›¾Transformeræ¨¡å‹ä»¥æå‡ç”¨æˆ·åå¥½å»ºæ¨¡çš„å‡†ç¡®æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.17584v1">LLM-Powered Text-Attributed Graph Anomaly Detection via Retrieval-Augmented Reasoning</a></td><td><details><summary>å±•å¼€</summary>Anomaly detection on attributed graphs plays an essential role in applications such as fraud detection, intrusion monitoring, and misinformation analysis. However, text-attributed graphs (TAGs), in which node information is expressed in natural language, remain underexplored, largely due to the absence of standardized benchmark datasets. In this work, we introduce TAG-AD, a comprehensive benchmark for anomaly node detection on TAGs. TAG-AD leverages large language models (LLMs) to generate realistic anomalous node texts directly in the raw text space, producing anomalies that are semantically coherent yet contextually inconsistent and thus more reflective of real-world irregularities. In addition, TAG-AD incorporates multiple other anomaly types, enabling thorough and reproducible evaluation of graph anomaly detection (GAD) methods. With these datasets, we further benchmark existing unsupervised GNN-based GAD methods as well as zero-shot LLMs for GAD.
  As part of our zero-shot detection setup, we propose a retrieval-augmented generation (RAG)-assisted, LLM-based zero-shot anomaly detection framework. The framework mitigates reliance on brittle, hand-crafted prompts by constructing a global anomaly knowledge base and distilling it into reusable analysis frameworks. Our experimental results reveal a clear division of strengths: LLMs are particularly effective at detecting contextual anomalies, whereas GNN-based methods remain superior for structural anomaly detection. Moreover, RAG-assisted prompting achieves performance comparable to human-designed prompts while eliminating manual prompt engineering, underscoring the practical value of our RAG-assisted zero-shot LLM anomaly detection framework.</details></td><td><details><summary>å±•å¼€</summary>æœ¬è®ºæ–‡æå‡ºTAG-ADåŸºå‡†æ•°æ®é›†ï¼Œç”¨äºæ–‡æœ¬å±æ€§å›¾ï¼ˆTAGï¼‰çš„å¼‚å¸¸èŠ‚ç‚¹æ£€æµ‹ï¼Œç»“åˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆè¯­ä¹‰è¿è´¯ä½†ä¸Šä¸‹æ–‡ä¸ä¸€è‡´çš„å¼‚å¸¸æ–‡æœ¬ã€‚ä½œè€…è®¾è®¡äº†ä¸€ç§åŸºäºRAGçš„é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºå…¨å±€å¼‚å¸¸çŸ¥è¯†åº“å‡å°‘å¯¹æ‰‹å·¥æç¤ºçš„ä¾èµ–ï¼Œå®éªŒè¡¨æ˜RAGè¾…åŠ©çš„LLMåœ¨ä¸Šä¸‹æ–‡å¼‚å¸¸æ£€æµ‹ä¸Šæ•ˆæœæ˜¾è‘—ï¼Œä¸GNNæ–¹æ³•å½¢æˆäº’è¡¥ä¼˜åŠ¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.12254v1">Mobile-Agent-RAG: Driving Smart Multi-Agent Coordination with Contextual Knowledge Empowerment for Long-Horizon Mobile Automation</a></td><td><details><summary>å±•å¼€</summary>Mobile agents show immense potential, yet current state-of-the-art (SoTA) agents exhibit inadequate success rates on real-world, long-horizon, cross-application tasks. We attribute this bottleneck to the agents' excessive reliance on static, internal knowledge within MLLMs, which leads to two critical failure points: 1) strategic hallucinations in high-level planning and 2) operational errors during low-level execution on user interfaces (UI). The core insight of this paper is that high-level planning and low-level UI operations require fundamentally distinct types of knowledge. Planning demands high-level, strategy-oriented experiences, whereas operations necessitate low-level, precise instructions closely tied to specific app UIs. Motivated by these insights, we propose Mobile-Agent-RAG, a novel hierarchical multi-agent framework that innovatively integrates dual-level retrieval augmentation. At the planning stage, we introduce Manager-RAG to reduce strategic hallucinations by retrieving human-validated comprehensive task plans that provide high-level guidance. At the execution stage, we develop Operator-RAG to improve execution accuracy by retrieving the most precise low-level guidance for accurate atomic actions, aligned with the current app and subtask. To accurately deliver these knowledge types, we construct two specialized retrieval-oriented knowledge bases. Furthermore, we introduce Mobile-Eval-RAG, a challenging benchmark for evaluating such agents on realistic multi-app, long-horizon tasks. Extensive experiments demonstrate that Mobile-Agent-RAG significantly outperforms SoTA baselines, improving task completion rate by 11.0% and step efficiency by 10.2%, establishing a robust paradigm for context-aware, reliable multi-agent mobile automation.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMobile-Agent-RAGçš„æ–°å‹åˆ†å±‚å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡é›†æˆåŒé‡æ£€ç´¢å¢å¼ºæŠ€æœ¯ï¼ˆManager-RAGå’ŒOperator-RAGï¼‰ï¼Œåˆ†åˆ«é’ˆå¯¹é«˜å±‚æ¬¡è§„åˆ’å’Œä½å±‚æ¬¡ç”¨æˆ·ç•Œé¢æ“ä½œæä¾›ç‰¹å®šçŸ¥è¯†æ£€ç´¢ï¼Œä»¥å‡å°‘æˆ˜ç•¥å¹»è§‰å’Œæ‰§è¡Œé”™è¯¯ï¼Œå¹¶ç»“åˆä¸¤ä¸ªä¸“é—¨çš„çŸ¥è¯†åº“å’Œè¯„ä¼°åŸºå‡†Mobile-Eval-RAGï¼Œæ˜¾è‘—æå‡äº†ç§»åŠ¨æ™ºèƒ½ä½“åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„å®Œæˆç‡å’Œæ•ˆç‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.12213v1">MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues</a></td><td><details><summary>å±•å¼€</summary>Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†MME-RAGæ¡†æ¶ï¼Œé€šè¿‡å°†ç»†ç²’åº¦å®ä½“è¯†åˆ«åˆ†è§£ä¸ºç±»å‹çº§åˆ¤æ–­å’Œç‰‡æ®µçº§æå–ä¸¤ä¸ªé˜¶æ®µï¼Œç»“åˆè½»é‡çº§ç®¡ç†å™¨å’Œä¸“ä¸šä¸“å®¶æ¨¡å—ï¼Œå¹¶åˆ©ç”¨KeyInfoæ£€ç´¢å™¨æ³¨å…¥è¯­ä¹‰å¯¹é½çš„å°‘æ ·æœ¬ç¤ºä¾‹ï¼Œå®ç°äº†æ— éœ€é¢å¤–è®­ç»ƒçš„é«˜ç²¾åº¦å’Œé¢†åŸŸè‡ªé€‚åº”å®ä½“æå–ã€‚å®éªŒè¡¨æ˜å…¶åœ¨å¤šä¸ªé¢†åŸŸä¼˜äºç°æœ‰åŸºçº¿ï¼Œä¸”åˆ†å±‚ç»“æ„å’Œæ£€ç´¢æœºåˆ¶å¯¹é²æ£’æ€§å’Œè·¨é¢†åŸŸæ³›åŒ–è‡³å…³é‡è¦ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.12208v1">Debate over Mixed-knowledge: A Robust Multi-Agent Framework for Incomplete Knowledge Graph Question Answering</a></td><td><details><summary>å±•å¼€</summary>Knowledge Graph Question Answering (KGQA) aims to improve factual accuracy by leveraging structured knowledge. However, real-world Knowledge Graphs (KGs) are often incomplete, leading to the problem of Incomplete KGQA (IKGQA). A common solution is to incorporate external data to fill knowledge gaps, but existing methods lack the capacity to adaptively and contextually fuse multiple sources, failing to fully exploit their complementary strengths. To this end, we propose Debate over Mixed-knowledge (DoM), a novel framework that enables dynamic integration of structured and unstructured knowledge for IKGQA. Built upon the Multi-Agent Debate paradigm, DoM assigns specialized agents to perform inference over knowledge graphs and external texts separately, and coordinates their outputs through iterative interaction. It decomposes the input question into sub-questions, retrieves evidence via dual agents (KG and Retrieval-Augmented Generation, RAG), and employs a judge agent to evaluate and aggregate intermediate answers. This collaboration exploits knowledge complementarity and enhances robustness to KG incompleteness. In addition, existing IKGQA datasets simulate incompleteness by randomly removing triples, failing to capture the irregular and unpredictable nature of real-world knowledge incompleteness. To address this, we introduce a new dataset, Incomplete Knowledge Graph WebQuestions, constructed by leveraging real-world knowledge updates. These updates reflect knowledge beyond the static scope of KGs, yielding a more realistic and challenging benchmark. Through extensive experiments, we show that DoM consistently outperforms state-of-the-art baselines.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œDebate over Mixed-knowledge (DoM)â€çš„æ–°æ¡†æ¶ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“è¾©è®ºæ¨¡å¼åŠ¨æ€æ•´åˆç»“æ„åŒ–å’Œéç»“æ„åŒ–çŸ¥è¯†ï¼ˆåŒ…æ‹¬çŸ¥è¯†å›¾è°±å’ŒRAGæ£€ç´¢çš„å¤–éƒ¨æ–‡æœ¬ï¼‰ï¼Œä»¥è§£å†³ä¸å®Œå…¨çŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆIKGQAï¼‰é—®é¢˜ã€‚åŒæ—¶æ„å»ºäº†æ›´çœŸå®çš„è¯„æµ‹æ•°æ®é›†ï¼Œå®éªŒéªŒè¯äº†DoMçš„ä¼˜è¶Šæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.12159v1">CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic</a></td><td><details><summary>å±•å¼€</summary>Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning based optimization, which often suffers from sparse outcome rewards, leading to inefficient exploration and unstable training. We introduce CriticSearch, a fine-grained credit-assignment framework that supplies dense, turn-level feedback via a retrospective critic mechanism. During training, a frozen, asymmetric critique LLM retrospectively evaluates each turn using privileged information from the full trajectory and gold answers, converting these assessments into stable, dense rewards that guide policy improvement. Experimental results across diverse multi-hop reasoning benchmarks demonstrate that CriticSearch consistently outperforms existing baselines, achieving faster convergence, improved training stability, and higher performance.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºCriticSearchçš„ç»†ç²’åº¦ä¿¡ç”¨åˆ†é…æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹è¿›åŸºäºæœç´¢å¼•æ“çš„å·¥å…·é›†æˆæ¨ç†ï¼ˆTIRï¼‰æ–¹æ³•ã€‚é€šè¿‡å¼•å…¥å›é¡¾æ€§è¯„ä»·æœºåˆ¶ï¼Œåˆ©ç”¨å†»ç»“çš„éå¯¹ç§°æ‰¹åˆ¤LLMå¯¹æ¯ä¸ªæ£€ç´¢æ­¥éª¤æä¾›å¯†é›†çš„åé¦ˆï¼Œè§£å†³äº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ä¸­ç¨€ç–å¥–åŠ±å¯¼è‡´çš„è®­ç»ƒä¸ç¨³å®šå’Œä½æ•ˆæ¢ç´¢é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šè·³æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå®ç°äº†æ›´å¿«æ”¶æ•›ã€æ›´é«˜ç¨³å®šæ€§å’Œæ€§èƒ½æå‡ï¼Œå±äºRAGæŠ€æœ¯åœ¨åŠ¨æ€çŸ¥è¯†æ£€ç´¢ä¸ç”ŸæˆååŒä¼˜åŒ–æ–¹å‘çš„æ‰©å±•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.12027v1">GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory</a></td><td><details><summary>å±•å¼€</summary>Long-video understanding remains a significant challenge for Multimodal Large Language Models (MLLMs) due to inherent token limitations and the complexity of capturing long-term temporal dependencies. Existing methods often fail to capture the global context and complex event relationships necessary for deep video reasoning. To address this, we introduce GCAgent, a novel Global-Context-Aware Agent framework that achieves comprehensive long-video understanding. Our core innovation is the Schematic and Narrative Episodic Memory. This memory structurally models events and their causal and temporal relations into a concise, organized context, fundamentally resolving the long-term dependency problem. Operating in a multi-stage Perception-Action-Reflection cycle, our GCAgent utilizes a Memory Manager to retrieve relevant episodic context for robust, context-aware inference. Extensive experiments confirm that GCAgent significantly enhances long-video understanding, achieving up to 23.5\% accuracy improvement on the Video-MME Long split over a strong MLLM baseline. Furthermore, our framework establishes state-of-the-art performance among comparable 7B-scale MLLMs, achieving 73.4\% accuracy on the Long split and the highest overall average (71.9\%) on the Video-MME benchmark, validating our agent-based reasoning paradigm and structured memory for cognitively-inspired long-video understanding.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†GCAgentæ¡†æ¶ï¼Œé€šè¿‡ç»“æ„åŒ–çš„äº‹ä»¶è®°å¿†ï¼ˆSchematic and Narrative Episodic Memoryï¼‰å’Œå¤šé˜¶æ®µçš„æ„ŸçŸ¥-åŠ¨ä½œ-åæ€å¾ªç¯ï¼Œè§£å†³é•¿è§†é¢‘ç†è§£ä¸­çš„é•¿æœŸä¾èµ–é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨Memory Manageræ£€ç´¢ç›¸å…³è®°å¿†ä¸Šä¸‹æ–‡ä»¥å¢å¼ºæ¨ç†ï¼Œç±»ä¼¼RAGçš„æ£€ç´¢å¢å¼ºæœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†é•¿è§†é¢‘ç†è§£çš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.12003v1">Look As You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning</a></td><td><details><summary>å±•å¼€</summary>Aiming to identify precise evidence sources from visual documents, visual evidence attribution for visual document retrieval-augmented generation (VD-RAG) ensures reliable and verifiable predictions from vision-language models (VLMs) in multimodal question answering. Most existing methods adopt end-to-end training to facilitate intuitive answer verification. However, they lack fine-grained supervision and progressive traceability throughout the reasoning process. In this paper, we introduce the Chain-of-Evidence (CoE) paradigm for VD-RAG. CoE unifies Chain-of-Thought (CoT) reasoning and visual evidence attribution by grounding reference elements in reasoning steps to specific regions with bounding boxes and page indexes. To enable VLMs to generate such evidence-grounded reasoning, we propose Look As You Think (LAT), a reinforcement learning framework that trains models to produce verifiable reasoning paths with consistent attribution. During training, LAT evaluates the attribution consistency of each evidence region and provides rewards only when the CoE trajectory yields correct answers, encouraging process-level self-verification. Experiments on vanilla Qwen2.5-VL-7B-Instruct with Paper- and Wiki-VISA benchmarks show that LAT consistently improves the vanilla model in both single- and multi-image settings, yielding average gains of 8.23% in soft exact match (EM) and 47.0% in IoU@0.5. Meanwhile, LAT not only outperforms the supervised fine-tuning baseline, which is trained to directly produce answers with attribution, but also exhibits stronger generalization across domains.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºChain-of-Evidence (CoE)çš„æ–°èŒƒå¼ï¼Œç”¨äºè§†è§‰æ–‡æ¡£æ£€ç´¢å¢å¼ºç”Ÿæˆ(VD-RAG)ï¼Œé€šè¿‡åœ¨æ¨ç†æ­¥éª¤ä¸­å°†å‚è€ƒå…ƒç´ ä¸è§†è§‰æ–‡æ¡£ä¸­çš„ç‰¹å®šåŒºåŸŸå¯¹åº”èµ·æ¥ï¼Œå®ç°ç»†ç²’åº¦ç›‘ç£å’Œæ¸è¿›å¯è¿½æº¯æ€§ã€‚ä½œè€…è¿˜ä»‹ç»äº†Look As You Think (LAT)å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä»¥è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆå¯éªŒè¯çš„è¯æ®é“¾ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.11990v1">Improving Autoformalization Using Direct Dependency Retrieval</a></td><td><details><summary>å±•å¼€</summary>The convergence of deep learning and formal mathematics has spurred research in formal verification. Statement autoformalization, a crucial first step in this process, aims to translate informal descriptions into machine-verifiable representations but remains a significant challenge. The core difficulty lies in the fact that existing methods often suffer from a lack of contextual awareness, leading to hallucination of formal definitions and theorems. Furthermore, current retrieval-augmented approaches exhibit poor precision and recall for formal library dependency retrieval, and lack the scalability to effectively leverage ever-growing public datasets. To bridge this gap, we propose a novel retrieval-augmented framework based on DDR (\textit{Direct Dependency Retrieval}) for statement autoformalization. Our DDR method directly generates candidate library dependencies from natural language mathematical descriptions and subsequently verifies their existence within the formal library via an efficient suffix array check. Leveraging this efficient search mechanism, we constructed a dependency retrieval dataset of over 500,000 samples and fine-tuned a high-precision DDR model. Experimental results demonstrate that our DDR model significantly outperforms SOTA methods in both retrieval precision and recall. Consequently, an autoformalizer equipped with DDR shows consistent performance advantages in both single-attempt accuracy and multi-attempt stability compared to models using traditional selection-based RAG methods.</details></td><td><details><summary>å±•å¼€</summary></details></td></tr></tbody></table>

### ğŸ“… 2025-11-14
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.11347v1">Privacy Challenges and Solutions in Retrieval-Augmented Generation-Enhanced LLMs for Healthcare Chatbots: A Review of Applications, Risks, and Future Directions</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) has rapidly emerged as a transformative approach for integrating large language models into clinical and biomedical workflows. However, privacy risks, such as protected health information (PHI) exposure, remain inconsistently mitigated. This review provides a thorough analysis of the current landscape of RAG applications in healthcare, including (i) sensitive data type across clinical scenarios, (ii) the associated privacy risks, (iii) current and emerging data-privacy protection mechanisms and (iv) future direction for patient data privacy protection. We synthesize 23 articles on RAG applications in healthcare and systematically analyze privacy challenges through a pipeline-structured framework encompassing data storage, transmission, retrieval and generation stages, delineating potential failure modes, their underlying causes in threat models and system mechanisms, and their practical implications. Building on this analysis, we critically review 17 articles on privacy-preserving strategies for RAG systems. Our evaluation reveals critical gaps, including insufficient clinical validation, absence of standardized evaluation frameworks, and lack of automated assessment tools. We propose actionable directions based on these limitations and conclude with a call to action. This review provides researchers and practitioners with a structured framework for understanding privacy vulnerabilities in healthcare RAG and offers a roadmap toward developing systems that achieve both clinical effectiveness and robust privacy preservation.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ˜¯å…³äºRAGåœ¨ä¸´åºŠå’Œç”Ÿç‰©åŒ»å­¦é¢†åŸŸåº”ç”¨çš„ç»¼è¿°ï¼Œé‡ç‚¹åˆ†æäº†éšç§é£é™©ï¼ˆå¦‚å—ä¿æŠ¤çš„å¥åº·ä¿¡æ¯æš´éœ²ï¼‰åŠå…¶ç¼“è§£æªæ–½ï¼ŒåŒ…æ‹¬æ•æ„Ÿæ•°æ®ç±»å‹ã€éšç§ä¿æŠ¤æœºåˆ¶ã€æœªæ¥æ–¹å‘ç­‰ï¼Œå¹¶åŸºäº23ç¯‡RAGåº”ç”¨æ–‡çŒ®å’Œ17ç¯‡éšç§ä¿æŠ¤ç­–ç•¥æ–‡çŒ®ï¼ŒæŒ‡å‡ºäº†å½“å‰ç ”ç©¶çš„ä¸è¶³ï¼ˆå¦‚ä¸´åºŠéªŒè¯ä¸è¶³ã€æ ‡å‡†åŒ–è¯„ä¼°æ¡†æ¶ç¼ºå¤±ï¼‰ï¼Œæå‡ºäº†æ”¹è¿›å»ºè®®å’Œè¡ŒåŠ¨è·¯çº¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.11104v1">CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation</a></td><td><details><summary>å±•å¼€</summary>Instruction-guided text-to-speech (TTS) research has reached a maturity level where excellent speech generation quality is possible on demand, yet two coupled biases persist: accent bias, where models default to dominant phonetic patterns, and linguistic bias, where dialect-specific lexical and cultural cues are ignored. These biases are interdependent, as authentic accent generation requires both accent fidelity and localized text. We present Contextual Linguistic Adaptation and Retrieval for Inclusive TTS sYnthesis (CLARITY), a backbone-agnostic framework that addresses these biases through dual-signal optimization: (i) contextual linguistic adaptation that localizes input text to the target dialect, and (ii) retrieval-augmented accent prompting (RAAP) that supplies accent-consistent speech prompts. Across twelve English accents, CLARITY improves accent accuracy and fairness while maintaining strong perceptual quality.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†CLARITYæ¡†æ¶ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºå£éŸ³æç¤ºï¼ˆRAAPï¼‰å’Œä¸Šä¸‹æ–‡è¯­è¨€é€‚åº”æŠ€æœ¯ï¼Œè§£å†³TTSç³»ç»Ÿä¸­çš„å£éŸ³å’Œè¯­è¨€åè§é—®é¢˜ï¼Œå±äºRAGåœ¨è¯­éŸ³åˆæˆé¢†åŸŸçš„åº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.10902v1">Multimodal Peer Review Simulation with Actionable To-Do Recommendations for Community-Aware Manuscript Revisions</a></td><td><details><summary>å±•å¼€</summary>While large language models (LLMs) offer promising capabilities for automating academic workflows, existing systems for academic peer review remain constrained by text-only inputs, limited contextual grounding, and a lack of actionable feedback. In this work, we present an interactive web-based system for multimodal, community-aware peer review simulation to enable effective manuscript revisions before paper submission. Our framework integrates textual and visual information through multimodal LLMs, enhances review quality via retrieval-augmented generation (RAG) grounded in web-scale OpenReview data, and converts generated reviews into actionable to-do lists using the proposed Action:Objective[\#] format, providing structured and traceable guidance. The system integrates seamlessly into existing academic writing platforms, providing interactive interfaces for real-time feedback and revision tracking. Experimental results highlight the effectiveness of the proposed system in generating more comprehensive and useful reviews aligned with expert standards, surpassing ablated baselines and advancing transparent, human-centered scholarly assistance.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„äº¤äº’å¼å­¦æœ¯åŒè¡Œè¯„å®¡ç³»ç»Ÿï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯åˆ©ç”¨OpenReviewæ•°æ®æå‡è¯„å®¡è´¨é‡ï¼Œå¹¶å°†ç”Ÿæˆçš„è¯„å®¡æ„è§è½¬åŒ–ä¸ºç»“æ„åŒ–å¯æ‰§è¡Œä»»åŠ¡åˆ—è¡¨ï¼Œå®éªŒè¡¨æ˜è¯¥ç³»ç»Ÿèƒ½ç”Ÿæˆæ›´å…¨é¢ä¸”ç¬¦åˆä¸“å®¶æ ‡å‡†çš„è¯„å®¡æ„è§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.10900v1">Expert-Guided Prompting and Retrieval-Augmented Generation for Emergency Medical Service Question Answering</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) have shown promise in medical question answering, yet they often overlook the domain-specific expertise that professionals depend on, such as the clinical subject areas (e.g., trauma, airway) and the certification level (e.g., EMT, Paramedic). Existing approaches typically apply general-purpose prompting or retrieval strategies without leveraging this structured context, limiting performance in high-stakes settings. We address this gap with EMSQA, an 24.3K-question multiple-choice dataset spanning 10 clinical subject areas and 4 certification levels, accompanied by curated, subject area-aligned knowledge bases (40K documents and 2M tokens). Building on EMSQA, we introduce (i) Expert-CoT, a prompting strategy that conditions chain-of-thought (CoT) reasoning on specific clinical subject area and certification level, and (ii) ExpertRAG, a retrieval-augmented generation pipeline that grounds responses in subject area-aligned documents and real-world patient data. Experiments on 4 LLMs show that Expert-CoT improves up to 2.05% over vanilla CoT prompting. Additionally, combining Expert-CoT with ExpertRAG yields up to a 4.59% accuracy gain over standard RAG baselines. Notably, the 32B expertise-augmented LLMs pass all the computer-adaptive EMS certification simulation exams.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†EMSQAæ•°æ®é›†å’ŒExpertRAGæ–¹æ³•ï¼Œé€šè¿‡ç»“åˆä¸´åºŠä¸»é¢˜é¢†åŸŸå’Œè®¤è¯çº§åˆ«çš„ç»“æ„åŒ–çŸ¥è¯†ï¼Œæ”¹è¿›æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨åŒ»ç–—é—®ç­”ä¸­çš„è¡¨ç°ã€‚è®ºæ–‡è¿˜å¼•å…¥äº†Expert-CoTæç¤ºç­–ç•¥ï¼Œå®éªŒæ˜¾ç¤ºExpertRAGä¸Expert-CoTç»“åˆæ˜¾è‘—æå‡äº†æ¨¡å‹å‡†ç¡®æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.10879v1">ICX360: In-Context eXplainability 360 Toolkit</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) have become ubiquitous in everyday life and are entering higher-stakes applications ranging from summarizing meeting transcripts to answering doctors' questions. As was the case with earlier predictive models, it is crucial that we develop tools for explaining the output of LLMs, be it a summary, list, response to a question, etc. With these needs in mind, we introduce In-Context Explainability 360 (ICX360), an open-source Python toolkit for explaining LLMs with a focus on the user-provided context (or prompts in general) that are fed to the LLMs. ICX360 contains implementations for three recent tools that explain LLMs using both black-box and white-box methods (via perturbations and gradients respectively). The toolkit, available at https://github.com/IBM/ICX360, contains quick-start guidance materials as well as detailed tutorials covering use cases such as retrieval augmented generation, natural language generation, and jailbreaking.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ICX360ï¼Œä¸€ä¸ªå¼€æºçš„Pythonå·¥å…·åŒ…ï¼Œä¸“æ³¨äºè§£é‡Šå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¾“å‡ºï¼Œç‰¹åˆ«å…³æ³¨ç”¨æˆ·æä¾›çš„ä¸Šä¸‹æ–‡æˆ–æç¤ºã€‚å…¶ä¸­æåˆ°å·¥å…·åŒ…çš„åº”ç”¨æ¡ˆä¾‹åŒ…æ‹¬æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ï¼Œè¯´æ˜è¯¥ç ”ç©¶æ”¯æŒRAGç›¸å…³çš„è§£é‡Šæ€§å·¥å…·å¼€å‘ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-13
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.10375v1">TruthfulRAG: Resolving Factual-level Conflicts in Retrieval-Augmented Generation with Knowledge Graphs</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has emerged as a powerful framework for enhancing the capabilities of Large Language Models (LLMs) by integrating retrieval-based methods with generative models. As external knowledge repositories continue to expand and the parametric knowledge within models becomes outdated, a critical challenge for RAG systems is resolving conflicts between retrieved external information and LLMs' internal knowledge, which can significantly compromise the accuracy and reliability of generated content. However, existing approaches to conflict resolution typically operate at the token or semantic level, often leading to fragmented and partial understanding of factual discrepancies between LLMs' knowledge and context, particularly in knowledge-intensive tasks. To address this limitation, we propose TruthfulRAG, the first framework that leverages Knowledge Graphs (KGs) to resolve factual-level knowledge conflicts in RAG systems. Specifically, TruthfulRAG constructs KGs by systematically extracting triples from retrieved content, utilizes query-based graph retrieval to identify relevant knowledge, and employs entropy-based filtering mechanisms to precisely locate conflicting elements and mitigate factual inconsistencies, thereby enabling LLMs to generate faithful and accurate responses. Extensive experiments reveal that TruthfulRAG outperforms existing methods, effectively alleviating knowledge conflicts and improving the robustness and trustworthiness of RAG systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºTruthfulRAGçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³RAGç³»ç»Ÿä¸­å¤–éƒ¨æ£€ç´¢ä¿¡æ¯ä¸å¤§å‹è¯­è¨€æ¨¡å‹å†…éƒ¨çŸ¥è¯†ä¹‹é—´çš„å†²çªé—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰åœ¨äº‹å®å±‚é¢è¯†åˆ«å’Œç¼“è§£çŸ¥è¯†å†²çªï¼ŒTruthfulRAGé‡‡ç”¨ä¸‰å…ƒç»„æå–ã€åŸºäºæŸ¥è¯¢çš„å›¾æ£€ç´¢å’ŒåŸºäºç†µçš„è¿‡æ»¤æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å‡è½»çŸ¥è¯†å†²çªå’Œå¢å¼ºRAGç³»ç»Ÿé²æ£’æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.09980v1">Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG</a></td><td><details><summary>å±•å¼€</summary>Dynamic retrieval-augmented generation (RAG) allows large language models (LLMs) to fetch external knowledge on demand, offering greater adaptability than static RAG. A central challenge in this setting lies in determining the optimal timing for retrieval. Existing methods often trigger retrieval based on low token-level confidence, which may lead to delayed intervention after errors have already propagated. We introduce Entropy-Trend Constraint (ETC), a training-free method that determines optimal retrieval timing by modeling the dynamics of token-level uncertainty. Specifically, ETC utilizes first- and second-order differences of the entropy sequence to detect emerging uncertainty trends, enabling earlier and more precise retrieval. Experiments on six QA benchmarks with three LLM backbones demonstrate that ETC consistently outperforms strong baselines while reducing retrieval frequency. ETC is particularly effective in domain-specific scenarios, exhibiting robust generalization capabilities. Ablation studies and qualitative analyses further confirm that trend-aware uncertainty modeling yields more effective retrieval timing. The method is plug-and-play, model-agnostic, and readily integrable into existing decoding pipelines. Implementation code is included in the supplementary materials.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºç†µè¶‹åŠ¿çº¦æŸï¼ˆETCï¼‰çš„åŠ¨æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ï¼Œä¸“æ³¨äºä¼˜åŒ–æ£€ç´¢æ—¶æœºã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–ä½ç½®ä¿¡åº¦è§¦å‘æ£€ç´¢ï¼Œå¯èƒ½å¯¼è‡´å»¶è¿Ÿå¹²é¢„ï¼Œè€ŒETCé€šè¿‡åˆ†æç†µåºåˆ—çš„ä¸€é˜¶å’ŒäºŒé˜¶å·®åˆ†æ¥æ£€æµ‹ä¸ç¡®å®šæ€§è¶‹åŠ¿ï¼Œå®ç°æ›´æ—©ã€æ›´ç²¾å‡†çš„æ£€ç´¢ã€‚å®éªŒè¡¨æ˜ï¼ŒETCåœ¨å…­ä¸ªQAåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå‡å°‘æ£€ç´¢é¢‘ç‡ï¼Œå°¤å…¶åœ¨é¢†åŸŸç‰¹å®šåœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”å…·æœ‰å³æ’å³ç”¨ã€æ¨¡å‹æ— å…³çš„ç‰¹æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.09984v1">Language Drift in Multilingual Retrieval-Augmented Generation: Characterization and Decoding-Time Mitigation</a></td><td><details><summary>å±•å¼€</summary>Multilingual Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to perform knowledge-intensive tasks in multilingual settings by leveraging retrieved documents as external evidence. However, when the retrieved evidence differs in language from the user query and in-context exemplars, the model often exhibits language drift by generating responses in an unintended language. This phenomenon is especially pronounced during reasoning-intensive decoding, such as Chain-of-Thought (CoT) generation, where intermediate steps introduce further language instability. In this paper, we systematically study output language drift in multilingual RAG across multiple datasets, languages, and LLM backbones. Our controlled experiments reveal that the drift results not from comprehension failure but from decoder-level collapse, where dominant token distributions and high-frequency English patterns dominate the intended generation language. We further observe that English serves as a semantic attractor under cross-lingual conditions, emerging as both the strongest interference source and the most frequent fallback language.
  To mitigate this, we propose Soft Constrained Decoding (SCD), a lightweight, training-free decoding strategy that gently steers generation toward the target language by penalizing non-target-language tokens. SCD is model-agnostic and can be applied to any generation algorithm without modifying the architecture or requiring additional data. Experiments across three multilingual datasets and multiple typologically diverse languages show that SCD consistently improves language alignment and task performance, providing an effective and generalizable solution in multilingual RAG.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¤šè¯­è¨€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­çš„è¾“å‡ºè¯­è¨€æ¼‚ç§»é—®é¢˜ï¼Œå‘ç°å½“æ£€ç´¢è¯æ®ä¸æŸ¥è¯¢è¯­è¨€ä¸ä¸€è‡´æ—¶ï¼Œæ¨¡å‹ä¼šå› è§£ç å™¨çº§å´©æºƒè€Œäº§ç”Ÿéç›®æ ‡è¯­è¨€çš„å“åº”ï¼Œå°¤å…¶æ˜¯è‹±è¯­æˆä¸ºä¸»è¦å¹²æ‰°æºã€‚ä½œè€…æå‡ºäº†ä¸€ç§è½»é‡çº§ã€æ— éœ€è®­ç»ƒçš„è½¯çº¦æŸè§£ç ï¼ˆSCDï¼‰ç­–ç•¥ï¼Œé€šè¿‡æƒ©ç½šéç›®æ ‡è¯­è¨€æ ‡è®°æ¥æ”¹å–„è¯­è¨€å¯¹é½å’Œä»»åŠ¡æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.09966v1">REAP: Enhancing RAG with Recursive Evaluation and Adaptive Planning for Multi-Hop Question Answering</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) has been extensively employed to mitigate hallucinations in large language models (LLMs). However, existing methods for multi-hop reasoning tasks often lack global planning, increasing the risk of falling into local reasoning impasses. Insufficient exploitation of retrieved content and the neglect of latent clues fail to ensure the accuracy of reasoning outcomes. To overcome these limitations, we propose Recursive Evaluation and Adaptive Planning (REAP), whose core idea is to explicitly maintain structured sub-tasks and facts related to the current task through the Sub-task Planner (SP) and Fact Extractor (FE) modules. SP maintains a global perspective, guiding the overall reasoning direction and evaluating the task state based on the outcomes of FE, enabling dynamic optimization of the task-solving trajectory. FE performs fine-grained analysis over retrieved content to extract reliable answers and clues. These two modules incrementally enrich a logically coherent representation of global knowledge, enhancing the reliability and the traceability of the reasoning process. Furthermore, we propose a unified task paradigm design that enables effective multi-task fine-tuning, significantly enhancing SP's performance on complex, data-scarce tasks. We conduct extensive experiments on multiple public multi-hop datasets, and the results demonstrate that our method significantly outperforms existing RAG methods in both in-domain and out-of-domain settings, validating its effectiveness in complex multi-hop reasoning tasks.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºREAPï¼ˆRecursive Evaluation and Adaptive Planningï¼‰çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰RAGæ–¹æ³•åœ¨å¤šè·³æ¨ç†ä»»åŠ¡ä¸­ç¼ºä¹å…¨å±€è§„åˆ’ã€æ£€ç´¢å†…å®¹åˆ©ç”¨ä¸è¶³ä»¥åŠå¿½ç•¥æ½œåœ¨çº¿ç´¢çš„é—®é¢˜ã€‚é€šè¿‡å­ä»»åŠ¡è§„åˆ’å™¨ï¼ˆSPï¼‰å’Œäº‹å®æå–å™¨ï¼ˆFEï¼‰æ¨¡å—ï¼ŒREAPåŠ¨æ€ä¼˜åŒ–æ¨ç†è·¯å¾„ï¼Œå¢å¼ºæ¨ç†è¿‡ç¨‹çš„å¯é æ€§å’Œå¯è¿½æº¯æ€§ï¼Œå¹¶åœ¨å¤šè·³æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰RAGæ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.10128v1">RAGFort: Dual-Path Defense Against Proprietary Knowledge Base Extraction in Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) systems deployed over proprietary knowledge bases face growing threats from reconstruction attacks that aggregate model responses to replicate knowledge bases. Such attacks exploit both intra-class and inter-class paths, progressively extracting fine-grained knowledge within topics and diffusing it across semantically related ones, thereby enabling comprehensive extraction of the original knowledge base. However, existing defenses target only one path, leaving the other unprotected. We conduct a systematic exploration to assess the impact of protecting each path independently and find that joint protection is essential for effective defense. Based on this, we propose RAGFort, a structure-aware dual-module defense combining "contrastive reindexing" for inter-class isolation and "constrained cascade generation" for intra-class protection. Experiments across security, performance, and robustness confirm that RAGFort significantly reduces reconstruction success while preserving answer quality, offering comprehensive defense against knowledge base extraction attacks.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æ¢è®¨äº†é’ˆå¯¹RAGç³»ç»Ÿçš„é‡å»ºæ”»å‡»å¨èƒï¼Œæå‡ºäº†ä¸€ç§åä¸ºRAGFortçš„åŒæ¨¡å—é˜²å¾¡æœºåˆ¶ï¼Œé€šè¿‡â€œå¯¹æ¯”é‡æ–°ç´¢å¼•â€å®ç°ç±»é—´éš”ç¦»å’Œâ€œçº¦æŸçº§è”ç”Ÿæˆâ€å®ç°ç±»å†…ä¿æŠ¤ï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆé™ä½çŸ¥è¯†åº“è¢«æå–çš„é£é™©ï¼ŒåŒæ—¶ä¿æŒå›ç­”è´¨é‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.09831v1">Answering Students' Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM</a></td><td><details><summary>å±•å¼€</summary>The course forums are increasingly significant and play vital role in facilitating student discussions and answering their questions related to the course. It provides a platform for students to post their questions related to the content and admin issues related to the course. However, there are several challenges due to the increase in the number of students enrolled in the course. The primary challenge is that students' queries cannot be responded immediately and the instructors have to face lots of repetitive questions. To mitigate these issues, we propose a question answering system based on large language model with retrieval augmented generation (RAG) method. This work focuses on designing a question answering system with open source Large Language Model (LLM) and fine-tuning it on the relevant course dataset. To further improve the performance, we use a local knowledge base and applied RAG method to retrieve relevant documents relevant to students' queries, where the local knowledge base contains all the course content. To mitigate the hallucination of LLMs, We also integrate it with multi chain-of-thought reasoning to overcome the challenge of hallucination in LLMs. In this work, we experiment fine-tuned LLM with RAG method on the HotpotQA dataset. The experimental results demonstrate that the fine-tuned LLM with RAG method has a strong performance on question answering task.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é—®ç­”ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³è¯¾ç¨‹è®ºå›ä¸­å­¦ç”Ÿé—®é¢˜å›å¤å»¶è¿Ÿå’Œé‡å¤æ€§é—®é¢˜ã€‚ç³»ç»Ÿé€šè¿‡æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³è¯¾ç¨‹å†…å®¹ï¼Œç»“åˆå¾®è°ƒçš„LLMå’Œå¤šé“¾å¼æ¨ç†ï¼ˆmulti chain-of-thought reasoningï¼‰æ¥æå‡å›ç­”å‡†ç¡®æ€§å¹¶å‡å°‘å¹»è§‰ã€‚å®éªŒåœ¨HotpotQAæ•°æ®é›†ä¸Šè¿›è¡Œï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.10523v1">Convomem Benchmark: Why Your First 150 Conversations Don't Need RAG</a></td><td><details><summary>å±•å¼€</summary>We introduce a comprehensive benchmark for conversational memory evaluation containing 75,336 question-answer pairs across diverse categories including user facts, assistant recall, abstention, preferences, temporal changes, and implicit connections. While existing benchmarks have advanced the field, our work addresses fundamental challenges in statistical power, data generation consistency, and evaluation flexibility that limit current memory evaluation frameworks. We examine the relationship between conversational memory and retrieval-augmented generation (RAG). While these systems share fundamental architectural patterns--temporal reasoning, implicit extraction, knowledge updates, and graph representations--memory systems have a unique characteristic: they start from zero and grow progressively with each conversation. This characteristic enables naive approaches that would be impractical for traditional RAG. Consistent with recent findings on long context effectiveness, we observe that simple full-context approaches achieve 70-82% accuracy even on our most challenging multi-message evidence cases, while sophisticated RAG-based memory systems like Mem0 achieve only 30-45% when operating on conversation histories under 150 interactions. Our analysis reveals practical transition points: long context excels for the first 30 conversations, remains viable with manageable trade-offs up to 150 conversations, and typically requires hybrid or RAG approaches beyond that point as costs and latencies become prohibitive. These patterns indicate that the small-corpus advantage of conversational memory--where exhaustive search and complete reranking are feasible--deserves dedicated research attention rather than simply applying general RAG solutions to conversation histories.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºè¯„ä¼°å¯¹è¯è®°å¿†çš„ç»¼åˆåŸºå‡†ï¼Œæ¢è®¨äº†å¯¹è¯è®°å¿†ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿçš„å…³ç³»ï¼ŒæŒ‡å‡ºä¸¤è€…åœ¨æ¶æ„ä¸Šç›¸ä¼¼ä½†å¯¹è¯è®°å¿†ç³»ç»Ÿå…·æœ‰ä»é›¶å¼€å§‹é€æ­¥å¢é•¿çš„ç‰¹æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œç®€å•çš„å…¨ä¸Šä¸‹æ–‡æ–¹æ³•åœ¨å°è§„æ¨¡å¯¹è¯ä¸­è¡¨ç°ä¼˜äºå¤æ‚RAGç³»ç»Ÿï¼Œä½†éšç€å¯¹è¯æ¬¡æ•°å¢åŠ ï¼Œæ··åˆæˆ–RAGæ–¹æ³•æ›´ä¸ºé€‚ç”¨ï¼Œå¼ºè°ƒäº†å¯¹è¯è®°å¿†çš„ç‹¬ç‰¹ä¼˜åŠ¿éœ€ä¸“é—¨ç ”ç©¶è€Œéç›´æ¥å¥—ç”¨é€šç”¨RAGæ–¹æ¡ˆã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.10014v1">fastbmRAG: A Fast Graph-Based RAG Framework for Efficient Processing of Large-Scale Biomedical Literature</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) are rapidly transforming various domains, including biomedicine and healthcare, and demonstrate remarkable potential from scientific research to new drug discovery. Graph-based retrieval-augmented generation (RAG) systems, as a useful application of LLMs, can improve contextual reasoning through structured entity and relationship identification from long-context knowledge, e.g. biomedical literature. Even though many advantages over naive RAGs, most of graph-based RAGs are computationally intensive, which limits their application to large-scale dataset. To address this issue, we introduce fastbmRAG, an fast graph-based RAG optimized for biomedical literature. Utilizing well organized structure of biomedical papers, fastbmRAG divides the construction of knowledge graph into two stages, first drafting graphs using abstracts; and second, refining them using main texts guided by vector-based entity linking, which minimizes redundancy and computational load. Our evaluations demonstrate that fastbmRAG is over 10x faster than existing graph-RAG tools and achieve superior coverage and accuracy to input knowledge. FastbmRAG provides a fast solution for quickly understanding, summarizing, and answering questions about biomedical literature on a large scale. FastbmRAG is public available in https://github.com/menggf/fastbmRAG.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†fastbmRAGï¼Œä¸€ç§é’ˆå¯¹ç”Ÿç‰©åŒ»å­¦æ–‡çŒ®ä¼˜åŒ–çš„å¿«é€Ÿå›¾åŸºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé€šè¿‡åˆ†é˜¶æ®µæ„å»ºçŸ¥è¯†å›¾ï¼ˆå…ˆåŸºäºæ‘˜è¦åˆç¨¿ï¼Œå†é€šè¿‡å‘é‡å®ä½“é“¾æ¥æŒ‡å¯¼çš„æ­£æ–‡ç²¾ç‚¼ï¼‰æ˜¾è‘—é™ä½äº†è®¡ç®—è´Ÿæ‹…ï¼Œé€Ÿåº¦æ¯”ç°æœ‰å›¾åŸºRAGå·¥å…·å¿«10å€ä»¥ä¸Šï¼ŒåŒæ—¶ä¿æŒäº†æ›´é«˜çš„çŸ¥è¯†è¦†ç›–ç‡å’Œå‡†ç¡®æ€§ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡ç”Ÿç‰©åŒ»å­¦æ–‡çŒ®çš„ç†è§£ä¸é—®ç­”ä»»åŠ¡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.10297v1">Local Hybrid Retrieval-Augmented Document QA</a></td><td><details><summary>å±•å¼€</summary>Organizations handling sensitive documents face a critical dilemma: adopt cloud-based AI systems that offer powerful question-answering capabilities but compromise data privacy, or maintain local processing that ensures security but delivers poor accuracy. We present a question-answering system that resolves this trade-off by combining semantic understanding with keyword precision, operating entirely on local infrastructure without internet access. Our approach demonstrates that organizations can achieve competitive accuracy on complex queries across legal, scientific, and conversational documents while keeping all data on their machines. By balancing two complementary retrieval strategies and using consumer-grade hardware acceleration, the system delivers reliable answers with minimal errors, letting banks, hospitals, and law firms adopt conversational document AI without transmitting proprietary information to external providers. This work establishes that privacy and performance need not be mutually exclusive in enterprise AI deployment.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æœ¬åœ°åŒ–è¿è¡Œçš„é—®ç­”ç³»ç»Ÿï¼Œé€šè¿‡ç»“åˆè¯­ä¹‰ç†è§£å’Œå…³é”®è¯ç²¾ç¡®æ£€ç´¢ä¸¤ç§äº’è¡¥ç­–ç•¥ï¼Œåœ¨ä¿è¯æ•°æ®éšç§ï¼ˆæ— éœ€è”ç½‘ï¼‰çš„åŒæ—¶ï¼Œå®ç°äº†å¯¹æ³•å¾‹ã€ç§‘å­¦ç­‰å¤æ‚æ–‡æ¡£çš„é«˜æ•ˆæ£€ç´¢ä¸å›ç­”ï¼Œè§£å†³äº†ä¼ä¸šçº§AIéƒ¨ç½²ä¸­éšç§ä¸æ€§èƒ½ä¸å¯å…¼å¾—çš„çŸ›ç›¾ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.09895v1">Simulator and Experience Enhanced Diffusion Model for Comprehensive ECG Generation</a></td><td><details><summary>å±•å¼€</summary>Cardiovascular disease (CVD) is a leading cause of mortality worldwide. Electrocardiograms (ECGs) are the most widely used non-invasive tool for cardiac assessment, yet large, well-annotated ECG corpora are scarce due to cost, privacy, and workflow constraints. Generating ECGs can be beneficial for the mechanistic understanding of cardiac electrical activity, enable the construction of large, heterogeneous, and unbiased datasets, and facilitate privacy-preserving data sharing. Generating realistic ECG signals from clinical context is important yet underexplored. Recent work has leveraged diffusion models for text-to-ECG generation, but two challenges remain: (i) existing methods often overlook the physiological simulator knowledge of cardiac activity; and (ii) they ignore broader, experience-based clinical knowledge grounded in real-world practice. To address these gaps, we propose SE-Diff, a novel physiological simulator and experience enhanced diffusion model for comprehensive ECG generation. SE-Diff integrates a lightweight ordinary differential equation (ODE)-based ECG simulator into the diffusion process via a beat decoder and simulator-consistent constraints, injecting mechanistic priors that promote physiologically plausible waveforms. In parallel, we design an LLM-powered experience retrieval-augmented strategy to inject clinical knowledge, providing more guidance for ECG generation. Extensive experiments on real-world ECG datasets demonstrate that SE-Diff improves both signal fidelity and text-ECG semantic alignment over baselines, proving its superiority for text-to-ECG generation. We further show that the simulator-based and experience-based knowledge also benefit downstream ECG classification.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºSE-Diffçš„æ–°å‹æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºä»æ–‡æœ¬ç”Ÿæˆå¿ƒç”µå›¾ï¼ˆECGï¼‰ã€‚è¯¥æ¨¡å‹é€šè¿‡æ•´åˆåŸºäºODEçš„ç”Ÿç†æ¨¡æ‹Ÿå™¨å’ŒLLMé©±åŠ¨çš„ä¸´åºŠç»éªŒæ£€ç´¢å¢å¼ºç­–ç•¥ï¼Œç»“åˆäº†å¿ƒè„æ´»åŠ¨çš„æœºåˆ¶æ€§å…ˆéªŒçŸ¥è¯†å’Œä¸´åºŠå®è·µçŸ¥è¯†ï¼Œä»è€Œæå‡äº†ç”ŸæˆECGçš„ç”Ÿç†åˆç†æ€§å’Œæ–‡æœ¬-ECGè¯­ä¹‰å¯¹é½æ•ˆæœï¼Œå¹¶éªŒè¯äº†å¯¹ä¸‹æ¸¸ECGåˆ†ç±»ä»»åŠ¡çš„ç›Šå¤„ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.10192v1">Text2SQL-Flow: A Robust SQL-Aware Data Augmentation Framework for Text-to-SQL</a></td><td><details><summary>å±•å¼€</summary>The data-centric paradigm has become pivotal in AI, especially for Text-to-SQL, where performance is limited by scarce, simplistic, and low-diversity datasets. To address this, we propose Text2SQL-Flow, a SQL-aware data augmentation framework that generates large-scale, semantically valid, and structurally diverse Text-to-SQL pairs from minimal seed data. It operates across six augmentation dimensions and integrates an end-to-end pipeline featuring SQL execution verification, natural language question generation, chain-of-thought reasoning traces, and data classification. A modular Database Manager ensures cross-database compatibility and scalability. Using this framework, we build SQLFlow, a high-quality dataset of 89,544 annotated examples. We evaluate SQLFlow in two settings: (1) For open-source LLMs, fine-tuning on SQLFlow consistently improves performance across benchmarks under the same data budget. (2) For closed-source LLMs, we introduce a masked alignment retrieval method that treats SQLFlow as both knowledge base and training data for the retriever. This enables structure-aware example matching by modeling fine-grained alignments between questions and SQL queries. Experiments show our retrieval strategy outperforms existing methods, underscoring the value of SQLFlow's high-fidelity data and our novel technique. Our work establishes a scalable, data-centric foundation for advancing Text-to-SQL systems and highlights the critical role of high-quality structured data in modern AI.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†Text2SQL-Flowæ¡†æ¶ï¼Œé€šè¿‡æ•°æ®å¢å¼ºç”Ÿæˆå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„Text-to-SQLæ•°æ®å¯¹ï¼Œå¹¶æ„å»ºäº†SQLFlowæ•°æ®é›†ã€‚å…¶ä¸­ï¼Œé’ˆå¯¹é—­æºå¤§è¯­è¨€æ¨¡å‹ï¼Œä½œè€…å¼•å…¥äº†ä¸€ç§åŸºäºSQLFlowæ•°æ®é›†ï¼ˆä½œä¸ºçŸ¥è¯†åº“ï¼‰çš„æ©ç å¯¹é½æ£€ç´¢æ–¹æ³•ï¼Œé€šè¿‡å»ºæ¨¡é—®é¢˜ä¸SQLæŸ¥è¯¢é—´çš„ç»†ç²’åº¦å¯¹é½å®ç°ç»“æ„æ„ŸçŸ¥çš„ç¤ºä¾‹åŒ¹é…ï¼Œæå‡äº†æ£€ç´¢æ€§èƒ½ã€‚è¿™ä¸€æ–¹æ³•ä½“ç°äº†RAGæŠ€æœ¯ä¸­æ£€ç´¢ä¸ç”Ÿæˆç»“åˆçš„æ ¸å¿ƒæ€æƒ³ï¼Œå› æ­¤å±äºRAGç›¸å…³ç ”ç©¶ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.10552v1">URaG: Unified Retrieval and Generation in Multimodal LLMs for Efficient Long Document Understanding</a></td><td><details><summary>å±•å¼€</summary>Recent multimodal large language models (MLLMs) still struggle with long document understanding due to two fundamental challenges: information interference from abundant irrelevant content, and the quadratic computational cost of Transformer-based architectures. Existing approaches primarily fall into two categories: token compression, which sacrifices fine-grained details; and introducing external retrievers, which increase system complexity and prevent end-to-end optimization. To address these issues, we conduct an in-depth analysis and observe that MLLMs exhibit a human-like coarse-to-fine reasoning pattern: early Transformer layers attend broadly across the document, while deeper layers focus on relevant evidence pages. Motivated by this insight, we posit that the inherent evidence localization capabilities of MLLMs can be explicitly leveraged to perform retrieval during the reasoning process, facilitating efficient long document understanding. To this end, we propose URaG, a simple-yet-effective framework that Unifies Retrieval and Generation within a single MLLM. URaG introduces a lightweight cross-modal retrieval module that converts the early Transformer layers into an efficient evidence selector, identifying and preserving the most relevant pages while discarding irrelevant content. This design enables the deeper layers to concentrate computational resources on pertinent information, improving both accuracy and efficiency. Extensive experiments demonstrate that URaG achieves state-of-the-art performance while reducing computational overhead by 44-56%. The code is available at https://github.com/shi-yx/URaG.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºURaGçš„æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡ç»Ÿä¸€æ£€ç´¢ä¸ç”Ÿæˆè¿‡ç¨‹æ¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é•¿æ–‡æ¡£ç†è§£ä¸­çš„ä¿¡æ¯å¹²æ‰°å’Œè®¡ç®—æˆæœ¬é—®é¢˜ã€‚URaGåˆ©ç”¨MLLMsè‡ªèº«çš„å±‚æ¬¡åŒ–æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†æ—©æœŸTransformerå±‚æ”¹é€ ä¸ºè½»é‡çº§è·¨æ¨¡æ€æ£€ç´¢æ¨¡å—ï¼ŒåŠ¨æ€ç­›é€‰ç›¸å…³è¯æ®é¡µé¢ä¾›æ·±å±‚å¤„ç†ï¼Œåœ¨æå‡å‡†ç¡®æ€§çš„åŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—å¼€é”€ï¼ˆ44-56%ï¼‰ï¼Œå±äºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯çš„åˆ›æ–°å˜ä½“ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.09918v1">MINDS: A Cross-cultural Dialogue Corpus for Social Norm Classification and Adherence Detection</a></td><td><details><summary>å±•å¼€</summary>Social norms are implicit, culturally grounded expectations that guide interpersonal communication. Unlike factual commonsense, norm reasoning is subjective, context-dependent, and varies across cultures, posing challenges for computational models. Prior works provide valuable normative annotations but mostly target isolated utterances or synthetic dialogues, limiting their ability to capture the fluid, multi-turn nature of real-world conversations. In this work, we present Norm-RAG, a retrieval-augmented, agentic framework for nuanced social norm inference in multi-turn dialogues. Norm-RAG models utterance-level attributes including communicative intent, speaker roles, interpersonal framing, and linguistic cues and grounds them in structured normative documentation retrieved via a novel Semantic Chunking approach. This enables interpretable and context-aware reasoning about norm adherence and violation across multilingual dialogues. We further introduce MINDS (Multilingual Interactions with Norm-Driven Speech), a bilingual dataset comprising 31 multi-turn Mandarin-English and Spanish-English conversations. Each turn is annotated for norm category and adherence status using multi-annotator consensus, reflecting cross-cultural and realistic norm expression. Our experiments show that Norm-RAG improves norm detection and generalization, demonstrates improved performance for culturally adaptive and socially intelligent dialogue systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†Norm-RAGï¼Œä¸€ä¸ªåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ¡†æ¶ï¼Œç”¨äºåœ¨å¤šè½®å¯¹è¯ä¸­è¿›è¡Œç»†è‡´çš„ç¤¾ä¼šè§„èŒƒæ¨ç†ã€‚Norm-RAGé€šè¿‡å»ºæ¨¡è¯è¯­çº§å±æ€§ï¼ˆå¦‚äº¤é™…æ„å›¾ã€è¯´è¯è€…è§’è‰²ç­‰ï¼‰å¹¶ç»“åˆé€šè¿‡æ–°å‹è¯­ä¹‰åˆ†å—æ–¹æ³•æ£€ç´¢çš„ç»“æ„åŒ–è§„èŒƒæ–‡æ¡£ï¼Œå®ç°äº†è·¨è¯­è¨€å¯¹è¯ä¸­è§„èŒƒéµå®ˆä¸è¿åçš„å¯è§£é‡Šå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨ç†ã€‚è®ºæ–‡è¿˜ä»‹ç»äº†åŒè¯­æ•°æ®é›†MINDSï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†Norm-RAGåœ¨è§„èŒƒæ£€æµ‹å’Œæ³›åŒ–æ–¹é¢çš„æ”¹è¿›æ•ˆæœã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.10240v1">ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) demonstrate strong reasoning capabilities but struggle with hallucinations and limited transparency. Recently, KG-enhanced LLMs that integrate knowledge graphs (KGs) have been shown to improve reasoning performance, particularly for complex, knowledge-intensive tasks. However, these methods still face significant challenges, including inaccurate retrieval and reasoning failures, often exacerbated by long input contexts that obscure relevant information or by context constructions that struggle to capture the richer logical directions required by different question types. Furthermore, many of these approaches rely on LLMs to directly retrieve evidence from KGs, and to self-assess the sufficiency of this evidence, which often results in premature or incorrect reasoning. To address the retrieval and reasoning failures, we propose ProgRAG, a multi-hop knowledge graph question answering (KGQA) framework that decomposes complex questions into sub-questions, and progressively extends partial reasoning paths by answering each sub-question. At each step, external retrievers gather candidate evidence, which is then refined through uncertainty-aware pruning by the LLM. Finally, the context for LLM reasoning is optimized by organizing and rearranging the partial reasoning paths obtained from the sub-question answers. Experiments on three well-known datasets demonstrate that ProgRAG outperforms existing baselines in multi-hop KGQA, offering improved reliability and reasoning quality.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºProgRAGçš„å¤šè·³çŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKGQAï¼‰æ¡†æ¶ï¼Œé€šè¿‡å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå­é—®é¢˜å¹¶é€æ­¥æ‰©å±•éƒ¨åˆ†æ¨ç†è·¯å¾„æ¥æ”¹è¿›æ£€ç´¢å’Œæ¨ç†ã€‚å®ƒç»“åˆå¤–éƒ¨æ£€ç´¢å™¨è·å–å€™é€‰è¯æ®ï¼Œå¹¶åˆ©ç”¨LLMè¿›è¡Œä¸ç¡®å®šæ€§æ„ŸçŸ¥å‰ªæï¼Œæœ€ç»ˆä¼˜åŒ–æ¨ç†ä¸Šä¸‹æ–‡ã€‚å®éªŒè¡¨æ˜ProgRAGåœ¨å¤šè·³KGQAä»»åŠ¡ä¸­ä¼˜äºç°æœ‰åŸºçº¿ï¼Œæå‡äº†å¯é æ€§å’Œæ¨ç†è´¨é‡ï¼Œå±äºRAGæŠ€æœ¯çš„æ‰©å±•åº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.10591v1">Mined Prompting and Metadata-Guided Generation for Wound Care Visual Question Answering</a></td><td><details><summary>å±•å¼€</summary>The rapid expansion of asynchronous remote care has intensified provider workload, creating demand for AI systems that can assist clinicians in managing patient queries more efficiently. The MEDIQA-WV 2025 shared task addresses this challenge by focusing on generating free-text responses to wound care queries paired with images. In this work, we present two complementary approaches developed for the English track. The first leverages a mined prompting strategy, where training data is embedded and the top-k most similar examples are retrieved to serve as few-shot demonstrations during generation. The second approach builds on a metadata ablation study, which identified four metadata attributes that consistently enhance response quality. We train classifiers to predict these attributes for test cases and incorporate them into the generation pipeline, dynamically adjusting outputs based on prediction confidence. Experimental results demonstrate that mined prompting improves response relevance, while metadata-guided generation further refines clinical precision. Together, these methods highlight promising directions for developing AI-driven tools that can provide reliable and efficient wound care support.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä¸¤ç§äº’è¡¥çš„æ–¹æ³•æ¥æå‡AIç³»ç»Ÿå¯¹ä¼¤å£æŠ¤ç†æŸ¥è¯¢çš„å“åº”è´¨é‡ï¼Œå…¶ä¸­ç¬¬ä¸€ç§æ–¹æ³•é‡‡ç”¨äº†åŸºäºåµŒå…¥å’Œæ£€ç´¢ç›¸ä¼¼è®­ç»ƒæ ·æœ¬çš„â€œmined promptingâ€ç­–ç•¥ï¼ˆç±»ä¼¼RAGçš„æ£€ç´¢å¢å¼ºæœºåˆ¶ï¼‰ï¼Œç¬¬äºŒç§æ–¹æ³•é€šè¿‡å…ƒæ•°æ®é¢„æµ‹åŠ¨æ€è°ƒæ•´ç”Ÿæˆå†…å®¹ã€‚ä¸¤ç§æ–¹æ³•å…±åŒæé«˜äº†å›ç­”çš„ç›¸å…³æ€§å’Œä¸´åºŠç²¾ç¡®æ€§ï¼Œå±äºRAGæŠ€æœ¯åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.10390v1">MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns</a></td><td><details><summary>å±•å¼€</summary>Document parsing is a core task in document intelligence, supporting applications such as information extraction, retrieval-augmented generation, and automated document analysis. However, real-world documents often feature complex layouts with multi-level tables, embedded images or formulas, and cross-page structures, which remain challenging for existing OCR systems. We introduce MonkeyOCR v1.5, a unified vision-language framework that enhances both layout understanding and content recognition through a two-stage parsing pipeline. The first stage employs a large multimodal model to jointly predict document layout and reading order, leveraging visual information to ensure structural and sequential consistency. The second stage performs localized recognition of text, formulas, and tables within detected regions, maintaining high visual fidelity while reducing error propagation. To address complex table structures, we propose a visual consistency-based reinforcement learning scheme that evaluates recognition quality via render-and-compare alignment, improving structural accuracy without manual annotations. Additionally, two specialized modules, Image-Decoupled Table Parsing and Type-Guided Table Merging, are introduced to enable reliable parsing of tables containing embedded images and reconstruction of tables crossing pages or columns. Comprehensive experiments on OmniDocBench v1.5 demonstrate that MonkeyOCR v1.5 achieves state-of-the-art performance, outperforming PPOCR-VL and MinerU 2.5 while showing exceptional robustness in visually complex document scenarios.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†MonkeyOCR v1.5ï¼Œä¸€ä¸ªå¢å¼ºå¤æ‚æ–‡æ¡£è§£æçš„è§†è§‰è¯­è¨€æ¡†æ¶ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè§£ææµç¨‹ï¼ˆå¸ƒå±€ç†è§£ä¸å†…å®¹è¯†åˆ«ï¼‰æå‡å¤šå…ƒç´ ï¼ˆæ–‡æœ¬ã€è¡¨æ ¼ã€å…¬å¼ç­‰ï¼‰å¤„ç†çš„å‡†ç¡®æ€§ï¼Œå¹¶ç‰¹åˆ«ä¼˜åŒ–äº†è¡¨æ ¼è§£ææŠ€æœ¯ã€‚å…¶åº”ç”¨åœºæ™¯æ˜ç¡®åŒ…å«æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ï¼Œæ—¨åœ¨ä¸ºä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚ä¿¡æ¯æå–å’ŒRAGï¼‰æä¾›é«˜è´¨é‡çš„æ–‡æ¡£è§£ææ”¯æŒã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-11
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.08181v1">MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System</a></td><td><details><summary>å±•å¼€</summary>Recommender systems (RS) are currently being studied to mitigate limitations during cold-start conditions by leveraging modality information or introducing Agent concepts based on the exceptional reasoning capabilities of Large Language Models (LLMs). Meanwhile, food and beverage recommender systems have traditionally used knowledge graph and ontology concepts due to the domain's unique data attributes and relationship characteristics. On this background, we propose MARC, a multimodal and multi-task cocktail recommender system based on Agentic Retrieval-Augmented Generation (RAG) utilizing graph database under cold-start conditions. The proposed system generates high-quality, contextually appropriate answers through two core processes: a task recognition router and a reflection process. The graph database was constructed by processing cocktail data from Kaggle, and its effectiveness was evaluated using 200 manually crafted questions. The evaluation used both LLM-as-a-judge and human evaluation to demonstrate that answers generated via the graph database outperformed those from a simple vector database in terms of quality. The code is available at https://github.com/diddbwls/cocktail_rec_agentrag</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMARCçš„å¤šæ¨¡æ€å¤šä»»åŠ¡é¸¡å°¾é…’æ¨èç³»ç»Ÿï¼ŒåŸºäºä»£ç†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆAgentic RAGï¼‰æŠ€æœ¯ï¼Œåˆ©ç”¨å›¾æ•°æ®åº“è§£å†³å†·å¯åŠ¨é—®é¢˜ã€‚ç³»ç»Ÿé€šè¿‡ä»»åŠ¡è¯†åˆ«è·¯ç”±å™¨å’Œåæ€è¿‡ç¨‹ç”Ÿæˆé«˜è´¨é‡å›ç­”ï¼Œå®éªŒè¡¨æ˜åŸºäºå›¾æ•°æ®åº“çš„ç­”æ¡ˆè´¨é‡ä¼˜äºç®€å•å‘é‡æ•°æ®åº“ï¼Œå¹¶é€šè¿‡äººå·¥å’ŒLLMè¯„ä¼°éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.08505v1">Structured RAG for Answering Aggregative Questions</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has become the dominant approach for answering questions over large corpora. However, current datasets and methods are highly focused on cases where only a small part of the corpus (usually a few paragraphs) is relevant per query, and fail to capture the rich world of aggregative queries. These require gathering information from a large set of documents and reasoning over them. To address this gap, we propose S-RAG, an approach specifically designed for such queries. At ingestion time, S-RAG constructs a structured representation of the corpus; at inference time, it translates natural-language queries into formal queries over said representation. To validate our approach and promote further research in this area, we introduce two new datasets of aggregative queries: HOTELS and WORLD CUP. Experiments with S-RAG on the newly introduced datasets, as well as on a public benchmark, demonstrate that it substantially outperforms both common RAG systems and long-context LLMs.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºS-RAGçš„æ–°æ–¹æ³•ï¼Œä¸“é—¨é’ˆå¯¹éœ€è¦ä»å¤§é‡æ–‡æ¡£ä¸­èšåˆä¿¡æ¯çš„æŸ¥è¯¢ä»»åŠ¡ï¼Œé€šè¿‡æ„å»ºç»“æ„åŒ–è¯­æ–™åº“è¡¨ç¤ºå’Œå°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºå½¢å¼åŒ–æŸ¥è¯¢æ¥æå‡æ€§èƒ½ï¼Œå¹¶åœ¨æ–°æ•°æ®é›†å’Œå…¬å¼€åŸºå‡†ä¸ŠéªŒè¯å…¶ä¼˜äºä¼ ç»ŸRAGç³»ç»Ÿå’Œé•¿ä¸Šä¸‹æ–‡å¤§è¯­è¨€æ¨¡å‹çš„æ•ˆæœã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.08245v1">Prompt Tuning for Natural Language to SQL with Embedding Fine-Tuning and RAG</a></td><td><details><summary>å±•å¼€</summary>This paper introduces an Error Correction through Prompt Tuning for NL-to-SQL, leveraging the latest advancements in generative pre-training-based LLMs and RAG. Our work addresses the crucial need for efficient and accurate translation of natural language queries into SQL expressions in various settings with the growing use of natural language interfaces. We explore the evolution of NLIDBs from early rule-based systems to advanced neural network-driven approaches. Drawing inspiration from the medical diagnostic process, we propose a novel framework integrating an error correction mechanism that diagnoses error types, identifies their causes, provides fixing instructions, and applies these corrections to SQL queries. This approach is further enriched by embedding fine-tuning and RAG, which harnesses external knowledge bases for improved accuracy and transparency. Through comprehensive experiments, we demonstrate that our framework achieves a significant 12 percent accuracy improvement over existing baselines, highlighting its potential to revolutionize data access and handling in contemporary data-driven environments.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆæç¤ºè°ƒä¼˜å’ŒRAGæŠ€æœ¯çš„è‡ªç„¶è¯­è¨€è½¬SQLé”™è¯¯æ ¡æ­£æ¡†æ¶ï¼Œé€šè¿‡è¯Šæ–­é”™è¯¯ç±»å‹ã€åˆ†æåŸå› å¹¶æä¾›ä¿®æ­£æŒ‡ä»¤ï¼Œç»“åˆå¤–éƒ¨çŸ¥è¯†åº“å¢å¼ºå‡†ç¡®æ€§å’Œé€æ˜åº¦ï¼Œå®éªŒæ˜¾ç¤ºå…¶æ€§èƒ½è¾ƒåŸºçº¿æå‡12%ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.08029v1">BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives</a></td><td><details><summary>å±•å¼€</summary>Hard negatives are essential for training effective retrieval models. Hard-negative mining typically relies on ranking documents using cross-encoders or static embedding models based on similarity metrics such as cosine distance. Hard negative mining becomes challenging for biomedical and scientific domains due to the difficulty in distinguishing between source and hard negative documents. However, referenced documents naturally share contextual relevance with the source document but are not duplicates, making them well-suited as hard negatives. In this work, we propose BiCA: Biomedical Dense Retrieval with Citation-Aware Hard Negatives, an approach for hard-negative mining by utilizing citation links in 20,000 PubMed articles for improving a domain-specific small dense retriever. We fine-tune the GTE_small and GTE_Base models using these citation-informed negatives and observe consistent improvements in zero-shot dense retrieval using nDCG@10 for both in-domain and out-of-domain tasks on BEIR and outperform baselines on long-tailed topics in LoTTE using Success@5. Our findings highlight the potential of leveraging document link structure to generate highly informative negatives, enabling state-of-the-art performance with minimal fine-tuning and demonstrating a path towards highly data-efficient domain adaptation.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºBiCAçš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨ç”Ÿç‰©åŒ»å­¦æ–‡çŒ®ä¸­çš„å¼•ç”¨é“¾æ¥æ¥æŒ–æ˜éš¾è´Ÿä¾‹ï¼ˆhard negativesï¼‰ï¼Œä»¥æ”¹è¿›é¢†åŸŸç‰¹å®šçš„å¯†é›†æ£€ç´¢æ¨¡å‹ï¼ˆdense retrieverï¼‰çš„æ€§èƒ½ã€‚ç ”ç©¶åœ¨PubMedæ–‡ç« ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•ï¼Œé€šè¿‡åœ¨GTE_smallå’ŒGTE_Baseæ¨¡å‹ä¸Šå¾®è°ƒï¼Œæ˜¾è‘—æå‡äº†é›¶æ ·æœ¬æ£€ç´¢æ•ˆæœï¼ˆå¦‚nDCG@10æŒ‡æ ‡ï¼‰ï¼Œå¹¶åœ¨BEIRå’ŒLoTTEåŸºå‡†æµ‹è¯•ä¸­è¶…è¶ŠåŸºçº¿ã€‚è¯¥å·¥ä½œå±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨æ–‡æ¡£é“¾æ¥ç»“æ„ç”Ÿæˆé«˜è´¨é‡è´Ÿä¾‹ï¼Œå®ç°é«˜æ•ˆé¢†åŸŸé€‚åº”ï¼Œè¿™ä¸RAGæŠ€æœ¯ä¸­æ£€ç´¢ç»„ä»¶çš„ä¼˜åŒ–å¯†åˆ‡ç›¸å…³ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.08507v1">Introducing A Bangla Sentence - Gloss Pair Dataset for Bangla Sign Language Translation and Research</a></td><td><details><summary>å±•å¼€</summary>Bangla Sign Language (BdSL) translation represents a low-resource NLP task due to the lack of large-scale datasets that address sentence-level translation. Correspondingly, existing research in this field has been limited to word and alphabet level detection. In this work, we introduce Bangla-SGP, a novel parallel dataset consisting of 1,000 human-annotated sentence-gloss pairs which was augmented with around 3,000 synthetically generated pairs using syntactic and morphological rules through a rule-based Retrieval-Augmented Generation (RAG) pipeline. The gloss sequences of the spoken Bangla sentences are made up of individual glosses which are Bangla sign supported words and serve as an intermediate representation for a continuous sign. Our dataset consists of 1000 high quality Bangla sentences that are manually annotated into a gloss sequence by a professional signer. The augmentation process incorporates rule-based linguistic strategies and prompt engineering techniques that we have adopted by critically analyzing our human annotated sentence-gloss pairs and by working closely with our professional signer. Furthermore, we fine-tune several transformer-based models such as mBart50, Google mT5, GPT4.1-nano and evaluate their sentence-to-gloss translation performance using BLEU scores, based on these evaluation metrics we compare the model's gloss-translation consistency across our dataset and the RWTH-PHOENIX-2014T benchmark.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ä»‹ç»äº†Bangla-SGPæ•°æ®é›†ï¼Œå®ƒæ˜¯ä¸€ä¸ªç”¨äºå­ŸåŠ æ‹‰æ‰‹è¯­ç¿»è¯‘çš„ä½èµ„æºå¹³è¡Œæ•°æ®é›†ï¼Œé€šè¿‡åŸºäºè§„åˆ™çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æµç¨‹æ‰©å……äº†äººå·¥æ ‡æ³¨çš„å¥å­-æ‰‹åŠ¿å¯¹ï¼Œå¹¶åˆ©ç”¨è¯­æ³•å’Œå½¢æ€è§„åˆ™ç”Ÿæˆäº†é¢å¤–çš„åˆæˆæ•°æ®ã€‚ä½œè€…è¿˜å¾®è°ƒäº†å¤šç§åŸºäºTransformerçš„æ¨¡å‹ï¼Œè¯„ä¼°äº†å®ƒä»¬åœ¨å¥å­åˆ°æ‰‹åŠ¿ç¿»è¯‘ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå¹¶ä¸åŸºå‡†æ•°æ®é›†è¿›è¡Œäº†æ¯”è¾ƒã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.08274v1">Multi-Agent GraphRAG: A Text-to-Cypher Framework for Labeled Property Graphs</a></td><td><details><summary>å±•å¼€</summary>While Retrieval-Augmented Generation (RAG) methods commonly draw information from unstructured documents, the emerging paradigm of GraphRAG aims to leverage structured data such as knowledge graphs. Most existing GraphRAG efforts focus on Resource Description Framework (RDF) knowledge graphs, relying on triple representations and SPARQL queries. However, the potential of Cypher and Labeled Property Graph (LPG) databases to serve as scalable and effective reasoning engines within GraphRAG pipelines remains underexplored in current research literature. To fill this gap, we propose Multi-Agent GraphRAG, a modular LLM agentic system for text-to-Cypher query generation serving as a natural language interface to LPG-based graph data. Our proof-of-concept system features an LLM-based workflow for automated Cypher queries generation and execution, using Memgraph as the graph database backend. Iterative content-aware correction and normalization, reinforced by an aggregated feedback loop, ensures both semantic and syntactic refinement of generated queries. We evaluate our system on the CypherBench graph dataset covering several general domains with diverse types of queries. In addition, we demonstrate performance of the proposed workflow on a property graph derived from the IFC (Industry Foundation Classes) data, representing a digital twin of a building. This highlights how such an approach can bridge AI with real-world applications at scale, enabling industrial digital automation use cases.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†GraphRAGçš„æ–°èŒƒå¼ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMulti-Agent GraphRAGçš„æ¨¡å—åŒ–LLMä»£ç†ç³»ç»Ÿï¼Œåˆ©ç”¨CypheræŸ¥è¯¢å’Œæ ‡è®°å±æ€§å›¾ï¼ˆLPGï¼‰æ•°æ®åº“ä½œä¸ºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­çš„å¯æ‰©å±•æ¨ç†å¼•æ“ã€‚è¯¥ç³»ç»Ÿé€šè¿‡è‡ªåŠ¨ç”Ÿæˆå’Œæ‰§è¡ŒCypheræŸ¥è¯¢ï¼Œç»“åˆè¯­ä¹‰å’Œè¯­æ³•ä¼˜åŒ–ï¼Œå¹¶åœ¨CypherBenchå’ŒIFCæ•°æ®ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå±•ç¤ºäº†å…¶åœ¨å·¥ä¸šæ•°å­—è‡ªåŠ¨åŒ–ç­‰å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.08343v1">JobSphere: An AI-Powered Multilingual Career Copilot for Government Employment Platforms</a></td><td><details><summary>å±•å¼€</summary>Users of government employment websites commonly face engagement and accessibility challenges linked to navigational complexity, a dearth of language options, and a lack of personalized support. This paper introduces JobSphere, an AI-powered career assistant that is redefining the employment platform in Punjab called PGRKAM. JobSphere employs Retrieval-Augmented Generation (RAG) architecture, and it is multilingual, available in English, Hindi and Punjabi. JobSphere technique uses 4-bit quantization, allowing the platform to deploy on consumer-grade GPUs (i.e., NVIDIA RTX 3050 4GB), making the implementation 89% cheaper than that of cloud-based systems. Key innovations include voice-enabled interaction with the assistant, automated mock tests, resume parsing with skills recognition, and embed-based job recommendation that achieves a precision@10 score of 68%. An evaluation of JobSphere's implementation reveals 94% factual accuracy, a median response time of 1.8 seconds, and a System Usability Scale score of 78.5/100, a 50% improvement compared to the baseline PGRKAM platform context. In conclusion, JobSphere effectively fills significant accessibility gaps for Punjab/Hindi-speaking users in rural locations, while also affirming the users access to trusted job content provided by government agencies.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†JobSphereï¼Œä¸€ä¸ªåŸºäºRAGæ¶æ„çš„å¤šè¯­è¨€AIèŒä¸šåŠ©æ‰‹ï¼Œæ—¨åœ¨æ”¹å–„å°åº¦æ—é®æ™®é‚¦æ”¿åºœå°±ä¸šå¹³å°PGRKAMçš„ç”¨æˆ·ä½“éªŒã€‚å®ƒé€šè¿‡4ä½é‡åŒ–æŠ€æœ¯é™ä½æˆæœ¬ï¼Œæä¾›è¯­éŸ³äº¤äº’ã€æ¨¡æ‹Ÿæµ‹è¯•ã€ç®€å†è§£æåŠç²¾å‡†èŒä½æ¨èï¼Œæ˜¾è‘—æå‡äº†å“åº”é€Ÿåº¦ã€äº‹å®å‡†ç¡®æ€§å’Œç”¨æˆ·æ»¡æ„åº¦ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.07943v1">Thinker: Training LLMs in Hierarchical Thinking for Deep Search via Multi-Turn Interaction</a></td><td><details><summary>å±•å¼€</summary>Efficient retrieval of external knowledge bases and web pages is crucial for enhancing the reasoning abilities of LLMs. Previous works on training LLMs to leverage external retrievers for solving complex problems have predominantly employed end-to-end reinforcement learning. However, these approaches neglect supervision over the reasoning process, making it difficult to guarantee logical coherence and rigor. To address these limitations, we propose Thinker, a hierarchical thinking model for deep search through multi-turn interaction, making the reasoning process supervisable and verifiable. It decomposes complex problems into independently solvable sub-problems, each dually represented in both natural language and an equivalent logical function to support knowledge base and web searches. Concurrently, dependencies between sub-problems are passed as parameters via these logical functions, enhancing the logical coherence of the problem-solving process. To avoid unnecessary external searches, we perform knowledge boundary determination to check if a sub-problem is within the LLM's intrinsic knowledge, allowing it to answer directly. Experimental results indicate that with as few as several hundred training samples, the performance of Thinker is competitive with established baselines. Furthermore, when scaled to the full training set, Thinker significantly outperforms these methods across various datasets and model sizes. The source code is available at https://github.com/OpenSPG/KAG-Thinker.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºThinkeræ¨¡å‹ï¼Œé€šè¿‡å¤šè½®äº¤äº’å¼åˆ†å±‚æ€è€ƒæœºåˆ¶å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤–éƒ¨çŸ¥è¯†æ£€ç´¢ä¸å¤æ‚é—®é¢˜è§£å†³èƒ½åŠ›ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š1ï¼‰å°†é—®é¢˜åˆ†è§£ä¸ºè‡ªç„¶è¯­è¨€ä¸é€»è¾‘å‡½æ•°åŒè¡¨å¾çš„å­é—®é¢˜ï¼Œç¡®ä¿æ¨ç†è¿‡ç¨‹å¯ç›‘ç£ä¸”é€»è¾‘è¿è´¯ï¼›2ï¼‰é€šè¿‡çŸ¥è¯†è¾¹ç•Œåˆ¤å®šé¿å…å†—ä½™æ£€ç´¢ï¼›3ï¼‰å®éªŒè¡¨æ˜ä»…éœ€æ•°ç™¾æ ·æœ¬å³å¯è¾¾åˆ°åŸºçº¿æ°´å¹³ï¼Œå®Œæ•´è®­ç»ƒåæ€§èƒ½æ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.07982v1">NOTAM-Evolve: A Knowledge-Guided Self-Evolving Optimization Framework with LLMs for NOTAM Interpretation</a></td><td><details><summary>å±•å¼€</summary>Accurate interpretation of Notices to Airmen (NOTAMs) is critical for aviation safety, yet their condensed and cryptic language poses significant challenges to both manual and automated processing. Existing automated systems are typically limited to shallow parsing, failing to extract the actionable intelligence needed for operational decisions. We formalize the complete interpretation task as deep parsing, a dual-reasoning challenge requiring both dynamic knowledge grounding (linking the NOTAM to evolving real-world aeronautical data) and schema-based inference (applying static domain rules to deduce operational status). To tackle this challenge, we propose NOTAM-Evolve, a self-evolving framework that enables a large language model (LLM) to autonomously master complex NOTAM interpretation. Leveraging a knowledge graph-enhanced retrieval module for data grounding, the framework introduces a closed-loop learning process where the LLM progressively improves from its own outputs, minimizing the need for extensive human-annotated reasoning traces. In conjunction with this framework, we introduce a new benchmark dataset of 10,000 expert-annotated NOTAMs. Our experiments demonstrate that NOTAM-Evolve achieves a 30.4% absolute accuracy improvement over the base LLM, establishing a new state of the art on the task of structured NOTAM interpretation.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†NOTAM-Evolveæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆçŸ¥è¯†å›¾è°±å¢å¼ºçš„æ£€ç´¢æ¨¡å—ï¼ˆåŠ¨æ€çŸ¥è¯†å…³è”ï¼‰å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é—­ç¯å­¦ä¹ æœºåˆ¶ï¼Œè§£å†³èˆªç©ºNOTAMsçš„æ·±åº¦è§£æé—®é¢˜ã€‚å…¶æ ¸å¿ƒåˆ©ç”¨æ£€ç´¢æŠ€æœ¯è·å–å®æ—¶èˆªç©ºæ•°æ®ä½œä¸ºä¸Šä¸‹æ–‡è¾“å…¥LLMï¼Œå¹¶åŸºäºè‡ªæ¼”åŒ–æœºåˆ¶æå‡æ¨¡å‹æ€§èƒ½ï¼Œå±äºRAGæŠ€æœ¯çš„åº”ç”¨ã€‚å®éªŒè¡¨æ˜è¯¥æ¡†æ¶æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼ŒåŒæ—¶å‘å¸ƒäº†ä¸“å®¶æ ‡æ³¨çš„æ•°æ®é›†ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-10
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.07328v1">Q-RAG: Long Context Multi-step Retrieval via Value-based Embedder Training</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) methods enhance LLM performance by
efficiently filtering relevant context for LLMs, reducing hallucinations and
inference cost. However, most existing RAG methods focus on single-step
retrieval, which is often insufficient for answering complex questions that
require multi-step search. Recently, multi-step retrieval approaches have
emerged, typically involving the fine-tuning of small LLMs to perform
multi-step retrieval. This type of fine-tuning is highly resource-intensive and
does not enable the use of larger LLMs. In this work, we propose Q-RAG, a novel
approach that fine-tunes the Embedder model for multi-step retrieval using
reinforcement learning (RL). Q-RAG offers a competitive, resource-efficient
alternative to existing multi-step retrieval methods for open-domain question
answering and achieves state-of-the-art results on the popular long-context
benchmarks Babilong and RULER for contexts up to 10M tokens.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºQ-RAGçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å¾®è°ƒEmbedderæ¨¡å‹æ¥å®ç°å¤šæ­¥æ£€ç´¢ï¼Œä»¥è§£å†³ç°æœ‰RAGæ–¹æ³•åœ¨å¤æ‚é—®é¢˜ä¸­å•æ­¥æ£€ç´¢ä¸è¶³çš„é—®é¢˜ï¼Œå¹¶åœ¨å¼€æ”¾åŸŸé—®ç­”ä»»åŠ¡ä¸­å–å¾—äº†å…ˆè¿›æˆæœã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.07262v1">AgenticSciML: Collaborative Multi-Agent Systems for Emergent Discovery in Scientific Machine Learning</a></td><td><details><summary>å±•å¼€</summary>Scientific Machine Learning (SciML) integrates data-driven inference with
physical modeling to solve complex problems in science and engineering.
However, the design of SciML architectures, loss formulations, and training
strategies remains an expert-driven research process, requiring extensive
experimentation and problem-specific insights. Here we introduce AgenticSciML,
a collaborative multi-agent system in which over 10 specialized AI agents
collaborate to propose, critique, and refine SciML solutions through structured
reasoning and iterative evolution. The framework integrates structured debate,
retrieval-augmented method memory, and ensemble-guided evolutionary search,
enabling the agents to generate and assess new hypotheses about architectures
and optimization procedures. Across physics-informed learning and operator
learning tasks, the framework discovers solution methods that outperform
single-agent and human-designed baselines by up to four orders of magnitude in
error reduction. The agents produce novel strategies -- including adaptive
mixture-of-expert architectures, decomposition-based PINNs, and
physics-informed operator learning models -- that do not appear explicitly in
the curated knowledge base. These results show that collaborative reasoning
among AI agents can yield emergent methodological innovation, suggesting a path
toward scalable, transparent, and autonomous discovery in scientific computing.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†AgenticSciMLï¼Œä¸€ä¸ªå¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºçš„æ–¹æ³•è®°å¿†ï¼ˆretrieval-augmented method memoryï¼‰ç­‰æœºåˆ¶ï¼Œè®©å¤šä¸ªAIæ™ºèƒ½ä½“åä½œæå‡ºã€è¯„ä¼°å’Œä¼˜åŒ–ç§‘å­¦æœºå™¨å­¦ä¹ ï¼ˆSciMLï¼‰è§£å†³æ–¹æ¡ˆã€‚è¯¥ç³»ç»Ÿåœ¨ç‰©ç†ä¿¡æ¯å­¦ä¹ å’Œç®—å­å­¦ä¹ ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½ç”Ÿæˆè¶…è¶Šäººå·¥è®¾è®¡çš„åˆ›æ–°ç­–ç•¥ï¼Œå±•ç¤ºäº†AIåä½œåœ¨ç§‘å­¦è®¡ç®—ä¸­çš„æ–¹æ³•è®ºåˆ›æ–°æ½œåŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.06973v1">Oh That Looks Familiar: A Novel Similarity Measure for Spreadsheet Template Discovery</a></td><td><details><summary>å±•å¼€</summary>Traditional methods for identifying structurally similar spreadsheets fail to
capture the spatial layouts and type patterns defining templates. To quantify
spreadsheet similarity, we introduce a hybrid distance metric that combines
semantic embeddings, data type information, and spatial positioning. In order
to calculate spreadsheet similarity, our method converts spreadsheets into
cell-level embeddings and then uses aggregation techniques like Chamfer and
Hausdorff distances. Experiments across template families demonstrate superior
unsupervised clustering performance compared to the graph-based Mondrian
baseline, achieving perfect template reconstruction (Adjusted Rand Index of
1.00 versus 0.90) on the FUSTE dataset. Our approach facilitates large-scale
automated template discovery, which in turn enables downstream applications
such as retrieval-augmented generation over tabular collections, model
training, and bulk data cleaning.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ··åˆè·ç¦»åº¦é‡æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆè¯­ä¹‰åµŒå…¥ã€æ•°æ®ç±»å‹å’Œç©ºé—´ä½ç½®æ¥é‡åŒ–ç”µå­è¡¨æ ¼çš„ç›¸ä¼¼æ€§ï¼Œä»è€Œæ”¯æŒå¤§è§„æ¨¡è‡ªåŠ¨åŒ–æ¨¡æ¿å‘ç°ï¼›å…¶åº”ç”¨åœºæ™¯æ˜ç¡®åŒ…å«æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œç”¨äºè¡¨æ ¼æ•°æ®çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.06738v1">Rethinking Retrieval-Augmented Generation for Medicine: A Large-Scale, Systematic Expert Evaluation and Practical Insights</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) are transforming the landscape of medicine, yet
two fundamental challenges persist: keeping up with rapidly evolving medical
knowledge and providing verifiable, evidence-grounded reasoning.
Retrieval-augmented generation (RAG) has been widely adopted to address these
limitations by supplementing model outputs with retrieved evidence. However,
whether RAG reliably achieves these goals remains unclear. Here, we present the
most comprehensive expert evaluation of RAG in medicine to date. Eighteen
medical experts contributed a total of 80,502 annotations, assessing 800 model
outputs generated by GPT-4o and Llama-3.1-8B across 200 real-world patient and
USMLE-style queries. We systematically decomposed the RAG pipeline into three
components: (i) evidence retrieval (relevance of retrieved passages), (ii)
evidence selection (accuracy of evidence usage), and (iii) response generation
(factuality and completeness of outputs). Contrary to expectation, standard RAG
often degraded performance: only 22% of top-16 passages were relevant, evidence
selection remained weak (precision 41-43%, recall 27-49%), and factuality and
completeness dropped by up to 6% and 5%, respectively, compared with non-RAG
variants. Retrieval and evidence selection remain key failure points for the
model, contributing to the overall performance drop. We further show that
simple yet effective strategies, including evidence filtering and query
reformulation, substantially mitigate these issues, improving performance on
MedMCQA and MedXpertQA by up to 12% and 8.2%, respectively. These findings call
for re-examining RAG's role in medicine and highlight the importance of
stage-aware evaluation and deliberate system design for reliable medical LLM
applications.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡å¯¹åŒ»å­¦é¢†åŸŸçš„RAGæŠ€æœ¯è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå‘ç°æ ‡å‡†RAGåœ¨è¯æ®æ£€ç´¢ã€é€‰æ‹©å’Œç”Ÿæˆç¯èŠ‚å­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼ˆå¦‚æ£€ç´¢ç›¸å…³æ€§ä½ã€è¯æ®ä½¿ç”¨ä¸å‡†ç¡®ï¼‰ï¼Œåè€Œé™ä½äº†æ¨¡å‹è¾“å‡ºçš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ï¼Œä½†é€šè¿‡è¯æ®è¿‡æ»¤å’ŒæŸ¥è¯¢é‡æ„ç­‰ç­–ç•¥å¯æœ‰æ•ˆæå‡æ€§èƒ½ã€‚ç ”ç©¶å‘¼åé‡æ–°å®¡è§†RAGåœ¨åŒ»å­¦ä¸­çš„ä½œç”¨ï¼Œå¹¶å¼ºè°ƒåˆ†é˜¶æ®µè¯„ä¼°å’Œç³»ç»Ÿè®¾è®¡çš„é‡è¦æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.06668v1">When Evidence Contradicts: Toward Safer Retrieval-Augmented Generation in Healthcare</a></td><td><details><summary>å±•å¼€</summary>In high-stakes information domains such as healthcare, where large language
models (LLMs) can produce hallucinations or misinformation, retrieval-augmented
generation (RAG) has been proposed as a mitigation strategy, grounding model
outputs in external, domain-specific documents. Yet, this approach can
introduce errors when source documents contain outdated or contradictory
information. This work investigates the performance of five LLMs in generating
RAG-based responses to medicine-related queries. Our contributions are
three-fold: i) the creation of a benchmark dataset using consumer medicine
information documents from the Australian Therapeutic Goods Administration
(TGA), where headings are repurposed as natural language questions, ii) the
retrieval of PubMed abstracts using TGA headings, stratified across multiple
publication years, to enable controlled temporal evaluation of outdated
evidence, and iii) a comparative analysis of the frequency and impact of
outdated or contradictory content on model-generated responses, assessing how
LLMs integrate and reconcile temporally inconsistent information. Our findings
show that contradictions between highly similar abstracts do, in fact, degrade
performance, leading to inconsistencies and reduced factual accuracy in model
answers. These results highlight that retrieval similarity alone is
insufficient for reliable medical RAG and underscore the need for
contradiction-aware filtering strategies to ensure trustworthy responses in
high-stakes domains.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†åœ¨åŒ»ç–—é¢†åŸŸä¸­ä½¿ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯æ—¶ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å› å¤–éƒ¨æ–‡æ¡£ä¸­çš„è¿‡æ—¶æˆ–çŸ›ç›¾ä¿¡æ¯è€Œäº§ç”Ÿçš„é”™è¯¯ã€‚ä½œè€…åˆ›å»ºäº†ä¸€ä¸ªåŸºäºæ¾³å¤§åˆ©äºšæ²»ç–—å•†å“ç®¡ç†å±€ï¼ˆTGAï¼‰è¯å“ä¿¡æ¯çš„åŸºå‡†æ•°æ®é›†ï¼Œé€šè¿‡PubMedæ‘˜è¦åˆ†å±‚æ£€ç´¢ä¸åŒå¹´ä»½çš„æ–‡çŒ®ï¼Œè¯„ä¼°äº†è¿‡æ—¶è¯æ®å¯¹æ¨¡å‹ç”Ÿæˆå›ç­”çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œé«˜åº¦ç›¸ä¼¼ä½†çŸ›ç›¾çš„æ‘˜è¦ä¼šé™ä½æ¨¡å‹æ€§èƒ½ï¼Œå¯¼è‡´å›ç­”ä¸ä¸€è‡´å’Œäº‹å®å‡†ç¡®æ€§ä¸‹é™ï¼Œå¼ºè°ƒä»…ä¾èµ–æ£€ç´¢ç›¸ä¼¼æ€§ä¸è¶³ä»¥ç¡®ä¿å¯é çš„åŒ»ç–—RAGï¼Œéœ€é‡‡ç”¨çŸ›ç›¾æ„ŸçŸ¥çš„è¿‡æ»¤ç­–ç•¥ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.06582v1">TabRAG: Tabular Document Retrieval via Structured Language Representations</a></td><td><details><summary>å±•å¼€</summary>Ingesting data for Retrieval-Augmented Generation (RAG) involves either
fine-tuning the embedding model directly on the target corpus or parsing
documents for embedding model encoding. The former, while accurate, incurs high
computational hardware requirements, while the latter suffers from suboptimal
performance when extracting tabular data. In this work, we address the latter
by presenting TabRAG, a parsing-based RAG pipeline designed to tackle
table-heavy documents via structured language representations. TabRAG
outperforms existing popular parsing-based methods for generation and
retrieval. Code is available at https://github.com/jacobyhsi/TabRAG.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†TabRAGï¼Œä¸€ç§é’ˆå¯¹è¡¨æ ¼å¯†é›†å‹æ–‡æ¡£è®¾è®¡çš„åŸºäºè§£æçš„RAGæµç¨‹ï¼Œé€šè¿‡ç»“æ„åŒ–è¯­è¨€è¡¨ç¤ºæå‡è¡¨æ ¼æ•°æ®æå–æ€§èƒ½ï¼Œä¼˜äºç°æœ‰è§£ææ–¹æ³•ï¼Œè§£å†³äº†ä¼ ç»Ÿè§£ææ–¹æ³•åœ¨è¡¨æ ¼æ•°æ®å¤„ç†ä¸Šçš„ä¸è¶³ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-09
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.07328v1">Q-RAG: Long Context Multi-step Retrieval via Value-based Embedder Training</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) methods enhance LLM performance by efficiently filtering relevant context for LLMs, reducing hallucinations and inference cost. However, most existing RAG methods focus on single-step retrieval, which is often insufficient for answering complex questions that require multi-step search. Recently, multi-step retrieval approaches have emerged, typically involving the fine-tuning of small LLMs to perform multi-step retrieval. This type of fine-tuning is highly resource-intensive and does not enable the use of larger LLMs. In this work, we propose Q-RAG, a novel approach that fine-tunes the Embedder model for multi-step retrieval using reinforcement learning (RL). Q-RAG offers a competitive, resource-efficient alternative to existing multi-step retrieval methods for open-domain question answering and achieves state-of-the-art results on the popular long-context benchmarks Babilong and RULER for contexts up to 10M tokens.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºQ-RAGçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å¾®è°ƒEmbedderæ¨¡å‹å®ç°å¤šæ­¥æ£€ç´¢ï¼Œä»¥è§£å†³ç°æœ‰å•æ­¥æ£€ç´¢RAGæ–¹æ³•åœ¨å›ç­”å¤æ‚é—®é¢˜æ—¶çš„ä¸è¶³ï¼Œå¹¶åœ¨å¼€æ”¾åŸŸé—®ç­”ä»»åŠ¡ä¸­å–å¾—äº†é«˜æ•ˆä¸”å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨é•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.07577v1">A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain</a></td><td><details><summary>å±•å¼€</summary>Existing retrieval-augmented generation (RAG) systems typically use a centralized architecture, causing a high cost of data collection, integration, and management, as well as privacy concerns. There is a great need for a decentralized RAG system that enables foundation models to utilize information directly from data owners who maintain full control over their sources. However, decentralization brings a challenge: the numerous independent data sources vary significantly in reliability, which can diminish retrieval accuracy and response quality. To address this, our decentralized RAG system has a novel reliability scoring mechanism that dynamically evaluates each source based on the quality of responses it contributes to generate and prioritizes high-quality sources during retrieval. To ensure transparency and trust, the scoring process is securely managed through blockchain-based smart contracts, creating verifiable and tamper-proof reliability records without relying on a central authority. We evaluate our decentralized system with two Llama models (3B and 8B) in two simulated environments where six data sources have different levels of reliability. Our system achieves a +10.7\% performance improvement over its centralized counterpart in the real world-like unreliable data environments. Notably, it approaches the upper-bound performance of centralized systems under ideally reliable data environments. The decentralized infrastructure enables secure and trustworthy scoring management, achieving approximately 56\% marginal cost savings through batched update operations. Our code and system are open-sourced at github.com/yining610/Reliable-dRAG.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å»ä¸­å¿ƒåŒ–çš„RAGç³»ç»Ÿï¼Œé€šè¿‡åŠ¨æ€å¯é æ€§è¯„åˆ†æœºåˆ¶å’ŒåŒºå—é“¾æ™ºèƒ½åˆçº¦è§£å†³åˆ†æ•£æ•°æ®æºçš„å¯é æ€§å·®å¼‚é—®é¢˜ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨ä¸å¯é æ•°æ®ç¯å¢ƒä¸­æ€§èƒ½ä¼˜äºé›†ä¸­å¼ç³»ç»Ÿï¼ŒåŒæ—¶å®ç°æˆæœ¬èŠ‚çº¦å’Œé€æ˜ç®¡ç†ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.06668v1">When Evidence Contradicts: Toward Safer Retrieval-Augmented Generation in Healthcare</a></td><td><details><summary>å±•å¼€</summary>In high-stakes information domains such as healthcare, where large language models (LLMs) can produce hallucinations or misinformation, retrieval-augmented generation (RAG) has been proposed as a mitigation strategy, grounding model outputs in external, domain-specific documents. Yet, this approach can introduce errors when source documents contain outdated or contradictory information. This work investigates the performance of five LLMs in generating RAG-based responses to medicine-related queries. Our contributions are three-fold: i) the creation of a benchmark dataset using consumer medicine information documents from the Australian Therapeutic Goods Administration (TGA), where headings are repurposed as natural language questions, ii) the retrieval of PubMed abstracts using TGA headings, stratified across multiple publication years, to enable controlled temporal evaluation of outdated evidence, and iii) a comparative analysis of the frequency and impact of outdated or contradictory content on model-generated responses, assessing how LLMs integrate and reconcile temporally inconsistent information. Our findings show that contradictions between highly similar abstracts do, in fact, degrade performance, leading to inconsistencies and reduced factual accuracy in model answers. These results highlight that retrieval similarity alone is insufficient for reliable medical RAG and underscore the need for contradiction-aware filtering strategies to ensure trustworthy responses in high-stakes domains.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ç ”ç©¶åŒ»ç–—é¢†åŸŸRAGæŠ€æœ¯åº”ç”¨ï¼Œé€šè¿‡æ„å»ºåŸºäºæ¾³å¤§åˆ©äºšè¯ç›‘å±€æ–‡æ¡£å’ŒPubMedæ‘˜è¦çš„åŸºå‡†æ•°æ®é›†ï¼Œåˆ†æäº”ç§å¤§è¯­è¨€æ¨¡å‹åœ¨æ•´åˆæ—¶æ•ˆæ€§/çŸ›ç›¾åŒ»å­¦ä¿¡æ¯æ—¶çš„è¡¨ç°ï¼Œå‘ç°ç›¸ä¼¼ä½†çŸ›ç›¾çš„æ£€ç´¢å†…å®¹ä¼šé™ä½ç­”æ¡ˆå‡†ç¡®æ€§ï¼ŒæŒ‡å‡ºéœ€çŸ›ç›¾æ„ŸçŸ¥è¿‡æ»¤ç­–ç•¥ä»¥ç¡®ä¿é«˜é£é™©é¢†åŸŸå¯é æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.06738v1">Rethinking Retrieval-Augmented Generation for Medicine: A Large-Scale, Systematic Expert Evaluation and Practical Insights</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) are transforming the landscape of medicine, yet two fundamental challenges persist: keeping up with rapidly evolving medical knowledge and providing verifiable, evidence-grounded reasoning. Retrieval-augmented generation (RAG) has been widely adopted to address these limitations by supplementing model outputs with retrieved evidence. However, whether RAG reliably achieves these goals remains unclear. Here, we present the most comprehensive expert evaluation of RAG in medicine to date. Eighteen medical experts contributed a total of 80,502 annotations, assessing 800 model outputs generated by GPT-4o and Llama-3.1-8B across 200 real-world patient and USMLE-style queries. We systematically decomposed the RAG pipeline into three components: (i) evidence retrieval (relevance of retrieved passages), (ii) evidence selection (accuracy of evidence usage), and (iii) response generation (factuality and completeness of outputs). Contrary to expectation, standard RAG often degraded performance: only 22% of top-16 passages were relevant, evidence selection remained weak (precision 41-43%, recall 27-49%), and factuality and completeness dropped by up to 6% and 5%, respectively, compared with non-RAG variants. Retrieval and evidence selection remain key failure points for the model, contributing to the overall performance drop. We further show that simple yet effective strategies, including evidence filtering and query reformulation, substantially mitigate these issues, improving performance on MedMCQA and MedXpertQA by up to 12% and 8.2%, respectively. These findings call for re-examining RAG's role in medicine and highlight the importance of stage-aware evaluation and deliberate system design for reliable medical LLM applications.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡å¯¹åŒ»å­¦é¢†åŸŸçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è¿›è¡Œäº†å…¨é¢ä¸“å®¶è¯„ä¼°ï¼Œå‘ç°æ ‡å‡†RAGæµç¨‹ï¼ˆåŒ…æ‹¬è¯æ®æ£€ç´¢ã€é€‰æ‹©å’Œç”Ÿæˆï¼‰å­˜åœ¨å…³é”®ç¼ºé™·ï¼ˆå¦‚æ£€ç´¢ç›¸å…³æ€§ä½ã€è¯æ®é€‰æ‹©ç²¾åº¦ä¸è¶³ï¼‰ï¼Œåè€Œé™ä½äº†æ¨¡å‹è¾“å‡ºçš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ï¼›ä½†é€šè¿‡ç®€å•ç­–ç•¥ï¼ˆå¦‚è¯æ®è¿‡æ»¤å’ŒæŸ¥è¯¢é‡æ„ï¼‰å¯æ˜¾è‘—æ”¹å–„æ€§èƒ½ã€‚ç ”ç©¶å‘¼åé‡æ–°å®¡è§†RAGåœ¨åŒ»å­¦ä¸­çš„ä½œç”¨ï¼Œå¹¶å¼ºè°ƒåˆ†é˜¶æ®µè¯„ä¼°å’Œç³»ç»Ÿè®¾è®¡çš„é‡è¦æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.07587v1">Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) face fundamental challenges in long-context reasoning: many documents exceed their finite context windows, while performance on texts that do fit degrades with sequence length, necessitating their augmentation with external memory frameworks. Current solutions, which have evolved from retrieval using semantic embeddings to more sophisticated structured knowledge graphs representations for improved sense-making and associativity, are tailored for fact-based retrieval and fail to build the space-time-anchored narrative representations required for tracking entities through episodic events. To bridge this gap, we propose the \textbf{Generative Semantic Workspace} (GSW), a neuro-inspired generative memory framework that builds structured, interpretable representations of evolving situations, enabling LLMs to reason over evolving roles, actions, and spatiotemporal contexts. Our framework comprises an \textit{Operator}, which maps incoming observations to intermediate semantic structures, and a \textit{Reconciler}, which integrates these into a persistent workspace that enforces temporal, spatial, and logical coherence. On the Episodic Memory Benchmark (EpBench) \cite{huet_episodic_2025} comprising corpora ranging from 100k to 1M tokens in length, GSW outperforms existing RAG based baselines by up to \textbf{20\%}. Furthermore, GSW is highly efficient, reducing query-time context tokens by \textbf{51\%} compared to the next most token-efficient baseline, reducing inference time costs considerably. More broadly, GSW offers a concrete blueprint for endowing LLMs with human-like episodic memory, paving the way for more capable agents that can reason over long horizons.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º"ç”Ÿæˆè¯­ä¹‰å·¥ä½œç©ºé—´"(GSW)çš„ç¥ç»å¯å‘å¼ç”Ÿæˆè®°å¿†æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸­çš„å±€é™æ€§ã€‚GSWé€šè¿‡æ„å»ºç»“æ„åŒ–ã€å¯è§£é‡Šçš„æ¼”åŒ–æƒ…å¢ƒè¡¨å¾ï¼Œä½¿LLMsèƒ½å¤Ÿå¯¹è§’è‰²ã€è¡Œä¸ºå’Œæ—¶ç©ºèƒŒæ™¯è¿›è¡Œæ¨ç†ã€‚è¯¥æ¡†æ¶åœ¨åŒ…å«10ä¸‡è‡³100ä¸‡tokençš„Episodic Memory Benchmarkä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ¯”ç°æœ‰RAGåŸºçº¿æ€§èƒ½æå‡20%ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘æŸ¥è¯¢æ—¶ä¸Šä¸‹æ–‡tokenæ•°é‡(51%)å’Œæ¨ç†æ—¶é—´æˆæœ¬ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.07637v1">Private-RAG: Answering Multiple Queries with LLMs while Keeping Your Data Private</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving documents from an external corpus at inference time. When this corpus contains sensitive information, however, unprotected RAG systems are at risk of leaking private information. Prior work has introduced differential privacy (DP) guarantees for RAG, but only in single-query settings, which fall short of realistic usage. In this paper, we study the more practical multi-query setting and propose two DP-RAG algorithms. The first, MURAG, leverages an individual privacy filter so that the accumulated privacy loss only depends on how frequently each document is retrieved rather than the total number of queries. The second, MURAG-ADA, further improves utility by privately releasing query-specific thresholds, enabling more precise selection of relevant documents. Our experiments across multiple LLMs and datasets demonstrate that the proposed methods scale to hundreds of queries within a practical DP budget ($\varepsilon\approx10$), while preserving meaningful utility.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­ä¿æŠ¤æ•æ„Ÿä¿¡æ¯çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸¤ç§å·®åˆ†éšç§ï¼ˆDPï¼‰ç®—æ³•ï¼ˆMURAGå’ŒMURAG-ADAï¼‰ä»¥åº”å¯¹å¤šæŸ¥è¯¢åœºæ™¯ä¸‹çš„éšç§æ³„éœ²é£é™©ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†è¿™äº›æ–¹æ³•åœ¨ä¿æŒå®ç”¨æ€§çš„åŒæ—¶èƒ½æœ‰æ•ˆæ§åˆ¶éšç§é¢„ç®—ï¼ˆ$\varepsilon\approx10$ï¼‰ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.06582v1">TabRAG: Tabular Document Retrieval via Structured Language Representations</a></td><td><details><summary>å±•å¼€</summary>Ingesting data for Retrieval-Augmented Generation (RAG) involves either fine-tuning the embedding model directly on the target corpus or parsing documents for embedding model encoding. The former, while accurate, incurs high computational hardware requirements, while the latter suffers from suboptimal performance when extracting tabular data. In this work, we address the latter by presenting TabRAG, a parsing-based RAG pipeline designed to tackle table-heavy documents via structured language representations. TabRAG outperforms existing popular parsing-based methods for generation and retrieval. Code is available at https://github.com/jacobyhsi/TabRAG.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†TabRAGï¼Œä¸€ç§é’ˆå¯¹è¡¨æ ¼å¯†é›†å‹æ–‡æ¡£çš„è§£æå¼RAGæµç¨‹ï¼Œé€šè¿‡ç»“æ„åŒ–è¯­è¨€è¡¨ç¤ºæå‡è¡¨æ ¼æ•°æ®æå–æ€§èƒ½ï¼Œä¼˜äºç°æœ‰è§£ææ–¹æ³•ï¼Œè§£å†³äº†ä¼ ç»Ÿè§£ææ–¹æ³•åœ¨è¡¨æ ¼æ•°æ®å¤„ç†ä¸Šçš„ä¸è¶³ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.06212v1">RAG-targeted Adversarial Attack on LLM-based Threat Detection and Mitigation Framework</a></td><td><details><summary>å±•å¼€</summary>The rapid expansion of the Internet of Things (IoT) is reshaping communication and operational practices across industries, but it also broadens the attack surface and increases susceptibility to security breaches. Artificial Intelligence has become a valuable solution in securing IoT networks, with Large Language Models (LLMs) enabling automated attack behavior analysis and mitigation suggestion in Network Intrusion Detection Systems (NIDS). Despite advancements, the use of LLMs in such systems further expands the attack surface, putting entire networks at risk by introducing vulnerabilities such as prompt injection and data poisoning. In this work, we attack an LLM-based IoT attack analysis and mitigation framework to test its adversarial robustness. We construct an attack description dataset and use it in a targeted data poisoning attack that applies word-level, meaning-preserving perturbations to corrupt the Retrieval-Augmented Generation (RAG) knowledge base of the framework. We then compare pre-attack and post-attack mitigation responses from the target model, ChatGPT-5 Thinking, to measure the impact of the attack on model performance, using an established evaluation rubric designed for human experts and judge LLMs. Our results show that small perturbations degrade LLM performance by weakening the linkage between observed network traffic features and attack behavior, and by reducing the specificity and practicality of recommended mitigations for resource-constrained devices.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ç ”ç©¶åŸºäºRAGçš„LLMæ¡†æ¶åœ¨ç‰©è”ç½‘å®‰å…¨ä¸­çš„è„†å¼±æ€§ï¼Œé€šè¿‡æ•°æ®æŠ•æ¯’æ”»å‡»ï¼ˆå¯¹æ£€ç´¢çŸ¥è¯†åº“è¿›è¡Œè¯­ä¹‰ä¿ç•™çš„è¯æ±‡æ‰°åŠ¨ï¼‰æµ‹è¯•å…¶å¯¹æŠ—é²æ£’æ€§ï¼Œå‘ç°å°è§„æ¨¡æ‰°åŠ¨ä¼šå‰Šå¼±ç½‘ç»œç‰¹å¾ä¸æ”»å‡»è¡Œä¸ºçš„å…³è”æ€§ï¼Œå¹¶é™ä½ç”Ÿæˆç¼“è§£æªæ–½çš„å®é™…é€‚ç”¨æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.06259v1">Breaking the Modality Barrier: Generative Modeling for Accurate Molecule Retrieval from Mass Spectra</a></td><td><details><summary>å±•å¼€</summary>Retrieving molecular structures from tandem mass spectra is a crucial step in rapid compound identification. Existing retrieval methods, such as traditional mass spectral library matching, suffer from limited spectral library coverage, while recent cross-modal representation learning frameworks often encounter modality misalignment, resulting in suboptimal retrieval accuracy and generalization. To address these limitations, we propose GLMR, a Generative Language Model-based Retrieval framework that mitigates the cross-modal misalignment through a two-stage process. In the pre-retrieval stage, a contrastive learning-based model identifies top candidate molecules as contextual priors for the input mass spectrum. In the generative retrieval stage, these candidate molecules are integrated with the input mass spectrum to guide a generative model in producing refined molecular structures, which are then used to re-rank the candidates based on molecular similarity. Experiments on both MassSpecGym and the proposed MassRET-20k dataset demonstrate that GLMR significantly outperforms existing methods, achieving over 40% improvement in top-1 accuracy and exhibiting strong generalizability.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºGLMRæ¡†æ¶ï¼Œé€šè¿‡ä¸¤é˜¶æ®µæ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ï¼ˆé¢„æ£€ç´¢é˜¶æ®µçš„å¯¹æ¯”å­¦ä¹ ç­›é€‰å€™é€‰åˆ†å­ï¼Œç”Ÿæˆæ£€ç´¢é˜¶æ®µçš„ç”Ÿæˆæ¨¡å‹ç»“åˆè´¨è°±è¾“å…¥ä¼˜åŒ–ç»“æ„å¹¶é‡æ’åºï¼‰ï¼Œæ˜¾è‘—æå‡è´¨è°±åˆ†å­ç»“æ„æ£€ç´¢çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.06405v1">TOOL4POI: A Tool-Augmented LLM Framework for Next POI Recommendation</a></td><td><details><summary>å±•å¼€</summary>Next Point-of-Interest (POI) recommendation is a fundamental task in location-based services. While recent advances leverage Large Language Model (LLM) for sequential modeling, existing LLM-based approaches face two key limitations: (i) strong reliance on the contextual completeness of user histories, resulting in poor performance on out-of-history (OOH) scenarios; (ii) limited scalability, due to the restricted context window of LLMs, which limits their ability to access and process a large number of candidate POIs. To address these challenges, we propose Tool4POI, a novel tool-augmented framework that enables LLMs to perform open-set POI recommendation through external retrieval and reasoning. Tool4POI consists of three key modules: preference extraction module, multi-turn candidate retrieval module, and reranking module, which together summarize long-term user interests, interact with external tools to retrieve relevant POIs, and refine final recommendations based on recent behaviors. Unlike existing methods, Tool4POI requires no task-specific fine-tuning and is compatible with off-the-shelf LLMs in a plug-and-play manner. Extensive experiments on three real-world datasets show that Tool4POI substantially outperforms state-of-the-art baselines, achieving up to 40% accuracy on challenging OOH scenarios where existing methods fail, and delivering average improvements of 20% and 30% on Acc@5 and Acc@10, respectively.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†Tool4POIæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå¤–éƒ¨æ£€ç´¢å·¥å…·ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è§£å†³POIæ¨èä¸­çš„ä¸Šä¸‹æ–‡ä¸å®Œæ•´å’Œå¯æ‰©å±•æ€§é—®é¢˜ã€‚å…¶æ ¸å¿ƒæ¨¡å—ï¼ˆåå¥½æå–ã€å¤šè½®å€™é€‰æ£€ç´¢ã€é‡æ’åºï¼‰å®ç°äº†ç±»ä¼¼RAGçš„æ£€ç´¢å¢å¼ºç”Ÿæˆæµç¨‹ï¼Œæ˜¾è‘—æå‡äº†OOHåœºæ™¯ä¸‹çš„æ¨èæ€§èƒ½ï¼Œä¸”æ— éœ€å¾®è°ƒå³å¯é€‚é…ç°æˆLLMã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.07555v1">LLM Optimization Unlocks Real-Time Pairwise Reranking</a></td><td><details><summary>å±•å¼€</summary>Efficiently reranking documents retrieved from information retrieval (IR) pipelines to enhance overall quality of Retrieval-Augmented Generation (RAG) system remains an important yet challenging problem. Recent studies have highlighted the importance of Large Language Models (LLMs) in reranking tasks. In particular, Pairwise Reranking Prompting (PRP) has emerged as a promising plug-and-play approach due to its usability and effectiveness. However, the inherent complexity of the algorithm, coupled with the high computational demands and latency incurred due to LLMs, raises concerns about its feasibility in real-time applications. To address these challenges, this paper presents a focused study on pairwise reranking, demonstrating that carefully applied optimization methods can significantly mitigate these issues. By implementing these methods, we achieve a remarkable latency reduction of up to 166 times, from 61.36 seconds to 0.37 seconds per query, with an insignificant drop in performance measured by Recall@k. Our study highlights the importance of design choices that were previously overlooked, such as using smaller models, limiting the reranked set, using lower precision, reducing positional bias with one-directional order inference, and restricting output tokens. These optimizations make LLM-based reranking substantially more efficient and feasible for latency-sensitive, real-world deployments.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æ¢è®¨äº†å¦‚ä½•ä¼˜åŒ–åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–‡æ¡£é‡æ’åºæ–¹æ³•ï¼ˆå¦‚Pairwise Reranking Prompting, PRPï¼‰ï¼Œä»¥æå‡RAGç³»ç»Ÿçš„æ•ˆç‡ï¼Œé€šè¿‡æ¨¡å‹å‹ç¼©ã€é™åˆ¶é‡æ’åºé›†ç­‰ç­–ç•¥å®ç°äº†166å€çš„å»¶è¿Ÿé™ä½ï¼ŒåŒæ—¶ä¿æŒæ£€ç´¢æ€§èƒ½ï¼Œä½¿å…¶æ›´é€‚åˆå®æ—¶åº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.07581v1">Think Before You Retrieve: Learning Test-Time Adaptive Search with Small Language Models</a></td><td><details><summary>å±•å¼€</summary>Effective information retrieval requires reasoning over partial evidence and refining strategies as information emerges. Yet current approaches fall short: neural retrievers lack reasoning capabilities, large language models (LLMs) provide semantic depth but at prohibitive cost, and query rewriting or decomposition limits improvement to static transformations. As a result, existing methods fail to capture the iterative dynamics of exploration, feedback, and revision that complex user queries demand. We introduce Orion, a training framework that enables compact models (350M-1.2B parameters) to perform iterative retrieval through learned search strategies. Orion combines: (1) synthetic trajectory generation and supervised fine-tuning to encourage diverse exploration patterns in models, (2) reinforcement learning (RL) that rewards effective query refinement and backtracking behaviors, and (3) inference-time beam search algorithms that exploit the self-reflection capabilities learned during RL. Despite using only 3% of the training data available, our 1.2B model achieves 77.6% success on SciFact (vs. 72.6% for prior retrievers), 25.2% on BRIGHT (vs. 22.1%), 63.2% on NFCorpus (vs. 57.8%), and remains competitive on FEVER, HotpotQA, and MSMarco. It outperforms retrievers up to 200-400x larger on five of six benchmarks. These findings suggest that retrieval performance can emerge from learned strategies, not just model scale, when models are trained to search, reflect, and revise.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†Orionæ¡†æ¶ï¼Œé€šè¿‡è®­ç»ƒç´§å‡‘æ¨¡å‹ï¼ˆ350M-1.2Bå‚æ•°ï¼‰å®ç°è¿­ä»£æ£€ç´¢ï¼Œç»“åˆåˆæˆè½¨è¿¹ç”Ÿæˆã€å¼ºåŒ–å­¦ä¹ å’Œæ¨ç†æ—¶æŸæœç´¢ç®—æ³•ï¼Œä½¿æ¨¡å‹å…·å¤‡åŠ¨æ€æŸ¥è¯¢ä¼˜åŒ–å’Œè‡ªåæ€èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†å¤šä¸ªä¿¡æ¯æ£€ç´¢åŸºå‡†çš„æ€§èƒ½ã€‚å…¶æ ¸å¿ƒæ€æƒ³ï¼ˆæ£€ç´¢ä¸ç”Ÿæˆçš„ååŒä¼˜åŒ–ï¼‰ä¸RAGæŠ€æœ¯ç›®æ ‡ä¸€è‡´ï¼Œå±äºRAGç›¸å…³ç ”ç©¶ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.06446v1">SR-KI: Scalable and Real-Time Knowledge Integration into LLMs via Supervised Attention</a></td><td><details><summary>å±•å¼€</summary>This paper proposes SR-KI, a novel approach for integrating real-time and large-scale structured knowledge bases (KBs) into large language models (LLMs). SR-KI begins by encoding KBs into key-value pairs using a pretrained encoder, and injects them into LLMs' KV cache. Building on this representation, we employ a two-stage training paradigm: first locating a dedicated retrieval layer within the LLM, and then applying an attention-based loss at this layer to explicitly supervise attention toward relevant KB entries. Unlike traditional retrieval-augmented generation methods that rely heavily on the performance of external retrievers and multi-stage pipelines, SR-KI supports end-to-end inference by performing retrieval entirely within the models latent space. This design enables efficient compression of injected knowledge and facilitates dynamic knowledge updates. Comprehensive experiments demonstrate that SR-KI enables the integration of up to 40K KBs into a 7B LLM on a single A100 40GB GPU, and achieves strong retrieval performance, maintaining over 98% Recall@10 on the best-performing task and exceeding 88% on average across all tasks. Task performance on question answering and KB ID generation also demonstrates that SR-KI maintains strong performance while achieving up to 99.75% compression of the injected KBs.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSR-KIçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å°†ç»“æ„åŒ–çŸ¥è¯†åº“ï¼ˆKBsï¼‰ç¼–ç ä¸ºé”®å€¼å¯¹å¹¶æ³¨å…¥å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„KVç¼“å­˜ä¸­ï¼Œå®ç°äº†å®æ—¶ã€å¤§è§„æ¨¡çŸ¥è¯†åº“çš„é›†æˆã€‚ä¸ä¼ ç»ŸRAGæ–¹æ³•ä¸åŒï¼ŒSR-KIåœ¨æ¨¡å‹æ½œåœ¨ç©ºé—´å†…å®Œæˆæ£€ç´¢ï¼Œæ”¯æŒç«¯åˆ°ç«¯æ¨ç†ï¼Œå¹¶å…è®¸åŠ¨æ€çŸ¥è¯†æ›´æ–°ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½é«˜æ•ˆå‹ç¼©æ³¨å…¥çš„çŸ¥è¯†åº“ï¼Œæ˜¾è‘—æå‡äº†æ£€ç´¢å’Œä»»åŠ¡æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.07061v1">Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and Curriculum Learning</a></td><td><details><summary>å±•å¼€</summary>Emotion Recognition in Conversation (ERC) is a crucial task for understanding human emotions and enabling natural human-computer interaction. Although Large Language Models (LLMs) have recently shown great potential in this field, their ability to capture the intrinsic connections between explicit and implicit emotions remains limited. We propose a novel ERC training framework, PRC-Emo, which integrates Prompt engineering, demonstration Retrieval, and Curriculum learning, with the goal of exploring whether LLMs can effectively perceive emotions in conversational contexts. Specifically, we design emotion-sensitive prompt templates based on both explicit and implicit emotional cues to better guide the model in understanding the speaker's psychological states. We construct the first dedicated demonstration retrieval repository for ERC, which includes training samples from widely used datasets, as well as high-quality dialogue examples generated by LLMs and manually verified. Moreover, we introduce a curriculum learning strategy into the LoRA fine-tuning process, incorporating weighted emotional shifts between same-speaker and different-speaker utterances to assign difficulty levels to dialogue samples, which are then organized in an easy-to-hard training sequence. Experimental results on two benchmark datasets -- IEMOCAP and MELD -- show that our method achieves new state-of-the-art (SOTA) performance, demonstrating the effectiveness and generalizability of our approach in improving LLM-based emotional understanding.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºPRC-Emoçš„æ–°æ¡†æ¶ï¼Œç»“åˆäº†æç¤ºå·¥ç¨‹ï¼ˆPromptï¼‰ã€æ£€ç´¢ï¼ˆRetrievalï¼‰å’Œè¯¾ç¨‹å­¦ä¹ ï¼ˆCurriculumï¼‰ï¼Œç”¨äºæå‡å¤§è¯­è¨€æ¨¡å‹åœ¨å¯¹è¯ä¸­æƒ…æ„Ÿè¯†åˆ«çš„èƒ½åŠ›ã€‚å…¶ä¸­ï¼Œé€šè¿‡æ„å»ºé¦–ä¸ªé’ˆå¯¹ERCä»»åŠ¡çš„ç¤ºèŒƒæ£€ç´¢åº“ï¼ˆåŒ…å«çœŸå®æ•°æ®å’ŒLLMç”Ÿæˆçš„é«˜è´¨é‡å¯¹è¯æ ·æœ¬ï¼‰ï¼Œå¹¶åˆ©ç”¨æ£€ç´¢åˆ°çš„å†…å®¹æŒ‡å¯¼æ¨¡å‹è®­ç»ƒï¼Œä½“ç°äº†RAGçš„æ ¸å¿ƒæ€æƒ³â€”â€”æ£€ç´¢å¤–éƒ¨ä¿¡æ¯å¢å¼ºç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•é€šè¿‡è¯¾ç¨‹å­¦ä¹ ç­–ç•¥è¿›ä¸€æ­¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œåœ¨IEMOCAPå’ŒMELDæ•°æ®é›†ä¸Šè¾¾åˆ°äº†SOTAæ•ˆæœã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.06973v2">Oh That Looks Familiar: A Novel Similarity Measure for Spreadsheet Template Discovery</a></td><td><details><summary>å±•å¼€</summary>Traditional methods for identifying structurally similar spreadsheets fail to capture the spatial layouts and type patterns defining templates. To quantify spreadsheet similarity, we introduce a hybrid distance metric that combines semantic embeddings, data type information, and spatial positioning. In order to calculate spreadsheet similarity, our method converts spreadsheets into cell-level embeddings and then uses aggregation techniques like Chamfer and Hausdorff distances. Experiments across template families demonstrate superior unsupervised clustering performance compared to the graph-based Mondrian baseline, achieving perfect template reconstruction (Adjusted Rand Index of 1.00 versus 0.90) on the FUSTE dataset. Our approach facilitates large-scale automated template discovery, which in turn enables downstream applications such as retrieval-augmented generation over tabular collections, model training, and bulk data cleaning.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§é‡åŒ–ç”µå­è¡¨æ ¼ç›¸ä¼¼æ€§çš„æ··åˆè·ç¦»åº¦é‡æ–¹æ³•ï¼Œç»“åˆè¯­ä¹‰åµŒå…¥ã€æ•°æ®ç±»å‹å’Œç©ºé—´ä½ç½®ä¿¡æ¯ï¼Œé€šè¿‡æ— ç›‘ç£èšç±»å®ç°æ¨¡æ¿è¯†åˆ«ï¼Œå¹¶æåˆ°å…¶æ–¹æ³•å¯æ”¯æŒåŸºäºè¡¨æ ¼é›†åˆçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­‰ä¸‹æ¸¸åº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.07682v1">Designing and Evaluating Malinowski's Lens: An AI-Native Educational Game for Ethnographic Learning</a></td><td><details><summary>å±•å¼€</summary>This study introduces 'Malinowski's Lens', the first AI-native educational game for anthropology that transforms Bronislaw Malinowski's 'Argonauts of the Western Pacific' (1922) into an interactive learning experience. The system combines Retrieval-Augmented Generation with DALL-E 3 text-to-image generation, creating consistent VGA-style visuals as players embody Malinowski during his Trobriand Islands fieldwork (1915-1918). To address ethical concerns, indigenous peoples appear as silhouettes while Malinowski is detailed, prompting reflection on anthropological representation. Two validation studies confirmed effectiveness: Study 1 with 10 non-specialists showed strong learning outcomes (average quiz score 7.5/10) and excellent usability (SUS: 83/100). Study 2 with 4 expert anthropologists confirmed pedagogical value, with one senior researcher discovering "new aspects" of Malinowski's work through gameplay. The findings demonstrate that AI-driven educational games can effectively convey complex anthropological concepts while sparking disciplinary curiosity. This study advances AI-native educational game design and provides a replicable model for transforming academic texts into engaging interactive experiences.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ä»‹ç»äº†ä¸€æ¬¾åä¸ºâ€œMalinowski's Lensâ€çš„AIåŸç”Ÿæ•™è‚²æ¸¸æˆï¼Œå°†äººç±»å­¦ç»å…¸è‘—ä½œè½¬åŒ–ä¸ºäº’åŠ¨å­¦ä¹ ä½“éªŒï¼Œç»“åˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’ŒDALL-E 3å›¾åƒç”ŸæˆæŠ€æœ¯ï¼Œå¹¶é€šè¿‡ä¸¤é¡¹éªŒè¯ç ”ç©¶è¯å®äº†å…¶åœ¨æ•™å­¦æ•ˆæœå’Œå­¦ç§‘å¥½å¥‡å¿ƒæ¿€å‘æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.07262v1">AgenticSciML: Collaborative Multi-Agent Systems for Emergent Discovery in Scientific Machine Learning</a></td><td><details><summary>å±•å¼€</summary>Scientific Machine Learning (SciML) integrates data-driven inference with physical modeling to solve complex problems in science and engineering. However, the design of SciML architectures, loss formulations, and training strategies remains an expert-driven research process, requiring extensive experimentation and problem-specific insights. Here we introduce AgenticSciML, a collaborative multi-agent system in which over 10 specialized AI agents collaborate to propose, critique, and refine SciML solutions through structured reasoning and iterative evolution. The framework integrates structured debate, retrieval-augmented method memory, and ensemble-guided evolutionary search, enabling the agents to generate and assess new hypotheses about architectures and optimization procedures. Across physics-informed learning and operator learning tasks, the framework discovers solution methods that outperform single-agent and human-designed baselines by up to four orders of magnitude in error reduction. The agents produce novel strategies -- including adaptive mixture-of-expert architectures, decomposition-based PINNs, and physics-informed operator learning models -- that do not appear explicitly in the curated knowledge base. These results show that collaborative reasoning among AI agents can yield emergent methodological innovation, suggesting a path toward scalable, transparent, and autonomous discovery in scientific computing.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†AgenticSciMLï¼Œä¸€ä¸ªå¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿï¼Œé€šè¿‡ç»“åˆæ£€ç´¢å¢å¼ºçš„æ–¹æ³•è®°å¿†ï¼ˆretrieval-augmented method memoryï¼‰ç­‰æœºåˆ¶ï¼Œè®©å¤šä¸ªAIæ™ºèƒ½ä½“åä½œæå‡ºã€è¯„ä¼°å’Œä¼˜åŒ–ç§‘å­¦æœºå™¨å­¦ä¹ ï¼ˆSciMLï¼‰è§£å†³æ–¹æ¡ˆã€‚è¯¥ç³»ç»Ÿåœ¨ç‰©ç†ä¿¡æ¯å­¦ä¹ å’Œç®—å­å­¦ä¹ ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½ç”Ÿæˆè¶…è¶Šäººå·¥è®¾è®¡çš„åˆ›æ–°ç­–ç•¥ï¼Œå±•ç¤ºäº†AIåä½œåœ¨ç§‘å­¦è®¡ç®—ä¸­çš„æ–¹æ³•è®ºåˆ›æ–°æ½œåŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.07585v1">LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows</a></td><td><details><summary>å±•å¼€</summary>Financial institutions deploy Large Language Models (LLMs) for reconciliations, regulatory reporting, and client communications, but nondeterministic outputs (output drift) undermine auditability and trust. We quantify drift across five model architectures (7B-120B parameters) on regulated financial tasks, revealing a stark inverse relationship: smaller models (Granite-3-8B, Qwen2.5-7B) achieve 100% output consistency at T=0.0, while GPT-OSS-120B exhibits only 12.5% consistency (95% CI: 3.5-36.0%) regardless of configuration (p<0.0001, Fisher's exact test). This finding challenges conventional assumptions that larger models are universally superior for production deployment.
  Our contributions include: (i) a finance-calibrated deterministic test harness combining greedy decoding (T=0.0), fixed seeds, and SEC 10-K structure-aware retrieval ordering; (ii) task-specific invariant checking for RAG, JSON, and SQL outputs using finance-calibrated materiality thresholds (plus or minus 5%) and SEC citation validation; (iii) a three-tier model classification system enabling risk-appropriate deployment decisions; and (iv) an audit-ready attestation system with dual-provider validation.
  We evaluated five models (Qwen2.5-7B via Ollama, Granite-3-8B via IBM watsonx.ai, Llama-3.3-70B, Mistral-Medium-2505, and GPT-OSS-120B) across three regulated financial tasks. Across 480 runs (n=16 per condition), structured tasks (SQL) remain stable even at T=0.2, while RAG tasks show drift (25-75%), revealing task-dependent sensitivity. Cross-provider validation confirms deterministic behavior transfers between local and cloud deployments. We map our framework to Financial Stability Board (FSB), Bank for International Settlements (BIS), and Commodity Futures Trading Commission (CFTC) requirements, demonstrating practical pathways for compliance-ready AI deployments.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é‡‘èä»»åŠ¡ä¸­çš„è¾“å‡ºä¸€è‡´æ€§ï¼ˆè¾“å‡ºæ¼‚ç§»ï¼‰é—®é¢˜ï¼Œç‰¹åˆ«å…³æ³¨äº†RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ä»»åŠ¡çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œè¾ƒå°çš„æ¨¡å‹åœ¨ç¡®å®šæ€§è®¾ç½®ä¸‹è¡¨ç°æ›´ç¨³å®šï¼Œè€Œè¾ƒå¤§çš„æ¨¡å‹ï¼ˆå¦‚GPT-OSS-120Bï¼‰è¾“å‡ºä¸€è‡´æ€§è¾ƒä½ã€‚è®ºæ–‡æå‡ºäº†ä¸€ä¸ªé‡‘èæ ¡å‡†çš„æµ‹è¯•æ¡†æ¶ï¼ŒåŒ…æ‹¬RAGè¾“å‡ºçš„ä»»åŠ¡ç‰¹å®šä¸å˜æ€§æ£€æŸ¥ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªä¸‰é˜¶æ¨¡å‹åˆ†ç±»ç³»ç»Ÿå’Œå®¡è®¡å°±ç»ªçš„éªŒè¯ç³»ç»Ÿï¼Œä»¥æ”¯æŒç¬¦åˆé‡‘èç›‘ç®¡è¦æ±‚çš„AIéƒ¨ç½²ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.06183v1">BookAsSumQA: An Evaluation Framework for Aspect-Based Book Summarization via Question Answering</a></td><td><details><summary>å±•å¼€</summary>Aspect-based summarization aims to generate summaries that highlight specific aspects of a text, enabling more personalized and targeted summaries. However, its application to books remains unexplored due to the difficulty of constructing reference summaries for long text. To address this challenge, we propose BookAsSumQA, a QA-based evaluation framework for aspect-based book summarization. BookAsSumQA automatically generates aspect-specific QA pairs from a narrative knowledge graph to evaluate summary quality based on its question-answering performance. Our experiments using BookAsSumQA revealed that while LLM-based approaches showed higher accuracy on shorter texts, RAG-based methods become more effective as document length increases, making them more efficient and practical for aspect-based book summarization.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºQAçš„è¯„ä¼°æ¡†æ¶BookAsSumQAï¼Œç”¨äºè¯„ä¼°é¢å‘ç‰¹å®šæ–¹é¢çš„ä¹¦ç±æ‘˜è¦è´¨é‡ï¼Œå¹¶é€šè¿‡å®éªŒå‘ç°éšç€æ–‡æ¡£é•¿åº¦å¢åŠ ï¼ŒåŸºäºRAGçš„æ–¹æ³•æ¯”åŸºäºLLMçš„æ–¹æ³•æ›´æœ‰æ•ˆå’Œå®ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.07685v1">ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents</a></td><td><details><summary>å±•å¼€</summary>Deep Research (DR) is an emerging agent application that leverages large language models (LLMs) to address open-ended queries. It requires the integration of several capabilities, including multi-step reasoning, cross-document synthesis, and the generation of evidence-backed, long-form answers. Evaluating DR remains challenging because responses are lengthy and diverse, admit many valid solutions, and often depend on dynamic information sources. We introduce ResearchRubrics, a standardized benchmark for DR built with over 2,800+ hours of human labor that pairs realistic, domain-diverse prompts with 2,500+ expert-written, fine-grained rubrics to assess factual grounding, reasoning soundness, and clarity. We also propose a new complexity framework for categorizing DR tasks along three axes: conceptual breadth, logical nesting, and exploration. In addition, we develop human and model-based evaluation protocols that measure rubric adherence for DR agents. We evaluate several state-of-the-art DR systems and find that even leading agents like Gemini's DR and OpenAI's DR achieve under 68% average compliance with our rubrics, primarily due to missed implicit context and inadequate reasoning about retrieved information. Our results highlight the need for robust, scalable assessment of deep research capabilities, to which end we release ResearchRubrics(including all prompts, rubrics, and evaluation code) to facilitate progress toward well-justified research assistants.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†â€œDeep Research (DR)â€è¿™ä¸€æ–°å…´çš„æ™ºèƒ½ä»£ç†åº”ç”¨ï¼Œå®ƒåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤„ç†å¼€æ”¾å¼æŸ¥è¯¢ï¼Œå¼ºè°ƒå¤šæ­¥æ¨ç†ã€è·¨æ–‡æ¡£æ•´åˆå’Œç”Ÿæˆæœ‰è¯æ®æ”¯æŒçš„é•¿ç¯‡å›ç­”ã€‚è®ºæ–‡æå‡ºäº†â€œResearchRubricsâ€è¿™ä¸€æ ‡å‡†åŒ–åŸºå‡†ï¼ŒåŒ…å«å¤šæ ·åŒ–çš„æç¤ºå’Œä¸“å®¶ç¼–å†™çš„è¯„ä¼°ç»†åˆ™ï¼Œç”¨äºè¡¡é‡äº‹å®ä¾æ®ã€æ¨ç†åˆç†æ€§å’Œæ¸…æ™°åº¦ã€‚åŒæ—¶ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ä¸ªå¤æ‚æ€§æ¡†æ¶æ¥åˆ†ç±»DRä»»åŠ¡ï¼Œå¹¶å¼€å‘äº†äººå·¥å’Œæ¨¡å‹è¯„ä¼°åè®®ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯å…ˆè¿›çš„DRç³»ç»Ÿä¹Ÿå­˜åœ¨å¯¹æ£€ç´¢ä¿¡æ¯æ¨ç†ä¸è¶³ç­‰é—®é¢˜ï¼Œçªæ˜¾äº†å¯¹æ·±åº¦ç ”ç©¶èƒ½åŠ›è¿›è¡Œç¨³å¥è¯„ä¼°çš„å¿…è¦æ€§ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-08
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.05901v1">Retrieval-Augmented Generation in Medicine: A Scoping Review of Technical Implementations, Clinical Applications, and Ethical Considerations</a></td><td><details><summary>å±•å¼€</summary>The rapid growth of medical knowledge and increasing complexity of clinical practice pose challenges. In this context, large language models (LLMs) have demonstrated value; however, inherent limitations remain. Retrieval-augmented generation (RAG) technologies show potential to enhance their clinical applicability. This study reviewed RAG applications in medicine. We found that research primarily relied on publicly available data, with limited application in private data. For retrieval, approaches commonly relied on English-centric embedding models, while LLMs were mostly generic, with limited use of medical-specific LLMs. For evaluation, automated metrics evaluated generation quality and task performance, whereas human evaluation focused on accuracy, completeness, relevance, and fluency, with insufficient attention to bias and safety. RAG applications were concentrated on question answering, report generation, text summarization, and information extraction. Overall, medical RAG remains at an early stage, requiring advances in clinical validation, cross-linguistic adaptation, and support for low-resource settings to enable trustworthy and responsible global use.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç»¼è¿°äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯åœ¨åŒ»å­¦é¢†åŸŸçš„åº”ç”¨ç°çŠ¶ï¼ŒæŒ‡å‡ºå½“å‰ç ”ç©¶ä¸»è¦ä¾èµ–å…¬å¼€æ•°æ®ã€è‹±è¯­åµŒå…¥æ¨¡å‹å’Œé€šç”¨å¤§è¯­è¨€æ¨¡å‹ï¼Œè¯„ä¼°å¤šå…³æ³¨ç”Ÿæˆè´¨é‡è€Œå¿½è§†åè§ä¸å®‰å…¨æ€§ï¼Œåº”ç”¨é›†ä¸­äºé—®ç­”ã€æŠ¥å‘Šç”Ÿæˆç­‰ä»»åŠ¡ï¼Œå¹¶å¼ºè°ƒæœªæ¥éœ€åŠ å¼ºä¸´åºŠéªŒè¯ã€è·¨è¯­è¨€é€‚é…åŠä½èµ„æºç¯å¢ƒæ”¯æŒä»¥å®ç°å¯é åº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.06125v1">Evaluation of retrieval-based QA on QUEST-LOFT</a></td><td><details><summary>å±•å¼€</summary>Despite the popularity of retrieval-augmented generation (RAG) as a solution for grounded QA in both academia and industry, current RAG methods struggle with questions where the necessary information is distributed across many documents or where retrieval needs to be combined with complex reasoning. Recently, the LOFT study has shown that this limitation also applies to approaches based on long-context language models, with the QUEST benchmark exhibiting particularly large headroom. In this paper, we provide an in-depth analysis of the factors contributing to the poor performance on QUEST-LOFT, publish updated numbers based on a thorough human evaluation, and demonstrate that RAG can be optimized to significantly outperform long-context approaches when combined with a structured output format containing reasoning and evidence, optionally followed by answer re-verification.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å½“å‰RAGæ–¹æ³•åœ¨å¤„ç†ä¿¡æ¯åˆ†æ•£æˆ–éœ€è¦å¤æ‚æ¨ç†çš„é—®é¢˜æ—¶çš„å±€é™æ€§ï¼Œåˆ†æäº†QUEST-LOFTåŸºå‡†è¡¨ç°ä¸ä½³çš„åŸå› ï¼Œå¹¶é€šè¿‡ç»“åˆç»“æ„åŒ–è¾“å‡ºæ ¼å¼ï¼ˆåŒ…å«æ¨ç†å’Œè¯æ®ï¼‰åŠç­”æ¡ˆé‡éªŒè¯ä¼˜åŒ–RAGï¼Œæ˜¾è‘—æå‡äº†å…¶æ€§èƒ½ï¼Œä¼˜äºé•¿ä¸Šä¸‹æ–‡æ¨¡å‹æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.05991v1">Ontology Learning and Knowledge Graph Construction: A Comparison of Approaches and Their Impact on RAG Performance</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) systems combine Large Language Models (LLMs) with external knowledge, and their performance depends heavily on how that knowledge is represented. This study investigates how different Knowledge Graph (KG) construction strategies influence RAG performance. We compare a variety of approaches: standard vector-based RAG, GraphRAG, and retrieval over KGs built from ontologies derived either from relational databases or textual corpora. Results show that ontology-guided KGs incorporating chunk information achieve competitive performance with state-of-the-art frameworks, substantially outperforming vector retrieval baselines. Moreover, the findings reveal that ontology-guided KGs built from relational databases perform competitively to ones built with ontologies extracted from text, with the benefit of offering a dual advantage: they require a one-time-only ontology learning process, substantially reducing LLM usage costs; and avoid the complexity of ontology merging inherent to text-based approaches.</details></td><td><details><summary>å±•å¼€</summary>è¯¥ç ”ç©¶æ¢è®¨äº†ä¸åŒçŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰æ„å»ºç­–ç•¥å¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿæ€§èƒ½çš„å½±å“ï¼Œæ¯”è¾ƒäº†åŸºäºå‘é‡çš„RAGã€GraphRAGä»¥åŠåŸºäºå…³ç³»æ•°æ®åº“æˆ–æ–‡æœ¬è¯­æ–™åº“æ„å»ºçš„KGæ–¹æ³•ï¼Œå‘ç°ç»“åˆåˆ†å—ä¿¡æ¯çš„æœ¬ä½“å¼•å¯¼KGæ€§èƒ½ä¼˜å¼‚ï¼Œä¸”åŸºäºå…³ç³»æ•°æ®åº“çš„KGåœ¨å‡å°‘æˆæœ¬å’Œå¤æ‚æ€§æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.05859v1">Predicting the Future by Retrieving the Past</a></td><td><details><summary>å±•å¼€</summary>Deep learning models such as MLP, Transformer, and TCN have achieved remarkable success in univariate time series forecasting, typically relying on sliding window samples from historical data for training. However, while these models implicitly compress historical information into their parameters during training, they are unable to explicitly and dynamically access this global knowledge during inference, relying only on the local context within the lookback window. This results in an underutilization of rich patterns from the global history. To bridge this gap, we propose Predicting the Future by Retrieving the Past (PFRP), a novel approach that explicitly integrates global historical data to enhance forecasting accuracy. Specifically, we construct a Global Memory Bank (GMB) to effectively store and manage global historical patterns. A retrieval mechanism is then employed to extract similar patterns from the GMB, enabling the generation of global predictions. By adaptively combining these global predictions with the outputs of any local prediction model, PFRP produces more accurate and interpretable forecasts. Extensive experiments conducted on seven real-world datasets demonstrate that PFRP significantly enhances the average performance of advanced univariate forecasting models by 8.4\%. Codes can be found in https://github.com/ddz16/PFRP.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºPFRPçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºå…¨å±€è®°å¿†åº“ï¼ˆGMBï¼‰å¹¶åˆ©ç”¨æ£€ç´¢æœºåˆ¶ä»å†å²æ•°æ®ä¸­æå–ç›¸ä¼¼æ¨¡å¼ï¼Œå°†å…¨å±€å†å²ä¿¡æ¯ä¸å±€éƒ¨é¢„æµ‹æ¨¡å‹ç»“åˆï¼Œæ˜¾è‘—æå‡äº†å•å˜é‡æ—¶é—´åºåˆ—é¢„æµ‹çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ï¼Œå…¶æ ¸å¿ƒæ€æƒ³ä¸RAGçš„æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯é«˜åº¦ç›¸ä¼¼ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.06212v1">RAG-targeted Adversarial Attack on LLM-based Threat Detection and Mitigation Framework</a></td><td><details><summary>å±•å¼€</summary>The rapid expansion of the Internet of Things (IoT) is reshaping communication and operational practices across industries, but it also broadens the attack surface and increases susceptibility to security breaches. Artificial Intelligence has become a valuable solution in securing IoT networks, with Large Language Models (LLMs) enabling automated attack behavior analysis and mitigation suggestion in Network Intrusion Detection Systems (NIDS). Despite advancements, the use of LLMs in such systems further expands the attack surface, putting entire networks at risk by introducing vulnerabilities such as prompt injection and data poisoning. In this work, we attack an LLM-based IoT attack analysis and mitigation framework to test its adversarial robustness. We construct an attack description dataset and use it in a targeted data poisoning attack that applies word-level, meaning-preserving perturbations to corrupt the Retrieval-Augmented Generation (RAG) knowledge base of the framework. We then compare pre-attack and post-attack mitigation responses from the target model, ChatGPT-5 Thinking, to measure the impact of the attack on model performance, using an established evaluation rubric designed for human experts and judge LLMs. Our results show that small perturbations degrade LLM performance by weakening the linkage between observed network traffic features and attack behavior, and by reducing the specificity and practicality of recommended mitigations for resource-constrained devices.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ç ”ç©¶äº†åœ¨åŸºäºLLMçš„ç‰©è”ç½‘æ”»å‡»åˆ†æä¸ç¼“è§£æ¡†æ¶ä¸­ï¼Œé’ˆå¯¹å…¶RAGçŸ¥è¯†åº“çš„é’ˆå¯¹æ€§æ•°æ®æŠ•æ¯’æ”»å‡»ï¼Œé€šè¿‡è¯çº§æ‰°åŠ¨ç ´åçŸ¥è¯†åº“å¹¶è¯„ä¼°æ”»å‡»å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå‘ç°å¾®å°æ‰°åŠ¨ä¼šé™ä½LLMçš„é˜²å¾¡å»ºè®®è´¨é‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.06259v1">Breaking the Modality Barrier: Generative Modeling for Accurate Molecule Retrieval from Mass Spectra</a></td><td><details><summary>å±•å¼€</summary>Retrieving molecular structures from tandem mass spectra is a crucial step in rapid compound identification. Existing retrieval methods, such as traditional mass spectral library matching, suffer from limited spectral library coverage, while recent cross-modal representation learning frameworks often encounter modality misalignment, resulting in suboptimal retrieval accuracy and generalization. To address these limitations, we propose GLMR, a Generative Language Model-based Retrieval framework that mitigates the cross-modal misalignment through a two-stage process. In the pre-retrieval stage, a contrastive learning-based model identifies top candidate molecules as contextual priors for the input mass spectrum. In the generative retrieval stage, these candidate molecules are integrated with the input mass spectrum to guide a generative model in producing refined molecular structures, which are then used to re-rank the candidates based on molecular similarity. Experiments on both MassSpecGym and the proposed MassRET-20k dataset demonstrate that GLMR significantly outperforms existing methods, achieving over 40% improvement in top-1 accuracy and exhibiting strong generalizability.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆè¯­è¨€æ¨¡å‹çš„æ£€ç´¢æ¡†æ¶GLMRï¼Œé€šè¿‡ä¸¤é˜¶æ®µè¿‡ç¨‹ï¼ˆé¢„æ£€ç´¢å’Œç”Ÿæˆæ£€ç´¢ï¼‰è§£å†³è·¨æ¨¡æ€å¯¹é½é—®é¢˜ï¼Œåˆ©ç”¨å€™é€‰åˆ†å­ä½œä¸ºä¸Šä¸‹æ–‡å…ˆéªŒæŒ‡å¯¼ç”Ÿæˆæ¨¡å‹ä¼˜åŒ–åˆ†å­ç»“æ„ï¼Œæ˜¾è‘—æå‡äº†è´¨è°±æ•°æ®æ£€ç´¢çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå±äºRAGæŠ€æœ¯çš„åº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.06405v1">TOOL4POI: A Tool-Augmented LLM Framework for Next POI Recommendation</a></td><td><details><summary>å±•å¼€</summary>Next Point-of-Interest (POI) recommendation is a fundamental task in location-based services. While recent advances leverage Large Language Model (LLM) for sequential modeling, existing LLM-based approaches face two key limitations: (i) strong reliance on the contextual completeness of user histories, resulting in poor performance on out-of-history (OOH) scenarios; (ii) limited scalability, due to the restricted context window of LLMs, which limits their ability to access and process a large number of candidate POIs. To address these challenges, we propose Tool4POI, a novel tool-augmented framework that enables LLMs to perform open-set POI recommendation through external retrieval and reasoning. Tool4POI consists of three key modules: preference extraction module, multi-turn candidate retrieval module, and reranking module, which together summarize long-term user interests, interact with external tools to retrieve relevant POIs, and refine final recommendations based on recent behaviors. Unlike existing methods, Tool4POI requires no task-specific fine-tuning and is compatible with off-the-shelf LLMs in a plug-and-play manner. Extensive experiments on three real-world datasets show that Tool4POI substantially outperforms state-of-the-art baselines, achieving up to 40% accuracy on challenging OOH scenarios where existing methods fail, and delivering average improvements of 20% and 30% on Acc@5 and Acc@10, respectively.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†Tool4POIï¼Œä¸€ä¸ªåŸºäºå·¥å…·å¢å¼ºçš„æ¡†æ¶ï¼Œé€šè¿‡å¤–éƒ¨æ£€ç´¢å’Œæ¨ç†ä½¿å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿè¿›è¡Œå¼€æ”¾é›†çš„ä¸‹ä¸€ä¸ªå…´è¶£ç‚¹ï¼ˆPOIï¼‰æ¨èã€‚è¯¥æ¡†æ¶åŒ…å«åå¥½æå–ã€å¤šè½®å€™é€‰æ£€ç´¢å’Œé‡æ–°æ’åºä¸‰ä¸ªæ¨¡å—ï¼Œè§£å†³äº†ç°æœ‰LLMæ–¹æ³•åœ¨ä¸Šä¸‹æ–‡å®Œæ•´æ€§å’Œå¯æ‰©å±•æ€§æ–¹é¢çš„é™åˆ¶ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡å¾®è°ƒå³å¯å®ç°é«˜æ€§èƒ½æ¨èã€‚å®éªŒè¡¨æ˜ï¼ŒTool4POIåœ¨OOHåœºæ™¯å’Œæ¨èå‡†ç¡®æ€§ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.06073v1">Stemming Hallucination in Language Models Using a Licensing Oracle</a></td><td><details><summary>å±•å¼€</summary>Language models exhibit remarkable natural language generation capabilities but remain prone to hallucinations, generating factually incorrect information despite producing syntactically coherent responses. This study introduces the Licensing Oracle, an architectural solution designed to stem hallucinations in LMs by enforcing truth constraints through formal validation against structured knowledge graphs. Unlike statistical approaches that rely on data scaling or fine-tuning, the Licensing Oracle embeds a deterministic validation step into the model's generative process, ensuring that only factually accurate claims are made. We evaluated the effectiveness of the Licensing Oracle through experiments comparing it with several state-of-the-art methods, including baseline language model generation, fine-tuning for factual recall, fine-tuning for abstention behavior, and retrieval-augmented generation (RAG). Our results demonstrate that although RAG and fine-tuning improve performance, they fail to eliminate hallucinations. In contrast, the Licensing Oracle achieved perfect abstention precision (AP = 1.0) and zero false answers (FAR-NE = 0.0), ensuring that only valid claims were generated with 89.1% accuracy in factual responses. This work shows that architectural innovations, such as the Licensing Oracle, offer a necessary and sufficient solution for hallucinations in domains with structured knowledge representations, offering guarantees that statistical methods cannot match. Although the Licensing Oracle is specifically designed to address hallucinations in fact-based domains, its framework lays the groundwork for truth-constrained generation in future AI systems, providing a new path toward reliable, epistemically grounded models.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºLicensing Oracleçš„æ¶æ„è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨é€šè¿‡ç»“æ„åŒ–çŸ¥è¯†å›¾è°±çš„å½¢å¼åŒ–éªŒè¯æ¥å‡å°‘è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜ã€‚ç ”ç©¶æ¯”è¾ƒäº†åŒ…æ‹¬RAGåœ¨å†…çš„å¤šç§æ–¹æ³•ï¼Œå‘ç°å°½ç®¡RAGå’Œå¾®è°ƒèƒ½æå‡æ€§èƒ½ï¼Œä½†ä»æ— æ³•å®Œå…¨æ¶ˆé™¤å¹»è§‰ã€‚Licensing Oracleé€šè¿‡ç¡®å®šæ€§éªŒè¯æ­¥éª¤å®ç°äº†é›¶é”™è¯¯å›ç­”å’Œé«˜å‡†ç¡®ç‡ï¼Œä¸ºåŸºäºç»“æ„åŒ–çŸ¥è¯†çš„é¢†åŸŸæä¾›äº†æ›´å¯é çš„ç”Ÿæˆæ–¹æ¡ˆã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.06446v1">SR-KI: Scalable and Real-Time Knowledge Integration into LLMs via Supervised Attention</a></td><td><details><summary>å±•å¼€</summary>This paper proposes SR-KI, a novel approach for integrating real-time and large-scale structured knowledge bases (KBs) into large language models (LLMs). SR-KI begins by encoding KBs into key-value pairs using a pretrained encoder, and injects them into LLMs' KV cache. Building on this representation, we employ a two-stage training paradigm: first locating a dedicated retrieval layer within the LLM, and then applying an attention-based loss at this layer to explicitly supervise attention toward relevant KB entries. Unlike traditional retrieval-augmented generation methods that rely heavily on the performance of external retrievers and multi-stage pipelines, SR-KI supports end-to-end inference by performing retrieval entirely within the models latent space. This design enables efficient compression of injected knowledge and facilitates dynamic knowledge updates. Comprehensive experiments demonstrate that SR-KI enables the integration of up to 40K KBs into a 7B LLM on a single A100 40GB GPU, and achieves strong retrieval performance, maintaining over 98% Recall@10 on the best-performing task and exceeding 88% on average across all tasks. Task performance on question answering and KB ID generation also demonstrates that SR-KI maintains strong performance while achieving up to 99.75% compression of the injected KBs.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºSR-KIæ–¹æ³•ï¼Œé€šè¿‡å°†ç»“æ„åŒ–çŸ¥è¯†åº“ç¼–ç ä¸ºé”®å€¼å¯¹å¹¶æ³¨å…¥å¤§è¯­è¨€æ¨¡å‹çš„KVç¼“å­˜ä¸­ï¼Œå®ç°ç«¯åˆ°ç«¯çš„çŸ¥è¯†æ£€ç´¢ä¸ç”Ÿæˆï¼Œé¿å…ä¼ ç»ŸRAGå¯¹å¤–éƒ¨æ£€ç´¢å™¨çš„ä¾èµ–ï¼Œæ”¯æŒé«˜æ•ˆçŸ¥è¯†å‹ç¼©ä¸åŠ¨æ€æ›´æ–°ï¼Œå®éªŒæ˜¾ç¤ºå…¶åœ¨é—®ç­”ç­‰ä»»åŠ¡ä¸­æ€§èƒ½ä¼˜è¶Šä¸”å†…å­˜å ç”¨æä½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.05804v1">Catching Contamination Before Generation: Spectral Kill Switches for Agents</a></td><td><details><summary>å±•å¼€</summary>Agentic language models compose multi step reasoning chains, yet intermediate steps can be corrupted by inconsistent context, retrieval errors, or adversarial inputs, which makes post hoc evaluation too late because errors propagate before detection. We introduce a diagnostic that requires no additional training and uses only the forward pass to emit a binary accept or reject signal during agent execution. The method analyzes token graphs induced by attention and computes two spectral statistics in early layers, namely the high frequency energy ratio and spectral entropy. We formalize these signals, establish invariances, and provide finite sample estimators with uncertainty quantification. Under a two regime mixture assumption with a monotone likelihood ratio property, we show that a single threshold on the high frequency energy ratio is optimal in the Bayes sense for detecting context inconsistency. Empirically, the high frequency energy ratio exhibits robust bimodality during context verification across multiple model families, which enables gating decisions with overhead below one millisecond on our hardware and configurations. We demonstrate integration into retrieval augmented agent pipelines and discuss deployment as an inline safety monitor. The approach detects contamination while the model is still processing the text, before errors commit to the reasoning chain.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ— éœ€é¢å¤–è®­ç»ƒçš„å®æ—¶è¯Šæ–­æ–¹æ³•ï¼Œé€šè¿‡åˆ†æè¯­è¨€æ¨¡å‹æ—©æœŸå±‚çš„æ³¨æ„åŠ›æœºåˆ¶ç”Ÿæˆçš„tokenå›¾çš„é¢‘è°±ç»Ÿè®¡é‡ï¼ˆé«˜é¢‘èƒ½é‡æ¯”å’Œé¢‘è°±ç†µï¼‰ï¼Œåœ¨æ£€ç´¢å¢å¼ºçš„æ™ºèƒ½ä½“æµç¨‹ä¸­æ£€æµ‹ä¸Šä¸‹æ–‡ä¸ä¸€è‡´æ€§ï¼Œä»¥é˜²æ­¢é”™è¯¯åœ¨æ¨ç†é“¾ä¸­ä¼ æ’­ï¼Œå¹¶æ¢è®¨äº†å°†å…¶ä½œä¸ºå†…è”å®‰å…¨ç›‘æ§å™¨çš„åº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.06183v1">BookAsSumQA: An Evaluation Framework for Aspect-Based Book Summarization via Question Answering</a></td><td><details><summary>å±•å¼€</summary>Aspect-based summarization aims to generate summaries that highlight specific aspects of a text, enabling more personalized and targeted summaries. However, its application to books remains unexplored due to the difficulty of constructing reference summaries for long text. To address this challenge, we propose BookAsSumQA, a QA-based evaluation framework for aspect-based book summarization. BookAsSumQA automatically generates aspect-specific QA pairs from a narrative knowledge graph to evaluate summary quality based on its question-answering performance. Our experiments using BookAsSumQA revealed that while LLM-based approaches showed higher accuracy on shorter texts, RAG-based methods become more effective as document length increases, making them more efficient and practical for aspect-based book summarization.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†BookAsSumQAæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°åŸºäºç‰¹å®šæ–¹é¢çš„ä¹¦ç±æ‘˜è¦ç”Ÿæˆè´¨é‡ï¼Œé€šè¿‡ä»å™äº‹çŸ¥è¯†å›¾è°±è‡ªåŠ¨ç”Ÿæˆé—®ç­”å¯¹æ¥æµ‹è¯•æ‘˜è¦æ•ˆæœã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•åœ¨çŸ­æ–‡æœ¬ä¸Šè¡¨ç°æ›´å¥½ï¼Œä½†éšç€æ–‡æœ¬é•¿åº¦å¢åŠ ï¼ŒåŸºäºRAGçš„æ–¹æ³•åœ¨æ•ˆç‡å’Œå®ç”¨æ€§ä¸Šæ›´å…·ä¼˜åŠ¿ï¼Œå°¤å…¶é€‚ç”¨äºé•¿æ–‡æœ¬çš„ç‰¹å®šæ–¹é¢æ‘˜è¦ç”Ÿæˆã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-07
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.05385v1">TeaRAG: A Token-Efficient Agentic Retrieval-Augmented Generation Framework</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) utilizes external knowledge to augment Large Language Models' (LLMs) reliability. For flexibility, agentic RAG employs autonomous, multi-round retrieval and reasoning to resolve queries. Although recent agentic RAG has improved via reinforcement learning, they often incur substantial token overhead from search and reasoning processes. This trade-off prioritizes accuracy over efficiency. To address this issue, this work proposes TeaRAG, a token-efficient agentic RAG framework capable of compressing both retrieval content and reasoning steps. 1) First, the retrieved content is compressed by augmenting chunk-based semantic retrieval with a graph retrieval using concise triplets. A knowledge association graph is then built from semantic similarity and co-occurrence. Finally, Personalized PageRank is leveraged to highlight key knowledge within this graph, reducing the number of tokens per retrieval. 2) Besides, to reduce reasoning steps, Iterative Process-aware Direct Preference Optimization (IP-DPO) is proposed. Specifically, our reward function evaluates the knowledge sufficiency by a knowledge matching mechanism, while penalizing excessive reasoning steps. This design can produce high-quality preference-pair datasets, supporting iterative DPO to improve reasoning conciseness. Across six datasets, TeaRAG improves the average Exact Match by 4% and 2% while reducing output tokens by 61% and 59% on Llama3-8B-Instruct and Qwen2.5-14B-Instruct, respectively. Code is available at https://github.com/Applied-Machine-Learning-Lab/TeaRAG.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTeaRAGçš„tokené«˜æ•ˆä»£ç†RAGæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å‹ç¼©æ£€ç´¢å†…å®¹å’Œæ¨ç†æ­¥éª¤æ¥è§£å†³ç°æœ‰ä»£ç†RAGæ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚å…·ä½“æ–¹æ³•åŒ…æ‹¬ï¼š1ï¼‰é€šè¿‡åŸºäºå›¾æ£€ç´¢çš„ä¸‰å…ƒç»„å‹ç¼©æ£€ç´¢å†…å®¹ï¼Œå‡å°‘æ¯æ¬¡æ£€ç´¢çš„tokenæ•°é‡ï¼›2ï¼‰æå‡ºè¿­ä»£è¿‡ç¨‹æ„ŸçŸ¥çš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆIP-DPOï¼‰æ¥å‡å°‘æ¨ç†æ­¥éª¤ï¼ŒåŒæ—¶é€šè¿‡çŸ¥è¯†åŒ¹é…æœºåˆ¶è¯„ä¼°çŸ¥è¯†å……åˆ†æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTeaRAGåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæé«˜äº†å‡†ç¡®ç‡å¹¶æ˜¾è‘—å‡å°‘äº†è¾“å‡ºtokenæ•°é‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.05901v1">Retrieval-Augmented Generation in Medicine: A Scoping Review of Technical Implementations, Clinical Applications, and Ethical Considerations</a></td><td><details><summary>å±•å¼€</summary>The rapid growth of medical knowledge and increasing complexity of clinical practice pose challenges. In this context, large language models (LLMs) have demonstrated value; however, inherent limitations remain. Retrieval-augmented generation (RAG) technologies show potential to enhance their clinical applicability. This study reviewed RAG applications in medicine. We found that research primarily relied on publicly available data, with limited application in private data. For retrieval, approaches commonly relied on English-centric embedding models, while LLMs were mostly generic, with limited use of medical-specific LLMs. For evaluation, automated metrics evaluated generation quality and task performance, whereas human evaluation focused on accuracy, completeness, relevance, and fluency, with insufficient attention to bias and safety. RAG applications were concentrated on question answering, report generation, text summarization, and information extraction. Overall, medical RAG remains at an early stage, requiring advances in clinical validation, cross-linguistic adaptation, and support for low-resource settings to enable trustworthy and responsible global use.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯åœ¨åŒ»å­¦é¢†åŸŸçš„åº”ç”¨ç°çŠ¶ï¼Œåˆ†æäº†å½“å‰ç ”ç©¶ä¸»è¦ä¾èµ–å…¬å¼€æ•°æ®ã€è‹±è¯­ä¸­å¿ƒåµŒå…¥æ¨¡å‹å’Œé€šç”¨å¤§è¯­è¨€æ¨¡å‹çš„å±€é™æ€§ï¼Œå¹¶æŒ‡å‡ºæœªæ¥éœ€åœ¨ä¸´åºŠéªŒè¯ã€è·¨è¯­è¨€é€‚åº”å’Œä½èµ„æºç¯å¢ƒæ”¯æŒç­‰æ–¹é¢å–å¾—è¿›å±•ä»¥å®ç°å¯ä¿¡èµ–çš„å…¨çƒåº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.06125v1">Evaluation of retrieval-based QA on QUEST-LOFT</a></td><td><details><summary>å±•å¼€</summary>Despite the popularity of retrieval-augmented generation (RAG) as a solution for grounded QA in both academia and industry, current RAG methods struggle with questions where the necessary information is distributed across many documents or where retrieval needs to be combined with complex reasoning. Recently, the LOFT study has shown that this limitation also applies to approaches based on long-context language models, with the QUEST benchmark exhibiting particularly large headroom. In this paper, we provide an in-depth analysis of the factors contributing to the poor performance on QUEST-LOFT, publish updated numbers based on a thorough human evaluation, and demonstrate that RAG can be optimized to significantly outperform long-context approaches when combined with a structured output format containing reasoning and evidence, optionally followed by answer re-verification.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡åˆ†æäº†å½“å‰RAGæ–¹æ³•åœ¨å¤„ç†éœ€è¦è·¨å¤šæ–‡æ¡£æ£€ç´¢æˆ–å¤æ‚æ¨ç†çš„é—®é¢˜æ—¶çš„å±€é™æ€§ï¼Œé€šè¿‡QUEST-LOFTåŸºå‡†å®éªŒæå‡ºä¼˜åŒ–æ–¹æ¡ˆï¼Œç»“åˆç»“æ„åŒ–è¾“å‡ºæ ¼å¼ä¸ç­”æ¡ˆéªŒè¯ï¼Œæ˜¾è‘—æå‡äº†RAGåœ¨é•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.04939v1">Search Is Not Retrieval: Decoupling Semantic Matching from Contextual Assembly in RAG</a></td><td><details><summary>å±•å¼€</summary>Retrieval systems are essential to contemporary AI pipelines, although most confuse two separate processes: finding relevant information and giving enough context for reasoning. We introduce the Search-Is-Not-Retrieve (SINR) framework, a dual-layer architecture that distinguishes between fine-grained search representations and coarse-grained retrieval contexts. SINR enhances the composability, scalability, and context fidelity of retrieval systems by directly connecting small, semantically accurate search chunks to larger, contextually complete retrieve chunks, all without incurring extra processing costs. This design changes retrieval from a passive step to an active one, making the system architecture more like how people process information. We discuss the SINR framework's conceptual foundation, formal structure, implementation issues, and qualitative outcomes. This provides a practical foundation for the next generation of AI systems that use retrieval.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†SINRï¼ˆSearch-Is-Not-Retrieveï¼‰æ¡†æ¶ï¼Œé€šè¿‡åŒºåˆ†ç»†ç²’åº¦æœç´¢è¡¨ç¤ºå’Œç²—ç²’åº¦æ£€ç´¢ä¸Šä¸‹æ–‡ï¼Œæ”¹è¿›äº†æ£€ç´¢ç³»ç»Ÿçš„å¯ç»„åˆæ€§ã€å¯æ‰©å±•æ€§å’Œä¸Šä¸‹æ–‡ä¿çœŸåº¦ã€‚è¯¥æ¡†æ¶å°†æ£€ç´¢ä»è¢«åŠ¨æ­¥éª¤è½¬å˜ä¸ºä¸»åŠ¨æ­¥éª¤ï¼Œç±»ä¼¼äºäººç±»å¤„ç†ä¿¡æ¯çš„æ–¹å¼ï¼Œä¸ºä¸‹ä¸€ä»£ä½¿ç”¨æ£€ç´¢çš„AIç³»ç»Ÿæä¾›äº†å®ç”¨åŸºç¡€ã€‚è™½ç„¶SINRæ¡†æ¶æœ¬èº«ä¸æ˜¯RAGï¼Œä½†å®ƒæ”¹è¿›äº†æ£€ç´¢ç³»ç»Ÿï¼Œè€Œæ£€ç´¢ç³»ç»Ÿæ˜¯RAGçš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œå› æ­¤ä¸RAGç›¸å…³ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.05991v1">Ontology Learning and Knowledge Graph Construction: A Comparison of Approaches and Their Impact on RAG Performance</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) systems combine Large Language Models (LLMs) with external knowledge, and their performance depends heavily on how that knowledge is represented. This study investigates how different Knowledge Graph (KG) construction strategies influence RAG performance. We compare a variety of approaches: standard vector-based RAG, GraphRAG, and retrieval over KGs built from ontologies derived either from relational databases or textual corpora. Results show that ontology-guided KGs incorporating chunk information achieve competitive performance with state-of-the-art frameworks, substantially outperforming vector retrieval baselines. Moreover, the findings reveal that ontology-guided KGs built from relational databases perform competitively to ones built with ontologies extracted from text, with the benefit of offering a dual advantage: they require a one-time-only ontology learning process, substantially reducing LLM usage costs; and avoid the complexity of ontology merging inherent to text-based approaches.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ç ”ç©¶äº†ä¸åŒçŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰æ„å»ºç­–ç•¥å¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿæ€§èƒ½çš„å½±å“ï¼Œæ¯”è¾ƒäº†åŸºäºå‘é‡çš„RAGã€GraphRAGåŠåŸºäºå…³ç³»æ•°æ®åº“æˆ–æ–‡æœ¬è¯­æ–™åº“æ„å»ºçš„KGæ–¹æ³•ï¼Œå‘ç°ç»“åˆåˆ†å—ä¿¡æ¯çš„æœ¬ä½“å¼•å¯¼KGæ€§èƒ½ä¼˜è¶Šï¼Œä¸”åŸºäºå…³ç³»æ•°æ®åº“çš„KGåœ¨é™ä½æˆæœ¬å’Œå¤æ‚åº¦æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.05297v1">Building Specialized Software-Assistant ChatBot with Graph-Based Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Digital Adoption Platforms (DAPs) have become essential tools for helping employees navigate complex enterprise software such as CRM, ERP, or HRMS systems. Companies like LemonLearning have shown how digital guidance can reduce training costs and accelerate onboarding. However, building and maintaining these interactive guides still requires extensive manual effort. Leveraging Large Language Models as virtual assistants is an appealing alternative, yet without a structured understanding of the target software, LLMs often hallucinate and produce unreliable answers. Moreover, most production-grade LLMs are black-box APIs, making fine-tuning impractical due to the lack of access to model weights. In this work, we introduce a Graph-based Retrieval-Augmented Generation framework that automatically converts enterprise web applications into state-action knowledge graphs, enabling LLMs to generate grounded and context-aware assistance. The framework was co-developed with the AI enterprise RAKAM, in collaboration with Lemon Learning. We detail the engineering pipeline that extracts and structures software interfaces, the design of the graph-based retrieval process, and the integration of our approach into production DAP workflows. Finally, we discuss scalability, robustness, and deployment lessons learned from industrial use cases.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå›¾çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆGraph-based RAGï¼‰æ¡†æ¶ï¼Œé€šè¿‡å°†ä¼ä¸šçº§Webåº”ç”¨ç¨‹åºè‡ªåŠ¨è½¬åŒ–ä¸ºçŠ¶æ€-åŠ¨ä½œçŸ¥è¯†å›¾è°±ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æä¾›ç»“æ„åŒ–è½¯ä»¶ç†è§£ï¼Œä»è€Œç”Ÿæˆå‡†ç¡®ä¸”ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¾…åŠ©å†…å®¹ï¼Œå¹¶åº”ç”¨äºæ•°å­—é‡‡ç”¨å¹³å°ï¼ˆDAPsï¼‰ä»¥è§£å†³ä¼ ç»ŸLLMsçš„å¹»è§‰å’Œä¸å¯é é—®é¢˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.05079v1">Wikipedia-based Datasets in Russian Information Retrieval Benchmark RusBEIR</a></td><td><details><summary>å±•å¼€</summary>In this paper, we present a novel series of Russian information retrieval datasets constructed from the "Did you know..." section of Russian Wikipedia. Our datasets support a range of retrieval tasks, including fact-checking, retrieval-augmented generation, and full-document retrieval, by leveraging interesting facts and their referenced Wikipedia articles annotated at the sentence level with graded relevance. We describe the methodology for dataset creation that enables the expansion of existing Russian Information Retrieval (IR) resources. Through extensive experiments, we extend the RusBEIR research by comparing lexical retrieval models, such as BM25, with state-of-the-art neural architectures fine-tuned for Russian, as well as multilingual models. Results of our experiments show that lexical methods tend to outperform neural models on full-document retrieval, while neural approaches better capture lexical semantics in shorter texts, such as in fact-checking or fine-grained retrieval. Using our newly created datasets, we also analyze the impact of document length on retrieval performance and demonstrate that combining retrieval with neural reranking consistently improves results. Our contribution expands the resources available for Russian information retrieval research and highlights the importance of accurate evaluation of retrieval models to achieve optimal performance. All datasets are publicly available at HuggingFace. To facilitate reproducibility and future research, we also release the full implementation on GitHub.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºä¿„è¯­ç»´åŸºç™¾ç§‘æ„å»ºçš„æ–°å‹ä¿¡æ¯æ£€ç´¢æ•°æ®é›†ç³»åˆ—ï¼Œæ”¯æŒåŒ…æ‹¬æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨å†…çš„å¤šç§ä»»åŠ¡ï¼Œé€šè¿‡å®éªŒæ¯”è¾ƒäº†ä¸åŒæ£€ç´¢æ¨¡å‹åœ¨ä¿„è¯­ä¿¡æ¯æ£€ç´¢ä¸­çš„è¡¨ç°ï¼Œå¹¶å±•ç¤ºäº†ç»“åˆç¥ç»é‡æ’åºå¯¹æ€§èƒ½çš„æå‡æ•ˆæœã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.04956v1">ORCHID: Orchestrated Retrieval-Augmented Classification with Human-in-the-Loop Intelligent Decision-Making for High-Risk Property</a></td><td><details><summary>å±•å¼€</summary>High-Risk Property (HRP) classification is critical at U.S. Department of Energy (DOE) sites, where inventories include sensitive and often dual-use equipment. Compliance must track evolving rules designated by various export control policies to make transparent and auditable decisions. Traditional expert-only workflows are time-consuming, backlog-prone, and struggle to keep pace with shifting regulatory boundaries. We demo ORCHID, a modular agentic system for HRP classification that pairs retrieval-augmented generation (RAG) with human oversight to produce policy-based outputs that can be audited. Small cooperating agents, retrieval, description refiner, classifier, validator, and feedback logger, coordinate via agent-to-agent messaging and invoke tools through the Model Context Protocol (MCP) for model-agnostic on-premise operation. The interface follows an Item to Evidence to Decision loop with step-by-step reasoning, on-policy citations, and append-only audit bundles (run-cards, prompts, evidence). In preliminary tests on real HRP cases, ORCHID improves accuracy and traceability over a non-agentic baseline while deferring uncertain items to Subject Matter Experts (SMEs). The demonstration shows single item submission, grounded citations, SME feedback capture, and exportable audit artifacts, illustrating a practical path to trustworthy LLM assistance in sensitive DOE compliance workflows.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ORCHIDç³»ç»Ÿï¼Œä¸€ä¸ªç”¨äºé«˜é£é™©è´¢äº§ï¼ˆHRPï¼‰åˆ†ç±»çš„æ¨¡å—åŒ–æ™ºèƒ½ç³»ç»Ÿï¼Œç»“åˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯å’Œäººå·¥ç›‘ç£ï¼Œä»¥æé«˜åˆ†ç±»çš„å‡†ç¡®æ€§å’Œå¯è¿½æº¯æ€§ã€‚ç³»ç»Ÿé€šè¿‡å¤šä¸ªåä½œä»£ç†ï¼ˆå¦‚æ£€ç´¢ã€åˆ†ç±»å™¨ã€éªŒè¯å™¨ç­‰ï¼‰åè°ƒå·¥ä½œï¼Œç”ŸæˆåŸºäºæ”¿ç­–çš„å¯å®¡è®¡è¾“å‡ºï¼Œå¹¶åœ¨åˆæ­¥æµ‹è¯•ä¸­æ˜¾ç¤ºå‡ºä¼˜äºéæ™ºèƒ½åŸºçº¿çš„æ€§èƒ½ï¼ŒåŒæ—¶å°†ä¸ç¡®å®šé¡¹ç›®äº¤ç”±ä¸“å®¶å¤„ç†ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.05859v1">Predicting the Future by Retrieving the Past</a></td><td><details><summary>å±•å¼€</summary>Deep learning models such as MLP, Transformer, and TCN have achieved remarkable success in univariate time series forecasting, typically relying on sliding window samples from historical data for training. However, while these models implicitly compress historical information into their parameters during training, they are unable to explicitly and dynamically access this global knowledge during inference, relying only on the local context within the lookback window. This results in an underutilization of rich patterns from the global history. To bridge this gap, we propose Predicting the Future by Retrieving the Past (PFRP), a novel approach that explicitly integrates global historical data to enhance forecasting accuracy. Specifically, we construct a Global Memory Bank (GMB) to effectively store and manage global historical patterns. A retrieval mechanism is then employed to extract similar patterns from the GMB, enabling the generation of global predictions. By adaptively combining these global predictions with the outputs of any local prediction model, PFRP produces more accurate and interpretable forecasts. Extensive experiments conducted on seven real-world datasets demonstrate that PFRP significantly enhances the average performance of advanced univariate forecasting models by 8.4\%. Codes can be found in https://github.com/ddz16/PFRP.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºPFRPçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºå…¨å±€è®°å¿†åº“ï¼ˆGMBï¼‰å’Œæ£€ç´¢æœºåˆ¶ï¼Œä»å…¨å±€å†å²æ•°æ®ä¸­åŠ¨æ€æ£€ç´¢ç›¸ä¼¼æ¨¡å¼æ¥å¢å¼ºæ—¶é—´åºåˆ—é¢„æµ‹çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ï¼Œç±»ä¼¼äºRAGæŠ€æœ¯ä¸­æ£€ç´¢å¤–éƒ¨çŸ¥è¯†è¾…åŠ©ç”Ÿæˆçš„æ€æƒ³ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.04921v1">AgentExpt: Automating AI Experiment Design with LLM-based Resource Retrieval Agent</a></td><td><details><summary>å±•å¼€</summary>Large language model agents are becoming increasingly capable at web-centric tasks such as information retrieval, complex reasoning. These emerging capabilities have given rise to surge research interests in developing LLM agent for facilitating scientific quest. One key application in AI research is to automate experiment design through agentic dataset and baseline retrieval. However, prior efforts suffer from limited data coverage, as recommendation datasets primarily harvest candidates from public portals and omit many datasets actually used in published papers, and from an overreliance on content similarity that biases model toward superficial similarity and overlooks experimental suitability. Harnessing collective perception embedded in the baseline and dataset citation network, we present a comprehensive framework for baseline and dataset recommendation. First, we design an automated data-collection pipeline that links roughly one hundred thousand accepted papers to the baselines and datasets they actually used. Second, we propose a collective perception enhanced retriever. To represent the position of each dataset or baseline within the scholarly network, it concatenates self-descriptions with aggregated citation contexts. To achieve efficient candidate recall, we finetune an embedding model on these representations. Finally, we develop a reasoning-augmented reranker that exact interaction chains to construct explicit reasoning chains and finetunes a large language model to produce interpretable justifications and refined rankings. The dataset we curated covers 85\% of the datasets and baselines used at top AI conferences over the past five years. On our dataset, the proposed method outperforms the strongest prior baseline with average gains of +5.85\% in Recall@20, +8.30\% in HitRate@5. Taken together, our results advance reliable, interpretable automation of experimental design.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºé›†ä½“æ„ŸçŸ¥å¢å¼ºçš„æ£€ç´¢æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨æ¨èå®éªŒè®¾è®¡ä¸­çš„æ•°æ®é›†å’ŒåŸºçº¿æ–¹æ³•ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªè¿æ¥è®ºæ–‡ä¸æ‰€ç”¨æ•°æ®é›†/åŸºçº¿çš„è‡ªåŠ¨åŒ–æ•°æ®æ”¶é›†æµç¨‹ï¼Œå¹¶ç»“åˆå¼•ç”¨ç½‘ç»œä¸­çš„é›†ä½“æ„ŸçŸ¥ä¿¡æ¯æ¥ä¼˜åŒ–æ£€ç´¢è¡¨ç¤ºï¼Œæœ€ç»ˆé€šè¿‡æ¨ç†å¢å¼ºçš„é‡æ–°æ’åºå™¨ç”Ÿæˆå¯è§£é‡Šçš„æ¨èç»“æœï¼Œæ˜¾è‘—æå‡äº†æ¨èè¦†ç›–ç‡ï¼ˆRecall@20æé«˜5.85%ï¼‰å’Œå‡†ç¡®æ€§ï¼ˆHitRate@5æé«˜8.30%ï¼‰ï¼Œå±äºRAGæŠ€æœ¯åœ¨å­¦æœ¯å®éªŒè®¾è®¡é¢†åŸŸçš„åˆ›æ–°åº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.05406v1">Large Language Models for Explainable Threat Intelligence</a></td><td><details><summary>å±•å¼€</summary>As cyber threats continue to grow in complexity, traditional security mechanisms struggle to keep up. Large language models (LLMs) offer significant potential in cybersecurity due to their advanced capabilities in text processing and generation. This paper explores the use of LLMs with retrieval-augmented generation (RAG) to obtain threat intelligence by combining real-time information retrieval with domain-specific data. The proposed system, RAGRecon, uses a LLM with RAG to answer questions about cybersecurity threats. Moreover, it makes this form of Artificial Intelligence (AI) explainable by generating and visually presenting to the user a knowledge graph for every reply. This increases the transparency and interpretability of the reasoning of the model, allowing analysts to better understand the connections made by the system based on the context recovered by the RAG system. We evaluated RAGRecon experimentally with two datasets and seven different LLMs and the responses matched the reference responses more than 91% of the time for the best combinations.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRAGReconçš„ç³»ç»Ÿï¼Œç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥è·å–ç½‘ç»œå®‰å…¨å¨èƒæƒ…æŠ¥ï¼Œé€šè¿‡å®æ—¶ä¿¡æ¯æ£€ç´¢ä¸é¢†åŸŸç‰¹å®šæ•°æ®å¢å¼ºå›ç­”å‡†ç¡®æ€§ï¼Œå¹¶åˆ©ç”¨çŸ¥è¯†å›¾è°±æå‡æ¨¡å‹æ¨ç†çš„å¯è§£é‡Šæ€§ï¼Œå®éªŒè¡¨æ˜å…¶æœ€ä½³ç»„åˆçš„å“åº”åŒ¹é…ç‡è¾¾91%ä»¥ä¸Šã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.04919v1">BudgetMem: Learning Selective Memory Policies for Cost-Efficient Long-Context Processing in Language Models</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) face significant computational and memory constraints when processing long contexts, despite growing demand for applications requiring reasoning over extensive documents, multi-session dialogues, and book length texts. While recent advances have extended context windows to 100K-1M tokens, such approaches incur prohibitive costs for resource constrained deployments. We propose BudgetMem, a novel memory augmented architecture that learns what to remember rather than remembering everything. Our system combines selective memory policies with feature based salience scoring (entity density, TF-IDF, discourse markers, position bias) to decide which information merits storage under strict budget constraints. Unlike existing retrieval augmented generation (RAG) systems that store all chunks, BudgetMem employs learned gating mechanisms coupled with BM25 sparse retrieval for efficient information access. Through comprehensive experiments on 700 question answer pairs across short (237 tokens) and long (5K-10K tokens) documents with Llama-3.2-3B-Instruct, we demonstrate that BudgetMem achieves remarkable results on long documents: only 1.0% F1 score degradation while saving 72.4% memory compared to baseline RAG. We validate our approach through budget sensitivity analysis (testing 7 budget ratios), naive baseline comparisons, and document length analysis, showing that BudgetMem's benefits increase with document length. Our work provides a practical pathway for deploying capable long context systems on modest hardware, democratizing access to advanced language understanding capabilities.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºBudgetMemçš„æ–°å‹å†…å­˜å¢å¼ºæ¶æ„ï¼Œé€šè¿‡é€‰æ‹©æ€§è®°å¿†ç­–ç•¥å’ŒåŸºäºç‰¹å¾çš„æ˜¾è‘—æ€§è¯„åˆ†æ¥ä¼˜åŒ–ä¿¡æ¯å­˜å‚¨ï¼Œä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶çš„é«˜æˆæœ¬é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„RAGç³»ç»Ÿä¸åŒï¼ŒBudgetMemé‡‡ç”¨å­¦ä¹ é—¨æ§æœºåˆ¶å’Œç¨€ç–æ£€ç´¢æŠ€æœ¯ï¼Œåœ¨ä¸¥æ ¼çš„å†…å­˜é¢„ç®—ä¸‹æ˜¾è‘—æå‡äº†æ•ˆç‡ï¼ˆèŠ‚çœ72.4%å†…å­˜ï¼‰ï¼ŒåŒæ—¶ä¿æŒäº†æ¥è¿‘åŸºçº¿RAGçš„å‡†ç¡®ç‡ï¼ˆä»…1.0% F1åˆ†æ•°ä¸‹é™ï¼‰ï¼Œç‰¹åˆ«é€‚åˆåœ¨èµ„æºå—é™çš„ç¡¬ä»¶ä¸Šéƒ¨ç½²é•¿æ–‡æœ¬å¤„ç†ç³»ç»Ÿã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.06073v1">Stemming Hallucination in Language Models Using a Licensing Oracle</a></td><td><details><summary>å±•å¼€</summary>Language models exhibit remarkable natural language generation capabilities but remain prone to hallucinations, generating factually incorrect information despite producing syntactically coherent responses. This study introduces the Licensing Oracle, an architectural solution designed to stem hallucinations in LMs by enforcing truth constraints through formal validation against structured knowledge graphs. Unlike statistical approaches that rely on data scaling or fine-tuning, the Licensing Oracle embeds a deterministic validation step into the model's generative process, ensuring that only factually accurate claims are made. We evaluated the effectiveness of the Licensing Oracle through experiments comparing it with several state-of-the-art methods, including baseline language model generation, fine-tuning for factual recall, fine-tuning for abstention behavior, and retrieval-augmented generation (RAG). Our results demonstrate that although RAG and fine-tuning improve performance, they fail to eliminate hallucinations. In contrast, the Licensing Oracle achieved perfect abstention precision (AP = 1.0) and zero false answers (FAR-NE = 0.0), ensuring that only valid claims were generated with 89.1% accuracy in factual responses. This work shows that architectural innovations, such as the Licensing Oracle, offer a necessary and sufficient solution for hallucinations in domains with structured knowledge representations, offering guarantees that statistical methods cannot match. Although the Licensing Oracle is specifically designed to address hallucinations in fact-based domains, its framework lays the groundwork for truth-constrained generation in future AI systems, providing a new path toward reliable, epistemically grounded models.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºLicensing Oracleçš„æ¶æ„è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨é€šè¿‡ç»“æ„åŒ–çŸ¥è¯†å›¾è°±çš„å½¢å¼åŒ–éªŒè¯æ¥å‡å°‘è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜ã€‚ç ”ç©¶æ¯”è¾ƒäº†åŒ…æ‹¬RAGåœ¨å†…çš„å¤šç§æ–¹æ³•ï¼Œå‘ç°å°½ç®¡RAGå’Œå¾®è°ƒèƒ½æå‡æ€§èƒ½ï¼Œä½†æ— æ³•å®Œå…¨æ¶ˆé™¤å¹»è§‰ï¼Œè€ŒLicensing Oracleåœ¨äº‹å®å‡†ç¡®æ€§ä¸Šè¡¨ç°æ›´ä¼˜ï¼Œå®ç°äº†å®Œç¾çš„ç²¾ç¡®åº¦å’Œé›¶é”™è¯¯å›ç­”ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.05626v1">LLMs as Packagers of HPC Software</a></td><td><details><summary>å±•å¼€</summary>High performance computing (HPC) software ecosystems are inherently heterogeneous, comprising scientific applications that depend on hundreds of external packages, each with distinct build systems, options, and dependency constraints. Tools such as Spack automate dependency resolution and environment management, but their effectiveness relies on manually written build recipes. As these ecosystems grow, maintaining existing specifications and creating new ones becomes increasingly labor-intensive. While large language models (LLMs) have shown promise in code generation, automatically producing correct and maintainable Spack recipes remains a significant challenge. We present a systematic analysis of how LLMs and context-augmentation methods can assist in the generation of Spack recipes. To this end, we introduce SpackIt, an end-to-end framework that combines repository analysis, retrieval of relevant examples, and iterative refinement through diagnostic feedback. We apply SpackIt to a representative subset of 308 open-source HPC packages to assess its effectiveness and limitations. Our results show that SpackIt increases installation success from 20% in a zero-shot setting to over 80% in its best configuration, demonstrating the value of retrieval and structured feedback for reliable package synthesis.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ£€ç´¢å¢å¼ºæ–¹æ³•ï¼ˆå¦‚ç»“åˆç›¸å…³ç¤ºä¾‹çš„ä¸Šä¸‹æ–‡ï¼‰æ¥è‡ªåŠ¨ç”Ÿæˆé«˜æ€§èƒ½è®¡ç®—ï¼ˆHPCï¼‰è½¯ä»¶ç”Ÿæ€ä¸­çš„Spackæ„å»ºé…æ–¹ï¼Œæå‡ºå¹¶è¯„ä¼°äº†ç«¯åˆ°ç«¯æ¡†æ¶SpackItï¼Œé€šè¿‡æ£€ç´¢å’Œè¿­ä»£åé¦ˆå°†å®‰è£…æˆåŠŸç‡ä»20%æå‡è‡³80%ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.05706v1">AdvisingWise: Supporting Academic Advising in Higher Educations Through a Human-in-the-Loop Multi-Agent Framework</a></td><td><details><summary>å±•å¼€</summary>Academic advising is critical to student success in higher education, yet high student-to-advisor ratios limit advisors' capacity to provide timely support, particularly during peak periods. Recent advances in Large Language Models (LLMs) present opportunities to enhance the advising process. We present AdvisingWise, a multi-agent system that automates time-consuming tasks, such as information retrieval and response drafting, while preserving human oversight. AdvisingWise leverages authoritative institutional resources and adaptively prompts students about their academic backgrounds to generate reliable, personalized responses. All system responses undergo human advisor validation before delivery to students. We evaluate AdvisingWise through a mixed-methods approach: (1) expert evaluation on responses of 20 sample queries, (2) LLM-as-a-judge evaluation of the information retrieval strategy, and (3) a user study with 8 academic advisors to assess the system's practical utility. Our evaluation shows that AdvisingWise produces accurate, personalized responses. Advisors reported increasingly positive perceptions after using AdvisingWise, as their initial concerns about reliability and personalization diminished. We conclude by discussing the implications of human-AI synergy on the practice of academic advising.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†AdvisingWiseï¼Œä¸€ä¸ªåŸºäºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å­¦æœ¯å’¨è¯¢è¾…åŠ©å·¥å…·ï¼Œé€šè¿‡ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæƒå¨æœºæ„èµ„æºè¿›è¡Œä¿¡æ¯æ£€ç´¢ä¸ä¸ªæ€§åŒ–å“åº”ç”Ÿæˆï¼Œå¹¶ä¿ç•™äººå·¥å®¡æ ¸ç¯èŠ‚ã€‚ç³»ç»Ÿé€šè¿‡æ··åˆè¯„ä¼°æ–¹æ³•éªŒè¯äº†å…¶ç”Ÿæˆç­”æ¡ˆçš„å‡†ç¡®æ€§ä¸å®ç”¨æ€§ï¼Œæœ€ç»ˆæå‡äº†å’¨è¯¢æ•ˆç‡å’Œäººæœºåä½œæ•ˆæœã€‚å…¶æ ¸å¿ƒæœºåˆ¶ï¼ˆæ£€ç´¢å¤–éƒ¨çŸ¥è¯†+ç”Ÿæˆå¢å¼ºï¼‰ç¬¦åˆRAGæŠ€æœ¯èŒƒå¼ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.05804v1">Catching Contamination Before Generation: Spectral Kill Switches for Agents</a></td><td><details><summary>å±•å¼€</summary>Agentic language models compose multi step reasoning chains, yet intermediate steps can be corrupted by inconsistent context, retrieval errors, or adversarial inputs, which makes post hoc evaluation too late because errors propagate before detection. We introduce a diagnostic that requires no additional training and uses only the forward pass to emit a binary accept or reject signal during agent execution. The method analyzes token graphs induced by attention and computes two spectral statistics in early layers, namely the high frequency energy ratio and spectral entropy. We formalize these signals, establish invariances, and provide finite sample estimators with uncertainty quantification. Under a two regime mixture assumption with a monotone likelihood ratio property, we show that a single threshold on the high frequency energy ratio is optimal in the Bayes sense for detecting context inconsistency. Empirically, the high frequency energy ratio exhibits robust bimodality during context verification across multiple model families, which enables gating decisions with overhead below one millisecond on our hardware and configurations. We demonstrate integration into retrieval augmented agent pipelines and discuss deployment as an inline safety monitor. The approach detects contamination while the model is still processing the text, before errors commit to the reasoning chain.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ— éœ€é¢å¤–è®­ç»ƒçš„å®æ—¶è¯Šæ–­æ–¹æ³•ï¼Œé€šè¿‡åˆ†æè¯­è¨€æ¨¡å‹æ³¨æ„åŠ›æœºåˆ¶ç”Ÿæˆçš„é«˜é¢‘èƒ½é‡æ¯”å’Œè°±ç†µç­‰å…‰è°±ç»Ÿè®¡é‡ï¼Œåœ¨æ£€ç´¢å¢å¼ºå‹æ™ºèƒ½ä½“ï¼ˆå¦‚RAGï¼‰æ‰§è¡Œè¿‡ç¨‹ä¸­æ£€æµ‹ä¸Šä¸‹æ–‡ä¸ä¸€è‡´æ€§ï¼ˆå¦‚æ£€ç´¢é”™è¯¯æˆ–å¯¹æŠ—æ€§è¾“å…¥ï¼‰ï¼Œä»¥æ¯«ç§’çº§å»¶è¿Ÿå®ç°æ—©æœŸé”™è¯¯æ‹¦æˆªï¼Œä»è€Œæå‡ç”Ÿæˆç»“æœçš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.05078v1">Reasoning-Guided Claim Normalization for Noisy Multilingual Social Media Posts</a></td><td><details><summary>å±•å¼€</summary>We address claim normalization for multilingual misinformation detection - transforming noisy social media posts into clear, verifiable statements across 20 languages. The key contribution demonstrates how systematic decomposition of posts using Who, What, Where, When, Why and How questions enables robust cross-lingual transfer despite training exclusively on English data. Our methodology incorporates finetuning Qwen3-14B using LoRA with the provided dataset after intra-post deduplication, token-level recall filtering for semantic alignment and retrieval-augmented few-shot learning with contextual examples during inference. Our system achieves METEOR scores ranging from 41.16 (English) to 15.21 (Marathi), securing third rank on the English leaderboard and fourth rank for Dutch and Punjabi. The approach shows 41.3% relative improvement in METEOR over baseline configurations and substantial gains over existing methods. Results demonstrate effective cross-lingual generalization for Romance and Germanic languages while maintaining semantic coherence across diverse linguistic structures.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šè¯­è¨€è°£è¨€æ£€æµ‹ä¸­çš„å£°æ˜è§„èŒƒåŒ–æ–¹æ³•ï¼Œé€šè¿‡åˆ†è§£ç¤¾äº¤åª’ä½“å¸–å­ä¸ºå¯éªŒè¯çš„è¦ç´ ï¼ˆWho/What/Whereç­‰ï¼‰ï¼Œç»“åˆæ£€ç´¢å¢å¼ºçš„å°‘æ ·æœ¬å­¦ä¹ ï¼ˆretrieval-augmented few-shot learningï¼‰å’ŒQwen3-14Bæ¨¡å‹çš„å¾®è°ƒï¼Œå®ç°äº†ä»…ç”¨è‹±è¯­è®­ç»ƒæ•°æ®å³å¯è·¨20ç§è¯­è¨€çš„é²æ£’è¿ç§»ï¼Œåœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šæ˜¾è‘—è¶…è¶ŠåŸºçº¿æ–¹æ³•å¹¶å–å¾—æ’è¡Œæ¦œå‰åˆ—æˆç»©ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.05696v1">AI-assisted workflow enables rapid, high-fidelity breast cancer clinical trial eligibility prescreening</a></td><td><details><summary>å±•å¼€</summary>Clinical trials play an important role in cancer care and research, yet participation rates remain low. We developed MSK-MATCH (Memorial Sloan Kettering Multi-Agent Trial Coordination Hub), an AI system for automated eligibility screening from clinical text. MSK-MATCH integrates a large language model with a curated oncology trial knowledge base and retrieval-augmented architecture providing explanations for all AI predictions grounded in source text. In a retrospective dataset of 88,518 clinical documents from 731 patients across six breast cancer trials, MSK-MATCH automatically resolved 61.9% of cases and triaged 38.1% for human review. This AI-assisted workflow achieved 98.6% accuracy, 98.4% sensitivity, and 98.7% specificity for patient-level eligibility classification, matching or exceeding performance of the human-only and AI-only comparisons. For the triaged cases requiring manual review, prepopulating eligibility screens with AI-generated explanations reduced screening time from 20 minutes to 43 seconds at an average cost of $0.96 per patient-trial pair.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ä»‹ç»äº†MSK-MATCHç³»ç»Ÿï¼Œä¸€ä¸ªç»“åˆå¤§è¯­è¨€æ¨¡å‹ä¸è‚¿ç˜¤å­¦è¯•éªŒçŸ¥è¯†åº“çš„AIè¾…åŠ©å·¥å…·ï¼Œé‡‡ç”¨æ£€ç´¢å¢å¼ºæ¶æ„ï¼ˆRAGï¼‰è‡ªåŠ¨ç­›é€‰ç™Œç—‡ä¸´åºŠè¯•éªŒèµ„æ ¼ï¼Œé€šè¿‡æ£€ç´¢ä¸´åºŠæ–‡æœ¬ç”Ÿæˆè§£é‡Šæ€§é¢„æµ‹ï¼Œæ˜¾è‘—æå‡ç­›é€‰æ•ˆç‡ï¼ˆå‡†ç¡®ç‡98.6%ï¼‰å¹¶å‡å°‘äººå·¥å®¡æ ¸æ—¶é—´ï¼ˆä»20åˆ†é’Ÿé™è‡³43ç§’ï¼‰ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-06
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.04560v1">BanglaMedQA and BanglaMMedBench: Evaluating Retrieval-Augmented Generation Strategies for Bangla Biomedical Question Answering</a></td><td><details><summary>å±•å¼€</summary>Developing accurate biomedical Question Answering (QA) systems in
low-resource languages remains a major challenge, limiting equitable access to
reliable medical knowledge. This paper introduces BanglaMedQA and
BanglaMMedBench, the first large-scale Bangla biomedical Multiple Choice
Question (MCQ) datasets designed to evaluate reasoning and retrieval in medical
artificial intelligence (AI). The study applies and benchmarks several
Retrieval-Augmented Generation (RAG) strategies, including Traditional,
Zero-Shot Fallback, Agentic, Iterative Feedback, and Aggregate RAG, combining
textbook-based and web retrieval with generative reasoning to improve factual
accuracy. A key novelty lies in integrating a Bangla medical textbook corpus
through Optical Character Recognition (OCR) and implementing an Agentic RAG
pipeline that dynamically selects between retrieval and reasoning strategies.
Experimental results show that the Agentic RAG achieved the highest accuracy
89.54% with openai/gpt-oss-120b, outperforming other configurations and
demonstrating superior rationale quality. These findings highlight the
potential of RAG-based methods to enhance the reliability and accessibility of
Bangla medical QA, establishing a foundation for future research in
multilingual medical artificial intelligence.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†é¦–ä¸ªå­ŸåŠ æ‹‰è¯­ç”Ÿç‰©åŒ»å­¦é—®ç­”æ•°æ®é›†BanglaMedQAå’ŒBanglaMMedBenchï¼Œå¹¶åˆ©ç”¨å¤šç§RAGç­–ç•¥ï¼ˆå¦‚ä¼ ç»Ÿã€é›¶æ ·æœ¬å›é€€ã€ä»£ç†ã€è¿­ä»£åé¦ˆå’ŒèšåˆRAGï¼‰è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚é€šè¿‡ç»“åˆæ•™ç§‘ä¹¦å’Œç½‘ç»œæ£€ç´¢ä¸ç”Ÿæˆæ¨ç†ï¼Œç ”ç©¶éªŒè¯äº†ä»£ç†RAGæ–¹æ³•åœ¨æå‡ç­”æ¡ˆå‡†ç¡®æ€§å’Œåˆç†æ€§æ–¹é¢çš„ä¼˜è¶Šæ€§ï¼ˆæœ€é«˜è¾¾89.54%ï¼‰ï¼Œä¸ºä½èµ„æºè¯­è¨€åŒ»å­¦QAç³»ç»Ÿæä¾›äº†å¯é è§£å†³æ–¹æ¡ˆã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.04502v1">RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific RAG</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) is a critical technique for grounding
Large Language Models (LLMs) in factual evidence, yet evaluating RAG systems in
specialized, safety-critical domains remains a significant challenge. Existing
evaluation frameworks often rely on heuristic-based metrics that fail to
capture domain-specific nuances and other works utilize LLM-as-a-Judge
approaches that lack validated alignment with human judgment. This paper
introduces RAGalyst, an automated, human-aligned agentic framework designed for
the rigorous evaluation of domain-specific RAG systems. RAGalyst features an
agentic pipeline that generates high-quality, synthetic question-answering (QA)
datasets from source documents, incorporating an agentic filtering step to
ensure data fidelity. The framework refines two key LLM-as-a-Judge
metrics-Answer Correctness and Answerability-using prompt optimization to
achieve a strong correlation with human annotations. Applying this framework to
evaluate various RAG components across three distinct domains (military
operations, cybersecurity, and bridge engineering), we find that performance is
highly context-dependent. No single embedding model, LLM, or hyperparameter
configuration proves universally optimal. Additionally, we provide an analysis
on the most common low Answer Correctness reasons in RAG. These findings
highlight the necessity of a systematic evaluation framework like RAGalyst,
which empowers practitioners to uncover domain-specific trade-offs and make
informed design choices for building reliable and effective RAG systems.
RAGalyst is available on our Github.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ä»‹ç»äº†RAGalystï¼Œä¸€ç§è‡ªåŠ¨åŒ–ä¸”ä¸äººç±»åˆ¤æ–­å¯¹é½çš„ä»£ç†æ¡†æ¶ï¼Œç”¨äºä¸¥æ ¼è¯„ä¼°é¢†åŸŸç‰¹å®šçš„RAGç³»ç»Ÿã€‚è¯¥æ¡†æ¶é€šè¿‡ç”Ÿæˆé«˜è´¨é‡çš„åˆæˆQAæ•°æ®é›†ã€ä¼˜åŒ–LLM-as-a-JudgeæŒ‡æ ‡ï¼ˆå¦‚ç­”æ¡ˆæ­£ç¡®æ€§å’Œå¯å›ç­”æ€§ï¼‰ï¼Œå¹¶åœ¨ä¸‰ä¸ªä¸“ä¸šé¢†åŸŸï¼ˆå†›äº‹è¡ŒåŠ¨ã€ç½‘ç»œå®‰å…¨å’Œæ¡¥æ¢å·¥ç¨‹ï¼‰éªŒè¯å…¶æ€§èƒ½ï¼Œæ­ç¤ºäº†RAGç»„ä»¶çš„è¡¨ç°é«˜åº¦ä¾èµ–ä¸Šä¸‹æ–‡ï¼Œå¹¶åˆ†æäº†ä½ç­”æ¡ˆæ­£ç¡®æ€§çš„å¸¸è§åŸå› ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.04234v1">Reusing Pre-Training Data at Test Time is a Compute Multiplier</a></td><td><details><summary>å±•å¼€</summary>Large language models learn from their vast pre-training corpora, gaining the
ability to solve an ever increasing variety of tasks; yet although researchers
work to improve these datasets, there is little effort to understand how
efficient the pre-training apparatus is at extracting ideas and knowledge from
the data. In this work, we use retrieval augmented generation along with
test-time compute as a way to quantify how much dataset value was left behind
by the process of pre-training, and how this changes across scale. We
demonstrate that pre-training then retrieving from standard and largely
open-sourced datasets results in significant accuracy gains in MMLU, Math-500,
and SimpleQA, which persist through decontamination. For MMLU we observe that
retrieval acts as a ~5x compute multiplier versus pre-training alone. We show
that these results can be further improved by leveraging additional compute at
test time to parse the retrieved context, demonstrating a 10 percentage point
improvement on MMLU for the public LLaMA 3.1 8B model. Overall, our results
suggest that today's pre-training methods do not make full use of the
information in existing pre-training datasets, leaving significant room for
progress.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡é‡‡ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œæµ‹è¯•æ—¶è®¡ç®—æ¥é‡åŒ–é¢„è®­ç»ƒè¿‡ç¨‹ä¸­æœªè¢«å……åˆ†åˆ©ç”¨çš„æ•°æ®é›†ä»·å€¼ï¼Œå¹¶æ¢è®¨å…¶åœ¨ä¸åŒè§„æ¨¡ä¸‹çš„å˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨æ ‡å‡†å¼€æºæ•°æ®é›†ä¸Šé¢„è®­ç»ƒåç»“åˆæ£€ç´¢èƒ½æ˜¾è‘—æå‡MMLUã€Math-500å’ŒSimpleQAçš„å‡†ç¡®æ€§ï¼Œæ£€ç´¢æ•ˆæœç›¸å½“äºé¢„è®­ç»ƒè®¡ç®—é‡çš„5å€å¢ç›Šã€‚é€šè¿‡æµ‹è¯•æ—¶é¢å¤–è®¡ç®—è§£ææ£€ç´¢ä¸Šä¸‹æ–‡ï¼Œè¿›ä¸€æ­¥ä½¿LLaMA 3.1 8Bæ¨¡å‹çš„MMLUæ€§èƒ½æå‡10ä¸ªç™¾åˆ†ç‚¹ï¼Œè¯å®å½“å‰é¢„è®­ç»ƒæ–¹æ³•æœªèƒ½å……åˆ†æŒ–æ˜æ•°æ®é›†ä¿¡æ¯ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.04205v1">LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member of the Polish National Board of Appeal</a></td><td><details><summary>å±•å¼€</summary>This study provides an empirical assessment of whether current large language
models (LLMs) can pass the official qualifying examination for membership in
Poland's National Appeal Chamber (Krajowa Izba Odwo{\l}awcza). The authors
examine two related ideas: using LLM as actual exam candidates and applying the
'LLM-as-a-judge' approach, in which model-generated answers are automatically
evaluated by other models. The paper describes the structure of the exam, which
includes a multiple-choice knowledge test on public procurement law and a
written judgment, and presents the hybrid information recovery and extraction
pipeline built to support the models. Several LLMs (including GPT-4.1, Claude 4
Sonnet and Bielik-11B-v2.6) were tested in closed-book and various
Retrieval-Augmented Generation settings. The results show that although the
models achieved satisfactory scores in the knowledge test, none met the passing
threshold in the practical written part, and the evaluations of the
'LLM-as-a-judge' often diverged from the judgments of the official examining
committee. The authors highlight key limitations: susceptibility to
hallucinations, incorrect citation of legal provisions, weaknesses in logical
argumentation, and the need for close collaboration between legal experts and
technical teams. The findings indicate that, despite rapid technological
progress, current LLMs cannot yet replace human judges or independent examiners
in Polish public procurement adjudication.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡è¯„ä¼°äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ³¢å…°å›½å®¶ä¸Šè¯‰æ³•åº­èµ„æ ¼è€ƒè¯•ä¸­çš„è¡¨ç°ï¼Œæµ‹è¯•äº†åŒ…æ‹¬GPT-4.1ã€Claude 4 Sonnetå’ŒBielik-11B-v2.6åœ¨å†…çš„å¤šä¸ªæ¨¡å‹åœ¨é—­å·å’Œå¤šç§æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è®¾ç½®ä¸‹çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡æ¨¡å‹åœ¨é€‰æ‹©é¢˜éƒ¨åˆ†è¡¨ç°å°šå¯ï¼Œä½†åœ¨å®è·µå†™ä½œéƒ¨åˆ†å‡æœªè¾¾æ ‡ï¼ŒåŒæ—¶æŒ‡å‡ºæ¨¡å‹å­˜åœ¨å¹»è§‰ã€é”™è¯¯å¼•ç”¨æ³•å¾‹æ¡æ¬¾ç­‰å±€é™æ€§ï¼Œå¼ºè°ƒå½“å‰LLMså°šæ— æ³•å–ä»£äººç±»æ³•å®˜æˆ–è€ƒå®˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.04080v1">Caption Injection for Optimization in Generative Search Engine</a></td><td><details><summary>å±•å¼€</summary>Generative Search Engines (GSEs) leverage Retrieval-Augmented Generation
(RAG) techniques and Large Language Models (LLMs) to integrate multi-source
information and provide users with accurate and comprehensive responses. Unlike
traditional search engines that present results in ranked lists, GSEs shift
users' attention from sequential browsing to content-driven subjective
perception, driving a paradigm shift in information retrieval. In this context,
enhancing the subjective visibility of content through Generative Search Engine
Optimization (G-SEO) methods has emerged as a new research focus. With the
rapid advancement of Multimodal Retrieval-Augmented Generation (MRAG)
techniques, GSEs can now efficiently integrate text, images, audio, and video,
producing richer responses that better satisfy complex information needs.
Existing G-SEO methods, however, remain limited to text-based optimization and
fail to fully exploit multimodal data. To address this gap, we propose Caption
Injection, the first multimodal G-SEO approach, which extracts captions from
images and injects them into textual content, integrating visual semantics to
enhance the subjective visibility of content in generative search scenarios. We
systematically evaluate Caption Injection on MRAMG, a benchmark for MRAG, under
both unimodal and multimodal settings. Experimental results show that Caption
Injection significantly outperforms text-only G-SEO baselines under the G-Eval
metric, demonstrating the necessity and effectiveness of multimodal integration
in G-SEO to improve user-perceived content visibility.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡è®¨è®ºäº†ä¸€ç§æ–°å‹çš„ç”Ÿæˆå¼æœç´¢å¼•æ“ï¼ˆGSEsï¼‰ï¼Œå®ƒåˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ•´åˆå¤šæºä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›æ›´å‡†ç¡®å’Œå…¨é¢çš„å“åº”ã€‚åŒºåˆ«äºä¼ ç»Ÿæœç´¢å¼•æ“ï¼ŒGSEså…³æ³¨å†…å®¹é©±åŠ¨çš„ä¸»è§‚æ„ŸçŸ¥ï¼Œæ¨åŠ¨ä¿¡æ¯æ£€ç´¢èŒƒå¼çš„è½¬å˜ã€‚è®ºæ–‡ç‰¹åˆ«æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€çš„ç”Ÿæˆå¼æœç´¢å¼•æ“ä¼˜åŒ–æ–¹æ³•ï¼ˆG-SEOï¼‰â€”â€”Caption Injectionï¼Œé€šè¿‡ä»å›¾åƒä¸­æå–æ ‡é¢˜å¹¶å°†å…¶æ³¨å…¥æ–‡æœ¬å†…å®¹ï¼Œç»¼åˆè§†è§‰è¯­ä¹‰ä»¥å¢å¼ºå†…å®¹åœ¨ç”Ÿæˆå¼æœç´¢ä¸­çš„ä¸»è§‚å¯è§æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€RAGåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä»…åŸºäºæ–‡æœ¬çš„G-SEOåŸºçº¿ï¼Œè¯æ˜äº†å¤šæ¨¡æ€æ•´åˆåœ¨æå‡ç”¨æˆ·æ„ŸçŸ¥å†…å®¹å¯è§æ€§æ–¹é¢çš„å¿…è¦æ€§å’Œæœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.04072v1">Plan of Knowledge: Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering</a></td><td><details><summary>å±•å¼€</summary>Temporal Knowledge Graph Question Answering (TKGQA) aims to answer
time-sensitive questions by leveraging factual information from Temporal
Knowledge Graphs (TKGs). While previous studies have employed pre-trained TKG
embeddings or graph neural networks to inject temporal knowledge, they fail to
fully understand the complex semantic information of time constraints.
Recently, Large Language Models (LLMs) have shown remarkable progress,
benefiting from their strong semantic understanding and reasoning
generalization capabilities. However, their temporal reasoning ability remains
limited. LLMs frequently suffer from hallucination and a lack of knowledge. To
address these limitations, we propose the Plan of Knowledge framework with a
contrastive temporal retriever, which is named PoK. Specifically, the proposed
Plan of Knowledge module decomposes a complex temporal question into a sequence
of sub-objectives from the pre-defined tools, serving as intermediate guidance
for reasoning exploration. In parallel, we construct a Temporal Knowledge Store
(TKS) with a contrastive retrieval framework, enabling the model to selectively
retrieve semantically and temporally aligned facts from TKGs. By combining
structured planning with temporal knowledge retrieval, PoK effectively enhances
the interpretability and factual consistency of temporal reasoning. Extensive
experiments on four benchmark TKGQA datasets demonstrate that PoK significantly
improves the retrieval precision and reasoning accuracy of LLMs, surpassing the
performance of the state-of-the-art TKGQA methods by 56.0% at most.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºPoKçš„æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆçŸ¥è¯†è§„åˆ’å’Œå¯¹æ¯”æ€§æ—¶åºæ£€ç´¢å™¨ï¼Œå°†å¤æ‚æ—¶åºé—®é¢˜åˆ†è§£ä¸ºå­ç›®æ ‡å¹¶ä»ä¸­æ£€ç´¢æ—¶åºçŸ¥è¯†å›¾è°±ä¸­çš„ç›¸å…³äº‹å®ï¼Œä»¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹åœ¨æ—¶åºçŸ¥è¯†å›¾è°±é—®ç­”ä¸­çš„æ¨ç†å‡†ç¡®æ€§å’Œäº‹å®ä¸€è‡´æ€§ï¼Œæ˜¾è‘—æå‡äº†æ£€ç´¢ç²¾åº¦å’Œæ¨ç†æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.04020v1">Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) enhanced with retrieval -- commonly referred to
as Retrieval-Augmented Generation (RAG) -- have demonstrated strong performance
in knowledge-intensive tasks. However, RAG pipelines often fail when retrieved
evidence is incomplete, leaving gaps in the reasoning process. In such cases,
\emph{abductive inference} -- the process of generating plausible missing
premises to explain observations -- offers a principled approach to bridge
these gaps. In this paper, we propose a framework that integrates abductive
inference into retrieval-augmented LLMs. Our method detects insufficient
evidence, generates candidate missing premises, and validates them through
consistency and plausibility checks. Experimental results on abductive
reasoning and multi-hop QA benchmarks show that our approach improves both
answer accuracy and reasoning faithfulness. This work highlights abductive
inference as a promising direction for enhancing the robustness and
explainability of RAG systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§å°†æº¯å› æ¨ç†ï¼ˆabductive inferenceï¼‰æ•´åˆåˆ°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­çš„æ¡†æ¶ï¼Œé€šè¿‡æ£€æµ‹ä¸å®Œæ•´çš„æ£€ç´¢è¯æ®ã€ç”Ÿæˆç¼ºå¤±å‰æå¹¶è¿›è¡ŒéªŒè¯ï¼Œæå‡äº†RAGç³»ç»Ÿåœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„ç­”æ¡ˆå‡†ç¡®æ€§å’Œæ¨ç†å¯ä¿¡åº¦ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-05
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.03782v1">Expert Evaluation of LLM World Models: A High-$T_c$ Superconductivity Case Study</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) show great promise as a powerful tool for
scientific literature exploration. However, their effectiveness in providing
scientifically accurate and comprehensive answers to complex questions within
specialized domains remains an active area of research. Using the field of
high-temperature cuprates as an exemplar, we evaluate the ability of LLM
systems to understand the literature at the level of an expert. We construct an
expert-curated database of 1,726 scientific papers that covers the history of
the field, and a set of 67 expert-formulated questions that probe deep
understanding of the literature. We then evaluate six different LLM-based
systems for answering these questions, including both commercially available
closed models and a custom retrieval-augmented generation (RAG) system capable
of retrieving images alongside text. Experts then evaluate the answers of these
systems against a rubric that assesses balanced perspectives, factual
comprehensiveness, succinctness, and evidentiary support. Among the six systems
two using RAG on curated literature outperformed existing closed models across
key metrics, particularly in providing comprehensive and well-supported
answers. We discuss promising aspects of LLM performances as well as critical
short-comings of all the models. The set of expert-formulated questions and the
rubric will be valuable for assessing expert level performance of LLM based
reasoning systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç§‘å­¦æ–‡çŒ®æ¢ç´¢ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜æ¸©é“œé…¸ç›é¢†åŸŸçš„ä¸“å®¶çº§ç†è§£èƒ½åŠ›ã€‚ä½œè€…æ„å»ºäº†ä¸€ä¸ªåŒ…å«1,726ç¯‡ç§‘å­¦è®ºæ–‡çš„ä¸“å®¶ç²¾é€‰æ•°æ®åº“å’Œ67ä¸ªä¸“å®¶æå‡ºçš„é—®é¢˜ï¼Œè¯„ä¼°äº†å…­ç§åŸºäºLLMçš„ç³»ç»Ÿï¼ˆåŒ…æ‹¬å•†ä¸šé—­æºæ¨¡å‹å’Œè‡ªå®šä¹‰çš„RAGç³»ç»Ÿï¼‰çš„å›ç­”è´¨é‡ã€‚ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨RAGçš„ç³»ç»Ÿåœ¨å…³é”®æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜äºå…¶ä»–é—­æºæ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨æä¾›å…¨é¢å’Œè¯æ®æ”¯æŒçš„ç­”æ¡ˆæ–¹é¢ã€‚è®ºæ–‡è¿˜è®¨è®ºäº†LLMçš„ä¼˜åŠ¿å’Œä¸è¶³ï¼Œå¹¶æå‡ºäº†è¯„ä¼°ä¸“å®¶çº§LLMæ¨ç†ç³»ç»Ÿçš„å·¥å…·å’Œæ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.03563v1">ASVRI-Legal: Fine-Tuning LLMs with Retrieval Augmented Generation for Enhanced Legal Regulation</a></td><td><details><summary>å±•å¼€</summary>In this study, we explore the fine-tuning of Large Language Models (LLMs) to
better support policymakers in their crucial work of understanding, analyzing,
and crafting legal regulations. To equip the model with a deep understanding of
legal texts, we curated a supervised dataset tailored to the specific needs of
the legal domain. Additionally, we integrated the Retrieval-Augmented
Generation (RAG) method, enabling the LLM to access and incorporate up-to-date
legal knowledge from external sources. This combination of fine-tuning and
RAG-based augmentation results in a tool that not only processes legal
information but actively assists policymakers in interpreting regulations and
drafting new ones that align with current needs. The results demonstrate that
this approach can significantly enhance the effectiveness of legal research and
regulation development, offering a valuable resource in the ever-evolving field
of law.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†é€šè¿‡å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¹¶ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œä¸ºæ”¿ç­–åˆ¶å®šè€…æä¾›æ³•å¾‹æ–‡æœ¬åˆ†æä¸æ³•è§„åˆ¶å®šçš„è¾…åŠ©å·¥å…·ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæå‡æ³•å¾‹ç ”ç©¶å’Œæ³•è§„å¼€å‘çš„æ•ˆæœã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.03475v1">RAGBoost: Efficient Retrieval-Augmented Generation with Accuracy-Preserving Context Reuse</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) enhances large language models (LLMs)
with retrieved context but often suffers from downgraded prefill performance as
modern applications demand longer and more complex inputs. Existing caching
techniques either preserve accuracy with low cache reuse or improve reuse at
the cost of degraded reasoning quality. We present RAGBoost, an efficient RAG
system that achieves high cache reuse without sacrificing accuracy through
accuracy-preserving context reuse. RAGBoost detects overlapping retrieved items
across concurrent sessions and multi-turn interactions, using efficient context
indexing, ordering, and de-duplication to maximize reuse, while lightweight
contextual hints maintain reasoning fidelity. It integrates seamlessly with
existing LLM inference engines and improves their prefill performance by 1.5-3X
over state-of-the-art methods, while preserving or even enhancing reasoning
accuracy across diverse RAG and agentic AI workloads. Our code is released at:
https://github.com/Edinburgh-AgenticAI/RAGBoost.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†RAGBoostç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­å› é•¿è€Œå¤æ‚çš„è¾“å…¥å¯¼è‡´çš„é¢„å¡«å……æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚é€šè¿‡é«˜æ•ˆçš„ä¸Šä¸‹æ–‡ç´¢å¼•ã€æ’åºå’Œå»é‡æŠ€æœ¯ï¼Œå®ç°äº†é«˜ç¼“å­˜å¤ç”¨ç‡ï¼ŒåŒæ—¶ä¿æŒäº†æ¨ç†å‡†ç¡®æ€§ï¼Œç›¸æ¯”ç°æœ‰æ–¹æ³•æå‡äº†1.5-3å€çš„é¢„å¡«å……æ€§èƒ½ï¼Œå¹¶å…¼å®¹ç°æœ‰å¤§è¯­è¨€æ¨¡å‹æ¨ç†å¼•æ“ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.03261v1">Comparing the Performance of LLMs in RAG-based Question-Answering: A Case Study in Computer Science Literature</a></td><td><details><summary>å±•å¼€</summary>Retrieval Augmented Generation (RAG) is emerging as a powerful technique to
enhance the capabilities of Generative AI models by reducing hallucination.
Thus, the increasing prominence of RAG alongside Large Language Models (LLMs)
has sparked interest in comparing the performance of different LLMs in
question-answering (QA) in diverse domains. This study compares the performance
of four open-source LLMs, Mistral-7b-instruct, LLaMa2-7b-chat,
Falcon-7b-instruct and Orca-mini-v3-7b, and OpenAI's trending GPT-3.5 over QA
tasks within the computer science literature leveraging RAG support. Evaluation
metrics employed in the study include accuracy and precision for binary
questions and ranking by a human expert, ranking by Google's AI model Gemini,
alongside cosine similarity for long-answer questions. GPT-3.5, when paired
with RAG, effectively answers binary and long-answer questions, reaffirming its
status as an advanced LLM. Regarding open-source LLMs, Mistral AI's
Mistral-7b-instruct paired with RAG surpasses the rest in answering both binary
and long-answer questions. However, among the open-source LLMs, Orca-mini-v3-7b
reports the shortest average latency in generating responses, whereas
LLaMa2-7b-chat by Meta reports the highest average latency. This research
underscores the fact that open-source LLMs, too, can go hand in hand with
proprietary models like GPT-3.5 with better infrastructure.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†Retrieval-Augmented Generation (RAG) æŠ€æœ¯åœ¨æå‡ç”Ÿæˆå¼AIæ¨¡å‹æ€§èƒ½ä¸­çš„åº”ç”¨ï¼Œé€šè¿‡æ¯”è¾ƒä¸åŒå¼€æºLLMsï¼ˆå¦‚Mistral-7b-instructã€LLaMa2-7b-chatç­‰ï¼‰ä¸GPT-3.5åœ¨è®¡ç®—æœºç§‘å­¦æ–‡çŒ®é—®ç­”ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œè¯„ä¼°äº†å®ƒä»¬åœ¨äºŒè¿›åˆ¶é—®é¢˜å’Œé•¿ç­”æ¡ˆé—®é¢˜ä¸Šçš„å‡†ç¡®æ€§ã€å»¶è¿Ÿç­‰æŒ‡æ ‡ï¼Œå‘ç°ç»“åˆRAGçš„å¼€æºæ¨¡å‹ï¼ˆå¦‚Mistralï¼‰æ€§èƒ½æ¥è¿‘GPT-3.5ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.03214v1">LGM: Enhancing Large Language Models with Conceptual Meta-Relations and Iterative Retrieval</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) exhibit strong semantic understanding, yet
struggle when user instructions involve ambiguous or conceptually misaligned
terms. We propose the Language Graph Model (LGM) to enhance conceptual clarity
by extracting meta-relations-inheritance, alias, and composition-from natural
language. The model further employs a reflection mechanism to validate these
meta-relations. Leveraging a Concept Iterative Retrieval Algorithm, these
relations and related descriptions are dynamically supplied to the LLM,
improving its ability to interpret concepts and generate accurate responses.
Unlike conventional Retrieval-Augmented Generation (RAG) approaches that rely
on extended context windows, our method enables large language models to
process texts of any length without the need for truncation. Experiments on
standard benchmarks demonstrate that the LGM consistently outperforms existing
RAG baselines.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºLanguage Graph Modelï¼ˆLGMï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡ä»è‡ªç„¶è¯­è¨€ä¸­æå–ç»§æ‰¿ã€åˆ«åå’Œç»„åˆç­‰å…ƒå…³ç³»æ¥å¢å¼ºæ¦‚å¿µæ¸…æ™°åº¦ï¼Œå¹¶åˆ©ç”¨æ¦‚å¿µè¿­ä»£æ£€ç´¢ç®—æ³•åŠ¨æ€åœ°å°†è¿™äº›å…³ç³»å’Œç›¸å…³æè¿°æä¾›ç»™å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä»¥æé«˜å…¶è§£é‡Šæ¦‚å¿µå’Œç”Ÿæˆå‡†ç¡®å›ç­”çš„èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„RAGæ–¹æ³•ä¸åŒï¼ŒLGMä¸éœ€è¦ä¾èµ–æ‰©å±•çš„ä¸Šä¸‹æ–‡çª—å£å³å¯å¤„ç†ä»»æ„é•¿åº¦çš„æ–‡æœ¬ï¼Œä¸”å®éªŒè¡¨æ˜å…¶åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰çš„RAGåŸºçº¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.03149v1">Forecast2Anomaly (F2A): Adapting Multivariate Time Series Foundation Models for Anomaly Prediction</a></td><td><details><summary>å±•å¼€</summary>Forecasting anomalies (anomaly prediction) in multivariate time series from
different real-world, dynamic, and complex systems is vital for preempting
critical failures, leading to a substantial minimization in operational costs
and human labor. Yet, existing methods are limited to specific systems while
failing to generalize to evolving anomaly patterns over time. In contrast,
pretrained Time Series Foundation Models (TSFMs) have recently demonstrated
strong generalization and zero-shot forecasting capabilities. However, their
potential remains untapped for anomaly prediction, a task fundamentally
different from forecasting normal behavior. Thus, we present Forecast2Anomaly
(F2A), a novel framework that empowers TSFMs with anomaly prediction abilities
through two key innovations. First, we propose a joint forecast-anomaly loss
that fine-tunes TSFMs to accurately forecast future signals even at anomalous
time points. Second, we introduce a Retrieval-Augmented Generation (RAG) module
that retrieves historically relevant horizons and conditions predictions on
them. This component dynamically adapts to distributional shifts at inference
time, enabling F2A to track evolving anomalies without requiring model updates.
By combining targeted fine-tuning with dynamic retrieval, F2A bridges the gap
between robust TSFM zero-shot forecasting and zero-shot anomaly prediction.
Extensive experiments across 16 diverse datasets and multiple TSFM backbones
show that F2A consistently outperforms state-of-the-art methods, offering a
scalable, zero-shot anomaly prediction solution for real-world applications.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†Forecast2Anomalyï¼ˆF2Aï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆé¢„è®­ç»ƒçš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼ˆTSFMsï¼‰å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¨¡å—ï¼Œæå‡äº†å¯¹å¤šå˜é‡æ—¶é—´åºåˆ—ä¸­å¼‚å¸¸è¡Œä¸ºçš„é¢„æµ‹èƒ½åŠ›ã€‚F2Aåˆ©ç”¨è”åˆé¢„æµ‹-å¼‚å¸¸æŸå¤±å¾®è°ƒæ¨¡å‹ï¼Œå¹¶é€šè¿‡RAGæ¨¡å—åŠ¨æ€æ£€ç´¢å†å²ç›¸å…³æ•°æ®ä»¥é€‚åº”åˆ†å¸ƒå˜åŒ–ï¼Œå®ç°äº†åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹çš„é«˜æ•ˆå¼‚å¸¸é¢„æµ‹ã€‚å®éªŒè¡¨æ˜å…¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.03138v1">A Proprietary Model-Based Safety Response Framework for AI Agents</a></td><td><details><summary>å±•å¼€</summary>With the widespread application of Large Language Models (LLMs), their
associated security issues have become increasingly prominent, severely
constraining their trustworthy deployment in critical domains. This paper
proposes a novel safety response framework designed to systematically safeguard
LLMs at both the input and output levels. At the input level, the framework
employs a supervised fine-tuning-based safety classification model. Through a
fine-grained four-tier taxonomy (Safe, Unsafe, Conditionally Safe, Focused
Attention), it performs precise risk identification and differentiated handling
of user queries, significantly enhancing risk coverage and business scenario
adaptability, and achieving a risk recall rate of 99.3%. At the output level,
the framework integrates Retrieval-Augmented Generation (RAG) with a
specifically fine-tuned interpretation model, ensuring all responses are
grounded in a real-time, trustworthy knowledge base. This approach eliminates
information fabrication and enables result traceability. Experimental results
demonstrate that our proposed safety control model achieves a significantly
higher safety score on public safety evaluation benchmarks compared to the
baseline model, TinyR1-Safety-8B. Furthermore, on our proprietary high-risk
test set, the framework's components attained a perfect 100% safety score,
validating their exceptional protective capabilities in complex risk scenarios.
This research provides an effective engineering pathway for building
high-security, high-trust LLM applications.</details></td><td><details><summary>å±•å¼€</summary></details></td></tr></tbody></table>

### ğŸ“… 2025-11-04
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.03048v1">ROBoto2: An Interactive System and Dataset for LLM-assisted Clinical Trial Risk of Bias Assessment</a></td><td><details><summary>å±•å¼€</summary>We present ROBOTO2, an open-source, web-based platform for large language
model (LLM)-assisted risk of bias (ROB) assessment of clinical trials. ROBOTO2
streamlines the traditionally labor-intensive ROB v2 (ROB2) annotation process
via an interactive interface that combines PDF parsing, retrieval-augmented LLM
prompting, and human-in-the-loop review. Users can upload clinical trial
reports, receive preliminary answers and supporting evidence for ROB2 signaling
questions, and provide real-time feedback or corrections to system suggestions.
ROBOTO2 is publicly available at https://roboto2.vercel.app/, with code and
data released to foster reproducibility and adoption. We construct and release
a dataset of 521 pediatric clinical trial reports (8954 signaling questions
with 1202 evidence passages), annotated using both manually and LLM-assisted
methods, serving as a benchmark and enabling future research. Using this
dataset, we benchmark ROB2 performance for 4 LLMs and provide an analysis into
current model capabilities and ongoing challenges in automating this critical
aspect of systematic review.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ROBOTO2ï¼Œä¸€ä¸ªåŸºäºç½‘ç»œçš„å¼€æºå¹³å°ï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¾…åŠ©çš„ä¸´åºŠè¯•éªŒåå€šé£é™©ï¼ˆROB2ï¼‰è¯„ä¼°ã€‚è¯¥å¹³å°é€šè¿‡ç»“åˆPDFè§£æã€æ£€ç´¢å¢å¼ºçš„LLMæç¤ºå’Œäººå·¥å‚ä¸å®¡æŸ¥çš„äº¤äº’ç•Œé¢ï¼Œç®€åŒ–äº†ä¼ ç»Ÿçš„ROB2æ³¨é‡Šæµç¨‹ã€‚æ–‡ç« è¿˜å…¬å¼€äº†ä¸€ä¸ªåŒ…å«521ä»½å„¿ç§‘ä¸´åºŠè¯•éªŒæŠ¥å‘Šçš„æ•°æ®é›†ï¼Œå¹¶è¯„ä¼°äº†4ç§LLMåœ¨ROB2ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œåˆ†æäº†å½“å‰æ¨¡å‹çš„æŒ‘æˆ˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.02979v1">Systematizing LLM Persona Design: A Four-Quadrant Technical Taxonomy for AI Companion Applications</a></td><td><details><summary>å±•å¼€</summary>The design and application of LLM-based personas in AI companionship is a
rapidly expanding but fragmented field, spanning from virtual emotional
companions and game NPCs to embodied functional robots. This diversity in
objectives, modality, and technical stacks creates an urgent need for a unified
framework. To address this gap, this paper systematizes the field by proposing
a Four-Quadrant Technical Taxonomy for AI companion applications. The framework
is structured along two critical axes: Virtual vs. Embodied and Emotional
Companionship vs. Functional Augmentation. Quadrant I (Virtual Companionship)
explores virtual idols, romantic companions, and story characters, introducing
a four-layer technical framework to analyze their challenges in maintaining
long-term emotional consistency. Quadrant II (Functional Virtual Assistants)
analyzes AI applications in work, gaming, and mental health, highlighting the
shift from "feeling" to "thinking and acting" and pinpointing key technologies
like enterprise RAG and on-device inference. Quadrants III & IV (Embodied
Intelligence) shift from the virtual to the physical world, analyzing home
robots and vertical-domain assistants, revealing core challenges in symbol
grounding, data privacy, and ethical liability. This taxonomy provides not only
a systematic map for researchers and developers to navigate the complex persona
design space but also a basis for policymakers to identify and address the
unique risks inherent in different application scenarios.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªAIä¼´ä¾£çš„å››è±¡é™æŠ€æœ¯åˆ†ç±»æ¡†æ¶ï¼Œå…¶ä¸­åœ¨ç¬¬äºŒè±¡é™ï¼ˆåŠŸèƒ½æ€§è™šæ‹ŸåŠ©æ‰‹ï¼‰æåˆ°äº†ä¼ä¸šRAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æŠ€æœ¯ï¼Œä½œä¸ºå®ç°å·¥ä½œã€æ¸¸æˆå’Œå¿ƒç†å¥åº·é¢†åŸŸAIåº”ç”¨çš„å…³é”®æŠ€æœ¯ä¹‹ä¸€ï¼Œçªå‡ºäº†RAGåœ¨ä»â€œæƒ…æ„Ÿâ€å‘â€œæ€è€ƒä¸è¡ŒåŠ¨â€è½¬å˜ä¸­çš„é‡è¦æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.02919v1">Cache Mechanism for Agent RAG Systems</a></td><td><details><summary>å±•å¼€</summary>Recent advances in Large Language Model (LLM)-based agents have been
propelled by Retrieval-Augmented Generation (RAG), which grants the models
access to vast external knowledge bases. Despite RAG's success in improving
agent performance, agent-level cache management, particularly constructing,
maintaining, and updating a compact, relevant corpus dynamically tailored to
each agent's need, remains underexplored. Therefore, we introduce ARC (Agent
RAG Cache Mechanism), a novel, annotation-free caching framework that
dynamically manages small, high-value corpora for each agent. By synthesizing
historical query distribution patterns with the intrinsic geometry of cached
items in the embedding space, ARC automatically maintains a high-relevance
cache. With comprehensive experiments on three retrieval datasets, our
experimental results demonstrate that ARC reduces storage requirements to
0.015% of the original corpus while offering up to 79.8% has-answer rate and
reducing average retrieval latency by 80%. Our results demonstrate that ARC can
drastically enhance efficiency and effectiveness in RAG-powered LLM agents.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ARCï¼ˆAgent RAG Cache Mechanismï¼‰ï¼Œä¸€ç§æ— éœ€æ ‡æ³¨çš„ç¼“å­˜æ¡†æ¶ï¼Œæ—¨åœ¨åŠ¨æ€ç®¡ç†é¢å‘RAGé©±åŠ¨çš„LLMä»£ç†çš„å°å‹é«˜ä»·å€¼è¯­æ–™åº“ã€‚é€šè¿‡ç»“åˆå†å²æŸ¥è¯¢åˆ†å¸ƒæ¨¡å¼å’ŒåµŒå…¥ç©ºé—´ä¸­ç¼“å­˜é¡¹çš„å†…åœ¨å‡ ä½•ç»“æ„ï¼ŒARCèƒ½è‡ªåŠ¨ç»´æŠ¤é«˜ç›¸å…³æ€§ç¼“å­˜ï¼Œæ˜¾è‘—å‡å°‘å­˜å‚¨éœ€æ±‚ï¼ˆè‡³åŸè¯­æ–™çš„0.015%ï¼‰ï¼Œæå‡å›ç­”ç‡ï¼ˆè¾¾79.8%ï¼‰å¹¶é™ä½80%çš„å¹³å‡æ£€ç´¢å»¶è¿Ÿï¼Œä»è€Œä¼˜åŒ–RAGä»£ç†çš„æ•ˆç‡å’Œæ•ˆæœã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.02749v1">Using Span Queries to Optimize for Cache and Attention Locality</a></td><td><details><summary>å±•å¼€</summary>Clients are evolving beyond chat completion, and now include a variety of
innovative inference-time scaling and deep reasoning techniques. At the same
time, inference servers remain heavily optimized for chat completion. Prior
work has shown that large improvements to KV cache hit rate are possible if
inference servers evolve towards these non-chat use cases. However, they offer
solutions that are also optimized for a single use case, RAG. In this paper, we
introduce the span query to generalize the interface to the inference server.
We demonstrate that chat, RAG, inference-time scaling, and agentic workloads
can all be expressed as span queries. We show how the critical distinction that
had been assumed by prior work lies in whether the order of the inputs matter
-- do they commute? In chat, they do not. In RAG, they often do. This paper
introduces span queries, which are expression trees of inference calls, linked
together with commutativity constraints. We describe span query syntax and
semantics. We show how they can be automatically optimized to improve KV cache
locality. We show how a small change to vLLM (affecting only 492 lines) can
enable high-performance execution of span queries. Using this stack, we
demonstrate that span queries can achieve 10-20x reductions in TTFT for two
distinct non-chat use cases. Finally, we show that span queries can also be
optimized to improve attention locality, so as to avoid the so-called
lost-in-the-middle problem. We demonstrate that an attention-optimized span
query on a 2b parameter model vastly outperforms the accuracy of a stock
inference server using an 8b model.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç§°ä¸ºâ€œspan queryâ€çš„é€šç”¨æ¥å£ï¼Œç”¨äºæ¨ç†æœåŠ¡å™¨ï¼Œèƒ½å¤Ÿç»Ÿä¸€å¤„ç†åŒ…æ‹¬RAGã€èŠå¤©ã€æ¨ç†æ—¶é—´æ‰©å±•å’Œä»£ç†å·¥ä½œè´Ÿè½½åœ¨å†…çš„å¤šç§ä»»åŠ¡ã€‚æ–‡ç« æå‡ºäº†span queryçš„è¯­æ³•å’Œè¯­ä¹‰ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•é€šè¿‡ä¼˜åŒ–KVç¼“å­˜å±€éƒ¨æ€§å’Œæ³¨æ„åŠ›å±€éƒ¨æ€§æ¥æå‡æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨éèŠå¤©ç”¨ä¾‹ä¸­æ˜¾è‘—å‡å°‘é¦–æ¬¡ä»¤ç‰Œç”Ÿæˆæ—¶é—´ï¼ˆTTFTï¼‰å’Œè§£å†³â€œè¿·å¤±åœ¨ä¸­é—´â€é—®é¢˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.02490v1">BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and Monitoring</a></td><td><details><summary>å±•å¼€</summary>As the global burden of Alzheimer's disease (AD) continues to grow, early and
accurate detection has become increasingly critical, especially in regions with
limited access to advanced diagnostic tools. We propose BRAINS (Biomedical
Retrieval-Augmented Intelligence for Neurodegeneration Screening) to address
this challenge. This novel system harnesses the powerful reasoning capabilities
of Large Language Models (LLMs) for Alzheimer's detection and monitoring.
BRAINS features a dual-module architecture: a cognitive diagnostic module and a
case-retrieval module. The Diagnostic Module utilizes LLMs fine-tuned on
cognitive and neuroimaging datasets -- including MMSE, CDR scores, and brain
volume metrics -- to perform structured assessments of Alzheimer's risk.
Meanwhile, the Case Retrieval Module encodes patient profiles into latent
representations and retrieves similar cases from a curated knowledge base.
These auxiliary cases are fused with the input profile via a Case Fusion Layer
to enhance contextual understanding. The combined representation is then
processed with clinical prompts for inference. Evaluations on real-world
datasets demonstrate BRAINS effectiveness in classifying disease severity and
identifying early signs of cognitive decline. This system not only shows strong
potential as an assistive tool for scalable, explainable, and early-stage
Alzheimer's disease detection, but also offers hope for future applications in
the field.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºBRAINSçš„ç³»ç»Ÿï¼Œåˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œç—…ä¾‹æ£€ç´¢æ¨¡å—ï¼Œé€šè¿‡ç»“æ„åŒ–è¯„ä¼°å’Œç±»ä¼¼ç—…ä¾‹çš„æ£€ç´¢èåˆï¼Œæé«˜äº†é˜¿å°”èŒ¨æµ·é»˜ç—…çš„æ—©æœŸæ£€æµ‹å’Œè¯Šæ–­å‡†ç¡®æ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨ä¸´åºŠè¾…åŠ©å·¥å…·ä¸­çš„æ½œåŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.02371v1">LUMA-RAG: Lifelong Multimodal Agents with Provably Stable Streaming Alignment</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has emerged as the dominant paradigm for
grounding large language model outputs in verifiable evidence. However, as
modern AI agents transition from static knowledge bases to continuous
multimodal streams encompassing text, images, video, and audio, two critical
challenges arise: maintaining index freshness without prohibitive re-indexing
costs, and preserving cross-modal semantic consistency across heterogeneous
embedding spaces. We present LUMA-RAG, a lifelong multimodal agent architecture
featuring three key innovations: (i) a streaming, multi-tier memory system that
dynamically spills embeddings from a hot HNSW tier to a compressed IVFPQ tier
under strict memory budgets; (ii) a streaming CLAP->CLIP alignment bridge that
maintains cross-modal consistency through incremental orthogonal Procrustes
updates; and (iii) stability-aware retrieval telemetry providing Safe@k
guarantees by jointly bounding alignment drift and quantization error.
Experiments demonstrate robust text-to-image retrieval (Recall@10 = 0.94),
graceful performance degradation under product quantization offloading, and
provably stable audio-to-image rankings (Safe@1 = 1.0), establishing LUMA-RAG
as a practical framework for production multimodal RAG systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºLUMA-RAGçš„æ–°å‹ç»ˆèº«å¤šæ¨¡æ€ä»£ç†æ¶æ„ï¼Œæ—¨åœ¨è§£å†³ç°ä»£AIä»£ç†åœ¨å¤„ç†è¿ç»­å¤šæ¨¡æ€æ•°æ®æµï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ï¼‰æ—¶é¢ä¸´çš„ç´¢å¼•æ›´æ–°æˆæœ¬é«˜å’Œè·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§ç­‰æŒ‘æˆ˜ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬åŠ¨æ€åˆ†å±‚çš„æµå¼è®°å¿†ç³»ç»Ÿã€å¢é‡æ­£äº¤æ›´æ–°çš„è·¨æ¨¡æ€å¯¹é½æ¡¥æ¥æŠ€æœ¯ï¼Œä»¥åŠå…¼å…·ç¨³å®šæ€§ä¿è¯çš„æ£€ç´¢æœºåˆ¶ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨å¤šæ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸­å…·æœ‰é«˜æ•ˆæ€§å’Œé²æ£’æ€§ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-03
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.02119v1">InsurAgent: A Large Language Model-Empowered Agent for Simulating Individual Behavior in Purchasing Flood Insurance</a></td><td><details><summary>å±•å¼€</summary>Flood insurance is an effective strategy for individuals to mitigate
disaster-related losses. However, participation rates among at-risk populations
in the United States remain strikingly low. This gap underscores the need to
understand and model the behavioral mechanisms underlying insurance decisions.
Large language models (LLMs) have recently exhibited human-like intelligence
across wide-ranging tasks, offering promising tools for simulating human
decision-making. This study constructs a benchmark dataset to capture insurance
purchase probabilities across factors. Using this dataset, the capacity of LLMs
is evaluated: while LLMs exhibit a qualitative understanding of factors, they
fall short in estimating quantitative probabilities. To address this
limitation, InsurAgent, an LLM-empowered agent comprising five modules
including perception, retrieval, reasoning, action, and memory, is proposed.
The retrieval module leverages retrieval-augmented generation (RAG) to ground
decisions in empirical survey data, achieving accurate estimation of marginal
and bivariate probabilities. The reasoning module leverages LLM common sense to
extrapolate beyond survey data, capturing contextual information that is
intractable for traditional models. The memory module supports the simulation
of temporal decision evolutions, illustrated through a roller coaster life
trajectory. Overall, InsurAgent provides a valuable tool for behavioral
modeling and policy analysis.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ç ”ç©¶äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨¡æ‹Ÿæ´ªæ°´ä¿é™©è´­ä¹°å†³ç­–è¡Œä¸ºçš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºInsurAgentçš„LLMå¢å¼ºä»£ç†ç³»ç»Ÿï¼Œå…¶ä¸­æ£€ç´¢æ¨¡å—é‡‡ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œé€šè¿‡ç»“åˆå®è¯è°ƒæŸ¥æ•°æ®æå‡æ¦‚ç‡ä¼°è®¡çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶ç»“åˆæ¨ç†å’Œè®°å¿†æ¨¡å—æ¨¡æ‹ŸåŠ¨æ€å†³ç­–è¿‡ç¨‹ï¼Œä¸ºè¡Œä¸ºå»ºæ¨¡å’Œæ”¿ç­–åˆ†ææä¾›äº†æ–°å·¥å…·ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.01668v1">Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics</a></td><td><details><summary>å±•å¼€</summary>As artificial intelligence permeates judicial forensics, ensuring the
veracity and traceability of legal question answering (QA) has become critical.
Conventional large language models (LLMs) are prone to hallucination, risking
misleading guidance in legal consultation, while static knowledge bases
struggle to keep pace with frequently updated statutes and case law. We present
a hybrid legal QA agent tailored for judicial settings that integrates
retrieval-augmented generation (RAG) with multi-model ensembling to deliver
reliable, auditable, and continuously updatable counsel. The system prioritizes
retrieval over generation: when a trusted legal repository yields relevant
evidence, answers are produced via RAG; otherwise, multiple LLMs generate
candidates that are scored by a specialized selector, with the top-ranked
answer returned. High-quality outputs then undergo human review before being
written back to the repository, enabling dynamic knowledge evolution and
provenance tracking. Experiments on the Law\_QA dataset show that our hybrid
approach significantly outperforms both a single-model baseline and a vanilla
RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm
the complementary contributions of retrieval prioritization, model ensembling,
and the human-in-the-loop update mechanism. The proposed system demonstrably
reduces hallucination while improving answer quality and legal compliance,
advancing the practical landing of media forensics technologies in judicial
scenarios.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å¸æ³•ç¯å¢ƒçš„æ··åˆæ³•å¾‹é—®ç­”ä»£ç†ï¼Œç»“åˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸å¤šæ¨¡å‹é›†æˆæŠ€æœ¯ï¼Œä»¥æé«˜æ³•å¾‹é—®ç­”çš„å¯é æ€§ã€å¯å®¡è®¡æ€§å’Œå¯æ›´æ–°æ€§ã€‚ç³»ç»Ÿä¼˜å…ˆé€šè¿‡æ£€ç´¢è·å–å¯ä¿¡æ³•å¾‹è¯æ®ï¼Œè‹¥æ£€ç´¢å¤±è´¥åˆ™è°ƒç”¨å¤šæ¨¡å‹ç”Ÿæˆå€™ç­”æ¡ˆå¹¶ç­›é€‰æœ€ä¼˜ç»“æœï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºå•ä¸€æ¨¡å‹æˆ–åŸºç¡€RAGæµç¨‹ï¼ŒåŒæ—¶é€šè¿‡äººå·¥å®¡æ ¸å¾ªç¯æ›´æ–°çŸ¥è¯†åº“ï¼Œæœ‰æ•ˆå‡å°‘äº†å¹»è§‰å¹¶æå‡æ³•å¾‹åˆè§„æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.01649v1">Evaluating Cultural Knowledge Processing in Large Language Models: A Cognitive Benchmarking Framework Integrating Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>This study proposes a cognitive benchmarking framework to evaluate how large
language models (LLMs) process and apply culturally specific knowledge. The
framework integrates Bloom's Taxonomy with Retrieval-Augmented Generation (RAG)
to assess model performance across six hierarchical cognitive domains:
Remembering, Understanding, Applying, Analyzing, Evaluating, and Creating.
Using a curated Taiwanese Hakka digital cultural archive as the primary
testbed, the evaluation measures LLM-generated responses' semantic accuracy and
cultural relevance.</details></td><td><details><summary>å±•å¼€</summary>è¯¥ç ”ç©¶æå‡ºä¸€ä¸ªè®¤çŸ¥è¯„ä¼°æ¡†æ¶ï¼Œåˆ©ç”¨Bloomåˆ†ç±»æ³•ä¸RAGæŠ€æœ¯ç›¸ç»“åˆï¼Œè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å’Œåº”ç”¨ç‰¹å®šæ–‡åŒ–çŸ¥è¯†ï¼ˆä»¥å°æ¹¾å®¢å®¶æ•°å­—æ–‡åŒ–æ¡£æ¡ˆä¸ºä¾‹ï¼‰æ—¶çš„è¡¨ç°ï¼Œæ¶µç›–å…­ä¸ªè®¤çŸ¥å±‚çº§ï¼Œå¹¶æ£€æµ‹ç”Ÿæˆå›ç­”çš„è¯­ä¹‰å‡†ç¡®æ€§å’Œæ–‡åŒ–ç›¸å…³æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.01643v1">A Graph-based RAG for Energy Efficiency Question Answering</a></td><td><details><summary>å±•å¼€</summary>In this work, we investigate the use of Large Language Models (LLMs) within a
graph-based Retrieval Augmented Generation (RAG) architecture for Energy
Efficiency (EE) Question Answering. First, the system automatically extracts a
Knowledge Graph (KG) from guidance and regulatory documents in the energy
field. Then, the generated graph is navigated and reasoned upon to provide
users with accurate answers in multiple languages. We implement a human-based
validation using the RAGAs framework properties, a validation dataset
comprising 101 question-answer pairs, and domain experts. Results confirm the
potential of this architecture and identify its strengths and weaknesses.
Validation results show how the system correctly answers in about three out of
four of the cases (75.2 +- 2.7%), with higher results on questions related to
more general EE answers (up to 81.0 +- 4.1%), and featuring promising
multilingual abilities (4.4% accuracy loss due to translation).</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå›¾ç»“æ„çš„RAGæ¶æ„ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œèƒ½æºæ•ˆç‡ï¼ˆEEï¼‰é¢†åŸŸçš„å¤šè¯­è¨€é—®ç­”ã€‚ç³»ç»Ÿä»èƒ½æºé¢†åŸŸçš„æŒ‡å¯¼æ–‡ä»¶å’Œæ³•è§„ä¸­è‡ªåŠ¨æå–çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ï¼Œå¹¶é€šè¿‡å›¾è°±å¯¼èˆªå’Œæ¨ç†ç”Ÿæˆç²¾ç¡®ç­”æ¡ˆã€‚ç ”ç©¶é‡‡ç”¨äººå·¥éªŒè¯ï¼ˆRAGAsæ¡†æ¶ã€101ç»„QAæ•°æ®é›†åŠä¸“å®¶è¯„ä¼°ï¼‰ï¼Œç»“æœæ˜¾ç¤ºæ¨¡å‹æ•´ä½“å‡†ç¡®ç‡è¾¾75.2%ï¼Œé€šç”¨ç±»é—®é¢˜å‡†ç¡®ç‡æ›´é«˜ï¼ˆ81%ï¼‰ï¼Œä¸”å¤šè¯­è¨€èƒ½åŠ›è¡¨ç°è‰¯å¥½ï¼ˆç¿»è¯‘ä»…å¯¼è‡´4.4%å‡†ç¡®ç‡ä¸‹é™ï¼‰ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.01581v1">ExplicitLM: Decoupling Knowledge from Parameters via Explicit Memory Banks</a></td><td><details><summary>å±•å¼€</summary>Large language models suffer from knowledge staleness and lack of
interpretability due to implicit knowledge storage across entangled network
parameters, preventing targeted updates and reasoning transparency. We propose
ExplicitLM, a novel architecture featuring a million-scale external memory bank
storing human-readable knowledge as token sequences, enabling direct inspection
and modification. We design a differentiable two-stage retrieval mechanism with
efficient coarse-grained filtering via product key decomposition (reducing
complexity from $\mathcal{O}(N \cdot |I|)$ to $\mathcal{O}(\sqrt{N} \cdot
|I|)$) and fine-grained Gumbel-Softmax matching for end-to-end training.
Inspired by dual-system cognitive theory, we partition knowledge into frozen
explicit facts (20%) and learnable implicit patterns (80%), maintained through
Exponential Moving Average updates for stability. ExplicitLM achieves up to
43.67% improvement on knowledge-intensive tasks versus standard Transformers,
with 3.62$\times$ gains in low-data regimes (10k samples). Analysis shows
strong correlations between memory retrieval and performance, with correct
predictions achieving 49% higher hit rates. Unlike RAG systems with frozen
retrieval, our jointly optimized architecture demonstrates that interpretable,
updatable models can maintain competitive performance while providing
unprecedented knowledge transparency.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºExplicitLMçš„æ–°æ¶æ„ï¼Œé€šè¿‡å¼•å…¥ä¸€ä¸ªå¤§è§„æ¨¡å¤–éƒ¨å­˜å‚¨åº“ï¼ˆå­˜å‚¨äººç±»å¯è¯»çš„çŸ¥è¯†ä½œä¸ºæ ‡è®°åºåˆ—ï¼‰æ¥è§£å†³å¤§è¯­è¨€æ¨¡å‹ä¸­çŸ¥è¯†è¿‡æ—¶å’Œå¯è§£é‡Šæ€§ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¶æ„é‡‡ç”¨å¯å¾®åˆ†çš„ä¸¤é˜¶æ®µæ£€ç´¢æœºåˆ¶ï¼ˆç²—ç²’åº¦ç­›é€‰å’Œç»†ç²’åº¦åŒ¹é…ï¼‰è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œå¹¶å°†çŸ¥è¯†åˆ†ä¸ºå†»ç»“çš„æ˜¾å¼äº‹å®ï¼ˆ20%ï¼‰å’Œå¯å­¦ä¹ çš„éšå¼æ¨¡å¼ï¼ˆ80%ï¼‰ã€‚ä¸æ ‡å‡†Transformerç›¸æ¯”ï¼ŒExplicitLMåœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šæ€§èƒ½æå‡è¾¾43.67%ï¼Œå¹¶åœ¨ä½æ•°æ®åœºæ™¯ä¸‹è¡¨ç°æ›´ä¼˜ã€‚å…¶æ ¸å¿ƒè´¡çŒ®æ˜¯é€šè¿‡è”åˆä¼˜åŒ–å®ç°å¯è§£é‡Šã€å¯æ›´æ–°çš„çŸ¥è¯†é€æ˜åŒ–æ¨¡å‹ï¼Œä¸RAGç±»ä¼¼ä½†çªç ´äº†ä¼ ç»ŸRAGæ£€ç´¢æ¨¡å—å›ºå®šçš„é™åˆ¶ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.01454v1">"Don't Teach Minerva": Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG</a></td><td><details><summary>å±•å¼€</summary>Translating a morphology-rich, low-resource language like Latin poses
significant challenges. This paper introduces a reproducible draft-based
refinement pipeline that elevates open-source Large Language Models (LLMs) to a
performance level statistically comparable to top-tier proprietary systems. Our
method first uses a fine-tuned NLLB-1.3B model to generate a high-quality,
structurally faithful draft. A zero-shot LLM (Llama-3.3 or Qwen3) then polishes
this draft, a process that can be further enhanced by augmenting the context
with retrieved out-context examples (RAG). We demonstrate the robustness of
this approach on two distinct benchmarks: a standard in-domain test set
(Rosenthal, 2023) and a new, challenging out-of-domain (OOD) set of
12th-century Latin letters (2025). Our central finding is that this open-source
RAG system achieves performance statistically comparable to the GPT-5 baseline,
without any task-specific LLM fine-tuning. We release the pipeline, the
Chartres OOD set, and evaluation scripts and models to facilitate replicability
and further research.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆå¾®è°ƒæ¨¡å‹ä¸é›¶æ ·æœ¬å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ç¿»è¯‘æµç¨‹ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å¼•å…¥å¤–éƒ¨ä¸Šä¸‹æ–‡ç¤ºä¾‹ä¼˜åŒ–ä½èµ„æºè¯­è¨€ï¼ˆæ‹‰ä¸è¯­ï¼‰çš„ç¿»è¯‘è´¨é‡ï¼Œåœ¨å¤šé¡¹æµ‹è¯•ä¸­è¾¾åˆ°ä¸é¡¶çº§ç§æœ‰ç³»ç»Ÿç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶å¼€æºäº†ç›¸å…³æ•°æ®é›†ä¸å·¥å…·ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.01409v2">LiveSearchBench: An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge</a></td><td><details><summary>å±•å¼€</summary>Evaluating large language models (LLMs) on question answering often relies on
static benchmarks that reward memorization and understate the role of
retrieval, failing to capture the dynamic nature of world knowledge. We present
LiveSearchBench, an automated pipeline for constructing retrieval-dependent
benchmarks from recent knowledge updates. Our method computes deltas between
successive Wikidata snapshots, filters candidate triples for quality, and
synthesizes natural-language questions at three levels of reasoning difficulty,
each guaranteed to admit a unique, verifiable answer through SPARQL validation.
The pipeline is fully automated, scalable across time, and minimizes human
intervention, enabling continual regeneration of temporally grounded
benchmarks. Experiments show a pronounced performance drop when models confront
facts that post-date pretraining, with the gap most salient on multi-hop
queries. Retrieval augmented methods and larger, instruction-tuned models
provide partial gains but fail to close this recency gap. By design,
LiveSearchBench shifts evaluation from static memorization toward tasks that
require up-to-date retrieval and reasoning, offering a foundation for
systematic, long-term assessment of LLMs under evolving knowledge.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†LiveSearchBenchï¼Œä¸€ä¸ªè‡ªåŠ¨åŒ–æ„å»ºä¾èµ–æ£€ç´¢çš„åŸºå‡†æµ‹è¯•çš„æµç¨‹ï¼Œæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ—¶æ•ˆæ€§çŸ¥è¯†æ—¶çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°æ¨¡å‹åœ¨é¢å¯¹è®­ç»ƒæ•°æ®ä¹‹åçš„æ–°äº‹å®æ—¶æ€§èƒ½ä¸‹é™ï¼Œå°¤å…¶æ˜¯å¤šè·³æŸ¥è¯¢ï¼Œæ£€ç´¢å¢å¼ºæ–¹æ³•å’Œå¤§æ¨¡å‹ä»…èƒ½éƒ¨åˆ†ç¼“è§£è¿™ä¸€é—®é¢˜ã€‚è¯¥æ–¹æ³•å¼ºè°ƒäº†æ£€ç´¢å’Œæ—¶æ•ˆæ€§æ¨ç†åœ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½ä¸­çš„é‡è¦æ€§ï¼Œä¸RAGæŠ€æœ¯çš„ç›®æ ‡ä¸€è‡´ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.01386v1">RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) quality depends on many interacting
choices across retrieval, ranking, augmentation, prompting, and generation, so
optimizing modules in isolation is brittle. We introduce RAGSmith, a modular
framework that treats RAG design as an end-to-end architecture search over nine
technique families and 46{,}080 feasible pipeline configurations. A genetic
search optimizes a scalar objective that jointly aggregates retrieval metrics
(recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic
similarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law,
Finance, Medicine, Defense Industry, Computer Science), each with 100 questions
spanning factual, interpretation, and long-answer types. RAGSmith finds
configurations that consistently outperform naive RAG baseline by +3.8\% on
average (range +1.2\% to +6.9\% across domains), with gains up to +12.5\% in
retrieval and +7.5\% in generation. The search typically explores $\approx
0.2\%$ of the space ($\sim 100$ candidates) and discovers a robust backbone --
vector retrieval plus post-generation reflection/revision -- augmented by
domain-dependent choices in expansion, reranking, augmentation, and prompt
reordering; passage compression is never selected. Improvement magnitude
correlates with question type, with larger gains on factual/long-answer mixes
than interpretation-heavy sets. These results provide practical, domain-aware
guidance for assembling effective RAG systems and demonstrate the utility of
evolutionary search for full-pipeline optimization.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†RAGSmithï¼Œä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¨¡å—åŒ–æ¡†æ¶ï¼Œé€šè¿‡é—ä¼ ç®—æ³•åœ¨46,080ç§å¯è¡Œçš„RAGé…ç½®ä¸­è¿›è¡Œæœç´¢ï¼Œä¼˜åŒ–æ£€ç´¢å’Œç”Ÿæˆçš„ç»¼åˆæŒ‡æ ‡ã€‚åœ¨å…­ä¸ªWikipediaè¡ç”Ÿé¢†åŸŸçš„å®éªŒä¸­ï¼Œè¯¥æ¡†æ¶æ‰¾åˆ°çš„é…ç½®å¹³å‡ä¼˜äºåŸºçº¿3.8%ï¼Œå¹¶æ­ç¤ºäº†é¢†åŸŸç›¸å…³çš„ä¼˜åŒ–ç­–ç•¥ï¼ˆå¦‚å‘é‡æ£€ç´¢å’Œåç”Ÿæˆåæ€ï¼‰ï¼Œä¸ºæ„å»ºé«˜æ•ˆRAGç³»ç»Ÿæä¾›å®è·µæŒ‡å¯¼ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.01305v1">DeepSpecs: Expert-Level Questions Answering in 5G</a></td><td><details><summary>å±•å¼€</summary>5G technology enables mobile Internet access for billions of users. Answering
expert-level questions about 5G specifications requires navigating thousands of
pages of cross-referenced standards that evolve across releases. Existing
retrieval-augmented generation (RAG) frameworks, including telecom-specific
approaches, rely on semantic similarity and cannot reliably resolve
cross-references or reason about specification evolution. We present DeepSpecs,
a RAG system enhanced by structural and temporal reasoning via three
metadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB
(line-level version diffs), and TDocDB (standardization meeting documents).
DeepSpecs explicitly resolves cross-references by recursively retrieving
referenced clauses through metadata lookup, and traces specification evolution
by mining changes and linking them to Change Requests that document design
rationale. We curate two 5G QA datasets: 573 expert-annotated real-world
questions from practitioner forums and educational resources, and 350
evolution-focused questions derived from approved Change Requests. Across
multiple LLM backends, DeepSpecs outperforms base models and state-of-the-art
telecom RAG systems; ablations confirm that explicit cross-reference resolution
and evolution-aware retrieval substantially improve answer quality,
underscoring the value of modeling the structural and temporal properties of 5G
standards.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†DeepSpecsï¼Œä¸€ä¸ªå¢å¼ºç‰ˆçš„RAGç³»ç»Ÿï¼Œä¸“é—¨ç”¨äºå¤„ç†5GæŠ€æœ¯è§„èŒƒçš„å¤æ‚æ€§é—®é¢˜ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ„å»ºä¸‰ä¸ªå…ƒæ•°æ®ä¸°å¯Œçš„æ•°æ®åº“ï¼ˆSpecDBã€ChangeDBå’ŒTDocDBï¼‰æ¥å®ç°ç»“æ„å’Œæ—¶é—´æ¨ç†ï¼Œèƒ½å¤Ÿæ˜¾å¼è§£æäº¤å‰å¼•ç”¨å¹¶è¿½è¸ªè§„èŒƒæ¼”å˜ã€‚DeepSpecsåœ¨å¤šä¸ªLLMåç«¯ä¸Šè¡¨ç°ä¼˜äºåŸºç¡€æ¨¡å‹å’Œå…¶ä»–å…ˆè¿›çš„ç”µä¿¡RAGç³»ç»Ÿï¼ŒéªŒè¯äº†å…¶ç»“æ„æ€§å’Œæ—¶é—´æ€§å»ºæ¨¡å¯¹æå‡ç­”æ¡ˆè´¨é‡çš„é‡è¦æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.01268v1">Rescuing the Unpoisoned: Efficient Defense against Knowledge Corruption Attacks on RAG Systems</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) are reshaping numerous facets of our daily
lives, leading widespread adoption as web-based services. Despite their
versatility, LLMs face notable challenges, such as generating hallucinated
content and lacking access to up-to-date information. Lately, to address such
limitations, Retrieval-Augmented Generation (RAG) has emerged as a promising
direction by generating responses grounded in external knowledge sources. A
typical RAG system consists of i) a retriever that probes a group of relevant
passages from a knowledge base and ii) a generator that formulates a response
based on the retrieved content. However, as with other AI systems, recent
studies demonstrate the vulnerability of RAG, such as knowledge corruption
attacks by injecting misleading information. In response, several defense
strategies have been proposed, including having LLMs inspect the retrieved
passages individually or fine-tuning robust retrievers. While effective, such
approaches often come with substantial computational costs.
  In this work, we introduce RAGDefender, a resource-efficient defense
mechanism against knowledge corruption (i.e., by data poisoning) attacks in
practical RAG deployments. RAGDefender operates during the post-retrieval
phase, leveraging lightweight machine learning techniques to detect and filter
out adversarial content without requiring additional model training or
inference. Our empirical evaluations show that RAGDefender consistently
outperforms existing state-of-the-art defenses across multiple models and
adversarial scenarios: e.g., RAGDefender reduces the attack success rate (ASR)
against the Gemini model from 0.89 to as low as 0.02, compared to 0.69 for
RobustRAG and 0.24 for Discern-and-Answer when adversarial passages outnumber
legitimate ones by a factor of four (4x).</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†RAGç³»ç»Ÿä¸­çš„å®‰å…¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯çŸ¥è¯†æ±¡æŸ“æ”»å‡»ï¼ˆå¦‚é€šè¿‡æ•°æ®æŠ•æ¯’æ³¨å…¥è¯¯å¯¼ä¿¡æ¯ï¼‰ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºRAGDefenderçš„è½»é‡çº§åæ£€ç´¢é˜¶æ®µé˜²å¾¡æœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ— éœ€é¢å¤–è®­ç»ƒæˆ–æ¨¡å‹æ¨ç†å³å¯é«˜æ•ˆæ£€æµ‹å¹¶è¿‡æ»¤å¯¹æŠ—æ€§å†…å®¹ï¼Œåœ¨å¤šé¡¹å®éªŒä¸­æ˜¾è‘—é™ä½äº†æ”»å‡»æˆåŠŸç‡ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-02
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.01059v1">Efficient Test-Time Retrieval Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Although Large Language Models (LLMs) demonstrate significant capabilities,
their reliance on parametric knowledge often leads to inaccuracies. Retrieval
Augmented Generation (RAG) mitigates this by incorporating external knowledge,
but these methods may introduce irrelevant retrieved documents, leading to
inaccurate responses. While the integration methods filter out incorrect
answers from multiple responses, but lack external knowledge like RAG methods,
and their high costs require balancing overhead with performance gains. To
address these issues, we propose an Efficient Test-Time Retrieval-Augmented
Generation Framework named ET2RAG to improve the performance of LLMs while
maintaining efficiency. Specifically, ET2RAG is a training-free method, that
first retrieves the most relevant documents and augments the LLMs to
efficiently generate diverse candidate responses by managing response length.
Then we compute the similarity of candidate responses and employ a majority
voting mechanism to select the most suitable response as the final output. In
particular, we discover that partial generation is sufficient to capture the
key information necessary for consensus calculation, allowing us to effectively
perform majority voting without the need for fully generated responses. Thus,
we can reach a balance between computational cost and performance by managing
the response length for the number of retrieved documents for majority voting.
Experimental results demonstrate that ET2RAG significantly enhances performance
across three tasks, including open-domain question answering, recipe generation
and image captioning.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºET2RAGçš„é«˜æ•ˆæµ‹è¯•æ—¶æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç»“åˆå¤–éƒ¨çŸ¥è¯†æ£€ç´¢å’Œå¤šæ•°æŠ•ç¥¨æœºåˆ¶æ¥æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ€§èƒ½å’Œæ•ˆç‡ï¼ŒåŒæ—¶å‡å°‘æ— å…³æ£€ç´¢æ–‡æ¡£å’Œè®¡ç®—æˆæœ¬çš„å½±å“ï¼Œå¹¶åœ¨å¼€æ”¾åŸŸé—®ç­”ã€é£Ÿè°±ç”Ÿæˆå’Œå›¾åƒæè¿°ç­‰ä»»åŠ¡ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.01052v1">Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports</a></td><td><details><summary>å±•å¼€</summary>Cancer staging is critical for patient prognosis and treatment planning, yet
extracting pathologic TNM staging from unstructured pathology reports poses a
persistent challenge. Existing natural language processing (NLP) and machine
learning (ML) strategies often depend on large annotated datasets, limiting
their scalability and adaptability. In this study, we introduce two Knowledge
Elicitation methods designed to overcome these limitations by enabling large
language models (LLMs) to induce and apply domain-specific rules for cancer
staging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses
an iterative prompting strategy to derive staging rules directly from
unannotated pathology reports, without requiring ground-truth labels. The
second, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG),
employs a variation of RAG where rules are pre-extracted from relevant
guidelines in a single step and then applied, enhancing interpretability and
avoiding repeated retrieval overhead. We leverage the ability of LLMs to apply
broad knowledge learned during pre-training to new tasks. Using breast cancer
pathology reports from the TCGA dataset, we evaluate their performance in
identifying T and N stages, comparing them against various baseline approaches
on two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG
when Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG
achieves better performance when ZSCOT inference is less effective. Both
methods offer transparent, interpretable interfaces by making the induced rules
explicit. These findings highlight the promise of our Knowledge Elicitation
methods as scalable, high-performing solutions for automated cancer staging
with enhanced interpretability, particularly in clinical settings with limited
annotated data.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸¤ç§çŸ¥è¯†æå–æ–¹æ³•ï¼ˆKEwLTMå’ŒKEwRAGï¼‰ï¼Œå…¶ä¸­KEwRAGé‡‡ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„å˜ä½“ï¼Œé€šè¿‡ä»æŒ‡å—ä¸­é¢„æå–è§„åˆ™å¹¶åº”ç”¨äºç™Œç—‡åˆ†æœŸä»»åŠ¡ï¼Œæå‡å¯è§£é‡Šæ€§å¹¶å‡å°‘é‡å¤æ£€ç´¢å¼€é”€ã€‚å®éªŒè¡¨æ˜ï¼Œä¸¤ç§æ–¹æ³•åœ¨ç¼ºä¹æ ‡æ³¨æ•°æ®çš„ä¸´åºŠç¯å¢ƒä¸­å‡èƒ½å®ç°é«˜æ€§èƒ½ä¸”é€æ˜çš„è‡ªåŠ¨ç™Œç—‡åˆ†æœŸï¼Œå…¶ä¸­KEwRAGåœ¨é›¶æ ·æœ¬æ€ç»´é“¾æ•ˆæœè¾ƒå·®æ—¶è¡¨ç°æ›´ä¼˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.00903v1">ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation has proven practical when models require
specialized knowledge or access to the latest data. However, existing methods
for multimodal document retrieval often replicate techniques developed for
text-only retrieval, whether in how they encode documents, define training
objectives, or compute similarity scores. To address these limitations, we
present ColMate, a document retrieval model that bridges the gap between
multimodal representation learning and document retrieval. ColMate utilizes a
novel OCR-based pretraining objective, a self-supervised masked contrastive
learning objective, and a late interaction scoring mechanism more relevant to
multimodal document structures and visual characteristics. ColMate obtains
3.61% improvements over existing retrieval models on the ViDoRe V2 benchmark,
demonstrating stronger generalization to out-of-domain benchmarks.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºColMateçš„å¤šæ¨¡æ€æ–‡æ¡£æ£€ç´¢æ¨¡å‹ï¼Œé’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šæ¨¡æ€æ–‡æ¡£æ—¶çš„å±€é™æ€§ï¼Œé€šè¿‡åˆ›æ–°çš„OCRé¢„è®­ç»ƒç›®æ ‡ã€è‡ªç›‘ç£æ©è”½å¯¹æ¯”å­¦ä¹ ç›®æ ‡åŠä¸å¤šæ¨¡æ€æ–‡æ¡£ç»“æ„æ›´ç›¸å…³çš„è¯„åˆ†æœºåˆ¶ï¼Œæå‡äº†æ£€ç´¢æ€§èƒ½ï¼Œå¹¶åœ¨ViDoRe V2åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æ¨¡å‹3.61%ï¼Œå±•ç°äº†æ›´å¼ºçš„è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-11-01
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.00739v1">A CPU-Centric Perspective on Agentic AI</a></td><td><details><summary>å±•å¼€</summary>Agentic AI frameworks add a decision-making orchestrator embedded with
external tools, including web search, Python interpreter, contextual database,
and others, on top of monolithic LLMs, turning them from passive text oracles
into autonomous problem-solvers that can plan, call tools, remember past steps,
and adapt on the fly.
  This paper aims to characterize and understand the system bottlenecks
introduced by agentic AI workloads from a largely overlooked CPU-centric
perspective. We first systematically characterize Agentic AI on the basis of
orchestrator/decision making component, inference path dynamics and
repetitiveness of the agentic flow which directly influences the system-level
performance. Thereafter, based on the characterization, we choose five
representative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow,
Langchain and SWE-Agent to profile latency, throughput and energy metrics and
demystify the significant impact of CPUs on these metrics relative to GPUs. We
observe that - 1. Tool processing on CPUs can take up to 90.6% of the total
latency; 2. Agentic throughput gets bottlenecked either by CPU factors -
coherence, synchronization and over-subscription of cores or GPU factors - main
memory capacity and bandwidth; \circled{3} CPU dynamic energy consumes up to
44% of the total dynamic energy at large batch sizes. Based on the profiling
insights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching
(CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and
heterogeneous agentic workloads respectively to demonstrate the potential to
improve the performance, efficiency, and scalability of agentic AI. We achieve
up to 2.1x and 1.41x P50 latency speedup compared to the multi-processing
benchmark for homogeneous and heterogeneous agentic workloads respectively.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»CPUä¸­å¿ƒè§†è§’åˆ†æäº†Agentic AIæ¡†æ¶ï¼ˆå¦‚Haystack RAGã€Langchainç­‰ï¼‰çš„ç³»ç»Ÿæ€§èƒ½ç“¶é¢ˆï¼Œæ­ç¤ºäº†CPUåœ¨å·¥å…·å¤„ç†å»¶è¿Ÿå’Œèƒ½è€—ä¸­çš„å…³é”®ä½œç”¨ï¼Œå¹¶æå‡ºäº†é’ˆå¯¹åŒè´¨/å¼‚æ„å·¥ä½œè´Ÿè½½çš„ä¼˜åŒ–æ–¹æ¡ˆï¼ˆCGAMå’ŒMAWSï¼‰ï¼Œæœ€ç»ˆå®ç°äº†æ˜¾è‘—çš„å»¶è¿Ÿé™ä½å’Œæ•ˆç‡æå‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.00505v2">Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation has shown remarkable results to address Large
Language Models' hallucinations, which usually uses a large external corpus to
supplement knowledge to LLMs. However, with the development of LLMs, the
internal knowledge of LLMs has expanded significantly, thus causing significant
knowledge redundancy between the external corpus and LLMs. On the one hand, the
indexing cost of dense retrieval is highly related to the corpus size and thus
significant redundant knowledge intensifies the dense retrieval's workload. On
the other hand, the redundant knowledge in the external corpus is not helpful
to LLMs and our exploratory analysis shows that it instead hurts the RAG
performance on those questions which the LLM can answer by itself. To address
these issues, we propose Zero-RAG to tackle these challenges. Specifically, we
first propose the Mastery-Score metric to identify redundant knowledge in the
RAG corpus to prune it. After pruning, answers to "mastered" questions rely
primarily on internal knowledge of the LLM. To better harness the internal
capacity, we propose Query Router and Noise-Tolerant Tuning to avoid the
irrelevant documents' distraction and thus further improve the LLM's
utilization of internal knowledge with pruned corpus. Experimental results show
that Zero-RAG prunes the Wikipedia corpus by 30\% and accelerates the retrieval
stage by 22\%, without compromising RAG's performance.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†RAGæŠ€æœ¯ä¸­å¤–éƒ¨çŸ¥è¯†åº“ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å†…éƒ¨çŸ¥è¯†å†—ä½™çš„é—®é¢˜ï¼Œæå‡ºäº†Zero-RAGæ–¹æ³•ï¼Œé€šè¿‡Mastery-ScoreæŒ‡æ ‡è¯†åˆ«å¹¶ä¿®å‰ªå†—ä½™çŸ¥è¯†ï¼Œç»“åˆQuery Routerå’ŒNoise-Tolerant Tuningä¼˜åŒ–LLMå¯¹å†…éƒ¨çŸ¥è¯†çš„åˆ©ç”¨ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å°†ç»´åŸºç™¾ç§‘è¯­æ–™åº“ç¼©å‡30%ï¼Œæ£€ç´¢é€Ÿåº¦æå‡22%ï¼ŒåŒæ—¶ä¿æŒRAGæ€§èƒ½ä¸å—å½±å“ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.00489v1">ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs), constrained by limited context windows, often
face significant performance degradation when reasoning over long contexts. To
address this, Retrieval-Augmented Generation (RAG) retrieves and reasons over
chunks but frequently sacrifices logical coherence due to its reliance on
similarity-based rankings. Similarly, divide-and-conquer frameworks (DCF) split
documents into small chunks for independent reasoning and aggregation. While
effective for local reasoning, DCF struggles to capture long-range dependencies
and risks inducing conflicts by processing chunks in isolation. To overcome
these limitations, we propose ToM, a novel Tree-oriented MapReduce framework
for long-context reasoning. ToM leverages the inherent hierarchical structure
of long documents (e.g., main headings and subheadings) by constructing a
DocTree through hierarchical semantic parsing and performing bottom-up
aggregation. Using a Tree MapReduce approach, ToM enables recursive reasoning:
in the Map step, rationales are generated at child nodes; in the Reduce step,
these rationales are aggregated across sibling nodes to resolve conflicts or
reach consensus at parent nodes. Experimental results on 70B+ LLMs show that
ToM significantly outperforms existing divide-and-conquer frameworks and
retrieval-augmented generation methods, achieving better logical coherence and
long-context reasoning. Our code is available at
https://github.com/gjn12-31/ToM .</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºToMï¼ˆTree-oriented MapReduceï¼‰çš„æ–°æ¡†æ¶ï¼Œç”¨äºè§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸­çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œåˆ†æ²»æ¡†æ¶ï¼ˆDCFï¼‰ç›¸æ¯”ï¼ŒToMé€šè¿‡åˆ©ç”¨é•¿æ–‡æ¡£çš„å±‚æ¬¡ç»“æ„ï¼ˆå¦‚ä¸»æ ‡é¢˜å’Œå‰¯æ ‡é¢˜ï¼‰ï¼Œæ„å»ºDocTreeå¹¶è¿›è¡Œè‡ªåº•å‘ä¸Šçš„èšåˆï¼Œä»è€Œæé«˜äº†é€»è¾‘è¿è´¯æ€§å’Œé•¿ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒToMåœ¨70B+çš„LLMsä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„RAGå’ŒDCFæ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.00340v1">Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities</a></td><td><details><summary>å±•å¼€</summary>The rapid integration of large language models (LLMs) into high-stakes legal
work has exposed a critical gap: no benchmark exists to systematically
stress-test their reliability against the nuanced, adversarial, and often
subtle flaws present in real-world contracts. To address this, we introduce
CLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an
LLM's legal reasoning. We study the capabilities of LLMs to detect and reason
about fine-grained discrepancies by producing over 7500 real-world perturbed
contracts from foundational datasets like CUAD and ContractNLI. Our novel,
persona-driven pipeline generates 10 distinct anomaly categories, which are
then validated against official statutes using a Retrieval-Augmented Generation
(RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs'
ability to detect embedded legal flaws and explain their significance. Our
analysis shows a key weakness: these models often miss subtle errors and
struggle even more to justify them legally. Our work outlines a path to
identify and correct such reasoning failures in legal AI.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†CLAUSEåŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ³•å¾‹æ¨ç†ä¸­çš„è„†å¼±æ€§ï¼Œé€šè¿‡ç”Ÿæˆ7500å¤šä»½æ‰°åŠ¨åˆåŒæ¥æµ‹è¯•æ¨¡å‹æ£€æµ‹ç»†å¾®å·®å¼‚çš„èƒ½åŠ›ï¼Œå¹¶ä½¿ç”¨äº†RAGç³»ç»ŸéªŒè¯å¼‚å¸¸ç±»åˆ«çš„æ³•å¾‹å¯ä¿¡æ€§ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨è¯†åˆ«å’Œè§£é‡Šæ³•å¾‹æ¼æ´æ–¹é¢çš„ä¸è¶³ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-31
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2511.00265v1">AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding</a></td><td><details><summary>å±•å¼€</summary>Traditional cybersecurity tabletop exercises (TTXs) provide valuable training
but are often scripted, resource-intensive, and difficult to scale. We
introduce AgentBnB, a browser-based re-imagining of the Backdoors & Breaches
game that integrates large language model teammates with a Bloom-aligned,
retrieval-augmented copilot (C2D2). The system expands a curated corpus into
factual, conceptual, procedural, and metacognitive snippets, delivering
on-demand, cognitively targeted hints. Prompt-engineered agents employ a
scaffolding ladder that gradually fades as learner confidence grows. In a
solo-player pilot with four graduate students, participants reported greater
intention to use the agent-based version compared to the physical card deck and
viewed it as more scalable, though a ceiling effect emerged on a simple
knowledge quiz. Despite limitations of small sample size, single-player focus,
and narrow corpus, these early findings suggest that large language model
augmented TTXs can provide lightweight, repeatable practice without the
logistical burden of traditional exercises. Planned extensions include
multi-player modes, telemetry-driven coaching, and comparative studies with
larger cohorts.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†AgentBnBç³»ç»Ÿï¼Œè¯¥ç½‘ç»œæµè§ˆå™¨ç³»ç»ŸåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹å’ŒRAGæŠ€æœ¯ï¼ˆé€šè¿‡æ£€ç´¢å¢å¼ºåŠ©æ‰‹C2D2ï¼‰ï¼Œæä¾›æŒ‰éœ€çš„è®¤çŸ¥ç›®æ ‡æç¤ºï¼Œæ—¨åœ¨æ”¹è¿›ä¼ ç»Ÿçš„ç½‘ç»œå®‰å…¨æ¡Œé¢ç»ƒä¹ ï¼ˆTTXsï¼‰ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•æ¯”ä¼ ç»Ÿæ–¹æ³•æ›´è½»é‡ã€å¯æ‰©å±•ä¸”æ˜“äºé‡å¤ï¼Œå°½ç®¡æ ·æœ¬è§„æ¨¡è¾ƒå°ä¸”å­˜åœ¨ä¸€äº›é™åˆ¶ã€‚è®¡åˆ’ä¸­çš„æ‰©å±•åŒ…æ‹¬å¤šäººæ¨¡å¼å’Œæ›´å¤§è§„æ¨¡çš„æ¯”è¾ƒç ”ç©¶ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.27569v1">MARAG-R1: Beyond Single Retriever via Reinforcement-Learned Multi-Tool Agentic Retrieval</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) excel at reasoning and generation but are
inherently limited by static pretraining data, resulting in factual
inaccuracies and weak adaptability to new information. Retrieval-Augmented
Generation (RAG) addresses this issue by grounding LLMs in external knowledge;
However, the effectiveness of RAG critically depends on whether the model can
adequately access relevant information. Existing RAG systems rely on a single
retriever with fixed top-k selection, restricting access to a narrow and static
subset of the corpus. As a result, this single-retriever paradigm has become
the primary bottleneck for comprehensive external information acquisition,
especially in tasks requiring corpus-level reasoning. To overcome this
limitation, we propose MARAG-R1, a reinforcement-learned multi-tool RAG
framework that enables LLMs to dynamically coordinate multiple retrieval
mechanisms for broader and more precise information access. MARAG-R1 equips the
model with four retrieval tools -- semantic search, keyword search, filtering,
and aggregation -- and learns both how and when to use them through a two-stage
training process: supervised fine-tuning followed by reinforcement learning.
This design allows the model to interleave reasoning and retrieval,
progressively gathering sufficient evidence for corpus-level synthesis.
Experiments on GlobalQA, HotpotQA, and 2WikiMultiHopQA demonstrate that
MARAG-R1 substantially outperforms strong baselines and achieves new
state-of-the-art results in corpus-level reasoning tasks.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤šå·¥å…·RAGæ¡†æ¶MARAG-R1ï¼Œé€šè¿‡åŠ¨æ€åè°ƒå››ç§æ£€ç´¢å·¥å…·ï¼ˆè¯­ä¹‰æœç´¢ã€å…³é”®è¯æœç´¢ã€è¿‡æ»¤å’Œèšåˆï¼‰æ¥æå‡å¤§è¯­è¨€æ¨¡å‹å¯¹å¤–éƒ¨çŸ¥è¯†çš„è·å–èƒ½åŠ›ï¼Œè§£å†³äº†ä¼ ç»Ÿå•æ£€ç´¢å™¨RAGåœ¨è¯­æ–™çº§æ¨ç†ä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯å…¶ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.27568v1">SIGMA: Search-Augmented On-Demand Knowledge Integration for Agentic Mathematical Reasoning</a></td><td><details><summary>å±•å¼€</summary>Solving mathematical reasoning problems requires not only accurate access to
relevant knowledge but also careful, multi-step thinking. However, current
retrieval-augmented models often rely on a single perspective, follow
inflexible search strategies, and struggle to effectively combine information
from multiple sources. We introduce SIGMA (Search-Augmented On-Demand Knowledge
Integration for AGentic Mathematical reAsoning), a unified framework that
orchestrates specialized agents to independently reason, perform targeted
searches, and synthesize findings through a moderator mechanism. Each agent
generates hypothetical passages to optimize retrieval for its analytic
perspective, ensuring knowledge integration is both context-sensitive and
computation-efficient. When evaluated on challenging benchmarks such as
MATH500, AIME, and PhD-level science QA GPQA, SIGMA consistently outperforms
both open- and closed-source systems, achieving an absolute performance
improvement of 7.4%. Our results demonstrate that multi-agent, on-demand
knowledge integration significantly enhances both reasoning accuracy and
efficiency, offering a scalable approach for complex, knowledge-intensive
problem-solving. We will release the code upon publication.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSIGMAçš„å¤šæ™ºèƒ½ä½“æ£€ç´¢å¢å¼ºæ¡†æ¶ï¼Œç”¨äºè§£å†³æ•°å­¦æ¨ç†é—®é¢˜ã€‚é€šè¿‡åè°ƒä¸“é—¨åŒ–çš„æ™ºèƒ½ä½“è¿›è¡Œç‹¬ç«‹æ¨ç†ã€å®šå‘æ£€ç´¢å’Œç»“æœåˆæˆï¼ŒSIGMAä¼˜åŒ–äº†ä¸Šä¸‹æ–‡æ•æ„Ÿä¸”é«˜æ•ˆçš„çŸ¥è¯†æ•´åˆï¼Œåœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰ç³»ç»Ÿï¼Œå®ç°äº†7.4%çš„ç»å¯¹æ€§èƒ½æå‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.27566v1">Interact-RAG: Reason and Interact with the Corpus, Beyond Black-Box Retrieval</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has significantly enhanced LLMs by
incorporating external information. However, prevailing agentic RAG approaches
are constrained by a critical limitation: they treat the retrieval process as a
black-box querying operation. This confines agents' actions to query issuing,
hindering its ability to tackle complex information-seeking tasks. To address
this, we introduce Interact-RAG, a new paradigm that elevates the LLM agent
from a passive query issuer into an active manipulator of the retrieval
process. We dismantle the black-box with a Corpus Interaction Engine, equipping
the agent with a set of action primitives for fine-grained control over
information retrieval. To further empower the agent on the entire RAG pipeline,
we first develop a reasoning-enhanced workflow, which enables both zero-shot
execution and the synthesis of interaction trajectories. We then leverage this
synthetic data to train a fully autonomous end-to-end agent via Supervised
Fine-Tuning (SFT), followed by refinement with Reinforcement Learning (RL).
Extensive experiments across six benchmarks demonstrate that Interact-RAG
significantly outperforms other advanced methods, validating the efficacy of
our reasoning-interaction strategy.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºInteract-RAGçš„æ–°èŒƒå¼ï¼Œé€šè¿‡å¼•å…¥Corpus Interaction Engineå’Œç»†ç²’åº¦æ§åˆ¶æœºåˆ¶ï¼Œå°†å¤§è¯­è¨€æ¨¡å‹ä»è¢«åŠ¨çš„æŸ¥è¯¢å‘èµ·è€…è½¬å˜ä¸ºæ£€ç´¢è¿‡ç¨‹çš„ä¸»åŠ¨æ“æ§è€…ï¼Œå¹¶åˆ©ç”¨å¢å¼ºçš„å·¥ä½œæµç¨‹å’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ•´ä¸ªRAGæµç¨‹ï¼Œå®éªŒè¯æ˜å…¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.27537v1">AstuteRAG-FQA: Task-Aware Retrieval-Augmented Generation Framework for Proprietary Data Challenges in Financial Question Answering</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) shows significant promise in
knowledge-intensive tasks by improving domain specificity, enhancing temporal
relevance, and reducing hallucinations. However, applying RAG to finance
encounters critical challenges: restricted access to proprietary datasets,
limited retrieval accuracy, regulatory constraints, and sensitive data
interpretation. We introduce AstuteRAG-FQA, an adaptive RAG framework tailored
for Financial Question Answering (FQA), leveraging task-aware prompt
engineering to address these challenges. The framework uses a hybrid retrieval
strategy integrating both open-source and proprietary financial data while
maintaining strict security protocols and regulatory compliance. A dynamic
prompt framework adapts in real time to query complexity, improving precision
and contextual relevance. To systematically address diverse financial queries,
we propose a four-tier task classification: explicit factual, implicit factual,
interpretable rationale, and hidden rationale involving implicit causal
reasoning. For each category, we identify key challenges, datasets, and
optimization techniques within the retrieval and generation process. The
framework incorporates multi-layered security mechanisms including differential
privacy, data anonymization, and role-based access controls to protect
sensitive financial information. Additionally, AstuteRAG-FQA implements
real-time compliance monitoring through automated regulatory validation systems
that verify responses against industry standards and legal obligations. We
evaluate three data integration techniques - contextual embedding, small model
augmentation, and targeted fine-tuning - analyzing their efficiency and
feasibility across varied financial environments.</details></td><td><details><summary>å±•å¼€</summary>AstuteRAG-FQAæ˜¯ä¸€ä¸ªä¸“ä¸ºé‡‘èé—®ç­”ï¼ˆFQAï¼‰è®¾è®¡çš„è‡ªé€‚åº”RAGæ¡†æ¶ï¼Œé€šè¿‡æ··åˆæ£€ç´¢ç­–ç•¥ã€åŠ¨æ€æç¤ºæ¡†æ¶å’Œå››å±‚ä»»åŠ¡åˆ†ç±»ä¼˜åŒ–æ£€ç´¢ä¸ç”Ÿæˆè¿‡ç¨‹ï¼Œå¹¶é‡‡ç”¨å¤šå±‚å®‰å…¨æœºåˆ¶å’Œå®æ—¶åˆè§„ç›‘æµ‹æ¥è§£å†³é‡‘èé¢†åŸŸçš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼ˆå¦‚æ•°æ®æ•æ„Ÿæ€§ã€ç›‘ç®¡é™åˆ¶ç­‰ï¼‰ã€‚ç ”ç©¶è¿˜æ¯”è¾ƒäº†ä¸‰ç§æ•°æ®é›†æˆæŠ€æœ¯çš„æ•ˆç‡ä¸å¯è¡Œæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2511.00122v1">Engineering.ai: A Platform for Teams of AI Engineers in Computational Design</a></td><td><details><summary>å±•å¼€</summary>In modern engineering practice, human engineers collaborate in specialized
teams to design complex products, with each expert completing their respective
tasks while communicating and exchanging results and data with one another.
While this division of expertise is essential for managing multidisciplinary
complexity, it demands substantial development time and cost. Recently, we
introduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer
for computational fluid dynamics, and turbulence.ai, which can conduct
end-to-end research in fluid mechanics draft publications and PhD theses.
Building upon these foundations, we present Engineering.ai, a platform for
teams of AI engineers in computational design. The framework employs a
hierarchical multi-agent architecture where a Chief Engineer coordinates
specialized agents consisting of Aerodynamics, Structural, Acoustic, and
Optimization Engineers, each powered by LLM with domain-specific knowledge.
Agent-agent collaboration is achieved through file-mediated communication for
data provenance and reproducibility, while a comprehensive memory system
maintains project context, execution history, and retrieval-augmented domain
knowledge to ensure reliable decision-making across the workflow. The system
integrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis,
enabling parallel multidisciplinary simulations while maintaining computational
accuracy. The framework is validated through UAV wing optimization. This work
demonstrates that agentic-AI-enabled AI engineers has the potential to perform
complex engineering tasks autonomously. Remarkably, the automated workflow
achieved a 100% success rate across over 400 parametric configurations, with
zero mesh generation failures, solver convergence issues, or manual
interventions required, validating that the framework is trustworthy.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†Engineering.aiå¹³å°ï¼Œé‡‡ç”¨å¤šæ™ºèƒ½ä½“æ¶æ„åä½œå®Œæˆè®¡ç®—è®¾è®¡ä»»åŠ¡ï¼Œå…¶ä¸­æ™ºèƒ½ä½“é€šè¿‡æ£€ç´¢å¢å¼ºçš„é¢†åŸŸçŸ¥è¯†ç¡®ä¿å†³ç­–å¯é æ€§ï¼Œå¹¶éªŒè¯äº†è¯¥ç³»ç»Ÿåœ¨æ— äººæœºç¿¼ä¼˜åŒ–ä¸­çš„æˆåŠŸåº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.27080v1">Adapting Large Language Models to Emerging Cybersecurity using Retrieval Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Security applications are increasingly relying on large language models
(LLMs) for cyber threat detection; however, their opaque reasoning often limits
trust, particularly in decisions that require domain-specific cybersecurity
knowledge. Because security threats evolve rapidly, LLMs must not only recall
historical incidents but also adapt to emerging vulnerabilities and attack
patterns. Retrieval-Augmented Generation (RAG) has demonstrated effectiveness
in general LLM applications, but its potential for cybersecurity remains
underexplored. In this work, we introduce a RAG-based framework designed to
contextualize cybersecurity data and enhance LLM accuracy in knowledge
retention and temporal reasoning. Using external datasets and the
Llama-3-8B-Instruct model, we evaluate baseline RAG, an optimized hybrid
retrieval approach, and conduct a comparative analysis across multiple
performance metrics. Our findings highlight the promise of hybrid retrieval in
strengthening the adaptability and reliability of LLMs for cybersecurity tasks.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†RAGåœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸçš„åº”ç”¨ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºRAGçš„æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨å¤–éƒ¨æ•°æ®é›†å’Œä¼˜åŒ–æ··åˆæ£€ç´¢æ–¹æ³•æå‡å¤§è¯­è¨€æ¨¡å‹åœ¨ç½‘ç»œå®‰å…¨ä»»åŠ¡ä¸­çš„çŸ¥è¯†è®°å¿†å’Œæ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†æ··åˆæ£€ç´¢åœ¨å¢å¼ºæ¨¡å‹é€‚åº”æ€§å’Œå¯é æ€§æ–¹é¢çš„æ½œåŠ›ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-30
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.27054v1">LLM-Centric RAG with Multi-Granular Indexing and Confidence Constraints</a></td><td><details><summary>å±•å¼€</summary>This paper addresses the issues of insufficient coverage, unstable results,
and limited reliability in retrieval-augmented generation under complex
knowledge environments, and proposes a confidence control method that
integrates multi-granularity memory indexing with uncertainty estimation. The
method builds a hierarchical memory structure that divides knowledge
representations into different levels of granularity, enabling dynamic indexing
and retrieval from local details to global context, and thus establishing
closer semantic connections between retrieval and generation. On this basis, an
uncertainty estimation mechanism is introduced to explicitly constrain and
filter low-confidence paths during the generation process, allowing the model
to maintain information coverage while effectively suppressing noise and false
content. The overall optimization objective consists of generation loss,
entropy constraints, and variance regularization, forming a unified confidence
control framework. In the experiments, comprehensive sensitivity tests and
comparative analyses were designed, covering hyperparameters, environmental
conditions, and data structures, to verify the stability and robustness of the
proposed method across different scenarios. The results show that the method
achieves superior performance over existing models in QA accuracy, retrieval
recall, ranking quality, and factual consistency, demonstrating the
effectiveness of combining multi-granularity indexing with confidence control.
This study not only provides a new technical pathway for retrieval-augmented
generation but also offers practical evidence for improving the reliability and
controllability of large models in complex contexts.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡é’ˆå¯¹å¤æ‚çŸ¥è¯†ç¯å¢ƒä¸‹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å­˜åœ¨çš„è¦†ç›–ä¸è¶³ã€ç»“æœä¸ç¨³å®šå’Œå¯é æ€§æœ‰é™ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§èåˆå¤šç²’åº¦è®°å¿†ç´¢å¼•ä¸ä¸ç¡®å®šæ€§ä¼°è®¡çš„ç½®ä¿¡åº¦æ§åˆ¶æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºåˆ†å±‚è®°å¿†ç»“æ„å®ç°åŠ¨æ€çŸ¥è¯†æ£€ç´¢ï¼Œå¹¶ç»“åˆç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä¸ç¡®å®šæ€§çº¦æŸè¿‡æ»¤ä½ç½®ä¿¡è·¯å¾„ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨é—®ç­”å‡†ç¡®æ€§ã€æ£€ç´¢å¬å›ç‡ã€æ’åºè´¨é‡å’Œäº‹å®ä¸€è‡´æ€§ä¸Šä¼˜äºç°æœ‰æ¨¡å‹ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.27051v1">Adaptive Data Flywheel: Applying MAPE Control Loops to AI Agent Improvement</a></td><td><details><summary>å±•å¼€</summary>Enterprise AI agents must continuously adapt to maintain accuracy, reduce
latency, and remain aligned with user needs. We present a practical
implementation of a data flywheel in NVInfo AI, NVIDIA's Mixture-of-Experts
(MoE) Knowledge Assistant serving over 30,000 employees. By operationalizing a
MAPE-driven data flywheel, we built a closed-loop system that systematically
addresses failures in retrieval-augmented generation (RAG) pipelines and
enables continuous learning. Over a 3-month post-deployment period, we
monitored feedback and collected 495 negative samples. Analysis revealed two
major failure modes: routing errors (5.25\%) and query rephrasal errors
(3.2\%). Using NVIDIA NeMo microservices, we implemented targeted improvements
through fine-tuning. For routing, we replaced a Llama 3.1 70B model with a
fine-tuned 8B variant, achieving 96\% accuracy, a 10x reduction in model size,
and 70\% latency improvement. For query rephrasal, fine-tuning yielded a 3.7\%
gain in accuracy and a 40\% latency reduction. Our approach demonstrates how
human-in-the-loop (HITL) feedback, when structured within a data flywheel,
transforms enterprise AI agents into self-improving systems. Key learnings
include approaches to ensure agent robustness despite limited user feedback,
navigating privacy constraints, and executing staged rollouts in production.
This work offers a repeatable blueprint for building robust, adaptive
enterprise AI agents capable of learning from real-world usage at scale.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†NVIDIAåœ¨å…¶ä¼ä¸šå†…éƒ¨çŸ¥è¯†åŠ©æ‰‹NVInfo AIä¸­å®æ–½çš„åŸºäºMAPEï¼ˆç›‘æµ‹-åˆ†æ-è§„åˆ’-æ‰§è¡Œï¼‰é©±åŠ¨çš„æ•°æ®é£è½®ç³»ç»Ÿï¼Œé€šè¿‡é—­ç¯åé¦ˆæŒç»­ä¼˜åŒ–æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç®¡é“çš„æ€§èƒ½ã€‚æ–‡ç« é‡ç‚¹åˆ†æäº†RAGæµç¨‹ä¸­è·¯ç”±é”™è¯¯å’ŒæŸ¥è¯¢é‡è¿°é”™è¯¯ä¸¤å¤§æ•…éšœæ¨¡å¼ï¼Œå¹¶åˆ©ç”¨å¾®è°ƒæŠ€æœ¯æ˜¾è‘—æå‡äº†æ¨¡å‹æ•ˆèƒ½ï¼ˆå¦‚ç”¨8Bæ¨¡å‹æ›¿ä»£70Bæ¨¡å‹å®ç°96%å‡†ç¡®ç‡ï¼‰ï¼Œæœ€ç»ˆæ„å»ºäº†ä¸€ä¸ªå¯è‡ªæˆ‘è¿­ä»£çš„ä¼ä¸šçº§AIä»£ç†æ¡†æ¶ï¼ŒåŒæ—¶æ¢è®¨äº†éšç§çº¦æŸå’Œåˆ†é˜¶æ®µéƒ¨ç½²ç­‰å®è·µç»éªŒã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.26941v1">LLM-based Multi-class Attack Analysis and Mitigation Framework in IoT/IIoT Networks</a></td><td><details><summary>å±•å¼€</summary>The Internet of Things has expanded rapidly, transforming communication and
operations across industries but also increasing the attack surface and
security breaches. Artificial Intelligence plays a key role in securing IoT,
enabling attack detection, attack behavior analysis, and mitigation suggestion.
Despite advancements, evaluations remain purely qualitative, and the lack of a
standardized, objective benchmark for quantitatively measuring AI-based attack
analysis and mitigation hinders consistent assessment of model effectiveness.
In this work, we propose a hybrid framework combining Machine Learning (ML) for
multi-class attack detection with Large Language Models (LLMs) for attack
behavior analysis and mitigation suggestion. After benchmarking several ML and
Deep Learning (DL) classifiers on the Edge-IIoTset and CICIoT2023 datasets, we
applied structured role-play prompt engineering with Retrieval-Augmented
Generation (RAG) to guide ChatGPT-o3 and DeepSeek-R1 in producing detailed,
context-aware responses. We introduce novel evaluation metrics for quantitative
assessment to guide us and an ensemble of judge LLMs, namely ChatGPT-4o,
DeepSeek-V3, Mixtral 8x7B Instruct, Gemini 2.5 Flash, Meta Llama 4, TII Falcon
H1 34B Instruct, xAI Grok 3, and Claude 4 Sonnet, to independently evaluate the
responses. Results show that Random Forest has the best detection model, and
ChatGPT-o3 outperformed DeepSeek-R1 in attack analysis and mitigation.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆæœºå™¨å­¦ä¹ å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ··åˆæ¡†æ¶ï¼Œç”¨äºç‰©è”ç½‘ï¼ˆIoTï¼‰å®‰å…¨ä¸­çš„å¤šç±»æ”»å‡»æ£€æµ‹ã€è¡Œä¸ºåˆ†æå’Œç¼“è§£å»ºè®®ï¼Œå¹¶åº”ç”¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯æ¥ä¼˜åŒ–LLMsçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥å“åº”ï¼ŒåŒæ—¶å¼•å…¥äº†æ–°çš„è¯„ä¼°æŒ‡æ ‡å’Œå¤šä¸ªLLMæ³•å®˜è¿›è¡Œç‹¬ç«‹è¯„ä¼°ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.26457v1">SecureReviewer: Enhancing Large Language Models for Secure Code Review through Secure-aware Fine-tuning</a></td><td><details><summary>å±•å¼€</summary>Identifying and addressing security issues during the early phase of the
development lifecycle is critical for mitigating the long-term negative impacts
on software systems. Code review serves as an effective practice that enables
developers to check their teammates' code before integration into the codebase.
To streamline the generation of review comments, various automated code review
approaches have been proposed, where LLM-based methods have significantly
advanced the capabilities of automated review generation. However, existing
models primarily focus on general-purpose code review, their effectiveness in
identifying and addressing security-related issues remains underexplored.
Moreover, adapting existing code review approaches to target security issues
faces substantial challenges, including data scarcity and inadequate evaluation
metrics. To address these limitations, we propose SecureReviewer, a new
approach designed for enhancing LLMs' ability to identify and resolve
security-related issues during code review. Specifically, we first construct a
dataset tailored for training and evaluating secure code review capabilities.
Leveraging this dataset, we fine-tune LLMs to generate code review comments
that can effectively identify security issues and provide fix suggestions with
our proposed secure-aware fine-tuning strategy. To mitigate hallucination in
LLMs and enhance the reliability of their outputs, we integrate the RAG
technique, which grounds the generated comments in domain-specific security
knowledge. Additionally, we introduce SecureBLEU, a new evaluation metric
designed to assess the effectiveness of review comments in addressing security
issues. Experimental results demonstrate that SecureReviewer outperforms
state-of-the-art baselines in both security issue detection accuracy and the
overall quality and practical utility of generated review comments.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºSecureRevieweræ–¹æ³•ï¼Œé€šè¿‡æ„å»ºå®‰å…¨ä»£ç å®¡æŸ¥æ•°æ®é›†ã€å¾®è°ƒLLMç”Ÿæˆå®‰å…¨å®¡æŸ¥æ„è§ï¼Œå¹¶å¼•å…¥RAGæŠ€æœ¯å¢å¼ºé¢†åŸŸå®‰å…¨çŸ¥è¯†å‚è€ƒä»¥å‡å°‘å¹»è§‰ï¼ŒåŒæ—¶è®¾è®¡SecureBLEUè¯„ä¼°æŒ‡æ ‡ï¼Œæ˜¾è‘—æå‡äº†ä»£ç å®‰å…¨é—®é¢˜çš„æ£€æµ‹ç²¾åº¦ä¸è¯„è®ºè´¨é‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.26345v1">MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data</a></td><td><details><summary>å±•å¼€</summary>Health-related misinformation is very prevalent and potentially harmful. It
is difficult to identify, especially when claims distort or misinterpret
scientific findings. We investigate the impact of synthetic data generation and
lightweight fine-tuning techniques on the ability of large language models
(LLMs) to recognize fallacious arguments using the MISSCI dataset and
framework. In this work, we propose MisSynth, a pipeline that applies
retrieval-augmented generation (RAG) to produce synthetic fallacy samples,
which are then used to fine-tune an LLM model. Our results show substantial
accuracy gains with fine-tuned models compared to vanilla baselines. For
instance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score
absolute improvement on the MISSCI test split over its vanilla baseline. We
demonstrate that introducing synthetic fallacy data to augment limited
annotated resources can significantly enhance zero-shot LLM classification
performance on real-world scientific misinformation tasks, even with limited
computational resources. The code and synthetic dataset are available on
https://github.com/mxpoliakov/MisSynth.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºMisSynthæ–¹æ³•ï¼Œåˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç”Ÿæˆè™šå‡è®ºç‚¹åˆæˆæ•°æ®ï¼Œå¹¶é€šè¿‡è½»é‡å¾®è°ƒæå‡å¤§è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦è°¬è¯¯è¯†åˆ«ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼ˆå¦‚LLaMA 3.1 8Bæ¨¡å‹F1å€¼æå‡35%ï¼‰ï¼Œè§£å†³äº†å¥åº·é¢†åŸŸé”™è¯¯ä¿¡æ¯æ£€æµ‹ä¸­æ ‡æ³¨æ•°æ®åŒ®ä¹çš„é—®é¢˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.26309v1">GraphCompliance: Aligning Policy and Context Graphs for LLM-Based Regulatory Compliance</a></td><td><details><summary>å±•å¼€</summary>Compliance at web scale poses practical challenges: each request may require
a regulatory assessment. Regulatory texts (e.g., the General Data Protection
Regulation, GDPR) are cross-referential and normative, while runtime contexts
are expressed in unstructured natural language. This setting motivates us to
align semantic information in unstructured text with the structured, normative
elements of regulations. To this end, we introduce GraphCompliance, a framework
that represents regulatory texts as a Policy Graph and runtime contexts as a
Context Graph, and aligns them. In this formulation, the policy graph encodes
normative structure and cross-references, whereas the context graph formalizes
events as subject-action-object (SAO) and entity-relation triples. This
alignment anchors the reasoning of a judge large language model (LLM) in
structured information and helps reduce the burden of regulatory interpretation
and event parsing, enabling a focus on the core reasoning step. In experiments
on 300 GDPR-derived real-world scenarios spanning five evaluation tasks,
GraphCompliance yields 4.1-7.2 percentage points (pp) higher micro-F1 than
LLM-only and RAG baselines, with fewer under- and over-predictions, resulting
in higher recall and lower false positive rates. Ablation studies indicate
contributions from each graph component, suggesting that structured
representations and a judge LLM are complementary for normative reasoning.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†GraphComplianceæ¡†æ¶ï¼Œé€šè¿‡å°†æ³•è§„æ–‡æœ¬å’Œè¿è¡Œæ—¶ä¸Šä¸‹æ–‡åˆ†åˆ«è¡¨ç¤ºä¸ºæ”¿ç­–å›¾å’Œä¸Šä¸‹æ–‡å›¾å¹¶å¯¹å…¶è¿›è¡Œå¯¹é½ï¼Œä»¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§„èŒƒæ€§æ¨ç†ä¸­çš„è¡¨ç°ã€‚å®éªŒè¡¨æ˜ï¼Œä¸çº¯LLMå’ŒRAGåŸºçº¿ç›¸æ¯”ï¼ŒGraphComplianceåœ¨å¤šä¸ªä»»åŠ¡ä¸­å±•ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.26242v1">Retrieval Augmented Generation-Enhanced Distributed LLM Agents for Generalizable Traffic Signal Control with Emergency Vehicles</a></td><td><details><summary>å±•å¼€</summary>With increasing urban traffic complexity, Traffic Signal Control (TSC) is
essential for optimizing traffic flow and improving road safety. Large Language
Models (LLMs) emerge as promising approaches for TSC. However, they are prone
to hallucinations in emergencies, leading to unreliable decisions that may
cause substantial delays for emergency vehicles. Moreover, diverse intersection
types present substantial challenges for traffic state encoding and
cross-intersection training, limiting generalization across heterogeneous
intersections. Therefore, this paper proposes Retrieval Augmented Generation
(RAG)-enhanced distributed LLM agents with Emergency response for Generalizable
TSC (REG-TSC). Firstly, this paper presents an emergency-aware reasoning
framework, which dynamically adjusts reasoning depth based on the emergency
scenario and is equipped with a novel Reviewer-based Emergency RAG (RERAG) to
distill specific knowledge and guidance from historical cases, enhancing the
reliability and rationality of agents' emergency decisions. Secondly, this
paper designs a type-agnostic traffic representation and proposes a
Reward-guided Reinforced Refinement (R3) for heterogeneous intersections. R3
adaptively samples training experience from diverse intersections with
environment feedback-based priority and fine-tunes LLM agents with a designed
reward-weighted likelihood loss, guiding REG-TSC toward high-reward policies
across heterogeneous intersections. On three real-world road networks with 17
to 177 heterogeneous intersections, extensive experiments show that REG-TSC
reduces travel time by 42.00%, queue length by 62.31%, and emergency vehicle
waiting time by 83.16%, outperforming other state-of-the-art methods.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºREG-TSCçš„äº¤é€šä¿¡å·æ§åˆ¶ç³»ç»Ÿï¼Œé€šè¿‡ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥ä¼˜åŒ–åº”æ€¥å“åº”å’Œå¼‚æ„è·¯å£çš„æ³›åŒ–èƒ½åŠ›ã€‚è®ºæ–‡è®¾è®¡äº†ç´§æ€¥æ„ŸçŸ¥æ¨ç†æ¡†æ¶ï¼ˆRERAGï¼‰å’Œå¥–åŠ±å¼•å¯¼çš„å¼ºåŒ–ç»†åŒ–æ–¹æ³•ï¼ˆR3ï¼‰ï¼Œæ˜¾è‘—å‡å°‘äº†äº¤é€šæ—¶é—´ã€æ’é˜Ÿé•¿åº¦å’Œç´§æ€¥è½¦è¾†ç­‰å¾…æ—¶é—´ï¼Œå¹¶åœ¨å¤šç§ç°å®è·¯ç½‘ä¸­éªŒè¯äº†å…¶ä¼˜è¶Šæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.26205v2">Towards Global Retrieval Augmented Generation: A Benchmark for Corpus-Level Reasoning</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) has emerged as a leading approach to
reducing hallucinations in large language models (LLMs). Current RAG evaluation
benchmarks primarily focus on what we call local RAG: retrieving relevant
chunks from a small subset of documents to answer queries that require only
localized understanding within specific text chunks. However, many real-world
applications require a fundamentally different capability -- global RAG --
which involves aggregating and analyzing information across entire document
collections to derive corpus-level insights (for example, "What are the top 10
most cited papers in 2023?"). In this paper, we introduce GlobalQA -- the first
benchmark specifically designed to evaluate global RAG capabilities, covering
four core task types: counting, extremum queries, sorting, and top-k
extraction. Through systematic evaluation across different models and
baselines, we find that existing RAG methods perform poorly on global tasks,
with the strongest baseline achieving only 1.51 F1 score. To address these
challenges, we propose GlobalRAG, a multi-tool collaborative framework that
preserves structural coherence through chunk-level retrieval, incorporates
LLM-driven intelligent filters to eliminate noisy documents, and integrates
aggregation modules for precise symbolic computation. On the Qwen2.5-14B model,
GlobalRAG achieves 6.63 F1 compared to the strongest baseline's 1.51 F1,
validating the effectiveness of our method.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°åŸºå‡†GlobalQAï¼Œç”¨äºæµ‹è¯•â€œå…¨å±€RAGâ€èƒ½åŠ›ï¼Œå³ä»æ•´ä¸ªæ–‡æ¡£é›†åˆä¸­èšåˆä¿¡æ¯ä»¥å›ç­”éœ€è¦ç»¼åˆåˆ†æçš„é—®é¢˜ï¼ˆå¦‚ç»Ÿè®¡æˆ–æ’åºä»»åŠ¡ï¼‰ã€‚ç ”ç©¶å‘ç°ç°æœ‰RAGæ–¹æ³•åœ¨å…¨å±€ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œå¹¶æå‡ºäº†GlobalRAGæ¡†æ¶ï¼ˆç»“åˆåˆ†å—æ£€ç´¢ã€æ™ºèƒ½è¿‡æ»¤å’Œèšåˆæ¨¡å—ï¼‰ï¼Œæ˜¾è‘—æå‡äº†Qwen2.5-14Bæ¨¡å‹åœ¨å…¨å±€ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼ˆF1åˆ†æ•°ä»1.51æå‡è‡³6.63ï¼‰ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.26130v2">Beyond Synthetic Benchmarks: Evaluating LLM Performance on Real-World Class-Level Code Generation</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) have demonstrated strong performance on
function-level code generation benchmarks, yet real-world software development
increasingly demands class-level implementations that integrate multiple
methods, attributes, and dependencies within authentic project contexts. This
gap between benchmark performance and practical utility raises critical
questions about LLMs' readiness for production code assistance, particularly
regarding their ability to generalize across familiar and novel codebases.
  We introduce a benchmark derived from real-world open-source repositories,
comprising classes divided into seen and unseen partitions to evaluate
generalization under practical conditions. We systematically examine how input
specification completeness and retrieval-augmented generation affect
class-level correctness across multiple state-of-the-art LLMs.
  Our evaluation reveals a substantial performance gap: while LLMs achieve 84
to 89% correctness on synthetic benchmarks, they attain only 25 to 34% on
real-world class tasks, with minimal distinction between familiar and novel
codebases. Comprehensive documentation provides marginal improvements (1 to
3%), whereas retrieval augmentation yields greater gains (4 to 7%) by supplying
concrete implementation patterns. Error analysis identifies AttributeError,
TypeError, and AssertionError as dominant failure modes, with distinct patterns
between synthetic and real-world scenarios.
  These findings provide actionable insights for enhancing context modelling,
documentation strategies, and retrieval integration in production code
assistance tools.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨çœŸå®ä¸–ç•Œç±»çº§åˆ«ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå‘ç°å…¶åœ¨åˆæˆåŸºå‡†æµ‹è¯•å’Œå®é™…ä»£ç åº“é—´å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œå¹¶ç³»ç»Ÿè¯„ä¼°äº†è¾“å…¥è§„èŒƒå®Œæ•´æ€§å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚ç»“æœè¡¨æ˜ï¼ŒRAGé€šè¿‡æä¾›å…·ä½“å®ç°æ¨¡å¼èƒ½æ˜¾è‘—æå‡æ¨¡å‹ç”Ÿæˆæ­£ç¡®æ€§ï¼ˆ4-7%ï¼‰ï¼ŒåŒæ—¶é”™è¯¯åˆ†ææ­ç¤ºäº†ä¸»è¦å¤±è´¥æ¨¡å¼åŠåˆæˆä¸çœŸå®åœºæ™¯çš„å·®å¼‚ã€‚ç ”ç©¶ä¸ºç”Ÿäº§ä»£ç è¾…åŠ©å·¥å…·ä¸­çš„ä¸Šä¸‹æ–‡å»ºæ¨¡ã€æ–‡æ¡£ç­–ç•¥å’Œæ£€ç´¢é›†æˆæä¾›äº†æ”¹è¿›æ–¹å‘ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-29
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.25724v1">BambooKG: A Neurobiologically-inspired Frequency-Weight Knowledge Graph</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation allows LLMs to access external knowledge,
reducing hallucinations and ageing-data issues. However, it treats retrieved
chunks independently and struggles with multi-hop or relational reasoning,
especially across documents. Knowledge graphs enhance this by capturing the
relationships between entities using triplets, enabling structured, multi-chunk
reasoning. However, these tend to miss information that fails to conform to the
triplet structure. We introduce BambooKG, a knowledge graph with
frequency-based weights on non-triplet edges which reflect link strength,
drawing on the Hebbian principle of "fire together, wire together". This
decreases information loss and results in improved performance on single- and
multi-hop reasoning, outperforming the existing solutions.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†RAGæŠ€æœ¯çš„å±€é™æ€§ï¼ˆå¦‚ç‹¬ç«‹å¤„ç†æ£€ç´¢ç‰‡æ®µã€å¤šè·³æ¨ç†å›°éš¾ï¼‰ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ”¹è¿›æ–¹æ³•BambooKGâ€”â€”é€šè¿‡åŸºäºé¢‘ç‡åŠ æƒçš„éä¸‰å…ƒç»„è¾¹å¢å¼ºçŸ¥è¯†å›¾è°±ç»“æ„ï¼Œå‡å°‘ä¿¡æ¯æŸå¤±å¹¶æå‡å•/å¤šè·³æ¨ç†æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.25718v1">Retrieval-Augmented Search for Large-Scale Map Collections with ColPali</a></td><td><details><summary>å±•å¼€</summary>Multimodal approaches have shown great promise for searching and navigating
digital collections held by libraries, archives, and museums. In this paper, we
introduce map-RAS: a retrieval-augmented search system for historic maps. In
addition to introducing our framework, we detail our publicly-hosted demo for
searching 101,233 map images held by the Library of Congress. With our system,
users can multimodally query the map collection via ColPali, summarize search
results using Llama 3.2, and upload their own collections to perform
inter-collection search. We articulate potential use cases for archivists,
curators, and end-users, as well as future work with our system in both machine
learning and the digital humanities. Our demo can be viewed at:
http://www.mapras.com.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ä»‹ç»äº†map-RASï¼Œä¸€ä¸ªé’ˆå¯¹å†å²åœ°å›¾çš„æ£€ç´¢å¢å¼ºæœç´¢ç³»ç»Ÿï¼Œç»“åˆå¤šæ¨¡æ€æŸ¥è¯¢ï¼ˆå¦‚ColPaliï¼‰ã€å¤§æ¨¡å‹ï¼ˆLlama 3.2ï¼‰ç”Ÿæˆæ‘˜è¦åŠè·¨é¦†è—æœç´¢åŠŸèƒ½ï¼Œå¹¶æä¾›äº†ç¾å›½å›½ä¼šå›¾ä¹¦é¦†10ä¸‡+åœ°å›¾çš„å…¬å¼€æ¼”ç¤ºï¼Œæ¢è®¨äº†å…¶åœ¨æ•°å­—äººæ–‡é¢†åŸŸçš„åº”ç”¨å‰æ™¯ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.25621v1">FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering</a></td><td><details><summary>å±•å¼€</summary>The advent of Large Language Models (LLMs) has revolutionized Natural
Language Processing, yet their application in high-stakes, specialized domains
like religious question answering is hindered by challenges like hallucination
and unfaithfulness to authoritative sources. This issue is particularly
critical for the Persian-speaking Muslim community, where accuracy and
trustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG)
systems, relying on simplistic single-pass pipelines, fall short on complex,
multi-hop queries requiring multi-step reasoning and evidence aggregation. To
address this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful
Advanced Question Answering in the Persian Islamic domain. FARSIQA is built
upon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative
Refinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting
process: it adaptively decomposes complex queries, assesses evidence
sufficiency, and enters an iterative loop to generate sub-queries,
progressively filling information gaps. Operating on a curated knowledge base
of over one million authoritative Islamic documents, FARSIQA demonstrates
superior performance. Rigorous evaluation on the challenging IslamicPCQA
benchmark shows state-of-the-art performance: the system achieves a remarkable
97.0% in Negative Rejection - a 40-point improvement over baselines - and a
high Answer Correctness score of 74.3%. Our work establishes a new standard for
Persian Islamic QA and validates that our iterative, adaptive architecture is
crucial for building faithful, reliable AI systems in sensitive domains.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºFARSIQAç³»ç»Ÿï¼ŒåŸºäºåˆ›æ–°çš„FAIR-RAGæ¶æ„ï¼ˆä¸€ç§å¿ å®ã€è‡ªé€‚åº”ã€è¿­ä»£ä¼˜åŒ–çš„RAGæ¡†æ¶ï¼‰ï¼Œé€šè¿‡åŠ¨æ€åˆ†è§£å¤æ‚æŸ¥è¯¢ã€è¯„ä¼°è¯æ®å……åˆ†æ€§å¹¶è¿­ä»£ç”Ÿæˆå­æŸ¥è¯¢æ¥è§£å†³æ³¢æ–¯ä¼Šæ–¯å…°é¢†åŸŸå¤šè·³é—®ç­”é—®é¢˜ï¼Œåœ¨æƒå¨çŸ¥è¯†åº“ä¸Šå®ç°äº†97%çš„è´Ÿä¾‹æ‹’ç»ç‡å’Œ74.3%ç­”æ¡ˆæ­£ç¡®ç‡ï¼Œæ˜¾è‘—æå‡äº†ä¸“ä¸šé¢†åŸŸé—®ç­”çš„å¯é æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.25518v1">Retrieval Augmented Generation (RAG) for Fintech: Agentic Design and Evaluation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) systems often face limitations in
specialized domains such as fintech, where domain-specific ontologies, dense
terminology, and acronyms complicate effective retrieval and synthesis. This
paper introduces an agentic RAG architecture designed to address these
challenges through a modular pipeline of specialized agents. The proposed
system supports intelligent query reformulation, iterative sub-query
decomposition guided by keyphrase extraction, contextual acronym resolution,
and cross-encoder-based context re-ranking. We evaluate our approach against a
standard RAG baseline using a curated dataset of 85 question--answer--reference
triples derived from an enterprise fintech knowledge base. Experimental results
demonstrate that the agentic RAG system outperforms the baseline in retrieval
precision and relevance, albeit with increased latency. These findings suggest
that structured, multi-agent methodologies offer a promising direction for
enhancing retrieval robustness in complex, domain-specific settings.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§é¢å‘é‡‘èç§‘æŠ€ç­‰ä¸“ä¸šé¢†åŸŸçš„å¤šæ™ºèƒ½ä½“RAGæ¶æ„ï¼Œé€šè¿‡æ¨¡å—åŒ–æ™ºèƒ½ä½“å®ç°æŸ¥è¯¢é‡æ„ã€å­æŸ¥è¯¢åˆ†è§£ã€æœ¯è¯­è§£æå’Œä¸Šä¸‹æ–‡é‡æ’åºï¼Œå®éªŒè¡¨æ˜å…¶åœ¨æ£€ç´¢ç²¾åº¦å’Œç›¸å…³æ€§ä¸Šä¼˜äºåŸºçº¿RAGç³»ç»Ÿï¼Œä½†å­˜åœ¨å»¶è¿Ÿå¢åŠ çš„é—®é¢˜ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-28
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.24652v1">Optimizing Retrieval for RAG via Reinforced Contrastive Learning</a></td><td><details><summary>å±•å¼€</summary>As retrieval-augmented generation (RAG) becomes increasingly widespread, the
role of information retrieval (IR) is shifting from retrieving information for
human users to retrieving contextual knowledge for artificial intelligence (AI)
systems, where relevance becomes difficult to define or annotate beforehand. To
address this challenge, we propose R3, a Retrieval framework optimized for RAG
through trialand-feedback Reinforced contrastive learning. Unlike prior
approaches that rely on annotated or synthetic data for supervised fine-tuning,
R3 enables the retriever to dynamically explore and optimize relevance within
the RAG environment. During training, the retrieved results interact with the
environment to produce contrastive signals that automatically guide the
retriever's self-improvement. Extensive experiments across diverse tasks
demonstrate that R3 improves RAG performance by 5.2% over the original
retriever and surpasses state-of-the-art retrievers by 4.9%, while achieving
comparable results to LLM-augmented retrieval and RAG systems built on
post-trained or instruction-tuned LLMs. It is both efficient and practical,
requiring only 4 GPUs and completing training within a single day.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºR3çš„æ£€ç´¢æ¡†æ¶ï¼Œé€šè¿‡åŸºäºè¯•é”™åé¦ˆçš„å¼ºåŒ–å¯¹æ¯”å­¦ä¹ ä¼˜åŒ–RAGä¸­çš„æ£€ç´¢è¿‡ç¨‹ï¼Œèƒ½å¤ŸåŠ¨æ€æ¢ç´¢å’Œä¼˜åŒ–ç›¸å…³æ€§ï¼Œæ— éœ€ä¾èµ–é¢„æ ‡æ³¨æˆ–åˆæˆæ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼ŒR3æ˜¾è‘—æå‡äº†RAGæ€§èƒ½ï¼Œä¼˜äºç°æœ‰æ£€ç´¢æ–¹æ³•ï¼Œä¸”è®­ç»ƒæ•ˆç‡é«˜ï¼ˆä»…éœ€4å—GPUå’Œ1å¤©æ—¶é—´ï¼‰ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.24476v1">Mitigating Hallucination in Large Language Models (LLMs): An Application-Oriented Survey on RAG, Reasoning, and Agentic Systems</a></td><td><details><summary>å±•å¼€</summary>Hallucination remains one of the key obstacles to the reliable deployment of
large language models (LLMs), particularly in real-world applications. Among
various mitigation strategies, Retrieval-Augmented Generation (RAG) and
reasoning enhancement have emerged as two of the most effective and widely
adopted approaches, marking a shift from merely suppressing hallucinations to
balancing creativity and reliability. However, their synergistic potential and
underlying mechanisms for hallucination mitigation have not yet been
systematically examined. This survey adopts an application-oriented perspective
of capability enhancement to analyze how RAG, reasoning enhancement, and their
integration in Agentic Systems mitigate hallucinations. We propose a taxonomy
distinguishing knowledge-based and logic-based hallucinations, systematically
examine how RAG and reasoning address each, and present a unified framework
supported by real-world applications, evaluations, and benchmarks.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„å¹»è§‰é—®é¢˜ï¼Œé‡ç‚¹åˆ†æäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œæ¨ç†å¢å¼ºä½œä¸ºç¼“è§£å¹»è§‰çš„æœ‰æ•ˆç­–ç•¥ï¼Œå¹¶ç ”ç©¶äº†å®ƒä»¬ååŒä½œç”¨çš„æ½œåŠ›åŠå…¶åœ¨æ™ºèƒ½ä»£ç†ç³»ç»Ÿä¸­çš„æ•´åˆã€‚æ–‡ç« æå‡ºäº†åŸºäºçŸ¥è¯†å’Œé€»è¾‘çš„å¹»è§‰åˆ†ç±»ï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°äº†RAGå’Œæ¨ç†æ–¹æ³•å¦‚ä½•åº”å¯¹å„ç±»å¹»è§‰ï¼Œå¹¶é€šè¿‡å®é™…åº”ç”¨å’ŒåŸºå‡†æµ‹è¯•æä¾›äº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.24469v1">Iterative Critique-Refine Framework for Enhancing LLM Personalization</a></td><td><details><summary>å±•å¼€</summary>Personalized text generation requires models not only to produce coherent
text but also to align with a target user's style, tone, and topical focus.
Existing retrieval-augmented approaches such as LaMP and PGraphRAG enrich
profiles with user and neighbor histories, but they stop at generation and
often yield outputs that drift in tone, topic, or style. We present PerFine, a
unified, training-free critique-refine framework that enhances personalization
through iterative, profile-grounded feedback. In each iteration, an LLM
generator produces a draft conditioned on the retrieved profile, and a critic
LLM - also conditioned on the same profile - provides structured feedback on
tone, vocabulary, sentence structure, and topicality. The generator then
revises, while a novel knockout strategy retains the stronger draft across
iterations. We further study additional inference-time strategies such as
Best-of-N and Topic Extraction to balance quality and efficiency. Across Yelp,
Goodreads, and Amazon datasets, PerFine consistently improves personalization
over PGraphRAG, with GEval gains of +7-13%, steady improvements over 3-5
refinement iterations, and scalability with increasing critic size. These
results highlight that post-hoc, profile-aware feedback offers a powerful
paradigm for personalized LLM generation that is both training-free and
model-agnostic.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºPerFineçš„è®­ç»ƒå…è´¹æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯å’Œè¿­ä»£å¼åé¦ˆä¼˜åŒ–ï¼Œæå‡ä¸ªæ€§åŒ–æ–‡æœ¬ç”Ÿæˆçš„è´¨é‡ã€‚PerFineåˆ©ç”¨æ£€ç´¢åˆ°çš„ç”¨æˆ·æ¡£æ¡ˆç”Ÿæˆåˆç¨¿ï¼Œå¹¶é€šè¿‡åŸºäºç›¸åŒæ¡£æ¡ˆçš„æ‰¹è¯„æ¨¡å‹æä¾›ç»“æ„åŒ–åé¦ˆï¼Œè¿›è€Œè¿­ä»£ä¼˜åŒ–ç”Ÿæˆç»“æœï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸­æ˜¾è‘—æå‡äº†ä¸ªæ€§åŒ–æŒ‡æ ‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.24427v1">SynthWorlds: Controlled Parallel Worlds for Disentangling Reasoning and Knowledge in Language Models</a></td><td><details><summary>å±•å¼€</summary>Evaluating the reasoning ability of language models (LMs) is complicated by
their extensive parametric world knowledge, where benchmark performance often
reflects factual recall rather than genuine reasoning. Existing datasets and
approaches (e.g., temporal filtering, paraphrasing, adversarial substitution)
cannot cleanly separate the two. We present SynthWorlds, a framework that
disentangles task reasoning complexity from factual knowledge. In SynthWorlds,
we construct parallel corpora representing two worlds with identical
interconnected structure: a real-mapped world, where models may exploit
parametric knowledge, and a synthetic-mapped world, where such knowledge is
meaningless. On top of these corpora, we design two mirrored tasks as case
studies: multi-hop question answering and page navigation, which maintain equal
reasoning difficulty across worlds. Experiments in parametric-only (e.g.,
closed-book QA) and knowledge-augmented (e.g., retrieval-augmented) LM settings
reveal a persistent knowledge advantage gap, defined as the performance boost
models gain from memorized parametric world knowledge. Knowledge acquisition
and integration mechanisms reduce but do not eliminate this gap, highlighting
opportunities for system improvements. Fully automatic and scalable,
SynthWorlds provides a controlled environment for evaluating LMs in ways that
were previously challenging, enabling precise and testable comparisons of
reasoning and memorization.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºSynthWorldsæ¡†æ¶ï¼Œé€šè¿‡æ„å»ºç»“æ„ç›¸åŒä½†çŸ¥è¯†èƒŒæ™¯ä¸åŒçš„å¹³è¡Œè¯­æ–™åº“ï¼ˆçœŸå®ä¸–ç•Œæ˜ å°„ä¸åˆæˆä¸–ç•Œæ˜ å°„ï¼‰ï¼Œåˆ†ç¦»è¯­è¨€æ¨¡å‹çš„ä»»åŠ¡æ¨ç†èƒ½åŠ›ä¸äº‹å®è®°å¿†èƒ½åŠ›ã€‚è™½ç„¶ä¸»è¦ç ”ç©¶æ¨¡å‹æ¨ç†ä¸è®°å¿†çš„åŒºåˆ†ï¼Œä½†æ˜ç¡®æŒ‡å‡ºå®éªŒæ¶‰åŠæ£€ç´¢å¢å¼ºï¼ˆRAGï¼‰ç­‰çŸ¥è¯†å¢å¼ºè®¾ç½®ï¼Œå¹¶æ¢è®¨çŸ¥è¯†æ•´åˆæœºåˆ¶å¯¹æ€§èƒ½å·®è·çš„å½±å“ï¼Œå› æ­¤ä¸RAGæŠ€æœ¯ç›¸å…³ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.24402v1">Metadata-Driven Retrieval-Augmented Generation for Financial Question Answering</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) struggles on long, structured financial
filings where relevant evidence is sparse and cross-referenced. This paper
presents a systematic investigation of advanced metadata-driven
Retrieval-Augmented Generation (RAG) techniques, proposing and evaluating a
novel, multi-stage RAG architecture that leverages LLM-generated metadata. We
introduce a sophisticated indexing pipeline to create contextually rich
document chunks and benchmark a spectrum of enhancements, including
pre-retrieval filtering, post-retrieval reranking, and enriched embeddings,
benchmarked on the FinanceBench dataset. Our results reveal that while a
powerful reranker is essential for precision, the most significant performance
gains come from embedding chunk metadata directly with text ("contextual
chunks"). Our proposed optimal architecture combines LLM-driven pre-retrieval
optimizations with these contextual embeddings to achieve superior performance.
Additionally, we present a custom metadata reranker that offers a compelling,
cost-effective alternative to commercial solutions, highlighting a practical
trade-off between peak performance and operational efficiency. This study
provides a blueprint for building robust, metadata-aware RAG systems for
financial document analysis.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡é’ˆå¯¹é‡‘èé•¿æ–‡æ¡£ä¸­ä¿¡æ¯ç¨€ç–å’Œäº¤å‰å¼•ç”¨çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºLLMç”Ÿæˆå…ƒæ•°æ®çš„å¤šé˜¶æ®µRAGæ¶æ„ï¼Œé€šè¿‡æ”¹è¿›ç´¢å¼•æµç¨‹ã€é¢„æ£€ç´¢è¿‡æ»¤ã€åæ£€ç´¢é‡æ’åºå’Œå¢å¼ºåµŒå…¥ç­‰æ–¹æ³•æå‡æ€§èƒ½ï¼Œå¹¶åœ¨FinanceBenchæ•°æ®é›†ä¸ŠéªŒè¯äº†ä¸Šä¸‹æ–‡åˆ†å—åµŒå…¥å’Œå®šåˆ¶å…ƒæ•°æ®é‡æ’åºå™¨çš„æœ‰æ•ˆæ€§ï¼Œä¸ºé‡‘èæ–‡æ¡£åˆ†ææä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.24390v1">Improving LLM Reasoning via Dependency-Aware Query Decomposition and Logic-Parallel Content Expansion</a></td><td><details><summary>å±•å¼€</summary>The integration of Large Language Models (LLMs) into real-time Web
applications, such as AI-powered search and conversational agents, presents a
fundamental Web infrastructure challenge: reconciling the demand for
high-quality, complex reasoning with the stringent low-latency and
high-throughput requirements of interactive services. Current LLM reasoning,
hindered by computationally inefficient sequential generation and rigid
reasoning strategies, creates a critical bottleneck for the Web services.
Existing approaches typically optimize the LLM reasoning for either efficiency
or quality but struggle to achieve both, and thus fail to meet the dual
requirements of modern Web platforms. To overcome these limitations, we propose
Orion, a novel and efficient reasoning framework that enables dependency-aware
query decomposition and logic-parallel content expansion. Concretely, Orion
decomposes a single query reasoning process into two synergistic phases: (1)
\textit{key point generation}, which distills logically structured key points
through retrieval-augmented few-shot prompting, and (2) \textit{content
parallel expansion}, which concurrently elaborates on these points based on a
dependency graph to ensure logical consistency. Furthermore, Orion introduces a
pipeline scheduling mechanism that exploits the complementary computational
characteristics of the two phases (generation imposes pressure on GPU computing
and expansion stresses on GPU memory) across multiple queries, enabling
cross-query parallelism and dramatically improving reasoning performance (\ie,
efficiency and quality). Experiments on diverse benchmarks show that Orion not
only delivers up to 4.33x higher token generation speed and 3.42x lower answer
latency over the baselines but also improves reasoning quality by up to 18.75%
through explicitly modeling inter-point dependencies.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºOrionæ¡†æ¶ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºçš„å°‘é‡ç¤ºä¾‹æç¤ºï¼ˆretrieval-augmented few-shot promptingï¼‰åˆ†è§£æŸ¥è¯¢ä¸ºé€»è¾‘å…³é”®ç‚¹ç”Ÿæˆå’Œå¹¶è¡Œå†…å®¹æ‰©å±•ä¸¤é˜¶æ®µï¼Œç»“åˆä¾èµ–å›¾å®ç°é«˜æ•ˆæ¨ç†ï¼Œæ˜¾è‘—æå‡å¤§è¯­è¨€æ¨¡å‹åœ¨å®æ—¶Webåº”ç”¨ä¸­çš„æ€§èƒ½ï¼ˆé€Ÿåº¦ã€å»¶è¿Ÿå’Œå›ç­”è´¨é‡ï¼‰ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.24303v1">Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental Forecasting</a></td><td><details><summary>å±•å¼€</summary>Judgmental forecasting is the task of making predictions about future events
based on human judgment. This task can be seen as a form of claim verification,
where the claim corresponds to a future event and the task is to assess the
plausibility of that event. In this paper, we propose a novel multi-agent
framework for claim verification, whereby different agents may disagree on
claim veracity and bring specific evidence for and against the claims,
represented as quantitative bipolar argumentation frameworks (QBAFs). We then
instantiate the framework for supporting claim verification, with a variety of
agents realised with Large Language Models (LLMs): (1) ArgLLM agents, an
existing approach for claim verification that generates and evaluates QBAFs;
(2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM)
from external sources is used to generate QBAFs; (3) RAG-ArgLLM agents,
extending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of
arguments from external sources. Finally, we conduct experiments with two
standard judgmental forecasting datasets, with instances of our framework with
two or three agents, empowered by six different base LLMs. We observe that
combining evidence from agents can improve forecasting accuracy, especially in
the case of three agents, while providing an explainable combination of
evidence for claim verification.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“æ¡†æ¶çš„å£°æ˜éªŒè¯æ–¹æ³•ï¼Œå…¶ä¸­éƒ¨åˆ†æ™ºèƒ½ä½“ï¼ˆå¦‚RAG-ArgLLMï¼‰é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ä»å¤–éƒ¨æ¥æºè·å–è®ºæ®ï¼Œå¹¶ç»“åˆå®šé‡åŒæè®ºè¯æ¡†æ¶ï¼ˆQBAFsï¼‰è¿›è¡ŒéªŒè¯ã€‚å®éªŒè¡¨æ˜ï¼Œå¤šæ™ºèƒ½ä½“ååŒèƒ½æå‡é¢„æµ‹å‡†ç¡®æ€§å¹¶æä¾›å¯è§£é‡Šçš„è¯æ®ç»„åˆã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.24242v1">Enabling Near-realtime Remote Sensing via Satellite-Ground Collaboration of Large Vision-Language Models</a></td><td><details><summary>å±•å¼€</summary>Large vision-language models (LVLMs) have recently demonstrated great
potential in remote sensing (RS) tasks (e.g., disaster monitoring) conducted by
low Earth orbit (LEO) satellites. However, their deployment in real-world LEO
satellite systems remains largely unexplored, hindered by limited onboard
computing resources and brief satellite-ground contacts. We propose Grace, a
satellite-ground collaborative system designed for near-realtime LVLM inference
in RS tasks. Accordingly, we deploy compact LVLM on satellites for realtime
inference, but larger ones on ground stations (GSs) to guarantee end-to-end
performance. Grace is comprised of two main phases that are asynchronous
satellite-GS Retrieval-Augmented Generation (RAG), and a task dispatch
algorithm. Firstly, we still the knowledge archive of GS RAG to satellite
archive with tailored adaptive update algorithm during limited satellite-ground
data exchange period. Secondly, propose a confidence-based test algorithm that
either processes the task onboard the satellite or offloads it to the GS.
Extensive experiments based on real-world satellite orbital data show that
Grace reduces the average latency by 76-95% compared to state-of-the-art
methods, without compromising inference accuracy.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºGraceçš„å«æ˜Ÿ-åœ°é¢åä½œç³»ç»Ÿï¼Œç”¨äºåœ¨é¥æ„Ÿä»»åŠ¡ä¸­å®ç°è¿‘å®æ—¶çš„è§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†ã€‚ç³»ç»Ÿé€šè¿‡å¼‚æ­¥çš„å«æ˜Ÿ-åœ°é¢æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œä»»åŠ¡è°ƒåº¦ç®—æ³•ï¼Œç»“åˆå«æ˜Ÿä¸Šçš„ç´§å‡‘æ¨¡å‹ä¸åœ°é¢ç«™çš„å¤§å‹æ¨¡å‹ï¼Œæ˜¾è‘—é™ä½äº†å»¶è¿Ÿï¼ˆ76-95%ï¼‰åŒæ—¶ä¿æŒæ¨ç†ç²¾åº¦ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.24120v1">Graph-Guided Concept Selection for Efficient Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Graph-based RAG constructs a knowledge graph (KG) from text chunks to enhance
retrieval in Large Language Model (LLM)-based question answering. It is
especially beneficial in domains such as biomedicine, law, and political
science, where effective retrieval often involves multi-hop reasoning over
proprietary documents. However, these methods demand numerous LLM calls to
extract entities and relations from text chunks, incurring prohibitive costs at
scale. Through a carefully designed ablation study, we observe that certain
words (termed concepts) and their associated documents are more important.
Based on this insight, we propose Graph-Guided Concept Selection (G2ConS). Its
core comprises a chunk selection method and an LLM-independent concept graph.
The former selects salient document chunks to reduce KG construction costs; the
latter closes knowledge gaps introduced by chunk selection at zero cost.
Evaluations on multiple real-world datasets show that G2ConS outperforms all
baselines in construction cost, retrieval effectiveness, and answering quality.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºçŸ¥è¯†å›¾è°±çš„RAGæ–¹æ³•ï¼ˆGraph-based RAGï¼‰ï¼Œé€šè¿‡ä»æ–‡æœ¬å—æ„å»ºçŸ¥è¯†å›¾è°±æ¥å¢å¼ºåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é—®ç­”æ£€ç´¢æ•ˆæœï¼Œç‰¹åˆ«é€‚ç”¨äºéœ€è¦å¤šè·³æ¨ç†çš„ä¸“ä¸šé¢†åŸŸï¼ˆå¦‚ç”Ÿç‰©åŒ»å­¦ã€æ³•å¾‹ç­‰ï¼‰ã€‚é’ˆå¯¹ä¼ ç»Ÿæ–¹æ³•å› é¢‘ç¹è°ƒç”¨LLMæå–å®ä½“å’Œå…³ç³»å¯¼è‡´çš„é«˜æˆæœ¬é—®é¢˜ï¼Œä½œè€…æå‡ºäº†Graph-Guided Concept Selectionï¼ˆG2ConSï¼‰ï¼ŒåŒ…å«ä¸€ä¸ªæ–‡æ¡£å—é€‰æ‹©æ–¹æ³•å’Œä¸€ä¸ªä¸ä¾èµ–LLMçš„æ¦‚å¿µå›¾è°±ï¼Œæ˜¾è‘—é™ä½äº†çŸ¥è¯†å›¾è°±æ„å»ºæˆæœ¬å¹¶å¡«è¡¥äº†çŸ¥è¯†ç©ºç™½ã€‚å®éªŒè¡¨æ˜ï¼ŒG2ConSåœ¨æ„å»ºæˆæœ¬ã€æ£€ç´¢æ•ˆæœå’Œå›ç­”è´¨é‡ä¸Šå‡ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.24049v1">Learning from History: A Retrieval-Augmented Framework for Spatiotemporal Prediction</a></td><td><details><summary>å±•å¼€</summary>Accurate and long-term spatiotemporal prediction for complex physical systems
remains a fundamental challenge in scientific computing. While deep learning
models, as powerful parametric approximators, have shown remarkable success,
they suffer from a critical limitation: the accumulation of errors during
long-term autoregressive rollouts often leads to physically implausible
artifacts. This deficiency arises from their purely parametric nature, which
struggles to capture the full constraints of a system's intrinsic dynamics. To
address this, we introduce a novel \textbf{Retrieval-Augmented Prediction
(RAP)} framework, a hybrid paradigm that synergizes the predictive power of
deep networks with the grounded truth of historical data. The core philosophy
of RAP is to leverage historical evolutionary exemplars as a non-parametric
estimate of the system's local dynamics. For any given state, RAP efficiently
retrieves the most similar historical analog from a large-scale database. The
true future evolution of this analog then serves as a \textbf{reference
target}. Critically, this target is not a hard constraint in the loss function
but rather a powerful conditional input to a specialized dual-stream
architecture. It provides strong \textbf{dynamic guidance}, steering the
model's predictions towards physically viable trajectories. In extensive
benchmarks across meteorology, turbulence, and fire simulation, RAP not only
surpasses state-of-the-art methods but also significantly outperforms a strong
\textbf{analog-only forecasting baseline}. More importantly, RAP generates
predictions that are more physically realistic by effectively suppressing error
divergence in long-term rollouts.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œæ£€ç´¢å¢å¼ºé¢„æµ‹ï¼ˆRAPï¼‰â€çš„æ··åˆæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆæ·±åº¦å­¦ä¹ æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›å’Œå†å²æ•°æ®çš„çœŸå®åŠ¨æ€ï¼Œåˆ©ç”¨æ£€ç´¢åˆ°çš„ç›¸ä¼¼å†å²æ¼”åŒ–ç¤ºä¾‹ä½œä¸ºéå‚æ•°ä¼°è®¡ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆæ›´ç¬¦åˆç‰©ç†è§„å¾‹çš„é•¿æœŸæ—¶ç©ºé¢„æµ‹ï¼Œæ˜¾è‘—æå‡äº†æ°”è±¡ã€æ¹æµå’Œç«ç¾æ¨¡æ‹Ÿç­‰é¢†åŸŸçš„é¢„æµ‹å‡†ç¡®æ€§å’Œç‰©ç†åˆç†æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.24003v1">META-RAG: Meta-Analysis-Inspired Evidence-Re-Ranking Method for Retrieval-Augmented Generation in Evidence-Based Medicine</a></td><td><details><summary>å±•å¼€</summary>Evidence-based medicine (EBM) holds a crucial role in clinical application.
Given suitable medical articles, doctors effectively reduce the incidence of
misdiagnoses. Researchers find it efficient to use large language models (LLMs)
techniques like RAG for EBM tasks. However, the EBM maintains stringent
requirements for evidence, and RAG applications in EBM struggle to efficiently
distinguish high-quality evidence. Therefore, inspired by the meta-analysis
used in EBM, we provide a new method to re-rank and filter the medical
evidence. This method presents multiple principles to filter the best evidence
for LLMs to diagnose. We employ a combination of several EBM methods to emulate
the meta-analysis, which includes reliability analysis, heterogeneity analysis,
and extrapolation analysis. These processes allow the users to retrieve the
best medical evidence for the LLMs. Ultimately, we evaluate these high-quality
articles and show an accuracy improvement of up to 11.4% in our experiments and
results. Our method successfully enables RAG to extract higher-quality and more
reliable evidence from the PubMed dataset. This work can reduce the infusion of
incorrect knowledge into responses and help users receive more effective
replies.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºRAGæŠ€æœ¯çš„æ”¹è¿›æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆå…ƒåˆ†æä¸­çš„å¯é æ€§åˆ†æã€å¼‚è´¨æ€§åˆ†æå’Œå¤–æ¨åˆ†æï¼Œå¯¹åŒ»å­¦è¯æ®è¿›è¡Œé‡æ–°æ’åºå’Œç­›é€‰ï¼Œä»¥æé«˜LLMsåœ¨å¾ªè¯åŒ»å­¦ä»»åŠ¡ä¸­çš„è¯Šæ–­å‡†ç¡®æ€§ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå‡†ç¡®æ€§æå‡äº†11.4%ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.23998v1">PICOs-RAG: PICO-supported Query Rewriting for Retrieval-Augmented Generation in Evidence-Based Medicine</a></td><td><details><summary>å±•å¼€</summary>Evidence-based medicine (EBM) research has always been of paramount
importance. It is important to find appropriate medical theoretical support for
the needs from physicians or patients to reduce the occurrence of medical
accidents. This process is often carried out by human querying relevant
literature databases, which lacks objectivity and efficiency. Therefore,
researchers utilize retrieval-augmented generation (RAG) to search for evidence
and generate responses automatically. However, current RAG methods struggle to
handle complex queries in real-world clinical scenarios. For example, when
queries lack certain information or use imprecise language, the model may
retrieve irrelevant evidence and generate unhelpful answers. To address this
issue, we present the PICOs-RAG to expand the user queries into a better
format. Our method can expand and normalize the queries into professional ones
and use the PICO format, a search strategy tool present in EBM, to extract the
most important information used for retrieval. This approach significantly
enhances retrieval efficiency and relevance, resulting in up to an 8.8\%
improvement compared to the baseline evaluated by our method. Thereby the
PICOs-RAG improves the performance of the large language models into a helpful
and reliable medical assistant in EBM.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºPICOs-RAGçš„æ–¹æ³•ï¼Œé€šè¿‡æ‰©å±•å’Œè§„èŒƒåŒ–ç”¨æˆ·æŸ¥è¯¢ï¼ˆé‡‡ç”¨EBMä¸­çš„PICOæ ¼å¼ï¼‰æ¥ä¼˜åŒ–æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨å¾ªè¯åŒ»å­¦ä¸­çš„åº”ç”¨ï¼Œè§£å†³äº†å¤æ‚ä¸´åºŠæŸ¥è¯¢ä¸‹æ£€ç´¢ä¸ç²¾å‡†çš„é—®é¢˜ï¼Œå®éªŒæ˜¾ç¤ºå…¶æ£€ç´¢æ•ˆç‡å’Œç›¸å…³æ€§è¾ƒåŸºçº¿æå‡8.8%ï¼Œä½¿å¤§è¯­è¨€æ¨¡å‹æˆä¸ºæ›´å¯é çš„åŒ»å­¦åŠ©æ‰‹ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.23995v1">M-Eval: A Heterogeneity-Based Framework for Multi-evidence Validation in Medical RAG Systems</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented Generation (RAG) has demonstrated potential in enhancing
medical question-answering systems through the integration of large language
models (LLMs) with external medical literature. LLMs can retrieve relevant
medical articles to generate more professional responses efficiently. However,
current RAG applications still face problems. They generate incorrect
information, such as hallucinations, and they fail to use external knowledge
correctly. To solve these issues, we propose a new method named M-Eval. This
method is inspired by the heterogeneity analysis approach used in
Evidence-Based Medicine (EBM). Our approach can check for factual errors in RAG
responses using evidence from multiple sources. First, we extract additional
medical literature from external knowledge bases. Then, we retrieve the
evidence documents generated by the RAG system. We use heterogeneity analysis
to check whether the evidence supports different viewpoints in the response. In
addition to verifying the accuracy of the response, we also assess the
reliability of the evidence provided by the RAG system. Our method shows an
improvement of up to 23.31% accuracy across various LLMs. This work can help
detect errors in current RAG-based medical systems. It also makes the
applications of LLMs more reliable and reduces diagnostic errors.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºM-Evalçš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³RAGåœ¨åŒ»ç–—é—®ç­”ç³»ç»Ÿä¸­å­˜åœ¨çš„ç”Ÿæˆé”™è¯¯ä¿¡æ¯ï¼ˆå¦‚å¹»è§‰ï¼‰å’Œæœªèƒ½æ­£ç¡®ä½¿ç”¨å¤–éƒ¨çŸ¥è¯†çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åŸºäºå¾ªè¯åŒ»å­¦çš„å¼‚è´¨æ€§åˆ†æï¼Œé€šè¿‡ä»å¤–éƒ¨çŸ¥è¯†åº“æ£€ç´¢é¢å¤–åŒ»å­¦æ–‡çŒ®å¹¶ä¸RAGç”Ÿæˆçš„è¯æ®æ–‡æ¡£å¯¹æ¯”ï¼ŒéªŒè¯å›ç­”çš„å‡†ç¡®æ€§å’Œè¯æ®çš„å¯é æ€§ï¼Œå®éªŒæ˜¾ç¤ºå…¶å¯å°†ä¸åŒå¤§è¯­è¨€æ¨¡å‹çš„å‡†ç¡®ç‡æå‡é«˜è¾¾23.31%ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-27
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.23601v1">Alita-G: Self-Evolving Generative Agent for Agent Generation</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) have been shown to perform better when
scaffolded into agents with memory, tools, and feedback. Beyond this,
self-evolving agents have emerged, but current work largely limits adaptation
to prompt rewriting or failure retries. Therefore, we present ALITA-G, a
self-evolution framework that transforms a general-purpose agent into a domain
expert by systematically generating, abstracting, and curating Model Context
Protocol (MCP) tools. In this framework, a generalist agent executes a curated
suite of target-domain tasks and synthesizes candidate MCPs from successful
trajectories. These are then abstracted to parameterized primitives and
consolidated into an MCP Box. At inference time, ALITA-G performs
retrieval-augmented MCP selection with the help of each tool's descriptions and
use cases, before executing an agent equipped with the MCP Executor. Across
several benchmarks GAIA, PathVQA, and Humanity's Last Exam, ALITA-G attains
strong gains while reducing computation costs. On GAIA validation, it achieves
83.03% pass@1 and 89.09% pass@3, establishing a new state-of-the-art result
while reducing mean tokens per example by approximately 15% relative to a
strong baseline agent. ALITA-G thus provides a principled pathway from
generalist capability to reusable, domain-specific competence, improving both
accuracy and efficiency on complex reasoning tasks.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ALITA-Gæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç”Ÿæˆã€æŠ½è±¡å’Œæ•´ç†æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰å·¥å…·ï¼Œå°†é€šç”¨æ™ºèƒ½ä»£ç†è½¬åŒ–ä¸ºé¢†åŸŸä¸“å®¶ã€‚ALITA-Gåœ¨æ¨ç†æ—¶é‡‡ç”¨æ£€ç´¢å¢å¼ºçš„MCPé€‰æ‹©æ–¹æ³•ï¼Œç»“åˆå·¥å…·æè¿°å’Œä½¿ç”¨æ¡ˆä¾‹è¿›è¡Œæ£€ç´¢ï¼Œä»è€Œæå‡ä»»åŠ¡æ‰§è¡Œçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡å’Œè®¡ç®—æˆæœ¬é™ä½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.23544v1">LimRank: Less is More for Reasoning-Intensive Information Reranking</a></td><td><details><summary>å±•å¼€</summary>Existing approaches typically rely on large-scale fine-tuning to adapt LLMs
for information reranking tasks, which is computationally expensive. In this
work, we demonstrate that modern LLMs can be effectively adapted using only
minimal, high-quality supervision. To enable this, we design
LIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating
diverse, challenging, and realistic reranking examples. Using this synthetic
data, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two
challenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and
FollowIR for instruction-following retrieval. Our experiments demonstrate that
LIMRANK achieves competitive performance, while being trained on less than 5%
of the data typically used in prior work. Further ablation studies demonstrate
the effectiveness of LIMRANK-SYNTHESIZER and the strong generalization
capabilities of LIMRANK across downstream tasks, including scientific
literature search and retrieval-augmented generation for knowledge-intensive
problem solving.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºLIMRANKçš„é«˜æ•ˆä¿¡æ¯é‡æ’æ¨¡å‹ï¼Œé€šè¿‡åˆæˆæ•°æ®ï¼ˆLIMRANK-SYNTHESIZERç”Ÿæˆï¼‰è¿›è¡Œå°è§„æ¨¡å¾®è°ƒï¼Œæ˜¾è‘—å‡å°‘è®­ç»ƒæ•°æ®éœ€æ±‚ã€‚ç ”ç©¶éªŒè¯äº†å…¶åœ¨æ¨ç†å¯†é›†å‹æ£€ç´¢ï¼ˆBRIGHTï¼‰å’ŒæŒ‡ä»¤è·Ÿéšæ£€ç´¢ï¼ˆFollowIRï¼‰ä¸­çš„ç«äº‰åŠ›ï¼Œå¹¶ç‰¹åˆ«æåˆ°è¯¥æ¨¡å‹åœ¨çŸ¥è¯†å¯†é›†å‹é—®é¢˜è§£å†³ï¼ˆå¦‚ç§‘å­¦æ–‡çŒ®æœç´¢å’Œæ£€ç´¢å¢å¼ºç”ŸæˆRAGï¼‰ä¸­çš„ä¸‹æ¸¸ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.23271v1">Mubeen AI: A Specialized Arabic Language Model for Heritage Preservation and User Intent Understanding</a></td><td><details><summary>å±•å¼€</summary>Mubeen is a proprietary Arabic language model developed by MASARAT SA,
optimized for deep understanding of Arabic linguistics, Islamic studies, and
cultural heritage. Trained on an extensive collection of authentic Arabic
sources significantly expanded by digitizing historical manuscripts via a
proprietary Arabic OCR engine, the model incorporates seminal scholarly works
in linguistics, jurisprudence, hadith, and Quranic exegesis, alongside
thousands of academic theses and peer-reviewed research papers. Conditioned
through a deep linguistic engineering framework, Mubeen masters not just the
meaning but the eloquence of Arabic, enabling precise understanding across
classical texts, contemporary writing, and regional dialects with focus on
comprehending user intent and delivering accurate, contextually relevant
responses. Unlike other Arabic models relying on translated English data that
often fail in intent detection or retrieval-augmented generation (RAG), Mubeen
uses native Arabic sources to ensure cultural authenticity and accuracy. Its
core innovation is the Practical Closure Architecture, designed to solve the
"Utility Gap Crisis" where factually correct answers fail to resolve users'
core needs, forcing them into frustrating cycles of re-prompting. By
prioritizing clarity and decisive guidance, Mubeen transforms from an
information repository into a decisive guide, aligning with Saudi Vision 2030.
The model's architecture combines deep heritage specialization with
multi-disciplinary expert modules, enabling robust performance across both
cultural preservation and general knowledge domains.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†Mubeenï¼Œä¸€ä¸ªä¸“æœ‰çš„é˜¿æ‹‰ä¼¯è¯­è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ç»“åˆåŸç”Ÿé˜¿æ‹‰ä¼¯è¯­èµ„æ–™å’Œæ·±åº¦è¯­è¨€å·¥ç¨‹æ¡†æ¶ï¼Œè§£å†³äº†ä¼ ç»Ÿé˜¿æ‹‰ä¼¯è¯­æ¨¡å‹åœ¨æ„å›¾æ£€æµ‹å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸Šçš„ä¸è¶³ã€‚å…¶æ ¸å¿ƒåˆ›æ–°â€œå®ç”¨é—­åˆæ¶æ„â€æ—¨åœ¨è§£å†³â€œæ•ˆç”¨å·®è·å±æœºâ€ï¼Œç¡®ä¿å›ç­”ä¸ä»…å‡†ç¡®ä¸”ç›´æ¥æ»¡è¶³ç”¨æˆ·éœ€æ±‚ï¼ŒåŒæ—¶å¼ºè°ƒäº†æ–‡åŒ–çœŸå®æ€§å’Œå¤šå­¦ç§‘ä¸“å®¶æ¨¡å—çš„æ•´åˆã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.23070v1">Quality-Aware Translation Tagging in Multilingual RAG system</a></td><td><details><summary>å±•å¼€</summary>Multilingual Retrieval-Augmented Generation (mRAG) often retrieves English
documents and translates them into the query language for low-resource
settings. However, poor translation quality degrades response generation
performance. Existing approaches either assume sufficient translation quality
or utilize the rewriting method, which introduces factual distortion and
hallucinations. To mitigate these problems, we propose Quality-Aware
Translation Tagging in mRAG (QTT-RAG), which explicitly evaluates translation
quality along three dimensions-semantic equivalence, grammatical accuracy, and
naturalness&fluency-and attach these scores as metadata without altering the
original content. We evaluate QTT-RAG against CrossRAG and DKM-RAG as baselines
in two open-domain QA benchmarks (XORQA, MKQA) using six instruction-tuned LLMs
ranging from 2.4B to 14B parameters, covering two low-resource languages
(Korean and Finnish) and one high-resource language (Chinese). QTT-RAG
outperforms the baselines by preserving factual integrity while enabling
generator models to make informed decisions based on translation reliability.
This approach allows for effective usage of cross-lingual documents in
low-resource settings with limited native language documents, offering a
practical and robust solution across multilingual domains.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºQTT-RAGçš„è´¨é‡æ„ŸçŸ¥ç¿»è¯‘æ ‡è®°æ–¹æ³•ï¼Œç”¨äºæ”¹è¿›å¤šè¯­è¨€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆmRAGï¼‰ä¸­çš„ç¿»è¯‘è´¨é‡é—®é¢˜ã€‚é€šè¿‡è¯„ä¼°ç¿»è¯‘çš„è¯­ä¹‰ç­‰ä»·æ€§ã€è¯­æ³•å‡†ç¡®æ€§å’Œæµç•…æ€§ï¼Œå¹¶å°†è¯„åˆ†ä½œä¸ºå…ƒæ•°æ®é™„åŠ åˆ°åŸæ–‡ï¼Œè¯¥æ–¹æ³•åœ¨ä½èµ„æºè¯­è¨€ï¼ˆå¦‚éŸ©è¯­ã€èŠ¬å…°è¯­ï¼‰å’Œé«˜èµ„æºè¯­è¨€ï¼ˆå¦‚ä¸­æ–‡ï¼‰çš„å¼€æ”¾åŸŸé—®ç­”ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒäº†äº‹å®å®Œæ•´æ€§å¹¶å‡å°‘äº†å¹»è§‰é—®é¢˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.22956v1">Tagging-Augmented Generation: Assisting Language Models in Finding Intricate Knowledge In Long Contexts</a></td><td><details><summary>å±•å¼€</summary>Recent investigations into effective context lengths of modern flagship large
language models (LLMs) have revealed major limitations in effective question
answering (QA) and reasoning over long and complex contexts for even the
largest and most impressive cadre of models. While approaches like
retrieval-augmented generation (RAG) and chunk-based re-ranking attempt to
mitigate this issue, they are sensitive to chunking, embedding and retrieval
strategies and models, and furthermore, rely on extensive pre-processing,
knowledge acquisition and indexing steps. In this paper, we propose
Tagging-Augmented Generation (TAG), a lightweight data augmentation strategy
that boosts LLM performance in long-context scenarios, without degrading and
altering the integrity and composition of retrieved documents. We validate our
hypothesis by augmenting two challenging and directly relevant
question-answering benchmarks -- NoLima and NovelQA -- and show that tagging
the context or even just adding tag definitions into QA prompts leads to
consistent performance gains over the baseline -- up to 17% for 32K token
contexts, and 2.9% in complex reasoning question-answering for multi-hop
queries requiring knowledge across a wide span of text. Additional details are
available at https://sites.google.com/view/tag-emnlp.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ç°ä»£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†é•¿ä¸”å¤æ‚ä¸Šä¸‹æ–‡æ—¶çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºâ€œæ ‡è®°å¢å¼ºç”Ÿæˆï¼ˆTAGï¼‰â€çš„è½»é‡çº§æ•°æ®å¢å¼ºç­–ç•¥ã€‚TAGé€šè¿‡åœ¨ä¸Šä¸‹æ–‡æˆ–æç¤ºä¸­æ·»åŠ æ ‡è®°æˆ–æ ‡è®°å®šä¹‰ï¼Œæ˜¾è‘—æå‡äº†LLMsåœ¨é•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸‹çš„æ€§èƒ½ï¼ˆå¦‚é—®ç­”å’Œå¤æ‚æ¨ç†ä»»åŠ¡ï¼‰ï¼Œè€Œæ— éœ€ä¾èµ–RAGç­‰ä¼ ç»Ÿæ–¹æ³•æ‰€éœ€çš„é¢„å¤„ç†å’Œç´¢å¼•æ­¥éª¤ã€‚å®éªŒè¡¨æ˜ï¼ŒTAGåœ¨32Kæ ‡è®°çš„ä¸Šä¸‹æ–‡å’Œå¤šè·³æŸ¥è¯¢ä»»åŠ¡ä¸­åˆ†åˆ«å®ç°äº†é«˜è¾¾17%å’Œ2.9%çš„æ€§èƒ½æå‡ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-26
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.22728v1">S-Chain: Structured Visual Chain-of-Thought For Medicine</a></td><td><details><summary>å±•å¼€</summary>Faithful reasoning in medical vision-language models (VLMs) requires not only
accurate predictions but also transparent alignment between textual rationales
and visual evidence. While Chain-of-Thought (CoT) prompting has shown promise
in medical visual question answering (VQA), no large-scale expert-level dataset
has captured stepwise reasoning with precise visual grounding. We introduce
S-Chain, the first large-scale dataset of 12,000 expert-annotated medical
images with bounding boxes and structured visual CoT (SV-CoT), explicitly
linking visual regions to reasoning steps. The dataset further supports 16
languages, totaling over 700k VQA pairs for broad multilingual applicability.
Using S-Chain, we benchmark state-of-the-art medical VLMs (ExGra-Med,
LLaVA-Med) and general-purpose VLMs (Qwen2.5-VL, InternVL2.5), showing that
SV-CoT supervision significantly improves interpretability, grounding fidelity,
and robustness. Beyond benchmarking, we study its synergy with
retrieval-augmented generation, revealing how domain knowledge and visual
grounding interact during autoregressive reasoning. Finally, we propose a new
mechanism that strengthens the alignment between visual evidence and reasoning,
improving both reliability and efficiency. S-Chain establishes a new benchmark
for grounded medical reasoning and paves the way toward more trustworthy and
explainable medical VLMs.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†S-Chainæ•°æ®é›†ï¼Œé€šè¿‡ç»“æ„åŒ–è§†è§‰é“¾å¼æ¨ç†ï¼ˆSV-CoTï¼‰å’Œè·¨è¯­è¨€VQAå¯¹å¢å¼ºåŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¯è§£é‡Šæ€§ä¸è§†è§‰è¯æ®å¯¹é½ï¼Œå¹¶æ¢è®¨äº†å…¶ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„ååŒä½œç”¨ï¼Œæ­ç¤ºäº†é¢†åŸŸçŸ¥è¯†ä¸è§†è§‰åŸºç¡€åœ¨æ¨ç†ä¸­çš„äº¤äº’æœºåˆ¶ï¼Œæœ€ç»ˆæå‡ºäº†ä¸€ç§æå‡è§†è§‰è¯æ®ä¸æ¨ç†å¯¹é½çš„æ–°æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.22710v1">RaCoT: Plug-and-Play Contrastive Example Generation Mechanism for Enhanced LLM Reasoning Reliability</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) faces a core bottleneck with
knowledge-sparse and semantically ambiguous long-tail queries, where retrieval
noise distorts reasoning and necessitates costly post-processing. To tackle
this, we propose RaCoT (Retrieval-aware Contrastive-of-Thought), a novel
framework that shifts contrastive thinking to the pre-retrieval stage. By
automatically generating a semantically adjacent yet differently answered
contrastive question and extracting a $\Delta$-Prompt to capture their key
differences, RaCoT guides the model to proactively focus on the ``critical
details that determine answer divergence." This approach allows it to suppress
semantic interference within a single retrieval pass, overcoming the
theoretical bottleneck of single-vector queries that struggle to simultaneously
encode signals for what to attend to and what to ignore. On six authoritative
benchmarks, including PopQA and TriviaQA-unfiltered, RaCoT outperforms strong
baselines like RankRAG and Self-RAG by 0.9-2.4 percentage points. It exhibits
superior robustness, with a performance drop of only 8.6\% in adversarial
tests, far surpassing the over 15\% degradation in other methods. Furthermore,
its low latency (3.12s) and token overhead (11.54) place it on the
accuracy-efficiency Pareto frontier, while ablation studies validate the
necessity of each component. Ultimately, RaCoT reframes the RAG paradigm from
``post-hoc context cleaning" to ``a priori shaping of discriminative
reasoning", offering an efficient and robust path toward reliable AI systems
for real-time, resource-constrained deployments.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºRaCoTæ¡†æ¶ï¼Œé€šè¿‡é¢„æ£€ç´¢é˜¶æ®µç”Ÿæˆå¯¹æ¯”æ€§é—®é¢˜å¹¶æå–å…³é”®å·®å¼‚æç¤ºï¼ˆÎ”-Promptï¼‰ï¼Œè§£å†³RAGä¸­é•¿å°¾æŸ¥è¯¢çš„è¯­ä¹‰æ¨¡ç³Šå’Œæ£€ç´¢å™ªå£°é—®é¢˜ï¼Œæ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ä¸é²æ£’æ€§ï¼Œå¹¶åœ¨æ•ˆç‡ä¸å‡†ç¡®æ€§ä¸Šè¾¾åˆ°å¸•ç´¯æ‰˜å‰æ²¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.22694v1">Windsock is Dancing: Adaptive Multimodal Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Multimodal Retrieval-Augmented Generation (MRAG) has emerged as a promising
method to generate factual and up-to-date responses of Multimodal Large
Language Models (MLLMs) by incorporating non-parametric knowledge from external
knowledge bases. However, existing MRAG approaches suffer from static retrieval
strategies, inflexible modality selection, and suboptimal utilization of
retrieved information, leading to three critical challenges: determining when
to retrieve, what modality to incorporate, and how to utilize retrieved
information effectively. To address these challenges, we introduce Windsock, a
query-dependent module making decisions on retrieval necessity and modality
selection, effectively reducing computational overhead and improving response
quality. Additionally, we propose Dynamic Noise-Resistance (DANCE) Instruction
Tuning, an adaptive training strategy that enhances MLLMs' ability to utilize
retrieved information while maintaining robustness against noise. Moreover, we
adopt a self-assessment approach leveraging knowledge within MLLMs to convert
question-answering datasets to MRAG training datasets. Extensive experiments
demonstrate that our proposed method significantly improves the generation
quality by 17.07% while reducing 8.95% retrieval times.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆMRAGï¼‰æ–¹æ³•Windsockï¼Œé€šè¿‡åŠ¨æ€å†³ç­–æ£€ç´¢å¿…è¦æ€§å’Œæ¨¡æ€é€‰æ‹©æ¥ä¼˜åŒ–ç”Ÿæˆè´¨é‡ï¼Œå¹¶ç»“åˆè‡ªé€‚åº”è®­ç»ƒç­–ç•¥DANCEæå‡æ¨¡å‹å¯¹æ£€ç´¢ä¿¡æ¯çš„åˆ©ç”¨èƒ½åŠ›ï¼Œå®éªŒè¯æ˜å…¶æ˜¾è‘—æå‡äº†ç”Ÿæˆè´¨é‡å¹¶å‡å°‘äº†æ£€ç´¢æ¬¡æ•°ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.22689v1">Rule-Based Explanations for Retrieval-Augmented LLM Systems</a></td><td><details><summary>å±•å¼€</summary>If-then rules are widely used to explain machine learning models; e.g., "if
employed = no, then loan application = rejected." We present the first proposal
to apply rules to explain the emerging class of large language models (LLMs)
with retrieval-augmented generation (RAG). Since RAG enables LLM systems to
incorporate retrieved information sources at inference time, rules linking the
presence or absence of sources can explain output provenance; e.g., "if a Times
Higher Education ranking article is retrieved, then the LLM ranks Oxford
first." To generate such rules, a brute force approach would probe the LLM with
all source combinations and check if the presence or absence of any sources
leads to the same output. We propose optimizations to speed up rule generation,
inspired by Apriori-like pruning from frequent itemset mining but redefined
within the scope of our novel problem. We conclude with qualitative and
quantitative experiments demonstrating our solutions' value and efficiency.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨if-thenè§„åˆ™è§£é‡Šæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¨¡å‹å†³ç­–çš„æ–¹æ³•ï¼Œé€šè¿‡åˆ†ææ£€ç´¢åˆ°çš„ä¿¡æ¯æºä¸è¾“å‡ºé—´çš„å› æœå…³ç³»ç”Ÿæˆè§„åˆ™ï¼ˆå¦‚â€œè‹¥æ£€ç´¢åˆ°æŸæ’åæ–‡ç« ï¼Œåˆ™æ¨¡å‹è¾“å‡ºç‰¹å®šç»“æœâ€ï¼‰ï¼Œå¹¶è®¾è®¡ä¼˜åŒ–ç®—æ³•åŠ é€Ÿè§„åˆ™æŒ–æ˜ï¼Œæœ€ç»ˆé€šè¿‡å®éªŒéªŒè¯äº†æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.22609v1">CLIN-LLM: A Safety-Constrained Hybrid Framework for Clinical Diagnosis and Treatment Generation</a></td><td><details><summary>å±•å¼€</summary>Accurate symptom-to-disease classification and clinically grounded treatment
recommendations remain challenging, particularly in heterogeneous patient
settings with high diagnostic risk. Existing large language model (LLM)-based
systems often lack medical grounding and fail to quantify uncertainty,
resulting in unsafe outputs. We propose CLIN-LLM, a safety-constrained hybrid
pipeline that integrates multimodal patient encoding, uncertainty-calibrated
disease classification, and retrieval-augmented treatment generation. The
framework fine-tunes BioBERT on 1,200 clinical cases from the Symptom2Disease
dataset and incorporates Focal Loss with Monte Carlo Dropout to enable
confidence-aware predictions from free-text symptoms and structured vitals.
Low-certainty cases (18%) are automatically flagged for expert review, ensuring
human oversight. For treatment generation, CLIN-LLM employs Biomedical
Sentence-BERT to retrieve top-k relevant dialogues from the 260,000-sample
MedDialog corpus. The retrieved evidence and patient context are fed into a
fine-tuned FLAN-T5 model for personalized treatment generation, followed by
post-processing with RxNorm for antibiotic stewardship and drug-drug
interaction (DDI) screening. CLIN-LLM achieves 98% accuracy and F1 score,
outperforming ClinicalBERT by 7.1% (p < 0.001), with 78% top-5 retrieval
precision and a clinician-rated validity of 4.2 out of 5. Unsafe antibiotic
suggestions are reduced by 67% compared to GPT-5. These results demonstrate
CLIN-LLM's robustness, interpretability, and clinical safety alignment. The
proposed system provides a deployable, human-in-the-loop decision support
framework for resource-limited healthcare environments. Future work includes
integrating imaging and lab data, multilingual extensions, and clinical trial
validation.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†CLIN-LLMï¼Œä¸€ä¸ªç»“åˆå¤šæ¨¡æ€æ‚£è€…ç¼–ç ã€ä¸ç¡®å®šæ€§æ ¡å‡†ç–¾ç—…åˆ†ç±»å’Œæ£€ç´¢å¢å¼ºæ²»ç–—ç”Ÿæˆçš„å®‰å…¨çº¦æŸæ··åˆæ¡†æ¶ã€‚å…¶ä¸­ï¼Œæ²»ç–—ç”Ÿæˆéƒ¨åˆ†é€šè¿‡Biomedical Sentence-BERTä»MedDialogè¯­æ–™åº“ä¸­æ£€ç´¢ç›¸å…³å¯¹è¯ï¼Œå¹¶å°†æ£€ç´¢ç»“æœä¸æ‚£è€…ä¸Šä¸‹æ–‡è¾“å…¥FLAN-T5æ¨¡å‹ç”Ÿæˆä¸ªæ€§åŒ–æ²»ç–—æ–¹æ¡ˆï¼Œä½“ç°äº†RAGæŠ€æœ¯çš„åº”ç”¨ã€‚è¯¥ç³»ç»Ÿåœ¨å‡†ç¡®æ€§ã€ä¸´åºŠå®‰å…¨æ€§å’Œæ£€ç´¢æ€§èƒ½æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—å‡å°‘äº†ä¸å®‰å…¨å»ºè®®ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.22521v1">Open Multimodal Retrieval-Augmented Factual Image Generation</a></td><td><details><summary>å±•å¼€</summary>Large Multimodal Models (LMMs) have achieved remarkable progress in
generating photorealistic and prompt-aligned images, but they often produce
outputs that contradict verifiable knowledge, especially when prompts involve
fine-grained attributes or time-sensitive events. Conventional
retrieval-augmented approaches attempt to address this issue by introducing
external information, yet they are fundamentally incapable of grounding
generation in accurate and evolving knowledge due to their reliance on static
sources and shallow evidence integration. To bridge this gap, we introduce
ORIG, an agentic open multimodal retrieval-augmented framework for Factual
Image Generation (FIG), a new task that requires both visual realism and
factual grounding. ORIG iteratively retrieves and filters multimodal evidence
from the web and incrementally integrates the refined knowledge into enriched
prompts to guide generation. To support systematic evaluation, we build
FIG-Eval, a benchmark spanning ten categories across perceptual, compositional,
and temporal dimensions. Experiments demonstrate that ORIG substantially
improves factual consistency and overall image quality over strong baselines,
highlighting the potential of open multimodal retrieval for factual image
generation.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºORIGçš„å¼€æ”¾å¼å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºæ¡†æ¶ï¼Œç”¨äºè§£å†³å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ç”Ÿæˆå›¾åƒæ—¶ä¸å¯éªŒè¯çŸ¥è¯†çŸ›ç›¾çš„é—®é¢˜ã€‚ORIGé€šè¿‡è¿­ä»£æ£€ç´¢å’Œè¿‡æ»¤ç½‘ç»œä¸­çš„å¤šæ¨¡æ€è¯æ®ï¼Œå¹¶å°†ç²¾ç‚¼çŸ¥è¯†é€æ­¥æ•´åˆåˆ°æç¤ºä¸­ä»¥æŒ‡å¯¼ç”Ÿæˆï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆå›¾åƒçš„äº‹å®ä¸€è‡´æ€§å’Œè´¨é‡ã€‚åŒæ—¶ï¼Œä½œè€…æ„å»ºäº†FIG-EvalåŸºå‡†è¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-25
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.22344v1">FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>While Retrieval-Augmented Generation (RAG) mitigates hallucination and
knowledge staleness in Large Language Models (LLMs), existing frameworks often
falter on complex, multi-hop queries that require synthesizing information from
disparate sources. Current advanced RAG methods, employing iterative or
adaptive strategies, lack a robust mechanism to systematically identify and
fill evidence gaps, often propagating noise or failing to gather a
comprehensive context. We introduce FAIR-RAG, a novel agentic framework that
transforms the standard RAG pipeline into a dynamic, evidence-driven reasoning
process. At its core is an Iterative Refinement Cycle governed by a module we
term Structured Evidence Assessment (SEA). The SEA acts as an analytical gating
mechanism: it deconstructs the initial query into a checklist of required
findings and audits the aggregated evidence to identify confirmed facts and,
critically, explicit informational gaps. These gaps provide a precise signal to
an Adaptive Query Refinement agent, which generates new, targeted sub-queries
to retrieve missing information. This cycle repeats until the evidence is
verified as sufficient, ensuring a comprehensive context for a final, strictly
faithful generation. We conducted experiments on challenging multi-hop QA
benchmarks, including HotpotQA, 2WikiMultiHopQA, and MusiQue. In a unified
experimental setup, FAIR-RAG significantly outperforms strong baselines. On
HotpotQA, it achieves an F1-score of 0.453 -- an absolute improvement of 8.3
points over the strongest iterative baseline -- establishing a new
state-of-the-art for this class of methods on these benchmarks. Our work
demonstrates that a structured, evidence-driven refinement process with
explicit gap analysis is crucial for unlocking reliable and accurate reasoning
in advanced RAG systems for complex, knowledge-intensive tasks.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºFAIR-RAGçš„æ–°å‹ä»£ç†æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥ç»“æ„åŒ–è¯æ®è¯„ä¼°ï¼ˆSEAï¼‰å’Œè‡ªé€‚åº”æŸ¥è¯¢ç»†åŒ–æœºåˆ¶ï¼Œæ”¹è¿›äº†ç°æœ‰RAGç³»ç»Ÿåœ¨å¤„ç†å¤æ‚å¤šè·³æŸ¥è¯¢æ—¶çš„ä¸è¶³ã€‚FAIR-RAGé€šè¿‡è¿­ä»£ä¼˜åŒ–è¯æ®æ”¶é›†è¿‡ç¨‹ï¼Œæ˜¾è‘—æå‡äº†åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨HotpotQAä¸Šå®ç°äº†8.3åˆ†çš„ç»å¯¹æå‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.22272v1">From Slides to Chatbots: Enhancing Large Language Models with University Course Materials</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) have advanced rapidly in recent years. One
application of LLMs is to support student learning in educational settings.
However, prior work has shown that LLMs still struggle to answer questions
accurately within university-level computer science courses. In this work, we
investigate how incorporating university course materials can enhance LLM
performance in this setting. A key challenge lies in leveraging diverse course
materials such as lecture slides and transcripts, which differ substantially
from typical textual corpora: slides also contain visual elements like images
and formulas, while transcripts contain spoken, less structured language. We
compare two strategies, Retrieval-Augmented Generation (RAG) and Continual
Pre-Training (CPT), to extend LLMs with course-specific knowledge. For lecture
slides, we further explore a multi-modal RAG approach, where we present the
retrieved content to the generator in image form. Our experiments reveal that,
given the relatively small size of university course materials, RAG is more
effective and efficient than CPT. Moreover, incorporating slides as images in
the multi-modal setting significantly improves performance over text-only
retrieval. These findings highlight practical strategies for developing AI
assistants that better support learning and teaching, and we hope they inspire
similar efforts in other educational contexts.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡æ•´åˆå¤§å­¦è¯¾ç¨‹ææ–™ï¼ˆå¦‚è®²ä¹‰å¹»ç¯ç‰‡å’Œè½¬å½•æ–‡æœ¬ï¼‰æ¥æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®¡ç®—æœºç§‘å­¦æ•™è‚²ä¸­çš„é—®ç­”æ€§èƒ½ï¼Œé‡ç‚¹æ¯”è¾ƒäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’ŒæŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰ä¸¤ç§ç­–ç•¥ï¼Œå¹¶æå‡ºå¤šæ¨¡æ€RAGæ–¹æ³•ï¼ˆä»¥å›¾åƒå½¢å¼å¤„ç†å¹»ç¯ç‰‡å†…å®¹ï¼‰ï¼Œå®éªŒè¡¨æ˜RAGåœ¨å°å‹è¯¾ç¨‹æ•°æ®é›†ä¸Šæ›´é«˜æ•ˆä¸”å¤šæ¨¡æ€æ£€ç´¢æ˜¾è‘—ä¼˜äºçº¯æ–‡æœ¬æ£€ç´¢ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.22210v1">LSPRAG: LSP-Guided RAG for Language-Agnostic Real-Time Unit Test Generation</a></td><td><details><summary>å±•å¼€</summary>Automated unit test generation is essential for robust software development,
yet existing approaches struggle to generalize across multiple programming
languages and operate within real-time development. While Large Language Models
(LLMs) offer a promising solution, their ability to generate high coverage test
code depends on prompting a concise context of the focal method. Current
solutions, such as Retrieval-Augmented Generation, either rely on imprecise
similarity-based searches or demand the creation of costly, language-specific
static analysis pipelines. To address this gap, we present LSPRAG, a framework
for concise-context retrieval tailored for real-time, language-agnostic unit
test generation. LSPRAG leverages off-the-shelf Language Server Protocol (LSP)
back-ends to supply LLMs with precise symbol definitions and references in real
time. By reusing mature LSP servers, LSPRAG provides an LLM with language-aware
context retrieval, requiring minimal per-language engineering effort. We
evaluated LSPRAG on open-source projects spanning Java, Go, and Python.
Compared to the best performance of baselines, LSPRAG increased line coverage
by up to 174.55% for Golang, 213.31% for Java, and 31.57% for Python.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†LSPRAGæ¡†æ¶ï¼Œé€šè¿‡é›†æˆè¯­è¨€æœåŠ¡å™¨åè®®ï¼ˆLSPï¼‰å®ç°å®æ—¶ã€è¯­è¨€æ— å…³çš„å•å…ƒæµ‹è¯•ç”Ÿæˆï¼Œåˆ©ç”¨ç²¾ç¡®çš„ç¬¦å·æ£€ç´¢å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡è¾“å…¥ï¼Œæ˜¾è‘—æå‡äº†å¤šè¯­è¨€æµ‹è¯•ä»£ç çš„è¦†ç›–ç‡ï¼Œè§£å†³äº†ä¼ ç»ŸRAGæ–¹æ³•åœ¨ç›¸ä¼¼æ€§æœç´¢æˆ–é™æ€åˆ†æä¸Šçš„å±€é™æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.22143v1">OlaMind: Towards Human-Like and Hallucination-Safe Customer Service for Retrieval-Augmented Dialogue</a></td><td><details><summary>å±•å¼€</summary>Intelligent customer service (ICS) systems via retrieval-augmented generation
(RAG) have been widely adopted in Web-based domains such as social platforms
and e-commerce, achieving remarkable improvements in automation and efficiency.
However, notable limitations still remain: these systems are prone to
hallucinations and often generate rigid, mechanical responses, which can
introduce business risks and undermine user experience, especially in Web-based
customer service interactions under the RAG scenarios. In this paper, we
introduce OlaMind, a human-like and hallucination-safe customer service
framework for retrieval-augmented dialogue. Specifically, it first leverages a
Learn-to-Think stage to learn the reasoning processes and response strategies
from human experts, and then employs a Learn-to-Respond stage to perform
cold-start supervised fine-tuning (SFT) combined with reinforcement learning
(RL) for basic-to-hard self-refinement. Our method significantly enhances
human-likeness and naturalness while effectively mitigating hallucinations and
critical business risks. We have conducted large-scale online A/B experiments
in an industry-level social customer service setting, and extensive
experimental results show that OlaMind achieves significant cumulative relative
improvements with intelligent resolution rates +28.92%/+18.42% and human
takeover rate -6.08%/-7.12% in community-support/livestream-interaction
scenarios, respectively, which highlights its consistent effectiveness across
diverse real-world applications. The code and data will be publicly available.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºOlaMindçš„æ™ºèƒ½å®¢æœæ¡†æ¶ï¼ŒåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰RAGç³»ç»Ÿåœ¨å®¢æœå¯¹è¯ä¸­æ˜“äº§ç”Ÿå¹»è§‰å’Œæœºæ¢°å›å¤çš„é—®é¢˜ã€‚OlaMindé€šè¿‡"Learn-to-Think"é˜¶æ®µå­¦ä¹ äººç±»ä¸“å®¶çš„æ¨ç†è¿‡ç¨‹ä¸åº”ç­”ç­–ç•¥ï¼Œå†é€šè¿‡"Learn-to-Respond"é˜¶æ®µç»“åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè‡ªæˆ‘ä¼˜åŒ–ï¼Œæ˜¾è‘—æå‡äº†å›ç­”çš„è‡ªç„¶åº¦å’Œå®‰å…¨æ€§ã€‚å·¥ä¸šçº§A/Bæµ‹è¯•è¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ç¤¾åŒºæ”¯æŒå’Œç›´æ’­äº’åŠ¨åœºæ™¯ä¸­æ˜¾è‘—æé«˜äº†æ™ºèƒ½è§£å†³ç‡å¹¶é™ä½äº†äººå·¥æ¥ç®¡ç‡ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-24
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.21933v1">A Comparison of Conversational Models and Humans in Answering Technical Questions: the Firefox Case</a></td><td><details><summary>å±•å¼€</summary>The use of Large Language Models (LLMs) to support tasks in software
development has steadily increased over recent years. From assisting developers
in coding activities to providing conversational agents that answer newcomers'
questions. In collaboration with the Mozilla Foundation, this study evaluates
the effectiveness of Retrieval-Augmented Generation (RAG) in assisting
developers within the Mozilla Firefox project. We conducted an empirical
analysis comparing responses from human developers, a standard GPT model, and a
GPT model enhanced with RAG, using real queries from Mozilla's developer chat
rooms. To ensure a rigorous evaluation, Mozilla experts assessed the responses
based on helpfulness, comprehensiveness, and conciseness. The results show that
RAG-assisted responses were more comprehensive than human developers (62.50% to
54.17%) and almost as helpful (75.00% to 79.17%), suggesting RAG's potential to
enhance developer assistance. However, the RAG responses were not as concise
and often verbose. The results show the potential to apply RAG-based tools to
Open Source Software (OSS) to minimize the load to core maintainers without
losing answer quality. Toning down retrieval mechanisms and making responses
even shorter in the future would enhance developer assistance in massive
projects like Mozilla Firefox.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡è¯„ä¼°äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨ååŠ©Mozilla Firefoxå¼€å‘è€…çš„æ•ˆæœï¼Œé€šè¿‡å¯¹æ¯”äººç±»å¼€å‘è€…ã€æ ‡å‡†GPTæ¨¡å‹å’ŒRAGå¢å¼ºGPTæ¨¡å‹çš„å›ç­”è´¨é‡ï¼ˆå¦‚å¸®åŠ©æ€§ã€å…¨é¢æ€§å’Œç®€æ´æ€§ï¼‰ï¼Œå‘ç°RAGåœ¨ç»¼åˆæ€§å’Œå¸®åŠ©æ€§ä¸Šæ¥è¿‘æˆ–ä¼˜äºäººç±»ï¼Œä½†å­˜åœ¨å†—é•¿é—®é¢˜ï¼Œæœªæ¥å¯é€šè¿‡ä¼˜åŒ–æ£€ç´¢æœºåˆ¶æå‡æ•ˆç‡ï¼Œå‡è½»å¼€æºé¡¹ç›®æ ¸å¿ƒç»´æŠ¤è€…è´Ÿæ‹…ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.21656v1">CMOMgen: Complex Multi-Ontology Alignment via Pattern-Guided In-Context Learning</a></td><td><details><summary>å±•å¼€</summary>Constructing comprehensive knowledge graphs requires the use of multiple
ontologies in order to fully contextualize data into a domain. Ontology
matching finds equivalences between concepts interconnecting ontologies and
creating a cohesive semantic layer. While the simple pairwise state of the art
is well established, simple equivalence mappings cannot provide full semantic
integration of related but disjoint ontologies. Complex multi-ontology matching
(CMOM) aligns one source entity to composite logical expressions of multiple
target entities, establishing more nuanced equivalences and provenance along
the ontological hierarchy.
  We present CMOMgen, the first end-to-end CMOM strategy that generates
complete and semantically sound mappings, without establishing any restrictions
on the number of target ontologies or entities. Retrieval-Augmented Generation
selects relevant classes to compose the mapping and filters matching reference
mappings to serve as examples, enhancing In-Context Learning. The strategy was
evaluated in three biomedical tasks with partial reference alignments. CMOMgen
outperforms baselines in class selection, demonstrating the impact of having a
dedicated strategy. Our strategy also achieves a minimum of 63% in F1-score,
outperforming all baselines and ablated versions in two out of three tasks and
placing second in the third. Furthermore, a manual evaluation of non-reference
mappings showed that 46% of the mappings achieve the maximum score, further
substantiating its ability to construct semantically sound mappings.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºCMOMgençš„ç«¯åˆ°ç«¯å¤æ‚å¤šæœ¬ä½“åŒ¹é…ç­–ç•¥ï¼Œåˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ä»å¤šä¸ªç›®æ ‡æœ¬ä½“ä¸­é€‰æ‹©ç›¸å…³ç±»å¹¶è¿‡æ»¤å‚è€ƒæ˜ å°„ä½œä¸ºç¤ºä¾‹ï¼Œä»¥æé«˜ä¸Šä¸‹æ–‡å­¦ä¹ æ•ˆæœï¼Œä»è€Œç”Ÿæˆå®Œæ•´ä¸”è¯­ä¹‰åˆç†çš„æ˜ å°„ã€‚è¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªç”Ÿç‰©åŒ»å­¦ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œè¯­ä¹‰å‡†ç¡®æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.21538v1">InterpDetect: Interpretable Signals for Detecting Hallucinations in Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) integrates external knowledge to
mitigate hallucinations, yet models often generate outputs inconsistent with
retrieved content. Accurate hallucination detection requires disentangling the
contributions of external context and parametric knowledge, which prior methods
typically conflate. We investigate the mechanisms underlying RAG hallucinations
and find they arise when later-layer FFN modules disproportionately inject
parametric knowledge into the residual stream. To address this, we explore a
mechanistic detection approach based on external context scores and parametric
knowledge scores. Using Qwen3-0.6b, we compute these scores across layers and
attention heads and train regression-based classifiers to predict
hallucinations. Our method is evaluated against state-of-the-art LLMs (GPT-5,
GPT-4.1) and detection baselines (RAGAS, TruLens, RefChecker). Furthermore,
classifiers trained on Qwen3-0.6b signals generalize to GPT-4.1-mini responses,
demonstrating the potential of proxy-model evaluation. Our results highlight
mechanistic signals as efficient, generalizable predictors for hallucination
detection in RAG systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†RAGç³»ç»Ÿä¸­çš„å¹»è§‰ç”Ÿæˆæœºåˆ¶ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤–éƒ¨è¯­å¢ƒè¯„åˆ†å’Œå‚æ•°çŸ¥è¯†è¯„åˆ†çš„æœºåˆ¶æ£€æµ‹æ–¹æ³•ï¼Œé€šè¿‡åˆ†ææ¨¡å‹å±‚é—´ä¿¡å·è®­ç»ƒåˆ†ç±»å™¨æ¥é¢„æµ‹å¹»è§‰ï¼Œå¹¶åœ¨å¤šç§æ¨¡å‹å’ŒåŸºçº¿æ–¹æ³•ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.21459v1">SBASH: a Framework for Designing and Evaluating RAG vs. Prompt-Tuned LLM Honeypots</a></td><td><details><summary>å±•å¼€</summary>Honeypots are decoy systems used for gathering valuable threat intelligence
or diverting attackers away from production systems. Maximising attacker
engagement is essential to their utility. However research has highlighted that
context-awareness, such as the ability to respond to new attack types, systems
and attacker agents, is necessary to increase engagement. Large Language Models
(LLMs) have been shown as one approach to increase context awareness but suffer
from several challenges including accuracy and timeliness of response time,
high operational costs and data-protection issues due to cloud deployment. We
propose the System-Based Attention Shell Honeypot (SBASH) framework which
manages data-protection issues through the use of lightweight local LLMs. We
investigate the use of Retrieval Augmented Generation (RAG) supported LLMs and
non-RAG LLMs for Linux shell commands and evaluate them using several different
metrics such as response time differences, realism from human testers, and
similarity to a real system calculated with Levenshtein distance, SBert, and
BertScore. We show that RAG improves accuracy for untuned models while models
that have been tuned via a system prompt that tells the LLM to respond like a
Linux system achieve without RAG a similar accuracy as untuned with RAG, while
having a slightly lower latency.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºSBASHçš„æ¡†æ¶ï¼Œé€šè¿‡ä½¿ç”¨è½»é‡çº§æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥è§£å†³æ•°æ®ä¿æŠ¤é—®é¢˜ï¼Œå¹¶ç ”ç©¶äº†åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’ŒéRAGçš„LLMsåœ¨Linux shellå‘½ä»¤ä¸­çš„åº”ç”¨æ•ˆæœï¼Œè¯„ä¼°äº†å“åº”æ—¶é—´ã€çœŸå®æ€§å’Œç³»ç»Ÿç›¸ä¼¼æ€§ç­‰æŒ‡æ ‡ï¼Œå‘ç°RAGèƒ½æé«˜æœªè°ƒä¼˜æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œè€Œé€šè¿‡ç³»ç»Ÿæç¤ºè°ƒä¼˜çš„æ¨¡å‹å³ä½¿ä¸ä½¿ç”¨RAGä¹Ÿèƒ½è¾¾åˆ°ç±»ä¼¼æ•ˆæœä½†å»¶è¿Ÿç•¥ä½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.21440v1">Redefining Retrieval Evaluation in the Era of LLMs</a></td><td><details><summary>å±•å¼€</summary>Traditional Information Retrieval (IR) metrics, such as nDCG, MAP, and MRR,
assume that human users sequentially examine documents with diminishing
attention to lower ranks. This assumption breaks down in Retrieval Augmented
Generation (RAG) systems, where search results are consumed by Large Language
Models (LLMs), which, unlike humans, process all retrieved documents as a whole
rather than sequentially. Additionally, traditional IR metrics do not account
for related but irrelevant documents that actively degrade generation quality,
rather than merely being ignored. Due to these two major misalignments, namely
human vs. machine position discount and human relevance vs. machine utility,
classical IR metrics do not accurately predict RAG performance. We introduce a
utility-based annotation schema that quantifies both the positive contribution
of relevant passages and the negative impact of distracting ones. Building on
this foundation, we propose UDCG (Utility and Distraction-aware Cumulative
Gain), a metric using an LLM-oriented positional discount to directly optimize
the correlation with the end-to-end answer accuracy. Experiments on five
datasets and six LLMs demonstrate that UDCG improves correlation by up to 36%
compared to traditional metrics. Our work provides a critical step toward
aligning IR evaluation with LLM consumers and enables more reliable assessment
of RAG components</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä¼ ç»Ÿä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰æŒ‡æ ‡åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­çš„å±€é™æ€§ï¼ŒæŒ‡å‡ºè¿™äº›æŒ‡æ ‡å› å‡è®¾äººç±»ç”¨æˆ·æŒ‰é¡ºåºæµè§ˆæ–‡æ¡£è€Œæ— æ³•å‡†ç¡®è¯„ä¼°LLMå¤„ç†æ£€ç´¢ç»“æœçš„è¡¨ç°ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŸºäºæ•ˆç”¨çš„æ ‡æ³¨æ¡†æ¶ï¼ˆUDCGï¼‰ï¼Œé‡åŒ–ç›¸å…³æ–‡æ¡£çš„ç§¯æä½œç”¨å’Œå¹²æ‰°æ–‡æ¡£çš„è´Ÿé¢å½±å“ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜UDCGä¸ä¼ ç»ŸæŒ‡æ ‡ç›¸æ¯”èƒ½æ˜¾è‘—æå‡ä¸ç«¯åˆ°ç«¯ç­”æ¡ˆå‡†ç¡®æ€§çš„ç›¸å…³æ€§ï¼ˆæœ€é«˜è¾¾36%ï¼‰ï¼Œä¸ºRAGç³»ç»Ÿçš„è¯„ä¼°æä¾›äº†æ›´å¯é çš„æŒ‡æ ‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.21144v1">NeuroGenPoisoning: Neuron-Guided Attacks on Retrieval-Augmented Generation of LLM via Genetic Optimization of External Knowledge</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) empowers Large Language Models (LLMs) to
dynamically integrate external knowledge during inference, improving their
factual accuracy and adaptability. However, adversaries can inject poisoned
external knowledge to override the model's internal memory. While existing
attacks iteratively manipulate retrieval content or prompt structure of RAG,
they largely ignore the model's internal representation dynamics and
neuron-level sensitivities. The underlying mechanism of RAG poisoning has not
been fully studied and the effect of knowledge conflict with strong parametric
knowledge in RAG is not considered. In this work, we propose NeuroGenPoisoning,
a novel attack framework that generates adversarial external knowledge in RAG
guided by LLM internal neuron attribution and genetic optimization. Our method
first identifies a set of Poison-Responsive Neurons whose activation strongly
correlates with contextual poisoning knowledge. We then employ a genetic
algorithm to evolve adversarial passages that maximally activate these neurons.
Crucially, our framework enables massive-scale generation of effective poisoned
RAG knowledge by identifying and reusing promising but initially unsuccessful
external knowledge variants via observed attribution signals. At the same time,
Poison-Responsive Neurons guided poisoning can effectively resolves knowledge
conflict. Experimental results across models and datasets demonstrate
consistently achieving high Population Overwrite Success Rate (POSR) of over
90% while preserving fluency. Empirical evidence shows that our method
effectively resolves knowledge conflict.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æŠ€æœ¯ä¸­å¤–éƒ¨çŸ¥è¯†è¢«æ¶æ„æ³¨å…¥ï¼ˆæŠ•æ¯’æ”»å‡»ï¼‰çš„å®‰å…¨é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºNeuroGenPoisoningçš„æ–°å‹æ”»å‡»æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†æå¤§è¯­è¨€æ¨¡å‹å†…éƒ¨ç¥ç»å…ƒæ¿€æ´»ä¸æŠ•æ¯’çŸ¥è¯†çš„ç›¸å…³æ€§ï¼Œç»“åˆé—ä¼ ç®—æ³•ç”Ÿæˆå¯¹æŠ—æ€§å¤–éƒ¨çŸ¥è¯†ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒæ–‡æœ¬æµç•…æ€§çš„åŒæ—¶é«˜æ•ˆè¦†ç›–æ¨¡å‹å†…éƒ¨å‚æ•°åŒ–çŸ¥è¯†ï¼ˆæˆåŠŸç‡è¶…90%ï¼‰ï¼Œå¹¶è§£å†³äº†çŸ¥è¯†å†²çªé—®é¢˜ã€‚å®éªŒéªŒè¯äº†å…¶åœ¨å¤šæ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.21093v1">MedAlign: A Synergistic Framework of Multimodal Preference Optimization and Federated Meta-Cognitive Reasoning</a></td><td><details><summary>å±•å¼€</summary>Recently, large models have shown significant potential for smart healthcare.
However, the deployment of Large Vision-Language Models (LVLMs) for clinical
services is currently hindered by three critical challenges: a tendency to
hallucinate answers not grounded in visual evidence, the inefficiency of
fixed-depth reasoning, and the difficulty of multi-institutional collaboration.
To address these challenges, in this paper, we develop MedAlign, a novel
framework to ensure visually accurate LVLM responses for Medical Visual
Question Answering (Med-VQA). Specifically, we first propose a multimodal
Direct Preference Optimization (mDPO) objective to explicitly align preference
learning with visual context. We then design a Retrieval-Aware
Mixture-of-Experts (RA-MoE) architecture that utilizes image and text
similarity to route queries to a specialized and context-augmented LVLM (i.e.,
an expert), thereby mitigating hallucinations in LVLMs. To achieve adaptive
reasoning and facilitate multi-institutional collaboration, we propose a
federated governance mechanism, where the selected expert, fine-tuned on
clinical datasets based on mDPO, locally performs iterative Chain-of-Thought
(CoT) reasoning via the local meta-cognitive uncertainty estimator. Extensive
experiments on three representative Med-VQA datasets demonstrate that MedAlign
achieves state-of-the-art performance, outperforming strong retrieval-augmented
baselines by up to $11.85\%$ in F1-score, and simultaneously reducing the
average reasoning length by $51.60\%$ compared with fixed-depth CoT approaches.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºMedAlignæ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€åå¥½ä¼˜åŒ–ï¼ˆmDPOï¼‰å’Œæ£€ç´¢æ„ŸçŸ¥çš„ä¸“å®¶æ··åˆæ¶æ„ï¼ˆRA-MoEï¼‰å¢å¼ºåŒ»å­¦è§†è§‰é—®ç­”ï¼ˆMed-VQAï¼‰çš„å‡†ç¡®æ€§ï¼Œåˆ©ç”¨æ£€ç´¢æœºåˆ¶é€‰æ‹©ä¸“å®¶æ¨¡å‹ä»¥å‡å°‘å¹»è§‰ï¼Œå¹¶ç»“åˆè”é‚¦æ²»ç†å®ç°è‡ªé€‚åº”æ¨ç†ï¼Œæ˜¾è‘—æå‡æ€§èƒ½å¹¶é™ä½æ¨ç†è€—æ—¶ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.21068v1">Bridging Language Gaps with Adaptive RAG: Improving Indonesian Language Question Answering</a></td><td><details><summary>å±•å¼€</summary>Question Answering (QA) has seen significant improvements with the
advancement of machine learning models, further studies enhanced this question
answering system by retrieving external information, called Retrieval-Augmented
Generation (RAG) to produce more accurate and informative answers. However,
these state-of-the-art-performance is predominantly in English language. To
address this gap we made an effort of bridging language gaps by incorporating
Adaptive RAG system to Indonesian language. Adaptive RAG system integrates a
classifier whose task is to distinguish the question complexity, which in turn
determines the strategy for answering the question. To overcome the limited
availability of Indonesian language dataset, our study employs machine
translation as data augmentation approach. Experiments show reliable question
complexity classifier; however, we observed significant inconsistencies in
multi-retrieval answering strategy which negatively impacted the overall
evaluation when this strategy was applied. These findings highlight both the
promise and challenges of question answering in low-resource language
suggesting directions for future improvement.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æ¢è®¨äº†å°†è‡ªé€‚åº”RAGç³»ç»Ÿåº”ç”¨äºå°å°¼è¯­ä»¥å¼¥è¡¥ä½èµ„æºè¯­è¨€é—®ç­”ç³»ç»Ÿçš„ä¸è¶³ï¼Œé€šè¿‡é—®é¢˜å¤æ‚åº¦åˆ†ç±»å™¨å’Œæœºå™¨ç¿»è¯‘å¢å¼ºæ•°æ®ï¼Œä½†å‘ç°å¤šæ£€ç´¢ç­–ç•¥å­˜åœ¨ä¸ä¸€è‡´æ€§å½±å“æ•´ä½“æ€§èƒ½ï¼Œæ­ç¤ºäº†ä½èµ„æºè¯­è¨€é—®ç­”çš„æ½œåŠ›ä¸æŒ‘æˆ˜ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-23
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.20797v1">Simple Context Compression: Mean-Pooling and Multi-Ratio Training</a></td><td><details><summary>å±•å¼€</summary>A common strategy to reduce the computational costs of using long contexts in
retrieval-augmented generation (RAG) with large language models (LLMs) is soft
context compression, where the input sequence is transformed into a shorter
continuous representation. We develop a lightweight and simple mean-pooling
approach that consistently outperforms the widely used compression-tokens
architecture, and study training the same compressor to output multiple
compression ratios. We conduct extensive experiments across in-domain and
out-of-domain QA datasets, as well as across model families, scales, and
compression ratios. Overall, our simple mean-pooling approach achieves the
strongest performance, with a relatively small drop when training for multiple
compression ratios. More broadly though, across architectures and training
regimes the trade-offs are more nuanced, illustrating the complex landscape of
compression methods.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„å‡å€¼æ± åŒ–æ–¹æ³•ï¼Œç”¨äºåœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­å‹ç¼©é•¿ä¸Šä¸‹æ–‡è¾“å…¥ï¼Œä»¥é™ä½è®¡ç®—æˆæœ¬ã€‚é€šè¿‡å®éªŒéªŒè¯ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§å‹ç¼©æ¯”å’Œæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶æ¢è®¨äº†å¤šå‹ç¼©æ¯”è®­ç»ƒçš„æƒè¡¡é—®é¢˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.20768v1">RAGRank: Using PageRank to Counter Poisoning in CTI LLM Pipelines</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has emerged as the dominant
architectural pattern to operationalize Large Language Model (LLM) usage in
Cyber Threat Intelligence (CTI) systems. However, this design is susceptible to
poisoning attacks, and previously proposed defenses can fail for CTI contexts
as cyber threat information is often completely new for emerging attacks, and
sophisticated threat actors can mimic legitimate formats, terminology, and
stylistic conventions. To address this issue, we propose that the robustness of
modern RAG defenses can be accelerated by applying source credibility
algorithms on corpora, using PageRank as an example. In our experiments, we
demonstrate quantitatively that our algorithm applies a lower authority score
to malicious documents while promoting trusted content, using the standardized
MS MARCO dataset. We also demonstrate proof-of-concept performance of our
algorithm on CTI documents and feeds.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åœ¨ç½‘ç»œå®‰å…¨å¨èƒæƒ…æŠ¥ï¼ˆCTIï¼‰ç³»ç»Ÿä¸­åº”ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯æ—¶é¢ä¸´çš„æŠ•æ¯’æ”»å‡»é£é™©ï¼Œå¹¶æå‡ºé€šè¿‡åŸºäºPageRankçš„æºå¯ä¿¡åº¦ç®—æ³•æ¥å¢å¼ºRAGé˜²å¾¡çš„é²æ£’æ€§ï¼Œå®éªŒéªŒè¯äº†è¯¥ç®—æ³•åœ¨åŒºåˆ†æ¶æ„æ–‡æ¡£å’Œå¯ä¿¡å†…å®¹ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.20609v1">Practical Code RAG at Scale: Task-Aware Retrieval Design Choices under Compute Budgets</a></td><td><details><summary>å±•å¼€</summary>We study retrieval design for code-focused generation tasks under realistic
compute budgets. Using two complementary tasks from Long Code Arena -- code
completion and bug localization -- we systematically compare retrieval
configurations across various context window sizes along three axes: (i)
chunking strategy, (ii) similarity scoring, and (iii) splitting granularity.
(1) For PL-PL, sparse BM25 with word-level splitting is the most effective and
practical, significantly outperforming dense alternatives while being an order
of magnitude faster. (2) For NL-PL, proprietary dense encoders (Voyager-3
family) consistently beat sparse retrievers, however requiring 100x larger
latency. (3) Optimal chunk size scales with available context: 32-64 line
chunks work best at small budgets, and whole-file retrieval becomes competitive
at 16000 tokens. (4) Simple line-based chunking matches syntax-aware splitting
across budgets. (5) Retrieval latency varies by up to 200x across
configurations; BPE-based splitting is needlessly slow, and BM25 + word
splitting offers the best quality-latency trade-off. Thus, we provide
evidence-based recommendations for implementing effective code-oriented RAG
systems based on task requirements, model constraints, and computational
efficiency.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶ä»£ç ç›¸å…³ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚ä»£ç è¡¥å…¨å’Œç¼ºé™·å®šä½ï¼‰ä¸­çš„æ£€ç´¢è®¾è®¡ï¼Œé€šè¿‡æ¯”è¾ƒä¸åŒæ£€ç´¢é…ç½®ï¼ˆåˆ†å—ç­–ç•¥ã€ç›¸ä¼¼æ€§è¯„åˆ†å’Œåˆ†å‰²ç²’åº¦ï¼‰åœ¨æœ‰é™è®¡ç®—èµ„æºä¸‹çš„è¡¨ç°ï¼Œä¸ºä»£ç å¯¼å‘çš„RAGç³»ç»Ÿæä¾›äº†åŸºäºå®è¯çš„ä¼˜åŒ–å»ºè®®ï¼ŒåŒ…æ‹¬ç¨€ç–æ£€ç´¢ï¼ˆBM25ï¼‰ä¸å¯†é›†æ£€ç´¢çš„é€‚ç”¨åœºæ™¯ã€æœ€ä½³åˆ†å—å¤§å°åŠæ•ˆç‡æƒè¡¡æ–¹æ¡ˆã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.20548v1">GlobalRAG: Enhancing Global Reasoning in Multi-hop Question Answering via Reinforcement Learning</a></td><td><details><summary>å±•å¼€</summary>Reinforcement learning has recently shown promise in improving
retrieval-augmented generation (RAG). Despite these advances, its effectiveness
in multi-hop question answering (QA) remains limited by two fundamental
limitations: (i) global planning absence to structure multi-step reasoning, and
(ii) unfaithful execution, which hinders effective query formulation and
consistent use of retrieved evidence. We propose GlobalRAG, a reinforcement
learning framework designed to enhance global reasoning in multi-hop QA.
GlobalRAG decomposes questions into subgoals, coordinates retrieval with
reasoning, and refines evidence iteratively. To guide this process, we
introduce Planning Quality Reward and SubGoal Completion Reward, which
encourage coherent planning and reliable subgoal execution. In addition, a
progressive weight annealing strategy balances process-oriented and
outcome-based objectives. Extensive experiments on both in-domain and
out-of-domain benchmarks demonstrate that GlobalRAG significantly outperforms
strong baselines while using only 8k training data (42% of the training data
used by strong baselines), achieving average improvements of 14.2% in both EM
and F1.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºGlobalRAGçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šè·³é—®ç­”ï¼ˆQAï¼‰ä¸­RAGæŠ€æœ¯çš„ä¸¤å¤§å±€é™â€”â€”ç¼ºä¹å…¨å±€è§„åˆ’å’Œæ‰§è¡Œä¸å¿ å®çš„é—®é¢˜ã€‚é€šè¿‡åˆ†è§£é—®é¢˜ä¸ºå­ç›®æ ‡ã€åè°ƒæ£€ç´¢ä¸æ¨ç†ã€è¿­ä»£ä¼˜åŒ–è¯æ®ï¼Œå¹¶ç»“åˆè§„åˆ’è´¨é‡å¥–åŠ±å’Œå­ç›®æ ‡å®Œæˆå¥–åŠ±ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå®éªŒæ˜¾ç¤ºå…¶åœ¨å°‘é‡è®­ç»ƒæ•°æ®ä¸‹ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.20535v1">ARC-Encoder: learning compressed text representations for large language models</a></td><td><details><summary>å±•å¼€</summary>Recent techniques such as retrieval-augmented generation or chain-of-thought
reasoning have led to longer contexts and increased inference costs. Context
compression techniques can reduce these costs, but the most effective
approaches require fine-tuning the target model or even modifying its
architecture. This can degrade its general abilities when not used for this
specific purpose. Here we explore an alternative approach: an encoder that
compresses the context into continuous representations which replace token
embeddings in decoder LLMs. First, we perform a systematic study of training
strategies and architecture choices for the encoder. Our findings led to the
design of an Adaptable text Representations Compressor, named ARC-Encoder,
which outputs $x$-times fewer continuous representations (typically
$x\!\in\!\{4,8\}$) than text tokens. We evaluate ARC-Encoder across a variety
of LLM usage scenarios, ranging from in-context learning to context window
extension, on both instruct and base decoders. Results show that ARC-Encoder
achieves state-of-the-art performance on several benchmarks while improving
computational efficiency at inference. Finally, we demonstrate that our models
can be adapted to multiple decoders simultaneously, allowing a single encoder
to generalize across different decoder LLMs. This makes ARC-Encoder a flexible
and efficient solution for portable encoders that work seamlessly with multiple
LLMs. We release a training code at https://github.com/kyutai-labs/ARC-Encoder
, fine-tuning dataset and pretrained models are available at
https://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047 .</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºARC-Encoderçš„ä¸Šä¸‹æ–‡å‹ç¼©æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³RAGå’Œæ€ç»´é“¾æ¨ç†ç­‰æŠ€æœ¯å¸¦æ¥çš„é•¿ä¸Šä¸‹æ–‡å’Œé«˜æ¨ç†æˆæœ¬é—®é¢˜ã€‚é€šè¿‡å°†ä¸Šä¸‹æ–‡å‹ç¼©ä¸ºæ›´å°‘çš„è¿ç»­è¡¨ç¤ºæ¥æ›¿æ¢è§£ç å™¨LLMä¸­çš„ä»¤ç‰ŒåµŒå…¥ï¼ŒARC-Encoderåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æé«˜äº†è®¡ç®—æ•ˆç‡ï¼Œå¹¶èƒ½é€‚é…å¤šç§è§£ç å™¨LLMã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.20505v1">Hierarchical Sequence Iteration for Heterogeneous Question Answering</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) remains brittle on multi-step questions
and heterogeneous evidence sources, trading accuracy against latency and
token/tool budgets. This paper introducesHierarchical Sequence (HSEQ) Iteration
for Heterogeneous Question Answering, a unified framework that (i) linearize
documents, tables, and knowledge graphs into a reversible hierarchical sequence
with lightweight structural tags, and (ii) perform structure-aware iteration to
collect just-enough evidence before answer synthesis. A Head Agent provides
guidance that leads retrieval, while an Iteration Agent selects and expands
HSeq via structure-respecting actions (e.g., parent/child hops, table
row/column neighbors, KG relations); Finally the head agent composes
canonicalized evidence to genearte the final answer, with an optional
refinement loop to resolve detected contradictions. Experiments on HotpotQA
(text), HybridQA/TAT-QA (table+text), and MetaQA (KG) show consistent EM/F1
gains over strong single-pass, multi-hop, and agentic RAG baselines with high
efficiency. Besides, HSEQ exhibits three key advantages: (1) a format-agnostic
unification that enables a single policy to operate across text, tables, and
KGs without per-dataset specialization; (2) guided, budget-aware iteration that
reduces unnecessary hops, tool calls, and tokens while preserving accuracy; and
(3) evidence canonicalization for reliable QA, improving answers consistency
and auditability.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºHSEQ Iterationçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ”¹è¿›æ¡†æ¶ï¼Œé€šè¿‡å°†æ–‡æ¡£ã€è¡¨æ ¼å’ŒçŸ¥è¯†å›¾è°±çº¿æ€§åŒ–ä¸ºå¯é€†çš„å±‚æ¬¡åºåˆ—ï¼Œå¹¶ç»“åˆç»“æ„æ„ŸçŸ¥çš„è¿­ä»£æ£€ç´¢ç­–ç•¥ï¼Œä»¥é«˜æ•ˆæ”¶é›†è¶³å¤Ÿè¯æ®å¹¶ç”Ÿæˆæ›´å‡†ç¡®çš„ç­”æ¡ˆã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§é—®ç­”ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰RAGåŸºçº¿ï¼Œå…·æœ‰è·¨æ ¼å¼ç»Ÿä¸€æ€§ã€é¢„ç®—æ„ŸçŸ¥è¿­ä»£å’Œè¯æ®è§„èŒƒåŒ–ä¸‰å¤§ä¼˜åŠ¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.20356v1">FreeChunker: A Cross-Granularity Chunking Framework</a></td><td><details><summary>å±•å¼€</summary>Chunking strategies significantly impact the effectiveness of
Retrieval-Augmented Generation (RAG) systems. Existing methods operate within
fixed-granularity paradigms that rely on static boundary identification,
limiting their adaptability to diverse query requirements. This paper presents
FreeChunker, a Cross-Granularity Encoding Framework that fundamentally
transforms the traditional chunking paradigm: the framework treats sentences as
atomic units and shifts from static chunk segmentation to flexible retrieval
supporting arbitrary sentence combinations. This paradigm shift not only
significantly reduces the computational overhead required for semantic boundary
detection but also enhances adaptability to complex queries. Experimental
evaluation on LongBench V2 demonstrates that FreeChunker achieves superior
retrieval performance compared to traditional chunking methods, while
significantly outperforming existing approaches in computational efficiency.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºFreeChunkerçš„è·¨ç²’åº¦ç¼–ç æ¡†æ¶ï¼Œé€šè¿‡å°†å¥å­ä½œä¸ºåŸå­å•å…ƒå¹¶æ”¯æŒçµæ´»çš„å¥å­ç»„åˆæ£€ç´¢ï¼Œæ”¹è¿›äº†ä¼ ç»ŸRAGç³»ç»Ÿä¸­å›ºå®šç²’åº¦çš„åˆ†å—ç­–ç•¥ï¼Œæ˜¾è‘—é™ä½äº†è¯­ä¹‰è¾¹ç•Œæ£€æµ‹çš„è®¡ç®—å¼€é”€ï¼Œæå‡äº†å¯¹å¤æ‚æŸ¥è¯¢çš„é€‚åº”æ€§ï¼Œå¹¶åœ¨LongBench V2å®éªŒä¸­è¡¨ç°å‡ºä¼˜äºä¼ ç»Ÿæ–¹æ³•çš„æ£€ç´¢æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.20303v1">Citation Failure: Definition, Analysis and Efficient Mitigation</a></td><td><details><summary>å±•å¼€</summary>Citations from LLM-based RAG systems are supposed to simplify response
verification. However, this does not hold for citation failure, when a model
generates a helpful response, but fails to cite complete evidence. In contrast
to previous work, we propose to disentangle this from response failure, where
the response itself is flawed, and citing complete evidence is impossible. To
address citation failure, this work follows a two-step approach: (1) We study
when citation failure occurs and (2) how it can be mitigated. For step 1, we
extend prior work by investigating how the relation between response and
evidence affects citation quality. We introduce CITECONTROL, a benchmark that
systematically varies this relation to analyze failure modes. Experiments show
that failures increase with relational complexity and suggest that combining
citation methods could improve performance, motivating step 2. To improve LLM
citation efficiently, we propose CITENTION, a framework integrating generative,
attention-based, and retrieval-based methods. Results demonstrate substantial
citation improvements on CITECONTROL and in transfer settings. We make our data
and code publicly available.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åŸºäºLLMçš„RAGç³»ç»Ÿä¸­å­˜åœ¨çš„â€œå¼•ç”¨å¤±è´¥â€é—®é¢˜ï¼ˆå³æ¨¡å‹ç”Ÿæˆæœ‰ç”¨å›ç­”ä½†æœªå®Œæ•´å¼•ç”¨è¯æ®ï¼‰ï¼Œæå‡ºå°†å…¶ä¸â€œå›ç­”å¤±è´¥â€åŒºåˆ†ï¼Œå¹¶é€šè¿‡CITECONTROLåŸºå‡†åˆ†æå¤±è´¥æ¨¡å¼ä¸è¯æ®-å›ç­”å…³ç³»ï¼Œæœ€ç»ˆæå‡ºæ•´åˆç”Ÿæˆã€æ³¨æ„åŠ›ä¸æ£€ç´¢æ–¹æ³•çš„CITENTIONæ¡†æ¶ä»¥æå‡å¼•ç”¨æ•ˆæœã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.20296v1">RAG-Stack: Co-Optimizing RAG Quality and Performance From the Vector Database Perspective</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) has emerged as one of the most prominent
applications of vector databases. By integrating documents retrieved from a
database into the prompt of a large language model (LLM), RAG enables more
reliable and informative content generation. While there has been extensive
research on vector databases, many open research problems remain once they are
considered in the wider context of end-to-end RAG pipelines. One practical yet
challenging problem is how to jointly optimize both system performance and
generation quality in RAG, which is significantly more complex than it appears
due to the numerous knobs on both the algorithmic side (spanning models and
databases) and the systems side (from software to hardware). In this paper, we
present RAG-Stack, a three-pillar blueprint for quality-performance
co-optimization in RAG systems. RAG-Stack comprises: (1) RAG-IR, an
intermediate representation that serves as an abstraction layer to decouple
quality and performance aspects; (2) RAG-CM, a cost model for estimating system
performance given an RAG-IR; and (3) RAG-PE, a plan exploration algorithm that
searches for high-quality, high-performance RAG configurations. We believe this
three-pillar blueprint will become the de facto paradigm for RAG
quality-performance co-optimization in the years to come.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œæå‡ºäº†ä¸€ç§åä¸ºRAG-Stackçš„ä¸‰æ”¯æŸ±è“å›¾ï¼Œæ—¨åœ¨å…±åŒä¼˜åŒ–RAGç³»ç»Ÿçš„æ€§èƒ½ä¸ç”Ÿæˆè´¨é‡ï¼ŒåŒ…æ‹¬RAG-IRä¸­é—´è¡¨ç¤ºã€RAG-CMæˆæœ¬æ¨¡å‹å’ŒRAG-PEè®¡åˆ’æ¢ç´¢ç®—æ³•ï¼Œä»¥è§£å†³ç°æœ‰ç ”ç©¶ä¸­çš„æŒ‘æˆ˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.20279v1">ResearchGPT: Benchmarking and Training LLMs for End-to-End Computer Science Research Workflows</a></td><td><details><summary>å±•å¼€</summary>As large language models (LLMs) advance, the ultimate vision for their role
in science is emerging: we could build an AI collaborator to effectively assist
human beings throughout the entire scientific research process. We refer to
this envisioned system as ResearchGPT. Given that scientific research
progresses through multiple interdependent phases, achieving this vision
requires rigorous benchmarks that evaluate the end-to-end workflow rather than
isolated sub-tasks. To this end, we contribute CS-54k, a high-quality corpus of
scientific Q&A pairs in computer science, built from 14k CC-licensed papers. It
is constructed through a scalable, paper-grounded pipeline that combines
retrieval-augmented generation (RAG) with multi-stage quality control to ensure
factual grounding. From this unified corpus, we derive two complementary
subsets: CS-4k, a carefully curated benchmark for evaluating AI's ability to
assist scientific research, and CS-50k, a large-scale training dataset.
Extensive experiments demonstrate that CS-4k stratifies state-of-the-art LLMs
into distinct capability tiers. Open models trained on CS-50k with supervised
training and reinforcement learning demonstrate substantial improvements. Even
7B-scale models, when properly trained, outperform many larger proprietary
systems, such as GPT-4.1, GPT-4o, and Gemini 2.5 Pro. This indicates that
making AI models better research assistants relies more on domain-aligned
training with high-quality data than on pretraining scale or general benchmark
performance. We release CS-4k and CS-50k in the hope of fostering AI systems as
reliable collaborators in CS research.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºResearchGPTæ„¿æ™¯ï¼Œé€šè¿‡æ„å»ºCS-54kç§‘å­¦é—®ç­”è¯­æ–™åº“ï¼ˆå«14kè®ºæ–‡ï¼‰æ”¯æŒAIè¾…åŠ©ç§‘ç ”ï¼Œå…¶æ•°æ®ç”Ÿæˆé‡‡ç”¨RAGæŠ€æœ¯ç¡®ä¿äº‹å®æ€§ï¼Œå¹¶æ‹†åˆ†å‡ºè¯„ä¼°åŸºå‡†CS-4kä¸è®­ç»ƒé›†CS-50kã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºè¯¥æ•°æ®è®­ç»ƒçš„7Bæ¨¡å‹å¯è¶…è¶ŠGPT-4ç­‰å•†ç”¨ç³»ç»Ÿï¼Œå‡¸æ˜¾é¢†åŸŸå¯¹é½æ•°æ®çš„é‡è¦æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.20260v1">Balancing Fine-tuning and RAG: A Hybrid Strategy for Dynamic LLM Recommendation Updates</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) empower recommendation systems through their
advanced reasoning and planning capabilities. However, the dynamic nature of
user interests and content poses a significant challenge: While initial
fine-tuning aligns LLMs with domain knowledge and user preferences, it fails to
capture such real-time changes, necessitating robust update mechanisms. This
paper investigates strategies for updating LLM-powered recommenders, focusing
on the trade-offs between ongoing fine-tuning and Retrieval-Augmented
Generation (RAG). Using an LLM-powered user interest exploration system as a
case study, we perform a comparative analysis of these methods across
dimensions like cost, agility, and knowledge incorporation. We propose a hybrid
update strategy that leverages the long-term knowledge adaptation of periodic
fine-tuning with the agility of low-cost RAG. We demonstrate through live A/B
experiments on a billion-user platform that this hybrid approach yields
statistically significant improvements in user satisfaction, offering a
practical and cost-effective framework for maintaining high-quality LLM-powered
recommender systems.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æ¢è®¨äº†åœ¨åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨èç³»ç»Ÿä¸­ï¼Œå¦‚ä½•é€šè¿‡æŒç»­å¾®è°ƒå’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸¤ç§ç­–ç•¥åŠ¨æ€é€‚åº”ç”¨æˆ·å…´è¶£å’Œå†…å®¹å˜åŒ–ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå®šæœŸå¾®è°ƒä¸ä½æˆæœ¬RAGçš„æ··åˆæ›´æ–°æ–¹æ³•ï¼Œå¹¶é€šè¿‡å¤§è§„æ¨¡A/Bå®éªŒéªŒè¯å…¶æ˜¾è‘—æå‡ç”¨æˆ·æ»¡æ„åº¦ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.20193v1">Multimedia-Aware Question Answering: A Review of Retrieval and Cross-Modal Reasoning Architectures</a></td><td><details><summary>å±•å¼€</summary>Question Answering (QA) systems have traditionally relied on structured text
data, but the rapid growth of multimedia content (images, audio, video, and
structured metadata) has introduced new challenges and opportunities for
retrieval-augmented QA. In this survey, we review recent advancements in QA
systems that integrate multimedia retrieval pipelines, focusing on
architectures that align vision, language, and audio modalities with user
queries. We categorize approaches based on retrieval methods, fusion
techniques, and answer generation strategies, and analyze benchmark datasets,
evaluation protocols, and performance tradeoffs. Furthermore, we highlight key
challenges such as cross-modal alignment, latency-accuracy tradeoffs, and
semantic grounding, and outline open problems and future research directions
for building more robust and context-aware QA systems leveraging multimedia
data.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç»¼è¿°äº†ç»“åˆå¤šåª’ä½“æ£€ç´¢ï¼ˆå¦‚å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ï¼‰çš„é—®ç­”ç³»ç»Ÿï¼ˆQAï¼‰çš„æœ€æ–°è¿›å±•ï¼Œèšç„¦äºé€šè¿‡è·¨æ¨¡æ€å¯¹é½ï¼ˆè§†è§‰ã€è¯­è¨€ã€éŸ³é¢‘ï¼‰å¢å¼ºæ£€ç´¢ä¸ç”Ÿæˆçš„æ¶æ„ï¼Œæ¢è®¨äº†æ£€ç´¢æ–¹æ³•ã€èåˆæŠ€æœ¯ã€ç”Ÿæˆç­–ç•¥åŠæŒ‘æˆ˜ï¼ˆå¦‚è·¨æ¨¡æ€å¯¹é½ã€è¯­ä¹‰ groundingï¼‰ï¼Œå±äºRAGæŠ€æœ¯åœ¨å¤šåª’ä½“é¢†åŸŸçš„æ‰©å±•åº”ç”¨ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-22
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.19723v1">From Answers to Guidance: A Proactive Dialogue System for Legal Documents</a></td><td><details><summary>å±•å¼€</summary>The accessibility of legal information remains a constant challenge,
particularly for laypersons seeking to understand and apply complex
institutional texts. While the European Union provides open access to
legislation, parliamentary responses, and regulatory documents, these resources
can be challenging for laypeople to explore. In this paper, we introduce
EUDial, a proactive multi-turn dialogue dataset constructed from 204 blogs
curated by the Citizens' Enquiries Unit (AskEP) of the European Parliamentary
Research Service. EUDial contains 880 dialogue turns (averaging 4.3 turns per
dialogue), where each dialogue includes initial questions, structured answers,
and follow-up questions. Beyond dataset construction, we propose the LexGuide
framework that leverages retrieval-augmented generation with hierarchical topic
organization to structure dialogue progression, ensuring both comprehensive
coverage of legal aspects and coherence across conversational turns. The
results demonstrate that proactive, structured navigation closes the gap
between the availability of legal information and citizen comprehension,
establishing EUDial and LexGuide as practical resources for advancing proactive
legal dialogue systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†EUDialæ•°æ®é›†å’ŒLexGuideæ¡†æ¶ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç»“åˆå±‚æ¬¡åŒ–ä¸»é¢˜ç»„ç»‡ï¼Œæ„å»ºä¸»åŠ¨å¤šè½®æ³•å¾‹å¯¹è¯ç³»ç»Ÿï¼Œå¸®åŠ©éä¸“ä¸šäººå£«ç†è§£æ¬§ç›Ÿå¤æ‚æ³•å¾‹æ–‡æœ¬ï¼Œæå‡ä¿¡æ¯å¯åŠæ€§ä¸å¯¹è¯è¿è´¯æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.19670v1">CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation</a></td><td><details><summary>å±•å¼€</summary>We present CoSense-LLM, an edge-first framework that turns continuous
multimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and
lightweight vision) into compact, verifiable semantic tokens and coordinates
with large language models under explicit latency, energy, bandwidth, and
privacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight
encoder that aligns sensor embeddings with language and compresses them into
short discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer
that grounds generation in site specific policies and notes; (iii)
PromptRouter, a cost and uncertainty aware policy that selects edge only
generation, edge plus retrieval, or compact cloud escalation; and (iv) Secure
Execution, an auditable redaction path that enforces data minimization so raw
waveforms never leave the device. The system works with modern serving
optimizations, including paged or streaming KV caches, FlashAttention style
kernels, speculative decoding, and quantized LoRA adapters, and supports on
device personalization and federated updates under non IID drift. Across home,
office, and clinic deployments, CoSense-LLM delivers grounded explanations
while meeting tight service level objectives: it sustains sub second (p95) end
to end latency on edge dominant paths, reduces inter tier token and bandwidth
costs by preferring local retrieval grounded responses, and preserves privacy
by transmitting only discrete codes and redacted metadata. Ablations show that
Edge-RAG improves factual consistency and reduces contradictions, calibrated
uncertainty enables selective abstention and controlled escalations, and KV
plus decoding accelerators lower energy per decision. The results support an
edge first design that treats semantics, privacy, and predictable latency as co
equal goals for large model deployments in interference prone environments.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†CoSense-LLMæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆè½»é‡çº§ä¼ æ„Ÿå™¨ç¼–ç ã€æœ¬åœ°æ··åˆæ£€ç´¢å±‚ï¼ˆEdge-RAGï¼‰å’Œæˆæœ¬æ„ŸçŸ¥ç­–ç•¥ï¼Œå°†å¤šæ¨¡æ€ä¼ æ„Ÿå™¨æ•°æ®è½¬åŒ–ä¸ºè¯­ä¹‰æ ‡è®°å¹¶ä¸å¤§è¯­è¨€æ¨¡å‹ååŒå·¥ä½œã€‚Edge-RAGä½œä¸ºæ ¸å¿ƒç»„ä»¶ï¼Œé€šè¿‡æœ¬åœ°æ£€ç´¢å¢å¼ºç”Ÿæˆè¿‡ç¨‹ï¼Œç¡®ä¿å›ç­”åŸºäºç‰¹å®šåœºæ™¯ç­–ç•¥å’Œè®°å½•ï¼Œæé«˜äº†äº‹å®ä¸€è‡´æ€§å¹¶å‡å°‘çŸ›ç›¾ï¼ŒåŒæ—¶æ»¡è¶³å»¶è¿Ÿã€éšç§å’Œå¸¦å®½çº¦æŸã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.19644v1">LLavaCode: Compressed Code Representations for Retrieval-Augmented Code Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation has emerged as one of the most effective
approaches for code completion, particularly when context from a surrounding
repository is essential. However, incorporating context significantly extends
sequence length, leading to slower inference - a critical limitation for
interactive settings such as IDEs. In this work, we introduce LlavaCode, a
framework that compresses code into compact, semantically rich representations
interpretable by code LLM, enhancing generation quality while reducing the
retrieved context to only a few compressed single-token vectors. Using a small
projector module we can significantly increase the EM and ES metrics of coding
model with negligible latency increase. Our experiments demonstrate that
compressed context enables 20-38% reduction in Time-to-First-Token (TTFT) on
line completion tasks compared to full-RAG pipelines.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†LlavaCodeæ¡†æ¶ï¼Œé€šè¿‡å°†ä»£ç å‹ç¼©æˆç´§å‡‘çš„è¯­ä¹‰ä¸°å¯Œè¡¨ç¤ºæ¥ä¼˜åŒ–RAGåœ¨ä»£ç è¡¥å…¨ä¸­çš„åº”ç”¨ï¼Œå‡å°‘äº†æ£€ç´¢ä¸Šä¸‹æ–‡çš„é•¿åº¦ï¼Œä»è€Œæå‡ç”Ÿæˆè´¨é‡å¹¶é™ä½å»¶è¿Ÿï¼Œå®éªŒæ˜¾ç¤ºå…¶èƒ½æ˜¾è‘—å‡å°‘Time-to-First-Tokenï¼ˆTTFTï¼‰ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.19331v1">Algorithmic Fairness in NLP: Persona-Infused LLMs for Human-Centric Hate Speech Detection</a></td><td><details><summary>å±•å¼€</summary>In this paper, we investigate how personalising Large Language Models
(Persona-LLMs) with annotator personas affects their sensitivity to hate
speech, particularly regarding biases linked to shared or differing identities
between annotators and targets. To this end, we employ Google's Gemini and
OpenAI's GPT-4.1-mini models and two persona-prompting methods: shallow persona
prompting and a deeply contextualised persona development based on
Retrieval-Augmented Generation (RAG) to incorporate richer persona profiles. We
analyse the impact of using in-group and out-group annotator personas on the
models' detection performance and fairness across diverse social groups. This
work bridges psychological insights on group identity with advanced NLP
techniques, demonstrating that incorporating socio-demographic attributes into
LLMs can address bias in automated hate speech detection. Our results highlight
both the potential and limitations of persona-based approaches in reducing
bias, offering valuable insights for developing more equitable hate speech
detection systems.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ç ”ç©¶äº†é€šè¿‡ä¸ªæ€§åŒ–å¤§è¯­è¨€æ¨¡å‹ï¼ˆPersona-LLMsï¼‰æ¥æå‡å¯¹ä»‡æ¨è¨€è®ºçš„æ•æ„Ÿæ€§ï¼Œç‰¹åˆ«å…³æ³¨äº†æ³¨é‡Šè€…ä¸ç›®æ ‡ç¾¤ä½“èº«ä»½å¼‚åŒå¸¦æ¥çš„åè§ã€‚ç ”ç©¶é‡‡ç”¨äº†Googleçš„Geminiå’ŒOpenAIçš„GPT-4.1-miniæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨äº†ä¸¤ç§äººç‰©æç¤ºæ–¹æ³•ï¼Œå…¶ä¸­ä¸€ç§åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¥æ•´åˆæ›´ä¸°å¯Œçš„äººç‰©æ¡£æ¡ˆã€‚è®ºæ–‡åˆ†æäº†ä½¿ç”¨å†…ç¾¤ä½“å’Œå¤–ç¾¤ä½“æ³¨é‡Šè€…äººç‰©å¯¹æ¨¡å‹æ£€æµ‹æ€§èƒ½å’Œå…¬å¹³æ€§çš„å½±å“ï¼Œå±•ç¤ºäº†å°†ç¤¾ä¼šäººå£å±æ€§èå…¥å¤§è¯­è¨€æ¨¡å‹ä»¥å‡å°‘è‡ªåŠ¨ä»‡æ¨è¨€è®ºæ£€æµ‹ä¸­çš„åè§çš„æ½œåŠ›ä¸å±€é™æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.19171v1">Think Straight, Stop Smart: Structured Reasoning for Efficient Multi-Hop RAG</a></td><td><details><summary>å±•å¼€</summary>Multi-hop retrieval-augmented generation (RAG) is a promising strategy for
complex reasoning, yet existing iterative prompting approaches remain
inefficient. They often regenerate predictable token sequences at every step
and rely on stochastic stopping, leading to excessive token usage and unstable
termination. We propose TSSS (Think Straight, Stop Smart), a structured
multi-hop RAG framework designed for efficiency. TSSS introduces (i) a
template-based reasoning that caches recurring prefixes and anchors sub-queries
to the main question, reducing token generation cost while promoting stable
reasoning, and (ii) a retriever-based terminator, which deterministically halts
reasoning once additional sub-queries collapse into repetition. This separation
of structured reasoning and termination control enables both faster inference
and more reliable answers. On HotpotQA, 2WikiMultiHop, and MuSiQue, TSSS
achieves state-of-the-art accuracy and competitive efficiency among RAG-CoT
approaches, highlighting its effectiveness in efficiency-constrained scenarios
such as on-device inference.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºTSSSï¼ˆThink Straight, Stop Smartï¼‰çš„é«˜æ•ˆå¤šè·³æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ¿åŒ–æ¨ç†å‡å°‘é‡å¤ç”Ÿæˆçš„ä»¤ç‰Œæˆæœ¬ï¼Œå¹¶å¼•å…¥åŸºäºæ£€ç´¢å™¨çš„ç»ˆæ­¢æœºåˆ¶ä»¥ç¨³å®šç»“æŸæ¨ç†ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†é«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-21
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.18821v1">Search Self-play: Pushing the Frontier of Agent Capability without Supervision</a></td><td><details><summary>å±•å¼€</summary>Reinforcement learning with verifiable rewards (RLVR) has become the
mainstream technique for training LLM agents. However, RLVR highly depends on
well-crafted task queries and corresponding ground-truth answers to provide
accurate rewards, which requires massive human efforts and hinders the RL
scaling processes, especially under agentic scenarios. Although a few recent
works explore task synthesis methods, the difficulty of generated agentic tasks
can hardly be controlled to provide effective RL training advantages. To
achieve agentic RLVR with higher scalability, we explore self-play training for
deep search agents, in which the learning LLM utilizes multi-turn search engine
calling and acts simultaneously as both a task proposer and a problem solver.
The task proposer aims to generate deep search queries with well-defined
ground-truth answers and increasing task difficulty. The problem solver tries
to handle the generated search queries and output the correct answer
predictions. To ensure that each generated search query has accurate ground
truth, we collect all the searching results from the proposer's trajectory as
external knowledge, then conduct retrieval-augmentation generation (RAG) to
test whether the proposed query can be correctly answered with all necessary
search documents provided. In this search self-play (SSP) game, the proposer
and the solver co-evolve their agent capabilities through both competition and
cooperation. With substantial experimental results, we find that SSP can
significantly improve search agents' performance uniformly on various
benchmarks without any supervision under both from-scratch and continuous RL
training setups. The code is at https://github.com/Alibaba-Quark/SSP.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒï¼ˆSSPï¼‰çš„æ·±åº¦æœç´¢ä»£ç†æ–¹æ³•ï¼Œé€šè¿‡è®©å¤§è¯­è¨€æ¨¡å‹åŒæ—¶å……å½“ä»»åŠ¡æå‡ºè€…å’Œé—®é¢˜è§£å†³è€…ï¼Œç”Ÿæˆå…·æœ‰æ˜ç¡®ç­”æ¡ˆå’Œé€’å¢éš¾åº¦çš„æœç´¢æŸ¥è¯¢ï¼Œå¹¶åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯éªŒè¯æŸ¥è¯¢çš„å¯å›ç­”æ€§ï¼Œä»è€Œåœ¨æ— ç›‘ç£æƒ…å†µä¸‹æ˜¾è‘—æå‡æœç´¢ä»£ç†çš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.18691v1">Investigating LLM Capabilities on Long Context Comprehension for Medical Question Answering</a></td><td><details><summary>å±•å¼€</summary>This study is the first to investigate LLM comprehension capabilities over
long-context (LC) medical QA of clinical relevance. Our comprehensive
assessment spans a range of content-inclusion settings based on their
relevance, LLM models of varying capabilities and datasets across task
formulations, revealing insights on model size effects, limitations, underlying
memorization issues and the benefits of reasoning models. Importantly, we
examine the effect of RAG on medical LC comprehension, uncover best settings in
single versus multi-document reasoning datasets and showcase RAG strategies for
improvements over LC. We shed light into some of the evaluation aspects using a
multi-faceted approach. Our qualitative and error analyses address open
questions on when RAG is beneficial over LC, revealing common failure cases.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡é¦–æ¬¡ç ”ç©¶äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é•¿ä¸Šä¸‹æ–‡ï¼ˆLCï¼‰åŒ»å­¦é—®ç­”ä¸­çš„ç†è§£èƒ½åŠ›ï¼Œè¯„ä¼°äº†ä¸åŒæ¨¡å‹ã€æ•°æ®é›†å’Œä»»åŠ¡è®¾ç½®ä¸‹çš„è¡¨ç°ï¼Œæ¢è®¨äº†æ¨¡å‹å¤§å°ã€è®°å¿†é—®é¢˜åŠæ¨ç†æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œé‡ç‚¹åˆ†æäº†RAGå¯¹åŒ»å­¦LCç†è§£çš„å½±å“ï¼Œæ¯”è¾ƒäº†å•æ–‡æ¡£ä¸å¤šæ–‡æ¡£æ¨ç†çš„æœ€ä½³è®¾ç½®ï¼Œå¹¶æå‡ºäº†æ”¹è¿›RAGçš„ç­–ç•¥ï¼ŒåŒæ—¶é€šè¿‡å¤šè§’åº¦è¯„ä¼°æ­ç¤ºäº†RAGçš„é€‚ç”¨åœºæ™¯å’Œå¸¸è§å¤±è´¥æ¡ˆä¾‹ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.18633v1">Query Decomposition for RAG: Balancing Exploration-Exploitation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) systems address complex user requests by
decomposing them into subqueries, retrieving potentially relevant documents for
each, and then aggregating them to generate an answer. Efficiently selecting
informative documents requires balancing a key trade-off: (i) retrieving
broadly enough to capture all the relevant material, and (ii) limiting
retrieval to avoid excessive noise and computational cost. We formulate query
decomposition and document retrieval in an exploitation-exploration setting,
where retrieving one document at a time builds a belief about the utility of a
given sub-query and informs the decision to continue exploiting or exploring an
alternative. We experiment with a variety of bandit learning methods and
demonstrate their effectiveness in dynamically selecting the most informative
sub-queries. Our main finding is that estimating document relevance using rank
information and human judgments yields a 35% gain in document-level precision,
15% increase in {\alpha}-nDCG, and better performance on the downstream task of
long-form generation.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åœ¨RAGç³»ç»Ÿä¸­å¦‚ä½•é€šè¿‡åˆ†è§£ç”¨æˆ·æŸ¥è¯¢ã€åŠ¨æ€æ£€ç´¢æ–‡æ¡£å¹¶å¹³è¡¡æ£€ç´¢å¹¿åº¦ä¸å™ªå£°çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ¢ç´¢-åˆ©ç”¨ç­–ç•¥çš„banditå­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨æ’åºä¿¡æ¯å’Œäººå·¥è¯„ä¼°æå‡æ–‡æ¡£é€‰æ‹©æ•ˆç‡ï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æ–‡æ¡£æ£€ç´¢ç²¾åº¦å’Œä¸‹æ¸¸é•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡çš„è¡¨ç°ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.18502v1">Zero-Shot Vehicle Model Recognition via Text-Based Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Vehicle make and model recognition (VMMR) is an important task in intelligent
transportation systems, but existing approaches struggle to adapt to newly
released models. Contrastive Language-Image Pretraining (CLIP) provides strong
visual-text alignment, yet its fixed pretrained weights limit performance
without costly image-specific finetuning. We propose a pipeline that integrates
vision language models (VLMs) with Retrieval-Augmented Generation (RAG) to
support zero-shot recognition through text-based reasoning. A VLM converts
vehicle images into descriptive attributes, which are compared against a
database of textual features. Relevant entries are retrieved and combined with
the description to form a prompt, and a language model (LM) infers the make and
model. This design avoids large-scale retraining and enables rapid updates by
adding textual descriptions of new vehicles. Experiments show that the proposed
method improves recognition by nearly 20% over the CLIP baseline, demonstrating
the potential of RAG-enhanced LM reasoning for scalable VMMR in smart-city
applications.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æµç¨‹ï¼Œç”¨äºé›¶æ ·æœ¬è½¦è¾†å“ç‰Œå’Œå‹å·è¯†åˆ«ï¼ˆVMMRï¼‰ã€‚é€šè¿‡å°†è½¦è¾†å›¾åƒè½¬æ¢ä¸ºæè¿°æ€§å±æ€§å¹¶ä¸æ–‡æœ¬ç‰¹å¾æ•°æ®åº“æ¯”å¯¹ï¼Œæ£€ç´¢ç›¸å…³ä¿¡æ¯åç”Ÿæˆæç¤ºï¼Œç”±è¯­è¨€æ¨¡å‹æ¨æ–­ç»“æœã€‚è¯¥æ–¹æ³•é¿å…äº†å¤§è§„æ¨¡é‡æ–°è®­ç»ƒï¼Œæ”¯æŒå¿«é€Ÿæ›´æ–°ï¼Œå®éªŒæ˜¾ç¤ºè¯†åˆ«ç‡æ¯”CLIPåŸºçº¿æå‡è¿‘20%ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.18468v1">IMB: An Italian Medical Benchmark for Question Answering</a></td><td><details><summary>å±•å¼€</summary>Online medical forums have long served as vital platforms where patients seek
professional healthcare advice, generating vast amounts of valuable knowledge.
However, the informal nature and linguistic complexity of forum interactions
pose significant challenges for automated question answering systems,
especially when dealing with non-English languages. We present two
comprehensive Italian medical benchmarks: \textbf{IMB-QA}, containing 782,644
patient-doctor conversations from 77 medical categories, and \textbf{IMB-MCQA},
comprising 25,862 multiple-choice questions from medical specialty
examinations. We demonstrate how Large Language Models (LLMs) can be leveraged
to improve the clarity and consistency of medical forum data while retaining
their original meaning and conversational style, and compare a variety of LLM
architectures on both open and multiple-choice question answering tasks. Our
experiments with Retrieval Augmented Generation (RAG) and domain-specific
fine-tuning reveal that specialized adaptation strategies can outperform
larger, general-purpose models in medical question answering tasks. These
findings suggest that effective medical AI systems may benefit more from domain
expertise and efficient information retrieval than from increased model scale.
We release both datasets and evaluation frameworks in our GitHub repository to
support further research on multilingual medical question answering:
https://github.com/PRAISELab-PicusLab/IMB.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ä»‹ç»äº†ä¸¤ä¸ªæ„å¤§åˆ©åŒ»å­¦åŸºå‡†æ•°æ®é›†ï¼ˆIMB-QAå’ŒIMB-MCQAï¼‰ï¼Œæ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æå‡åŒ»å­¦è®ºå›æ•°æ®çš„æ¸…æ™°åº¦ä¸ä¸€è‡´æ€§çš„æ–¹æ³•ï¼Œå¹¶é€šè¿‡å®éªŒå¯¹æ¯”äº†RAGä¸é¢†åŸŸå¾®è°ƒåœ¨åŒ»å­¦é—®ç­”ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå‘ç°é¢†åŸŸé€‚é…ç­–ç•¥ä¼˜äºé€šç”¨å¤§æ¨¡å‹ï¼Œæœ€ç»ˆå¼€æºäº†æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ä»¥æ”¯æŒå¤šè¯­è¨€åŒ»å­¦é—®ç­”ç ”ç©¶ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.18455v1">ChronoPlay: A Framework for Modeling Dual Dynamics and Authenticity in Game RAG Benchmarks</a></td><td><details><summary>å±•å¼€</summary>Retrieval Augmented Generation (RAG) systems are increasingly vital in
dynamic domains like online gaming, yet the lack of a dedicated benchmark has
impeded standardized evaluation in this area. The core difficulty lies in Dual
Dynamics: the constant interplay between game content updates and the shifting
focus of the player community. Furthermore, the necessity of automating such a
benchmark introduces a critical requirement for player-centric authenticity to
ensure generated questions are realistic. To address this integrated challenge,
we introduce ChronoPlay, a novel framework for the automated and continuous
generation of game RAG benchmarks. ChronoPlay utilizes a dual-dynamic update
mechanism to track both forms of change, and a dual-source synthesis engine
that draws from official sources and player community to ensure both factual
correctness and authentic query patterns. We instantiate our framework on three
distinct games to create the first dynamic RAG benchmark for the gaming domain,
offering new insights into model performance under these complex and realistic
conditions. Code is avaliable at: https://github.com/hly1998/ChronoPlay.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºChronoPlayçš„è‡ªåŠ¨åŒ–åŠ¨æ€RAGåŸºå‡†ç”Ÿæˆæ¡†æ¶ï¼Œé’ˆå¯¹åœ¨çº¿æ¸¸æˆé¢†åŸŸè®¾è®¡ï¼Œé€šè¿‡åŒåŠ¨æ€æ›´æ–°æœºåˆ¶å’ŒåŒæºåˆæˆå¼•æ“ï¼ˆå®˜æ–¹èµ„æ–™ä¸ç©å®¶ç¤¾åŒºï¼‰è§£å†³æ¸¸æˆå†…å®¹æ›´æ–°ä¸ç©å®¶å…³æ³¨ç‚¹å˜åŒ–çš„åŒé‡æŒ‘æˆ˜ï¼Œå¹¶æ„å»ºäº†é¦–ä¸ªæ¸¸æˆé¢†åŸŸçš„åŠ¨æ€RAGåŸºå‡†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨å¤æ‚ç°å®æ¡ä»¶ä¸‹çš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.18355v1">KrishokBondhu: A Retrieval-Augmented Voice-Based Agricultural Advisory Call Center for Bengali Farmers</a></td><td><details><summary>å±•å¼€</summary>In Bangladesh, many farmers continue to face challenges in accessing timely,
expert-level agricultural guidance. This paper presents KrishokBondhu, a
voice-enabled, call-centre-integrated advisory platform built on a
Retrieval-Augmented Generation (RAG) framework, designed specifically for
Bengali-speaking farmers. The system aggregates authoritative agricultural
handbooks, extension manuals, and NGO publications; applies Optical Character
Recognition (OCR) and document-parsing pipelines to digitize and structure the
content; and indexes this corpus in a vector database for efficient semantic
retrieval. Through a simple phone-based interface, farmers can call the system
to receive real-time, context-aware advice: speech-to-text converts the Bengali
query, the RAG module retrieves relevant content, a large language model (Gemma
3-4B) generates a context-grounded response, and text-to-speech delivers the
answer in natural spoken Bengali. In a pilot evaluation, KrishokBondhu produced
high-quality responses for 72.7% of diverse agricultural queries covering crop
management, disease control, and cultivation practices. Compared to the
KisanQRS benchmark, the system achieved a composite score of 4.53 (vs. 3.13) on
a 5-point scale, a 44.7% improvement, with especially large gains in contextual
richness (+367%) and completeness (+100.4%), while maintaining comparable
relevance and technical specificity. Semantic similarity analysis further
revealed a strong correlation between retrieved context and answer quality,
emphasizing the importance of grounding generative responses in curated
documentation. KrishokBondhu demonstrates the feasibility of integrating
call-centre accessibility, multilingual voice interaction, and modern RAG
techniques to deliver expert-level agricultural guidance to remote Bangladeshi
farmers, paving the way toward a fully AI-driven agricultural advisory
ecosystem.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†KrishokBondhuï¼Œä¸€ä¸ªåŸºäºRAGæ¡†æ¶çš„è¯­éŸ³å†œä¸šå’¨è¯¢å¹³å°ï¼Œä¸“ä¸ºå­ŸåŠ æ‹‰è¯­å†œæ°‘è®¾è®¡ï¼Œé€šè¿‡æ•´åˆæƒå¨å†œä¸šèµ„æ–™ã€OCRæŠ€æœ¯å’Œè¯­éŸ³äº¤äº’ï¼Œæä¾›å®æ—¶å†œä¸šå»ºè®®ï¼Œå¹¶åœ¨è¯•ç‚¹è¯„ä¼°ä¸­å±•ç°å‡ºé«˜è´¨é‡å›ç­”å’Œæ˜¾è‘—æ€§èƒ½æå‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.18339v1">ECG-LLM-- training and evaluation of domain-specific large language models for electrocardiography</a></td><td><details><summary>å±•å¼€</summary>Domain-adapted open-weight large language models (LLMs) offer promising
healthcare applications, from queryable knowledge bases to multimodal
assistants, with the crucial advantage of local deployment for privacy
preservation. However, optimal adaptation strategies, evaluation methodologies,
and performance relative to general-purpose LLMs remain poorly characterized.
We investigated these questions in electrocardiography, an important area of
cardiovascular medicine, by finetuning open-weight models on domain-specific
literature and implementing a multi-layered evaluation framework comparing
finetuned models, retrieval-augmented generation (RAG), and Claude Sonnet 3.7
as a representative general-purpose model. Finetuned Llama 3.1 70B achieved
superior performance on multiple-choice evaluations and automatic text metrics,
ranking second to Claude 3.7 in LLM-as-a-judge assessments. Human expert
evaluation favored Claude 3.7 and RAG approaches for complex queries. Finetuned
models significantly outperformed their base counterparts across nearly all
evaluation modes. Our findings reveal substantial performance heterogeneity
across evaluation methodologies, underscoring assessment complexity.
Nevertheless, domain-specific adaptation through finetuning and RAG achieves
competitive performance with proprietary models, supporting the viability of
privacy-preserving, locally deployable clinical solutions.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†åœ¨åŒ»ç–—é¢†åŸŸï¼ˆç‰¹åˆ«æ˜¯å¿ƒç”µå›¾å­¦ï¼‰ä¸­ï¼Œé€šè¿‡å¾®è°ƒå¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œé‡‡ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯æ¥æå‡æ¨¡å‹æ€§èƒ½çš„æ–¹æ³•ã€‚ç ”ç©¶æ¯”è¾ƒäº†å¾®è°ƒæ¨¡å‹ã€RAGæ–¹æ³•å’Œé€šç”¨æ¨¡å‹Claude Sonnet 3.7çš„è¡¨ç°ï¼Œå‘ç°å¾®è°ƒæ¨¡å‹åœ¨å¤šé¡¹è¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè€ŒRAGå’ŒClaude 3.7åœ¨å¤æ‚æŸ¥è¯¢ä¸­æ›´å—ä¸“å®¶é’çã€‚è®ºæ–‡å¼ºè°ƒäº†è¯„ä¼°æ–¹æ³•çš„å¤æ‚æ€§ï¼Œå¹¶è¯æ˜äº†é¢†åŸŸç‰¹å®šé€‚é…ï¼ˆåŒ…æ‹¬RAGï¼‰åœ¨éšç§ä¿æŠ¤å‹æœ¬åœ°ä¸´åºŠè§£å†³æ–¹æ¡ˆä¸­çš„å¯è¡Œæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.18297v1">From Retrieval to Generation: Unifying External and Parametric Knowledge for Medical Question Answering</a></td><td><details><summary>å±•å¼€</summary>Medical question answering (QA) requires extensive access to domain-specific
knowledge. A promising direction is to enhance large language models (LLMs)
with external knowledge retrieved from medical corpora or parametric knowledge
stored in model parameters. Existing approaches typically fall into two
categories: Retrieval-Augmented Generation (RAG), which grounds model reasoning
on externally retrieved evidence, and Generation-Augmented Generation (GAG),
which depends solely on the models internal knowledge to generate contextual
documents. However, RAG often suffers from noisy or incomplete retrieval, while
GAG is vulnerable to hallucinated or inaccurate information due to
unconstrained generation. Both issues can mislead reasoning and undermine
answer reliability. To address these challenges, we propose MedRGAG, a unified
retrieval-generation augmented framework that seamlessly integrates external
and parametric knowledge for medical QA. MedRGAG comprises two key modules:
Knowledge-Guided Context Completion (KGCC), which directs the generator to
produce background documents that complement the missing knowledge revealed by
retrieval; and Knowledge-Aware Document Selection (KADS), which adaptively
selects an optimal combination of retrieved and generated documents to form
concise yet comprehensive evidence for answer generation. Extensive experiments
on five medical QA benchmarks demonstrate that MedRGAG achieves a 12.5%
improvement over MedRAG and a 4.5% gain over MedGENIE, highlighting the
effectiveness of unifying retrieval and generation for knowledge-intensive
reasoning. Our code and data are publicly available at
https://anonymous.4open.science/r/MedRGAG</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºMedRGAGæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆå¤–éƒ¨æ£€ç´¢ï¼ˆRAGï¼‰å’Œå†…éƒ¨ç”Ÿæˆï¼ˆGAGï¼‰çš„åŒ»å­¦çŸ¥è¯†ï¼Œç»“åˆçŸ¥è¯†å¼•å¯¼çš„ä¸Šä¸‹æ–‡è¡¥å…¨ï¼ˆKGCCï¼‰å’ŒçŸ¥è¯†æ„ŸçŸ¥æ–‡æ¡£é€‰æ‹©ï¼ˆKADSï¼‰æ¨¡å—ï¼Œæ˜¾è‘—æå‡åŒ»ç–—é—®ç­”çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œå®éªŒè¯æ˜å…¶æ€§èƒ½ä¼˜äºçº¯æ£€ç´¢æˆ–çº¯ç”Ÿæˆæ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.18204v1">RESCUE: Retrieval Augmented Secure Code Generation</a></td><td><details><summary>å±•å¼€</summary>Despite recent advances, Large Language Models (LLMs) still generate
vulnerable code. Retrieval-Augmented Generation (RAG) has the potential to
enhance LLMs for secure code generation by incorporating external security
knowledge. However, the conventional RAG design struggles with the noise of raw
security-related documents, and existing retrieval methods overlook the
significant security semantics implicitly embedded in task descriptions. To
address these issues, we propose RESCUE, a new RAG framework for secure code
generation with two key innovations. First, we propose a hybrid knowledge base
construction method that combines LLM-assisted cluster-then-summarize
distillation with program slicing, producing both high-level security
guidelines and concise, security-focused code examples. Second, we design a
hierarchical multi-faceted retrieval to traverse the constructed knowledge base
from top to bottom and integrates multiple security-critical facts at each
hierarchical level, ensuring comprehensive and accurate retrieval. We evaluated
RESCUE on four benchmarks and compared it with five state-of-the-art secure
code generation methods on six LLMs. The results demonstrate that RESCUE
improves the SecurePass@1 metric by an average of 4.8 points, establishing a
new state-of-the-art performance for security. Furthermore, we performed
in-depth analysis and ablation studies to rigorously validate the effectiveness
of individual components in RESCUE.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRESCUEçš„æ–°å‹RAGæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ”¹è¿›æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯æ¥æå‡å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå®‰å…¨ä»£ç çš„èƒ½åŠ›ã€‚RESCUEé€šè¿‡ç»“åˆLLMè¾…åŠ©çš„èšç±»ä¸æ‘˜è¦è’¸é¦æ–¹æ³•ä»¥åŠç¨‹åºåˆ‡ç‰‡æŠ€æœ¯æ„å»ºæ··åˆçŸ¥è¯†åº“ï¼Œå¹¶é‡‡ç”¨åˆ†å±‚å¤šé¢æ£€ç´¢ç­–ç•¥ï¼Œæœ‰æ•ˆæ•´åˆé«˜å±‚å®‰å…¨æŒ‡å—å’Œèšç„¦å®‰å…¨çš„ä»£ç ç¤ºä¾‹ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆä»£ç çš„å®‰å…¨æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒRESCUEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹³å‡æå‡SecurePass@1æŒ‡æ ‡4.8åˆ†ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-20
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.17795v1">Executable Knowledge Graphs for Replicating AI Research</a></td><td><details><summary>å±•å¼€</summary>Replicating AI research is a crucial yet challenging task for large language
model (LLM) agents. Existing approaches often struggle to generate executable
code, primarily due to insufficient background knowledge and the limitations of
retrieval-augmented generation (RAG) methods, which fail to capture latent
technical details hidden in referenced papers. Furthermore, previous approaches
tend to overlook valuable implementation-level code signals and lack structured
knowledge representations that support multi-granular retrieval and reuse. To
overcome these challenges, we propose Executable Knowledge Graphs (xKG), a
modular and pluggable knowledge base that automatically integrates technical
insights, code snippets, and domain-specific knowledge extracted from
scientific literature. When integrated into three agent frameworks with two
different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on
PaperBench, demonstrating its effectiveness as a general and extensible
solution for automated AI research replication. Code will released at
https://github.com/zjunlp/xKG.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºExecutable Knowledge Graphsï¼ˆxKGï¼‰çš„å¯æ’æ‹”çŸ¥è¯†åº“ï¼Œç”¨äºè§£å†³ç°æœ‰æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•åœ¨å¤åˆ¶AIç ”ç©¶æ—¶éš¾ä»¥ç”Ÿæˆå¯æ‰§è¡Œä»£ç çš„é—®é¢˜ã€‚xKGé€šè¿‡æ•´åˆç§‘å­¦æ–‡çŒ®ä¸­çš„æŠ€æœ¯ç»†èŠ‚ã€ä»£ç ç‰‡æ®µå’Œé¢†åŸŸçŸ¥è¯†ï¼Œæ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†åœ¨è‡ªåŠ¨åŒ–AIç ”ç©¶å¤ç°ä¸­çš„æ€§èƒ½è¡¨ç°ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.17733v1">Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations</a></td><td><details><summary>å±•å¼€</summary>Language models often generate factually incorrect information unsupported by
their training data, a phenomenon known as extrinsic hallucination. Existing
mitigation approaches often degrade performance on open-ended generation and
downstream tasks, limiting their practical utility. We propose an online
reinforcement learning method using a novel binary retrieval-augmented reward
(RAR) to address this tradeoff. Unlike continuous reward schemes, our approach
assigns a reward of one only when the model's output is entirely factually
correct, and zero otherwise. We evaluate our method on Qwen3 reasoning models
across diverse tasks. For open-ended generation, binary RAR achieves a 39.3%
reduction in hallucination rates, substantially outperforming both supervised
training and continuous-reward RL baselines. In short-form question answering,
the model learns calibrated abstention, strategically outputting "I don't know"
when faced with insufficient parametric knowledge. This yields 44.4% and 21.7%
fewer incorrect answers on PopQA and GPQA, respectively. Crucially, these
factuality gains come without performance degradation on instruction following,
math, or code, whereas continuous-reward RL, despite improving factuality,
induces quality regressions.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ£€ç´¢å¢å¼ºå¥–åŠ±ï¼ˆRARï¼‰æ–¹æ³•ï¼Œé€šè¿‡äºŒå…ƒå¥–åŠ±æœºåˆ¶ï¼ˆè¾“å‡ºå®Œå…¨æ­£ç¡®æ—¶å¥–åŠ±ä¸º1ï¼Œå¦åˆ™ä¸º0ï¼‰å‡å°‘è¯­è¨€æ¨¡å‹çš„å¤–æºæ€§å¹»è§‰é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¼€æ”¾ç”Ÿæˆä»»åŠ¡ä¸­æ˜¾è‘—é™ä½å¹»è§‰ç‡ï¼Œå¹¶èƒ½åœ¨é—®ç­”ä»»åŠ¡ä¸­å®ç°ç­–ç•¥æ€§â€œæœªçŸ¥â€å›ç­”ï¼ŒåŒæ—¶ä¿æŒæŒ‡ä»¤éµå¾ªã€æ•°å­¦å’Œä»£ç ç­‰ä»»åŠ¡çš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.17590v1">MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning</a></td><td><details><summary>å±•å¼€</summary>Misinformation spreads across web platforms through billions of daily
multimodal posts that combine text and images, overwhelming manual
fact-checking capacity. Supervised detection models require domain-specific
training data and fail to generalize across diverse manipulation tactics. We
present MIRAGE, an inference-time, model-pluggable agentic framework that
decomposes multimodal verification into four sequential modules: visual
veracity assessment detects AI-generated images, cross-modal consistency
analysis identifies out-of-context repurposing, retrieval-augmented factual
checking grounds claims in web evidence through iterative question generation,
and a calibrated judgment module integrates all signals. MIRAGE orchestrates
vision-language model reasoning with targeted web retrieval, outputs structured
and citation-linked rationales. On MMFakeBench validation set (1,000 samples),
MIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming
the strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65
points while maintaining 34.3% false positive rate versus 97.3% for a
judge-only baseline. Test set results (5,000 samples) confirm generalization
with 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification
contributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97
points. Our results demonstrate that decomposed agentic reasoning with web
retrieval can match supervised detector performance without domain-specific
training, enabling misinformation detection across modalities where labeled
data remains scarce.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†MIRAGEæ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€éªŒè¯å’Œæ£€ç´¢å¢å¼ºçš„äº‹å®æ ¸æŸ¥ï¼ˆretrieval-augmented factual checkingï¼‰æ¥æ£€æµ‹ç½‘ç»œä¸Šçš„é”™è¯¯ä¿¡æ¯ï¼Œåˆ©ç”¨æ£€ç´¢åˆ°çš„ç½‘ç»œè¯æ®å¢å¼ºç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚GPT-4o-miniï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†æ£€æµ‹å‡†ç¡®æ€§å¹¶ç”Ÿæˆç»“æ„åŒ–è§£é‡Šã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.17476v1">Disparities in Multilingual LLM-Based Healthcare Q&A</a></td><td><details><summary>å±•å¼€</summary>Equitable access to reliable health information is vital when integrating AI
into healthcare. Yet, information quality varies across languages, raising
concerns about the reliability and consistency of multilingual Large Language
Models (LLMs). We systematically examine cross-lingual disparities in
pre-training source and factuality alignment in LLM answers for multilingual
healthcare Q&A across English, German, Turkish, Chinese (Mandarin), and
Italian. We (i) constructed Multilingual Wiki Health Care
(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed
cross-lingual healthcare coverage; (iii) assessed LLM response alignment with
these references; and (iv) conducted a case study on factual alignment through
the use of contextual information and Retrieval-Augmented Generation (RAG). Our
findings reveal substantial cross-lingual disparities in both Wikipedia
coverage and LLM factual alignment. Across LLMs, responses align more with
English Wikipedia, even when the prompts are non-English. Providing contextual
excerpts from non-English Wikipedia at inference time effectively shifts
factual alignment toward culturally relevant knowledge. These results highlight
practical pathways for building more equitable, multilingual AI systems for
healthcare.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»ç–—é—®ç­”ä¸­çš„äº‹å®å‡†ç¡®æ€§åŠå…¶ä¸ç»´åŸºç™¾ç§‘é¢„è®­ç»ƒæ•°æ®çš„è·¨è¯­è¨€å·®å¼‚ï¼Œå¹¶é€šè¿‡æ„å»ºå¤šè¯­è¨€æ•°æ®é›†ï¼ˆMultiWikiHealthCareï¼‰å’Œå¼•å…¥æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼ŒéªŒè¯äº†ä¸Šä¸‹æ–‡ä¿¡æ¯èƒ½æœ‰æ•ˆæå‡éè‹±è¯­å›ç­”çš„æ–‡åŒ–ç›¸å…³æ€§ä¸äº‹å®å¯¹é½ï¼Œä»è€Œæ¨åŠ¨æ›´å…¬å¹³çš„å¤šè¯­è¨€åŒ»ç–—AIç³»ç»Ÿå‘å±•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.17354v1">Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for
enhancing large language models (LLMs) by retrieving relevant documents from an
external corpus. However, existing RAG systems primarily focus on unimodal text
documents, and often fall short in real-world scenarios where both queries and
documents may contain mixed modalities (such as text and images). In this
paper, we address the challenge of Universal Retrieval-Augmented Generation
(URAG), which involves retrieving and reasoning over mixed-modal information to
improve vision-language generation. To this end, we propose Nyx, a unified
mixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate
the scarcity of realistic mixed-modal data, we introduce a four-stage automated
pipeline for generation and filtering, leveraging web documents to construct
NyxQA, a dataset comprising diverse mixed-modal question-answer pairs that
better reflect real-world information needs. Building on this high-quality
dataset, we adopt a two-stage training framework for Nyx: we first perform
pre-training on NyxQA along with a variety of open-source retrieval datasets,
followed by supervised fine-tuning using feedback from downstream
vision-language models (VLMs) to align retrieval outputs with generative
preferences. Experimental results demonstrate that Nyx not only performs
competitively on standard text-only RAG benchmarks, but also excels in the more
general and realistic URAG setting, significantly improving generation quality
in vision-language tasks.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºNyxçš„ç»Ÿä¸€æ··åˆæ¨¡æ€æ£€ç´¢å™¨ï¼Œç”¨äºè§£å†³é€šç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆURAGï¼‰ä¸­å¤šæ¨¡æ€ï¼ˆå¦‚æ–‡æœ¬å’Œå›¾åƒï¼‰ä¿¡æ¯çš„æ£€ç´¢ä¸æ¨ç†é—®é¢˜ï¼Œå¹¶é€šè¿‡è‡ªåŠ¨æ„å»ºæ•°æ®é›†å’Œæ”¹è¿›è®­ç»ƒæ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰è¯­è¨€ä»»åŠ¡çš„ç”Ÿæˆè´¨é‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.17309v1">RubiSCoT: A Framework for AI-Supported Academic Assessment</a></td><td><details><summary>å±•å¼€</summary>The evaluation of academic theses is a cornerstone of higher education,
ensuring rigor and integrity. Traditional methods, though effective, are
time-consuming and subject to evaluator variability. This paper presents
RubiSCoT, an AI-supported framework designed to enhance thesis evaluation from
proposal to final submission. Using advanced natural language processing
techniques, including large language models, retrieval-augmented generation,
and structured chain-of-thought prompting, RubiSCoT offers a consistent,
scalable solution. The framework includes preliminary assessments,
multidimensional assessments, content extraction, rubric-based scoring, and
detailed reporting. We present the design and implementation of RubiSCoT,
discussing its potential to optimize academic assessment processes through
consistent, scalable, and transparent evaluation.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ä»‹ç»äº†RubiSCoTæ¡†æ¶ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œæ€ç»´é“¾æç¤ºç­‰æŠ€æœ¯ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€å¤„ç†æå‡å­¦æœ¯è®ºæ–‡è¯„ä¼°çš„æ•ˆç‡å’Œä¸€è‡´æ€§ï¼Œæ¶µç›–ä»ææ¡ˆåˆ°ç»ˆç¨¿çš„å…¨æµç¨‹è‡ªåŠ¨åŒ–è¯„åˆ†ä¸æŠ¥å‘ŠåŠŸèƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.17301v1">Comprehending Spatio-temporal Data via Cinematic Storytelling using Large Language Models</a></td><td><details><summary>å±•å¼€</summary>Spatio-temporal data captures complex dynamics across both space and time,
yet traditional visualizations are complex, require domain expertise and often
fail to resonate with broader audiences. Here, we propose MapMuse, a
storytelling-based framework for interpreting spatio-temporal datasets,
transforming them into compelling, narrative-driven experiences. We utilize
large language models and employ retrieval augmented generation (RAG) and
agent-based techniques to generate comprehensive stories. Drawing on principles
common in cinematic storytelling, we emphasize clarity, emotional connection,
and audience-centric design. As a case study, we analyze a dataset of taxi
trajectories. Two perspectives are presented: a captivating story based on a
heat map that visualizes millions of taxi trip endpoints to uncover urban
mobility patterns; and a detailed narrative following a single long taxi
journey, enriched with city landmarks and temporal shifts. By portraying
locations as characters and movement as plot, we argue that data storytelling
drives insight, engagement, and action from spatio-temporal information. The
case study illustrates how MapMuse can bridge the gap between data complexity
and human understanding. The aim of this short paper is to provide a glimpse to
the potential of the cinematic storytelling technique as an effective
communication tool for spatio-temporal data, as well as to describe open
problems and opportunities for future research.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†MapMuseï¼Œä¸€ä¸ªåŸºäºå™äº‹çš„æ•°æ®å¯è§†åŒ–æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œæ™ºèƒ½ä½“æŠ€æœ¯ï¼Œå°†å¤æ‚çš„æ—¶ç©ºæ•°æ®ï¼ˆå¦‚å‡ºç§Ÿè½¦è½¨è¿¹ï¼‰è½¬åŒ–ä¸ºå¼•äººå…¥èƒœçš„æ•…äº‹ï¼Œå¼ºè°ƒæƒ…æ„Ÿè¿æ¥å’Œè§‚ä¼—ç†è§£ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿå¯è§†åŒ–æ–¹æ³•çš„å±€é™æ€§å¹¶æå‡æ•°æ®ä¼ æ’­æ•ˆæœã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.17098v1">Can Transformer Memory Be Corrupted? Investigating Cache-Side Vulnerabilities in Large Language Models</a></td><td><details><summary>å±•å¼€</summary>Even when prompts and parameters are secured, transformer language models
remain vulnerable because their key-value (KV) cache during inference
constitutes an overlooked attack surface. This paper introduces Malicious Token
Injection (MTI), a modular framework that systematically perturbs cached key
vectors at selected layers and timesteps through controlled magnitude and
frequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A
theoretical analysis quantifies how these perturbations propagate through
attention, linking logit deviations to the Frobenius norm of corruption and
softmax Lipschitz dynamics. Empirical results show that MTI significantly
alters next-token distributions and downstream task performance across GPT-2
and LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic
reasoning pipelines. These findings identify cache integrity as a critical yet
underexplored vulnerability in current LLM deployments, positioning cache
corruption as a reproducible and theoretically grounded threat model for future
robustness and security research.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†åä¸ºâ€œæ¶æ„ä»¤ç‰Œæ³¨å…¥â€ï¼ˆMTIï¼‰çš„æ”»å‡»æ¡†æ¶ï¼Œé€šè¿‡å¹²æ‰°Transformerè¯­è¨€æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­çš„é”®å€¼ç¼“å­˜ï¼ˆKV cacheï¼‰ï¼Œåˆ©ç”¨é«˜æ–¯å™ªå£°ã€å½’é›¶å’Œæ­£äº¤æ—‹è½¬ç­‰æ–¹æ³•ç³»ç»Ÿæ€§ç ´åç¼“å­˜æ•°æ®ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒMTIä¸ä»…èƒ½æ˜¾è‘—æ”¹å˜æ¨¡å‹çš„next-tokenåˆ†å¸ƒå’Œä»»åŠ¡æ€§èƒ½ï¼Œè¿˜ä¼šå½±å“æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œæ™ºèƒ½ä»£ç†æ¨ç†æµç¨‹çš„ç¨³å®šæ€§ï¼Œæ­ç¤ºäº†KVç¼“å­˜å®Œæ•´æ€§åœ¨å½“å‰LLMéƒ¨ç½²ä¸­çš„æ½œåœ¨å®‰å…¨æ¼æ´ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.17064v1">A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation</a></td><td><details><summary>å±•å¼€</summary>Single-cell RNA sequencing has transformed our ability to identify diverse
cell types and their transcriptomic signatures. However, annotating these
signatures-especially those involving poorly characterized genes-remains a
major challenge. Traditional methods, such as Gene Set Enrichment Analysis
(GSEA), depend on well-curated annotations and often perform poorly in these
contexts. Large Language Models (LLMs) offer a promising alternative but
struggle to represent complex biological knowledge within structured
ontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID:
https://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that
integrates free-text descriptions with ontology labels to enable more accurate
and robust gene set annotation. By incorporating retrieval-augmented generation
(RAG), we developed a robust agentic workflow that refines predictions using
relevant PubMed literature, reducing hallucinations and enhancing
interpretability. Using this workflow, we achieved correct annotations for 77%
of mouse gene sets among their top predictions. Applying this approach, we
annotated 5,322 brain cell clusters from the comprehensive mouse brain cell
atlas generated by the BRAIN Initiative Cell Census Network, enabling novel
insights into brain cell function by identifying region-specific gene
co-expression patterns and inferring functional roles of gene ensembles.
BRAINCELL-AID also identifies Basal Ganglia-related cell types with
neurologically meaningful descriptions. Hence, we create a valuable resource to
support community-driven cell type annotation.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†BRAINCELL-AIDï¼Œä¸€ä¸ªæ•´åˆè‡ªç”±æ–‡æœ¬æè¿°å’Œæœ¬ä½“æ ‡ç­¾çš„å¤šæ™ºèƒ½ä½“AIç³»ç»Ÿï¼Œåˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ä»PubMedæ–‡çŒ®ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œä»¥æé«˜åŸºå› é›†æ³¨é‡Šçš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ï¼Œå¹¶å°†å…¶åº”ç”¨äºå°é¼ å¤§è„‘ç»†èƒå›¾è°±çš„æ³¨é‡Šã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-19
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.16980v1">Towards Interpretable and Trustworthy Time Series Reasoning: A BlueSky Vision</a></td><td><details><summary>å±•å¼€</summary>Time series reasoning is emerging as the next frontier in temporal analysis,
aiming to move beyond pattern recognition towards explicit, interpretable, and
trustworthy inference. This paper presents a BlueSky vision built on two
complementary directions. One builds robust foundations for time series
reasoning, centered on comprehensive temporal understanding, structured
multi-step reasoning, and faithful evaluation frameworks. The other advances
system-level reasoning, moving beyond language-only explanations by
incorporating multi-agent collaboration, multi-modal context, and
retrieval-augmented approaches. Together, these directions outline a flexible
and extensible framework for advancing time series reasoning, aiming to deliver
interpretable and trustworthy temporal intelligence across diverse domains.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªå…³äºæ—¶é—´åºåˆ—æ¨ç†çš„è“å›¾ï¼Œç»“åˆäº†ç¨³å¥çš„åŸºç¡€æ„å»ºå’Œç³»ç»Ÿçº§æ¨ç†ä¸¤å¤§æ–¹å‘ï¼Œå…¶ä¸­ç‰¹åˆ«æåˆ°äº†é€šè¿‡å¤šæ™ºèƒ½ä½“åä½œã€å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å’Œæ£€ç´¢å¢å¼ºæ–¹æ³•ï¼ˆretrieval-augmented approachesï¼‰æ¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚è¿™è¡¨æ˜è¯¥ç ”ç©¶åœ¨ä¸€å®šç¨‹åº¦ä¸Šæ¶‰åŠäº†RAGæŠ€æœ¯ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.16724v1">A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications</a></td><td><details><summary>å±•å¼€</summary>The advent of large language models (LLMs) has transformed information access
and reasoning through open-ended natural language interaction. However, LLMs
remain limited by static knowledge, factual hallucinations, and the inability
to retrieve real-time or domain-specific information. Retrieval-Augmented
Generation (RAG) mitigates these issues by grounding model outputs in external
evidence, but traditional RAG pipelines are often single turn and heuristic,
lacking adaptive control over retrieval and reasoning. Recent advances in
agentic search address these limitations by enabling LLMs to plan, retrieve,
and reflect through multi-step interaction with search environments. Within
this paradigm, reinforcement learning (RL) offers a powerful mechanism for
adaptive and self-improving search behavior. This survey provides the first
comprehensive overview of \emph{RL-based agentic search}, organizing the
emerging field along three complementary dimensions: (i) What RL is for
(functional roles), (ii) How RL is used (optimization strategies), and (iii)
Where RL is applied (scope of optimization). We summarize representative
methods, evaluation protocols, and applications, and discuss open challenges
and future directions toward building reliable and scalable RL driven agentic
search systems. We hope this survey will inspire future research on the
integration of RL and agentic search. Our repository is available at
https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„è‡ªä¸»æœç´¢ï¼ˆRL-based agentic searchï¼‰å¦‚ä½•ä¼˜åŒ–ä¼ ç»Ÿçš„RAGæ¡†æ¶ï¼Œé€šè¿‡å¤šæ­¥äº¤äº’å’Œè‡ªé€‚åº”æ§åˆ¶è§£å†³å•ä¸€æ£€ç´¢å›åˆå’Œå¯å‘å¼æ–¹æ³•çš„ä¸è¶³ï¼Œæ—¨åœ¨æå‡æ£€ç´¢ä¸æ¨ç†çš„åŠ¨æ€æ€§å’Œå¯é æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.16715v1">Right Answer at the Right Time - Temporal Retrieval-Augmented Generation via Graph Summarization</a></td><td><details><summary>å±•å¼€</summary>Question answering in temporal knowledge graphs requires retrieval that is
both time-consistent and efficient. Existing RAG methods are largely semantic
and typically neglect explicit temporal constraints, which leads to
time-inconsistent answers and inflated token usage. We propose STAR-RAG, a
temporal GraphRAG framework that relies on two key ideas: building a
time-aligned rule graph and conducting propagation on this graph to narrow the
search space and prioritize semantically relevant, time-consistent evidence.
This design enforces temporal proximity during retrieval, reduces the candidate
set of retrieval results, and lowers token consumption without sacrificing
accuracy. Compared with existing temporal RAG approaches, STAR-RAG eliminates
the need for heavy model training and fine-tuning, thereby reducing
computational cost and significantly simplifying deployment.Extensive
experiments on real-world temporal KG datasets show that our method achieves
improved answer accuracy while consuming fewer tokens than strong GraphRAG
baselines.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†STAR-RAGï¼Œä¸€ç§é’ˆå¯¹æ—¶åºçŸ¥è¯†å›¾è°±çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡æ„å»ºæ—¶é—´å¯¹é½çš„è§„åˆ™å›¾è°±å’Œåœ¨å…¶ä¸Šè¿›è¡Œä¼ æ’­æ¥ä¼˜åŒ–æ£€ç´¢è¿‡ç¨‹ï¼Œç¡®ä¿æ—¶é—´ä¸€è‡´æ€§å’Œé«˜æ•ˆæ€§ï¼ŒåŒæ—¶å‡å°‘ä»¤ç‰Œæ¶ˆè€—ï¼Œæ— éœ€å¤æ‚æ¨¡å‹è®­ç»ƒå³å¯æå‡ç­”æ¡ˆå‡†ç¡®æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.16695v1">Resolution-Aware Retrieval Augmented Zero-Shot Forecasting</a></td><td><details><summary>å±•å¼€</summary>Zero-shot forecasting aims to predict outcomes for previously unseen
conditions without direct historical data, posing a significant challenge for
traditional forecasting methods. We introduce a Resolution-Aware
Retrieval-Augmented Forecasting model that enhances predictive accuracy by
leveraging spatial correlations and temporal frequency characteristics. By
decomposing signals into different frequency components, our model employs
resolution-aware retrieval, where lower-frequency components rely on broader
spatial context, while higher-frequency components focus on local influences.
This allows the model to dynamically retrieve relevant data and adapt to new
locations with minimal historical context.
  Applied to microclimate forecasting, our model significantly outperforms
traditional forecasting methods, numerical weather prediction models, and
modern foundation time series models, achieving 71% lower MSE than HRRR and 34%
lower MSE than Chronos on the ERA5 dataset.
  Our results highlight the effectiveness of retrieval-augmented and
resolution-aware strategies, offering a scalable and data-efficient solution
for zero-shot forecasting in microclimate modeling and beyond.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåˆ†è¾¨ç‡æ„ŸçŸ¥çš„æ£€ç´¢å¢å¼ºé¢„æµ‹æ¨¡å‹ï¼Œé€šè¿‡åˆ†è§£ä¿¡å·é¢‘ç‡å¹¶ç»“åˆç©ºé—´ç›¸å…³æ€§è¿›è¡ŒåŠ¨æ€æ•°æ®æ£€ç´¢ï¼Œæ˜¾è‘—æå‡äº†é›¶æ ·æœ¬å¾®æ°”å€™é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œåœ¨ERA5æ•°æ®é›†ä¸Šä¼˜äºä¼ ç»Ÿæ–¹æ³•å’Œç°ä»£æ—¶åºæ¨¡å‹ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-18
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.16643v1">Structured Interfaces for Automated Reasoning with 3D Scene Graphs</a></td><td><details><summary>å±•å¼€</summary>In order to provide a robot with the ability to understand and react to a
user's natural language inputs, the natural language must be connected to the
robot's underlying representations of the world. Recently, large language
models (LLMs) and 3D scene graphs (3DSGs) have become a popular choice for
grounding natural language and representing the world. In this work, we address
the challenge of using LLMs with 3DSGs to ground natural language. Existing
methods encode the scene graph as serialized text within the LLM's context
window, but this encoding does not scale to large or rich 3DSGs. Instead, we
propose to use a form of Retrieval Augmented Generation to select a subset of
the 3DSG relevant to the task. We encode a 3DSG in a graph database and provide
a query language interface (Cypher) as a tool to the LLM with which it can
retrieve relevant data for language grounding. We evaluate our approach on
instruction following and scene question-answering tasks and compare against
baseline context window and code generation methods. Our results show that
using Cypher as an interface to 3D scene graphs scales significantly better to
large, rich graphs on both local and cloud-based models. This leads to large
performance improvements in grounded language tasks while also substantially
reducing the token count of the scene graph content. A video supplement is
available at https://www.youtube.com/watch?v=zY_YI9giZSA.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºRAGçš„æ–¹æ³•ï¼Œåˆ©ç”¨å›¾æ•°æ®åº“ï¼ˆCypheræŸ¥è¯¢è¯­è¨€ï¼‰æ£€ç´¢3Dåœºæ™¯å›¾ä¸­ä¸ä»»åŠ¡ç›¸å…³çš„å­é›†ï¼Œæ›¿ä»£ä¼ ç»Ÿçš„å¤§è¯­è¨€æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£ç¼–ç æ–¹å¼ï¼Œä»¥æé«˜è‡ªç„¶è¯­è¨€åœ¨æœºå™¨äººä»»åŠ¡ä¸­çš„ç†è§£èƒ½åŠ›ï¼Œå¹¶åœ¨æŒ‡ä»¤è·Ÿéšå’Œåœºæ™¯é—®ç­”ä»»åŠ¡ä¸­éªŒè¯äº†å…¶æ‰©å±•æ€§å’Œæ€§èƒ½ä¼˜åŠ¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.16609v1">Prior Makes It Possible: From Sublinear Graph Algorithms to LLM Test-Time Methods</a></td><td><details><summary>å±•å¼€</summary>Test-time augmentation, such as Retrieval-Augmented Generation (RAG) or tool
use, critically depends on an interplay between a model's parametric knowledge
and externally retrieved information. However, the theoretical underpinnings of
this relationship remain poorly understood. Specifically, it is not clear how
much pre-training knowledge is required to answer queries with a small number
of augmentation steps, which is a desirable property in practice. To address
this question, we formulate multi-step reasoning as an $s$-$t$ connectivity
problem on a knowledge graph. We represent a model's pre-training parametric
knowledge as a partial, potentially noisy subgraph. We view augmentation as
querying an oracle for true edges that augment the model's knowledge. Then, we
characterize the necessary and sufficient number of augmentation steps for the
model to generate an accurate answer given partial prior knowledge. One key
result shows a phase transition: if the prior knowledge graph over $n$ vertices
is disconnected into small components, then finding a path via augmentation is
inefficient and requires $\Omega(\sqrt{n})$ queries. On the other hand, once
the density of correct knowledge surpasses a threshold, forming a giant
component, we can find paths with an expected constant number of queries.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†RAGç­‰æŠ€æœ¯ä¸­æ¨¡å‹é¢„è®­ç»ƒçŸ¥è¯†ä¸å¤–éƒ¨æ£€ç´¢ä¿¡æ¯ä¹‹é—´çš„ç†è®ºå…³ç³»ï¼Œå°†å¤šæ­¥æ¨ç†å»ºæ¨¡ä¸ºçŸ¥è¯†å›¾ä¸Šçš„è¿é€šæ€§é—®é¢˜ï¼Œåˆ†æäº†å…ˆéªŒçŸ¥è¯†å¯†åº¦å’Œå¢å¼ºæ­¥éª¤æ•°é‡å¯¹ç­”æ¡ˆå‡†ç¡®æ€§çš„å½±å“ï¼Œå¹¶æ­ç¤ºäº†çŸ¥è¯†å›¾è¿æ¥æ€§å¯¹æŸ¥è¯¢æ•ˆç‡çš„ä¸´ç•Œç›¸å˜ç°è±¡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.16582v1">Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need?</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances
large language models (LLMs) by providing structured and interpretable external
knowledge. However, existing KG-based RAG methods struggle to retrieve accurate
and diverse information from text-rich KGs for complex real-world queries.
Process Reward Models (PRMs) offer a way to align the retrieval process of
KG-based RAG with query-specific knowledge requirements, but they heavily rely
on process-level supervision signals that are expensive and hard to obtain on
KGs. To address this challenge, we propose GraphFlow, a framework that
efficiently retrieves accurate and diverse knowledge required for real-world
queries from text-rich KGs. GraphFlow employs a transition-based flow matching
objective to jointly optimize a retrieval policy and a flow estimator. The flow
estimator factorizes the reward of the retrieval outcome into the intermediate
retrieval states. Such reward factorization guides the retrieval policy to
retrieve candidates from KGs in proportion to their reward. This allows
GraphFlow to explore high-quality regions of KGs that yield diverse and
relevant results. We evaluate GraphFlow on the STaRK benchmark, which includes
real-world queries from multiple domains over text-rich KGs. GraphFlow
outperforms strong KG-RAG baselines, including GPT-4o, by 10% on average in hit
rate and recall. It also shows strong generalization to unseen KGs,
demonstrating its effectiveness and robustness.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºGraphFlowçš„æ¡†æ¶ï¼Œé€šè¿‡åŸºäºçŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ï¼Œä¼˜åŒ–äº†ä»æ–‡æœ¬ä¸°å¯Œçš„KGä¸­æ£€ç´¢å‡†ç¡®ä¸”å¤šæ ·åŒ–çŸ¥è¯†çš„è¿‡ç¨‹ã€‚GraphFlowåˆ©ç”¨è½¬ç§»æµåŒ¹é…ç›®æ ‡è”åˆä¼˜åŒ–æ£€ç´¢ç­–ç•¥å’Œæµä¼°è®¡å™¨ï¼Œå°†æ£€ç´¢ç»“æœçš„å¥–åŠ±åˆ†è§£åˆ°ä¸­é—´çŠ¶æ€ï¼Œä»è€Œåœ¨STaRKåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰KG-RAGåŸºçº¿ï¼ˆåŒ…æ‹¬GPT-4oï¼‰ï¼Œå¹¶åœ¨å‘½ä¸­ç‡å’Œå¬å›ç‡ä¸Šå¹³å‡æå‡10%ï¼ŒåŒæ—¶å±•ç°å‡ºå¯¹æœªè§KGçš„å¼ºæ³›åŒ–èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.16392v1">RGMem: Renormalization Group-based Memory Evolution for Language Agent User Profile</a></td><td><details><summary>å±•å¼€</summary>Personalized and continuous interactions are the key to enhancing user
experience in today's large language model (LLM)-based conversational systems,
however, the finite context windows and static parametric memory make it
difficult to model the cross-session long-term user states and behavioral
consistency. Currently, the existing solutions to this predicament, such as
retrieval-augmented generation (RAG) and explicit memory systems, primarily
focus on fact-level storage and retrieval, lacking the capability to distill
latent preferences and deep traits from the multi-turn dialogues, which limits
the long-term and effective user modeling, directly leading to the personalized
interactions remaining shallow, and hindering the cross-session continuity. To
realize the long-term memory and behavioral consistency for Language Agents in
LLM era, we propose a self-evolving memory framework RGMem, inspired by the
ideology of classic renormalization group (RG) in physics, this framework
enables to organize the dialogue history in multiple scales: it first extracts
semantics and user insights from episodic fragments, then through hierarchical
coarse-graining and rescaling operations, progressively forms a
dynamically-evolved user profile. The core innovation of our work lies in
modeling memory evolution as a multi-scale process of information compression
and emergence, which accomplishes the high-level and accurate user profiles
from noisy and microscopic-level interactions.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRGMemçš„è‡ªæˆ‘è¿›åŒ–è®°å¿†æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¯¹è¯ç³»ç»Ÿä¸­é•¿æœŸç”¨æˆ·çŠ¶æ€å’Œè¡Œä¸ºä¸€è‡´æ€§çš„å»ºæ¨¡é—®é¢˜ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•ï¼ˆå¦‚RAGå’Œæ˜¾å¼è®°å¿†ç³»ç»Ÿï¼‰ä¸»è¦å…³æ³¨äº‹å®çº§åˆ«çš„å­˜å‚¨å’Œæ£€ç´¢ï¼Œä½†RGMemé€šè¿‡å¤šå°ºåº¦å¯¹è¯å†å²ç»„ç»‡å’Œåˆ†å±‚ç²—ç²’åº¦åŒ–æ“ä½œï¼Œèƒ½å¤Ÿä»å¤šè½®å¯¹è¯ä¸­æç‚¼æ½œåœ¨åå¥½å’Œæ·±å±‚ç‰¹å¾ï¼Œä»è€Œå®ç°æ›´é«˜å±‚æ¬¡å’Œå‡†ç¡®çš„ç”¨æˆ·ç”»åƒï¼Œæå‡ä¸ªæ€§åŒ–äº¤äº’çš„æ·±åº¦å’Œè·¨ä¼šè¯è¿ç»­æ€§ã€‚å› æ­¤ï¼ŒRGMemå¯¹RAGæŠ€æœ¯è¿›è¡Œäº†æ‰©å±•å’Œæ”¹è¿›ï¼Œå±äºRAGç›¸å…³çš„ç ”ç©¶ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.16302v1">DTKG: Dual-Track Knowledge Graph-Verified Reasoning Framework for Multi-Hop QA</a></td><td><details><summary>å±•å¼€</summary>Multi-hop reasoning for question answering (QA) plays a critical role in
retrieval-augmented generation (RAG) for modern large language models (LLMs).
The accurate answer can be obtained through retrieving relational structure of
entities from knowledge graph (KG). Regarding the inherent relation-dependency
and reasoning pattern, multi-hop reasoning can be in general classified into
two categories: i) parallel fact-verification multi-hop reasoning question,
i.e., requiring simultaneous verifications of multiple independent
sub-questions; and ii) chained multi-hop reasoning questions, i.e., demanding
sequential multi-step inference with intermediate conclusions serving as
essential premises for subsequent reasoning. Currently, the multi-hop reasoning
approaches singly employ one of two techniques: LLM response-based fact
verification and KG path-based chain construction. Nevertheless, the former
excels at parallel fact-verification but underperforms on chained reasoning
tasks, while the latter demonstrates proficiency in chained multi-hop reasoning
but suffers from redundant path retrieval when handling parallel
fact-verification reasoning. These limitations deteriorate the efficiency and
accuracy for multi-hop QA tasks. To address this challenge, we propose a novel
dual-track KG verification and reasoning framework DTKG, which is inspired by
the Dual Process Theory in cognitive science. Specifically, DTKG comprises two
main stages: the Classification Stage and the Branch Processing Stage.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åœ¨å¤šè·³æ¨ç†é—®ç­”ï¼ˆQAï¼‰ä»»åŠ¡ä¸­å¦‚ä½•é€šè¿‡çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰æå‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ•ˆæœï¼Œæå‡ºäº†ä¸€ç§åŒè½¨æ¡†æ¶DTKGï¼Œç»“åˆäº†LLMçš„äº‹å®éªŒè¯å’ŒKGè·¯å¾„æ„å»ºï¼Œä»¥ä¼˜åŒ–å¹¶è¡Œäº‹å®éªŒè¯å’Œé“¾å¼å¤šè·³æ¨ç†çš„æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-17
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.15828v1">GENESIS: A Generative Model of Episodic-Semantic Interaction</a></td><td><details><summary>å±•å¼€</summary>A central challenge in cognitive neuroscience is to explain how semantic and
episodic memory, two major forms of declarative memory, typically associated
with cortical and hippocampal processing, interact to support learning, recall,
and imagination. Despite significant advances, we still lack a unified
computational framework that jointly accounts for core empirical phenomena
across both semantic and episodic processing domains. Here, we introduce the
Generative Episodic-Semantic Integration System (GENESIS), a computational
model that formalizes memory as the interaction between two limited-capacity
generative systems: a Cortical-VAE, supporting semantic learning and
generalization, and a Hippocampal-VAE, supporting episodic encoding and
retrieval within a retrieval-augmented generation (RAG) architecture. GENESIS
reproduces hallmark behavioral findings, including generalization in semantic
memory, recognition, serial recall effects and gist-based distortions in
episodic memory, and constructive episodic simulation, while capturing their
dynamic interactions. The model elucidates how capacity constraints shape the
fidelity and memorability of experiences, how semantic processing introduces
systematic distortions in episodic recall, and how episodic replay can
recombine previous experiences. Together, these results provide a principled
account of memory as an active, constructive, and resource-bounded process.
GENESIS thus advances a unified theoretical framework that bridges semantic and
episodic memory, offering new insights into the generative foundations of human
cognition.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†GENESISæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡ç»“åˆè¯­ä¹‰è®°å¿†ï¼ˆCortical-VAEï¼‰å’Œæƒ…æ™¯è®°å¿†ï¼ˆHippocampal-VAEï¼‰çš„è®¡ç®—æ¡†æ¶ï¼Œåˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¶æ„æ”¯æŒè®°å¿†çš„ç¼–ç ã€æ£€ç´¢å’Œæƒ³è±¡ï¼Œæ­ç¤ºäº†è®°å¿†çš„ç”Ÿæˆæ€§ã€ä¸»åŠ¨æ€§å’Œèµ„æºé™åˆ¶ç‰¹æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.15782v1">Demo: Guide-RAG: Evidence-Driven Corpus Curation for Retrieval-Augmented Generation in Long COVID</a></td><td><details><summary>å±•å¼€</summary>As AI chatbots gain adoption in clinical medicine, developing effective
frameworks for complex, emerging diseases presents significant challenges. We
developed and evaluated six Retrieval-Augmented Generation (RAG) corpus
configurations for Long COVID (LC) clinical question answering, ranging from
expert-curated sources to large-scale literature databases. Our evaluation
employed an LLM-as-a-judge framework across faithfulness, relevance, and
comprehensiveness metrics using LongCOVID-CQ, a novel dataset of
expert-generated clinical questions. Our RAG corpus configuration combining
clinical guidelines with high-quality systematic reviews consistently
outperformed both narrow single-guideline approaches and large-scale literature
databases. Our findings suggest that for emerging diseases, retrieval grounded
in curated secondary reviews provides an optimal balance between narrow
consensus documents and unfiltered primary literature, supporting clinical
decision-making while avoiding information overload and oversimplified
guidance. We propose Guide-RAG, a chatbot system and accompanying evaluation
framework that integrates both curated expert knowledge and comprehensive
literature databases to effectively answer LC clinical questions.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ç ”ç©¶å’Œè¯„ä¼°äº†å…­ç§é’ˆå¯¹é•¿æœŸæ–°å† è‚ºç‚ï¼ˆLong COVIDï¼‰ä¸´åºŠé—®ç­”çš„RAGè¯­æ–™åº“é…ç½®ï¼Œæå‡ºç»“åˆä¸´åºŠæŒ‡å—ä¸é«˜è´¨é‡ç³»ç»Ÿè¯„ä»·çš„é…ç½®æ•ˆæœæœ€ä½³ï¼Œå¹¶å¼€å‘äº†åä¸ºGuide-RAGçš„èŠå¤©æœºå™¨äººç³»ç»Ÿï¼Œæ•´åˆä¸“å®¶çŸ¥è¯†åº“ä¸æ–‡çŒ®æ•°æ®åº“ä»¥ä¼˜åŒ–ä¸´åºŠå†³ç­–æ”¯æŒã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.15722v1">The 3rd Place Solution of CCIR CUP 2025: A Framework for Retrieval-Augmented Generation in Multi-Turn Legal Conversation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation has made significant progress in the field of
natural language processing. By combining the advantages of information
retrieval and large language models, RAG can generate relevant and contextually
appropriate responses based on items retrieved from reliable sources. This
technology has demonstrated outstanding performance across multiple domains,
but its application in the legal field remains in its exploratory phase. In
this paper, we introduce our approach for "Legal Knowledge Retrieval and
Generation" in CCIR CUP 2025, which leverages large language models and
information retrieval systems to provide responses based on laws in response to
user questions.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æ¢è®¨äº†RAGæŠ€æœ¯åœ¨æ³•å¾‹é¢†åŸŸçš„åº”ç”¨ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå¤§è¯­è¨€æ¨¡å‹å’Œä¿¡æ¯æ£€ç´¢ç³»ç»Ÿçš„æ–¹æ³•ï¼Œç”¨äºåŸºäºæ³•å¾‹æ¡æ–‡ç”Ÿæˆå›ç­”ç”¨æˆ·é—®é¢˜çš„å“åº”ï¼Œå¹¶ä»‹ç»äº†åœ¨CCIR CUP 2025ä¸­çš„â€œLegal Knowledge Retrieval and Generationâ€æ–¹æ¡ˆã€‚ç›®å‰è¯¥æŠ€æœ¯åœ¨æ³•å¾‹é¢†åŸŸçš„åº”ç”¨ä»å¤„äºæ¢ç´¢é˜¶æ®µã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.15719v1">Cost-Aware Retrieval-Augmentation Reasoning Models with Adaptive Retrieval Depth</a></td><td><details><summary>å±•å¼€</summary>Reasoning models have gained significant attention due to their strong
performance, particularly when enhanced with retrieval augmentation. However,
these models often incur high computational costs, as both retrieval and
reasoning tokens contribute substantially to the overall resource usage. In
this work, we make the following contributions: (1) we propose a
retrieval-augmented reasoning model that dynamically adjusts the length of the
retrieved document list based on the query and retrieval results; (2) we
develop a cost-aware advantage function for training of efficient
retrieval-augmented reasoning models through reinforcement learning; and (3) we
explore both memory- and latency-bound implementations of the proposed
cost-aware framework for both proximal and group relative policy optimization
algorithms. We evaluate our approach on seven public question answering
datasets and demonstrate significant efficiency gains, without compromising
effectiveness. In fact, we observed that the model latency decreases by ~16-20%
across datasets, while its effectiveness increases by ~5% on average, in terms
of exact match.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŠ¨æ€è°ƒæ•´æ£€ç´¢æ–‡æ¡£é•¿åº¦çš„æ£€ç´¢å¢å¼ºæ¨ç†æ¨¡å‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒæˆæœ¬æ„ŸçŸ¥ä¼˜åŠ¿å‡½æ•°ä»¥æé«˜æ•ˆç‡ï¼Œå¹¶åœ¨å¤šä¸ªé—®ç­”æ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶åœ¨é™ä½å»¶è¿Ÿ16-20%çš„åŒæ—¶æå‡å‡†ç¡®ç‡çº¦5%çš„æ•ˆæœï¼Œæœªç‰ºç‰²æ¨¡å‹æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.15682v1">SQuAI: Scientific Question-Answering with Multi-Agent Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>We present SQuAI (https://squai.scads.ai/), a scalable and trustworthy
multi-agent retrieval-augmented generation (RAG) framework for scientific
question answering (QA) with large language models (LLMs). SQuAI addresses key
limitations of existing RAG systems in the scholarly domain, where complex,
open-domain questions demand accurate answers, explicit claims with citations,
and retrieval across millions of scientific documents. Built on over 2.3
million full-text papers from arXiv.org, SQuAI employs four collaborative
agents to decompose complex questions into sub-questions, retrieve targeted
evidence via hybrid sparse-dense retrieval, and adaptively filter documents to
improve contextual relevance. To ensure faithfulness and traceability, SQuAI
integrates in-line citations for each generated claim and provides supporting
sentences from the source documents. Our system improves faithfulness, answer
relevance, and contextual relevance by up to +0.088 (12%) over a strong RAG
baseline. We further release a benchmark of 1,000 scientific
question-answer-evidence triplets to support reproducibility. With transparent
reasoning, verifiable citations, and domain-wide scalability, SQuAI
demonstrates how multi-agent RAG enables more trustworthy scientific QA with
LLMs.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSQuAIçš„å¯æ‰©å±•ä¸”å¯ä¿¡çš„å¤šæ™ºèƒ½ä½“æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ï¼Œç”¨äºåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ç§‘å­¦é—®ç­”ï¼ˆQAï¼‰ã€‚SQuAIé’ˆå¯¹å­¦æœ¯é¢†åŸŸç°æœ‰RAGç³»ç»Ÿçš„å±€é™æ€§ï¼Œé€šè¿‡åˆ†è§£å¤æ‚é—®é¢˜ã€æ··åˆç¨€ç–-å¯†é›†æ£€ç´¢ã€è‡ªé€‚åº”è¿‡æ»¤æ–‡æ¡£ç­‰æŠ€æœ¯æé«˜å›ç­”çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ï¼Œå¹¶ä¸ºæ¯ä¸ªç”Ÿæˆçš„ä¸»å¼ æä¾›å†…è”å¼•ç”¨å’Œæ¥æºæ”¯æŒã€‚å®éªŒè¡¨æ˜ï¼ŒSQuAIåœ¨å¿ å®æ€§ã€ç­”æ¡ˆç›¸å…³æ€§å’Œä¸Šä¸‹æ–‡ç›¸å…³æ€§ä¸Šæ¯”åŸºçº¿RAGç³»ç»Ÿæå‡äº†12%ï¼ŒåŒæ—¶å‘å¸ƒäº†åŒ…å«1000ä¸ªç§‘å­¦é—®ç­”è¯æ®ä¸‰å…ƒç»„çš„åŸºå‡†æ•°æ®é›†ä»¥æ”¯æŒå¯å¤ç°æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.15681v1">ProofBridge: Auto-Formalization of Natural Language Proofs in Lean via Joint Embeddings</a></td><td><details><summary>å±•å¼€</summary>Translating human-written mathematical theorems and proofs from natural
language (NL) into formal languages (FLs) like Lean 4 has long been a
significant challenge for AI. Most state-of-the-art methods address this
separately, first translating theorems and then generating proofs, creating a
fundamental disconnect vis-a-vis true proof auto-formalization. This two-step
process and its limitations were evident even in AlphaProof's silver-medal
performance at the 2024 IMO, where problem statements needed manual translation
before automated proof synthesis.
  We present ProofBridge, a unified framework for automatically translating
entire NL theorems and proofs into Lean 4. At its core is a joint embedding
model that aligns NL and FL (NL-FL) theorem-proof pairs in a shared semantic
space, enabling cross-modal retrieval of semantically relevant FL examples to
guide translation. Our training ensures that NL-FL theorems (and their proofs)
are mapped close together in this space if and only if the NL-FL pairs are
semantically equivalent. ProofBridge integrates retrieval-augmented fine-tuning
with iterative proof repair, leveraging Lean's type checker and semantic
equivalence feedback to ensure both syntactic correctness and semantic
fidelity. Experiments show substantial improvements in proof auto-formalization
over strong baselines (including GPT-5, Gemini-2.5, Kimina-Prover,
DeepSeek-Prover), with our retrieval-augmented approach yielding significant
gains in semantic correctness (SC, via proving bi-directional equivalence) and
type correctness (TC, via type-checking theorem+proof) across pass@k metrics on
miniF2F-Test-PF, a dataset we curated. In particular, ProofBridge improves
cross-modal retrieval quality by up to 3.28x Recall@1 over all-MiniLM-L6-v2,
and achieves +31.14% SC and +1.64% TC (pass@32) compared to the baseline
Kimina-Prover-RL-1.7B.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ProofBridgeï¼Œä¸€ä¸ªå°†è‡ªç„¶è¯­è¨€æ•°å­¦å®šç†å’Œè¯æ˜è‡ªåŠ¨ç¿»è¯‘ä¸ºLean 4çš„ç»Ÿä¸€æ¡†æ¶ã€‚å®ƒé€šè¿‡è”åˆåµŒå…¥æ¨¡å‹å¯¹é½è‡ªç„¶è¯­è¨€å’Œå½¢å¼è¯­è¨€çš„è¯­ä¹‰ç©ºé—´ï¼Œåˆ©ç”¨æ£€ç´¢å¢å¼ºçš„å¾®è°ƒå’Œè¿­ä»£è¯æ˜ä¿®å¤æ¥æé«˜ç¿»è¯‘çš„å‡†ç¡®æ€§å’Œè¯­ä¹‰ä¿çœŸåº¦ï¼Œå®éªŒæ˜¾ç¤ºåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.15620v1">GRATING: Low-Latency and Memory-Efficient Semantic Selection on Device</a></td><td><details><summary>å±•å¼€</summary>Semantic top-K selection with cross-encoder rerankers underpins of on-device
AI services, such as retrieval-augmented generation, agent memory, and
personalized recommendation. However, its latency and memory demands dominate
end-to-end budgets on edge hardware. Revisiting the objective of top-K
selection, we reveal that only relative rankings matter, not exact
per-candidate scores. We further observe sequence-level sparsity: relative
rankings stabilize early in intermediate layers, allowing pruning opportunities
prior to completing full inference.
  Building on this insight, we propose monolithic forwarding and develop a
training-free inference system, GRATING. By maintaining a global view of all
candidates, it reduces latency through progressive cluster pruning. It also
bounds peak memory usage by strategically overlapping I/O with computation via
dual-layer sliding window and chunked execution. We evaluate GRATING against
state-of-the-art baselines on rerankers from 0.6B to 8B parameters across Apple
M2 and RTX 5070. GRATING consistently reduces latency by up to 89.0% and peak
memory by up to 94.9% in microbenchmarks, without any loss in precision. Across
three real-world on-device AI applications, GRATING lowers latency by
11.6%-51.0% and peak memory by 18.6%-77.8%, demonstrating substantial
improvements in efficiency and deployability.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºGRATINGçš„é«˜æ•ˆæ¨ç†ç³»ç»Ÿï¼Œä¸“æ³¨äºä¼˜åŒ–äº¤å‰ç¼–ç å™¨é‡æ’åºå™¨ï¼ˆcross-encoder rerankersï¼‰åœ¨è¯­ä¹‰Top-Ké€‰æ‹©ä¸­çš„è®¡ç®—æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å»¶è¿Ÿå’Œå†…å­˜å ç”¨é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨åºåˆ—çº§ç¨€ç–æ€§å’Œç›¸å¯¹æ’åçš„æ—©æœŸç¨³å®šæ€§ï¼Œå®ç°äº†æ— éœ€è®­ç»ƒå³å¯æ˜¾è‘—é™ä½å»¶è¿Ÿï¼ˆé«˜è¾¾89.0%ï¼‰å’Œå³°å€¼å†…å­˜ï¼ˆé«˜è¾¾94.9%ï¼‰çš„æŠ€æœ¯ï¼Œå¹¶ç›´æ¥åº”ç”¨äºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­‰è®¾å¤‡ç«¯AIæœåŠ¡åœºæ™¯ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.15552v1">Think Parallax: Solving Multi-Hop Problems via Multi-View Knowledge-Graph-Based Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) excel at language understanding but often
hallucinate and struggle with multi-hop reasoning. Knowledge-graph-based
retrieval-augmented generation (KG-RAG) offers grounding, yet most methods rely
on flat embeddings and noisy path exploration. We propose ParallaxRAG, a
framework that symmetrically decouples queries and graph triples into
multi-view spaces, enabling a robust retrieval architecture that explicitly
enforces head diversity while constraining weakly related paths. Central to our
approach is the observation that different attention heads specialize in
semantic relations at distinct reasoning stages, contributing to different hops
of the reasoning chain. This specialization allows ParallaxRAG to construct
cleaner subgraphs and guide LLMs through grounded, step-wise reasoning.
Experiments on WebQSP and CWQ, under our unified, reproducible setup (BGE-M3 +
Llama3.1-8B), demonstrate competitive retrieval and QA performance, alongside
reduced hallucination and good generalization. Our results highlight multi-view
head specialization as a principled direction for knowledge-grounded multi-hop
reasoning. Our implementation will be released as soon as the paper is
accepted.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºParallaxRAGçš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡å¤šè§†è§’ç©ºé—´å¯¹ç§°è§£è€¦æŸ¥è¯¢å’ŒçŸ¥è¯†å›¾è°±ä¸‰å…ƒç»„ï¼Œå¢å¼ºäº†åŸºäºçŸ¥è¯†å›¾è°±çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆKG-RAGï¼‰ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä¸åŒæ³¨æ„åŠ›å¤´åœ¨æ¨ç†é“¾ä¸åŒé˜¶æ®µçš„è¯­ä¹‰å…³ç³» specializationï¼Œæ„å»ºæ›´æ¸…æ™°çš„å­å›¾å¹¶æŒ‡å¯¼å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œé€æ­¥æ¨ç†ï¼Œä»è€Œå‡å°‘å¹»è§‰å¹¶æå‡å¤šè·³æ¨ç†æ€§èƒ½ã€‚å®éªŒåœ¨WebQSPå’ŒCWQæ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶åœ¨æ£€ç´¢å’Œé—®ç­”ä»»åŠ¡ä¸­çš„ç«äº‰åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.15428v1">Fault Cause Identification across Manufacturing Lines through Ontology-Guided and Process-Aware FMEA Graph Learning with LLMs</a></td><td><details><summary>å±•å¼€</summary>Fault cause identification in automated manufacturing lines is challenging
due to the system's complexity, frequent reconfigurations, and the limited
reusability of existing Failure Mode and Effects Analysis (FMEA) knowledge.
Although FMEA worksheets contain valuable expert insights, their reuse across
heterogeneous lines is hindered by natural language variability, inconsistent
terminology, and process differences. To address these limitations, this study
proposes a process-aware framework that enhances FMEA reusability by combining
manufacturing-domain conceptualization with graph neural network (GNN)
reasoning. First, FMEA worksheets from multiple manufacturing lines are
transformed into a unified knowledge graph through ontology-guided large
language model (LLM) extraction, capturing domain concepts such as actions,
states, components, and parameters. Second, a Relational Graph Convolutional
Network (RGCN) with the process-aware scoring function learns embeddings that
respect both semantic relationships and sequential process flows. Finally, link
prediction is employed to infer and rank candidate fault causes consistent with
the target line's process flow.
  A case study on automotive pressure sensor assembly lines demonstrates that
the proposed method outperforms a state-of-the-art retrieval-augmented
generation (RAG) baseline (F1@20 = 0.267) and an RGCN approach (0.400),
achieving the best performance (0.523) in fault cause identification. Ablation
studies confirm the contributions of both LLM-driven domain conceptualization
and process-aware learning. These results indicate that the proposed framework
significantly improves the transferability of FMEA knowledge across
heterogeneous lines, thereby supporting operators in diagnosing failures more
reliably and paving the way for future domain-adaptive LLM applications in
smart manufacturing.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§ç»“åˆåˆ¶é€ ä¸šé¢†åŸŸæ¦‚å¿µåŒ–å’Œå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æ¨ç†çš„æµç¨‹æ„ŸçŸ¥æ¡†æ¶ï¼Œç”¨äºæå‡æ•…éšœæ¨¡å¼ä¸æ•ˆåº”åˆ†æï¼ˆFMEAï¼‰çŸ¥è¯†çš„å¯é‡ç”¨æ€§ï¼Œå¹¶é€šè¿‡çŸ¥è¯†å›¾è°±æ„å»ºå’Œé“¾æ¥é¢„æµ‹å®ç°æ•…éšœåŸå› è¯†åˆ«ã€‚ç ”ç©¶æ˜¾ç¤ºè¯¥æ–¹æ³•ä¼˜äºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åŸºçº¿å’Œå…¶ä»–æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†FMEAçŸ¥è¯†åœ¨å¼‚æ„ç”Ÿäº§çº¿é—´çš„è¿ç§»èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.15418v1">Fine-Tuning MedGemma for Clinical Captioning to Enhance Multimodal RAG over Malaysia CPGs</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation systems are essential for providing fact-based
guidance from Malaysian Clinical Practice Guidelines. However, their
effectiveness with image-based queries is limited, as general Vision-Language
Model captions often lack clinical specificity and factual grounding. This
study proposes and validates a framework to specialize the MedGemma model for
generating high-fidelity captions that serve as superior queries. To overcome
data scarcity, we employ a knowledge distillation pipeline to create a
synthetic dataset across dermatology, fundus, and chest radiography domains,
and fine-tune MedGemma using the parameter-efficient QLoRA method. Performance
was rigorously assessed through a dual framework measuring both classification
accuracy and, via a novel application of the RAGAS framework, caption
faithfulness, relevancy, and correctness. The fine-tuned model demonstrated
substantial improvements in classification performance, while RAGAS evaluation
confirmed significant gains in caption faithfulness and correctness, validating
the models ability to produce reliable, factually grounded descriptions. This
work establishes a robust pipeline for specializing medical VLMs and validates
the resulting model as a high-quality query generator, laying the groundwork
for enhancing multimodal RAG systems in evidence-based clinical decision
support.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºå¹¶éªŒè¯äº†ä¸€ä¸ªä¸“ä¸ºåŒ»å­¦å›¾åƒç”Ÿæˆé«˜ä¿çœŸå­—å¹•çš„æ¡†æ¶ï¼Œæ—¨åœ¨æå‡åŸºäºå›¾åƒæŸ¥è¯¢çš„å¤šæ¨¡æ€RAGç³»ç»Ÿæ€§èƒ½ã€‚é€šè¿‡çŸ¥è¯†è’¸é¦åˆ›å»ºåˆæˆæ•°æ®é›†å¹¶å¾®è°ƒMedGemmaæ¨¡å‹ï¼Œç ”ç©¶æ˜¾è‘—æ”¹è¿›äº†å­—å¹•çš„å‡†ç¡®æ€§å’Œä¸´åºŠç›¸å…³æ€§ï¼Œä¸ºå¾ªè¯ä¸´åºŠå†³ç­–æ”¯æŒä¸­çš„RAGç³»ç»Ÿå¢å¼ºå¥ å®šäº†åŸºç¡€ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.15339v1">AutoGraph-R1: End-to-End Reinforcement Learning for Knowledge Graph Construction</a></td><td><details><summary>å±•å¼€</summary>Building effective knowledge graphs (KGs) for Retrieval-Augmented Generation
(RAG) is pivotal for advancing question answering (QA) systems. However, its
effectiveness is hindered by a fundamental disconnect: the knowledge graph (KG)
construction process is decoupled from its downstream application, yielding
suboptimal graph structures. To bridge this gap, we introduce AutoGraph-R1, the
first framework to directly optimize KG construction for task performance using
Reinforcement Learning (RL). AutoGraph-R1 trains an LLM constructor by framing
graph generation as a policy learning problem, where the reward is derived from
the graph's functional utility in a RAG pipeline. We design two novel,
task-aware reward functions, one for graphs as knowledge carriers and another
as knowledge indices. Across multiple QA benchmarks, AutoGraph-R1 consistently
enables graph RAG methods to achieve significant performance gains over using
task-agnostic baseline graphs. Our work shows it is possible to close the loop
between construction and application, shifting the paradigm from building
intrinsically ``good'' graphs to building demonstrably ``useful'' ones.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†AutoGraph-R1æ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰æ„å»ºè¿‡ç¨‹ï¼Œä½¿å…¶åœ¨RAGæµç¨‹ä¸­æ›´æœ‰æ•ˆåœ°æ”¯æŒé—®ç­”ç³»ç»Ÿï¼Œè®¾è®¡äº†ä¸¤ç§ä»»åŠ¡æ„ŸçŸ¥çš„å¥–åŠ±å‡½æ•°ä»¥æå‡å›¾è°±ä½œä¸ºçŸ¥è¯†è½½ä½“å’Œç´¢å¼•çš„æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†å…¶ä¼˜è¶Šæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.15261v1">AUGUSTUS: An LLM-Driven Multimodal Agent System with Contextualized User Memory</a></td><td><details><summary>å±•å¼€</summary>Riding on the success of LLMs with retrieval-augmented generation (RAG),
there has been a growing interest in augmenting agent systems with external
memory databases. However, the existing systems focus on storing text
information in their memory, ignoring the importance of multimodal signals.
Motivated by the multimodal nature of human memory, we present AUGUSTUS, a
multimodal agent system aligned with the ideas of human memory in cognitive
science. Technically, our system consists of 4 stages connected in a loop: (i)
encode: understanding the inputs; (ii) store in memory: saving important
information; (iii) retrieve: searching for relevant context from memory; and
(iv) act: perform the task. Unlike existing systems that use vector databases,
we propose conceptualizing information into semantic tags and associating the
tags with their context to store them in a graph-structured multimodal
contextual memory for efficient concept-driven retrieval. Our system
outperforms the traditional multimodal RAG approach while being 3.5 times
faster for ImageNet classification and outperforming MemGPT on the MSC
benchmark.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†AUGUSTUSï¼Œä¸€ç§å¤šæ¨¡æ€ä»£ç†ç³»ç»Ÿï¼Œå—åˆ°äººç±»è®°å¿†å¯å‘ï¼Œæå‡ºäº†ä¸€ä¸ªåŒ…å«ç¼–ç ã€å­˜å‚¨ã€æ£€ç´¢å’Œæ‰§è¡Œå››ä¸ªé˜¶æ®µçš„å¾ªç¯æ¡†æ¶ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºå‘é‡æ•°æ®åº“çš„ç³»ç»Ÿä¸åŒï¼Œå®ƒé‡‡ç”¨å›¾ç»“æ„çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡è®°å¿†è¿›è¡Œæ¦‚å¿µé©±åŠ¨æ£€ç´¢ï¼Œåœ¨ImageNetåˆ†ç±»å’ŒMSCåŸºå‡†ä¸Šè¡¨ç°ä¼˜äºä¼ ç»Ÿå¤šæ¨¡æ€RAGæ–¹æ³•å’ŒMemGPTã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.15253v1">Scaling Beyond Context: A Survey of Multimodal Retrieval-Augmented Generation for Document Understanding</a></td><td><details><summary>å±•å¼€</summary>Document understanding is critical for applications from financial analysis
to scientific discovery. Current approaches, whether OCR-based pipelines
feeding Large Language Models (LLMs) or native Multimodal LLMs (MLLMs), face
key limitations: the former loses structural detail, while the latter struggles
with context modeling. Retrieval-Augmented Generation (RAG) helps ground models
in external data, but documents' multimodal nature, i.e., combining text,
tables, charts, and layout, demands a more advanced paradigm: Multimodal RAG.
This approach enables holistic retrieval and reasoning across all modalities,
unlocking comprehensive document intelligence. Recognizing its importance, this
paper presents a systematic survey of Multimodal RAG for document
understanding. We propose a taxonomy based on domain, retrieval modality, and
granularity, and review advances involving graph structures and agentic
frameworks. We also summarize key datasets, benchmarks, and applications, and
highlight open challenges in efficiency, fine-grained representation, and
robustness, providing a roadmap for future progress in document AI.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ˜¯å…³äºå¤šæ¨¡æ€RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰åœ¨æ–‡æ¡£ç†è§£ä¸­çš„åº”ç”¨ï¼Œç³»ç»Ÿæ€§åœ°ç»¼è¿°äº†è¯¥é¢†åŸŸçš„è¿›å±•ã€æå‡ºåˆ†ç±»æ³•ï¼Œæ€»ç»“äº†å…³é”®æ•°æ®é›†å’Œåº”ç”¨ï¼Œå¹¶æŒ‡å‡ºæœªæ¥ç ”ç©¶æ–¹å‘å¦‚æ•ˆç‡å’Œç»†ç²’åº¦è¡¨ç¤ºç­‰æŒ‘æˆ˜ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-16
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.14944v1">MetaBench: A Multi-task Benchmark for Assessing LLMs in Metabolomics</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) have demonstrated remarkable capabilities on
general text; however, their proficiency in specialized scientific domains that
require deep, interconnected knowledge remains largely uncharacterized.
Metabolomics presents unique challenges with its complex biochemical pathways,
heterogeneous identifier systems, and fragmented databases. To systematically
evaluate LLM capabilities in this domain, we introduce MetaBench, the first
benchmark for metabolomics assessment. Curated from authoritative public
resources, MetaBench evaluates five capabilities essential for metabolomics
research: knowledge, understanding, grounding, reasoning, and research. Our
evaluation of 25 open- and closed-source LLMs reveals distinct performance
patterns across metabolomics tasks: while models perform well on text
generation tasks, cross-database identifier grounding remains challenging even
with retrieval augmentation. Model performance also decreases on long-tail
metabolites with sparse annotations. With MetaBench, we provide essential
infrastructure for developing and evaluating metabolomics AI systems, enabling
systematic progress toward reliable computational tools for metabolomics
research.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†MetaBenchï¼Œé¦–ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£è°¢ç»„å­¦é¢†åŸŸèƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡æ¨¡å‹åœ¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†å³ä½¿ä½¿ç”¨äº†æ£€ç´¢å¢å¼ºæŠ€æœ¯ï¼ˆRAGï¼‰ï¼Œè·¨æ•°æ®åº“æ ‡è¯†ç¬¦çš„åŒ¹é…ä»å…·æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶å¯¹æ³¨é‡Šç¨€ç–çš„é•¿å°¾ä»£è°¢ç‰©æ€§èƒ½ä¸‹é™ï¼Œå¼ºè°ƒäº†RAGåœ¨ä¸“ä¸šç§‘å­¦é¢†åŸŸçš„å±€é™æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.14915v1">Harmonizing Diverse Models: A Layer-wise Merging Strategy for Consistent Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) systems leverage Large Language Models
(LLMs) to generate accurate and reliable responses that are grounded in
retrieved context. However, LLMs often generate inconsistent outputs for
semantically equivalent inputs, a problem compounded by the scarcity of
consistency-focused training data and the limitations of current fine-tuning
techniques in enhancing output consistency. We propose a new approach combining
systematic synthetic data generation, triplet loss for better embeddings, and a
novel layer-wise model merging approach. Using consistency-aware weights
derived from intermediate layer activations, our method effectively integrates
knowledge from specialized models. Experimental results how that our merged
model significantly enhances output consistency, achieving a ~47.5\%
improvement in response similarity over the baseline, thus offering a practical
solution for increasing the reliability of an industrial RAG system.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡é’ˆå¯¹RAGç³»ç»Ÿä¸­å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹è¯­ä¹‰ç›¸åŒè¾“å…¥äº§ç”Ÿä¸ä¸€è‡´è¾“å‡ºçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆåˆæˆæ•°æ®ç”Ÿæˆã€ä¸‰å…ƒç»„æŸå¤±æ”¹è¿›åµŒå…¥åŠåˆ†å±‚æ¨¡å‹èåˆçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡ä¸€è‡´æ€§æ„ŸçŸ¥æƒé‡æ•´åˆä¸“ä¸šæ¨¡å‹çŸ¥è¯†ï¼Œå®éªŒæ˜¾ç¤ºåˆå¹¶åçš„æ¨¡å‹ä½¿å“åº”ç›¸ä¼¼æ€§æå‡çº¦47.5%ï¼Œæ˜¾è‘—å¢å¼ºäº†å·¥ä¸šçº§RAGç³»ç»Ÿçš„å¯é æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.14900v1">Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent That Improves Without Labels or Model Updates</a></td><td><details><summary>å±•å¼€</summary>The Enterprise Intelligence Platform must integrate logs from numerous
third-party vendors in order to perform various downstream tasks. However,
vendor documentation is often unavailable at test time. It is either misplaced,
mismatched, poorly formatted, or incomplete, which makes schema mapping
challenging. We introduce a reinforcement learning agent that can self-improve
without labeled examples or model weight updates. During inference, the agent:
1) Identifies ambiguous field-mapping attempts. 2) Generates targeted
web-search queries to gather external evidence. 3) Applies a confidence-based
reward to iteratively refine its mappings. To demonstrate this concept, we
converted Microsoft Defender for Endpoint logs into a common schema. Our method
increased mapping accuracy from 56.4\%(LLM-only) to 72.73\%(RAG) to 93.94\%
over 100 iterations using GPT-4o. At the same time, it reduced the number of
low-confidence mappings requiring expert review by 85\%. This new approach
provides an evidence-driven, transparent method for solving future industry
problems, paving the way for more robust, accountable, scalable, efficient,
flexible, adaptable, and collaborative solutions.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ™ºèƒ½ä»£ç†ï¼Œé€šè¿‡å®æ—¶æ£€ç´¢å¤–éƒ¨è¯æ®ï¼ˆå¦‚ç½‘ç»œæœç´¢ï¼‰è§£å†³ä¼ä¸šæ—¥å¿—æ¨¡å¼æ˜ å°„ä¸­çš„æ¨¡ç³Šæ€§é—®é¢˜ï¼Œå¹¶è¿­ä»£ä¼˜åŒ–æ˜ å°„ç»“æœã€‚å®éªŒè¡¨æ˜ï¼Œç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•æ˜¾è‘—æå‡äº†GPT-4oçš„æ˜ å°„å‡†ç¡®ç‡ï¼ˆä»56.4%æå‡è‡³93.94%ï¼‰ï¼ŒåŒæ—¶å‡å°‘äº†85%éœ€ä¸“å®¶å®¡æ ¸çš„ä½ç½®ä¿¡åº¦æ˜ å°„ï¼Œå‡¸æ˜¾äº†RAGåœ¨æå‡æ¨¡å‹è¯æ®é©±åŠ¨å†³ç­–å’Œé€æ˜åº¦æ–¹é¢çš„ä»·å€¼ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.14629v1">MR.Rec: Synergizing Memory and Reasoning for Personalized Recommendation Assistant with LLMs</a></td><td><details><summary>å±•å¼€</summary>The application of Large Language Models (LLMs) in recommender systems faces
key challenges in delivering deep personalization and intelligent reasoning,
especially for interactive scenarios. Current methods are often constrained by
limited context windows and single-turn reasoning, hindering their ability to
capture dynamic user preferences and proactively reason over recommendation
contexts. To address these limitations, we propose MR.Rec, a novel framework
that synergizes memory and reasoning for LLM-based recommendations. To achieve
personalization, we develop a comprehensive Retrieval-Augmented Generation
(RAG) system that efficiently indexes and retrieves relevant external memory to
enhance LLM personalization capabilities. Furthermore, to enable the synergy
between memory and reasoning, our RAG system goes beyond conventional
query-based retrieval by integrating reasoning enhanced memory retrieval.
Finally, we design a reinforcement learning framework that trains the LLM to
autonomously learn effective strategies for both memory utilization and
reasoning refinement. By combining dynamic memory retrieval with adaptive
reasoning, this approach ensures more accurate, context-aware, and highly
personalized recommendations. Extensive experiments demonstrate that MR.Rec
significantly outperforms state-of-the-art baselines across multiple metrics,
validating its efficacy in delivering intelligent and personalized
recommendations. We will release code and data upon paper notification.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†MR.Recæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿå’Œå¼ºåŒ–å­¦ä¹ ï¼Œå¢å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨èç³»ç»Ÿä¸­çš„è®°å¿†æ£€ç´¢ä¸æ¨ç†èƒ½åŠ›ï¼Œä»¥å®ç°æ›´ç²¾å‡†ã€ä¸ªæ€§åŒ–çš„æ¨èã€‚RAGç”¨äºé«˜æ•ˆç´¢å¼•å’Œæ£€ç´¢å¤–éƒ¨è®°å¿†æ•°æ®ï¼ŒåŒæ—¶å¼•å…¥æ¨ç†å¢å¼ºçš„æ£€ç´¢æœºåˆ¶ï¼Œæœ€ç»ˆé€šè¿‡å®éªŒéªŒè¯å…¶ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.14605v1">Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering</a></td><td><details><summary>å±•å¼€</summary>Knowledge-based visual question answering (KB-VQA) requires visual language
models (VLMs) to integrate visual understanding with external knowledge
retrieval. Although retrieval-augmented generation (RAG) achieves significant
advances in this task by combining knowledge-base querying, it still struggles
with the quality of multimodal queries and the relevance of retrieved results.
To overcome these challenges, we propose a novel three-stage method, termed
Wiki-PRF, including Processing, Retrieval and Filtering stages. The processing
stage dynamically invokes visual tools to extract precise multimodal
information for retrieval. The retrieval stage integrates visual and text
features to achieve multimodal knowledge retrieval. The filtering stage
performs relevance filtering and concentration on retrieval results. To this
end, we introduce a visual language model trained with answer accuracy and
format consistency as reward signals via a reinforcement learning manner. This
enhances the model's reasoning, tool invocation for accurate queries, and
filtering of irrelevant content. Experiments on benchmark datasets (E-VQA and
InfoSeek) show significant improvements~(36.0 and 42.8) in answer quality,
achieving state-of-the-art performance. Code is available at
https://github.com/cqu-student/Wiki-PRF</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºWiki-PRFçš„ä¸‰é˜¶æ®µæ–¹æ³•ï¼ˆå¤„ç†ã€æ£€ç´¢ã€è¿‡æ»¤ï¼‰ï¼Œé€šè¿‡åŠ¨æ€è°ƒç”¨è§†è§‰å·¥å…·æå–å¤šæ¨¡æ€ä¿¡æ¯ï¼Œå¢å¼ºçŸ¥è¯†æ£€ç´¢ä¸è¿‡æ»¤èƒ½åŠ›ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†åŸºäºçŸ¥è¯†çš„è§†è§‰é—®ç­”ï¼ˆKB-VQAï¼‰ä»»åŠ¡ä¸­çš„ç­”æ¡ˆè´¨é‡ï¼Œåœ¨E-VQAå’ŒInfoSeekæ•°æ®é›†ä¸Šè¾¾åˆ°æœ€ä¼˜æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.14592v1">Multimodal RAG for Unstructured Data:Leveraging Modality-Aware Knowledge Graphs with Hybrid Retrieval</a></td><td><details><summary>å±•å¼€</summary>Current Retrieval-Augmented Generation (RAG) systems primarily operate on
unimodal textual data, limiting their effectiveness on unstructured multimodal
documents. Such documents often combine text, images, tables, equations, and
graphs, each contributing unique information. In this work, we present a
Modality-Aware Hybrid retrieval Architecture (MAHA), designed specifically for
multimodal question answering with reasoning through a modality-aware knowledge
graph. MAHA integrates dense vector retrieval with structured graph traversal,
where the knowledge graph encodes cross-modal semantics and relationships. This
design enables both semantically rich and context-aware retrieval across
diverse modalities. Evaluations on multiple benchmark datasets demonstrate that
MAHA substantially outperforms baseline methods, achieving a ROUGE-L score of
0.486, providing complete modality coverage. These results highlight MAHA's
ability to combine embeddings with explicit document structure, enabling
effective multimodal retrieval. Our work establishes a scalable and
interpretable retrieval framework that advances RAG systems by enabling
modality-aware reasoning over unstructured multimodal data.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMAHAçš„æ¨¡æ€æ„ŸçŸ¥æ··åˆæ£€ç´¢æ¶æ„ï¼Œæ—¨åœ¨è§£å†³å½“å‰RAGç³»ç»Ÿåœ¨å¤„ç†å¤šæ¨¡æ€æ–‡æ¡£ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒã€è¡¨æ ¼ç­‰ï¼‰æ—¶çš„å±€é™æ€§ã€‚é€šè¿‡ç»“åˆå¯†é›†å‘é‡æ£€ç´¢å’Œç»“æ„åŒ–çŸ¥è¯†å›¾è°±éå†ï¼ŒMAHAèƒ½å¤Ÿå®ç°è·¨æ¨¡æ€çš„è¯­ä¹‰ä¸°å¯Œå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ£€ç´¢ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€é—®ç­”ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.14400v1">MedTrust-RAG: Evidence Verification and Trust Alignment for Biomedical Question Answering</a></td><td><details><summary>å±•å¼€</summary>Biomedical question answering (QA) requires accurate interpretation of
complex medical knowledge. Large language models (LLMs) have shown promising
capabilities in this domain, with retrieval-augmented generation (RAG) systems
enhancing performance by incorporating external medical literature. However,
RAG-based approaches in biomedical QA suffer from hallucinations due to
post-retrieval noise and insufficient verification of retrieved evidence,
undermining response reliability. We propose MedTrust-Guided Iterative RAG, a
framework designed to enhance factual consistency and mitigate hallucinations
in medical QA. Our method introduces three key innovations. First, it enforces
citation-aware reasoning by requiring all generated content to be explicitly
grounded in retrieved medical documents, with structured Negative Knowledge
Assertions used when evidence is insufficient. Second, it employs an iterative
retrieval-verification process, where a verification agent assesses evidence
adequacy and refines queries through Medical Gap Analysis until reliable
information is obtained. Third, it integrates the MedTrust-Align Module (MTAM)
that combines verified positive examples with hallucination-aware negative
samples, leveraging Direct Preference Optimization to reinforce
citation-grounded reasoning while penalizing hallucination-prone response
patterns. Experiments on MedMCQA, MedQA, and MMLU-Med demonstrate that our
approach consistently outperforms competitive baselines across multiple model
architectures, achieving the best average accuracy with gains of 2.7% for
LLaMA3.1-8B-Instruct and 2.4% for Qwen3-8B.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMedTrust-Guided Iterative RAGçš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¼•ç”¨æ„ŸçŸ¥æ¨ç†ã€è¿­ä»£æ£€ç´¢éªŒè¯å’ŒMedTrust-Alignæ¨¡å—æ¥å‡å°‘åŒ»å­¦é—®ç­”ä¸­çš„å¹»è§‰é—®é¢˜ï¼Œæé«˜äº‹å®ä¸€è‡´æ€§ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•çš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.14377v1">PluriHop: Exhaustive, Recall-Sensitive QA over Distractor-Rich Corpora</a></td><td><details><summary>å±•å¼€</summary>Recent advances in large language models (LLMs) and retrieval-augmented
generation (RAG) have enabled progress on question answering (QA) when relevant
evidence is in one (single-hop) or multiple (multi-hop) passages. Yet many
realistic questions about recurring report data - medical records, compliance
filings, maintenance logs - require aggregation across all documents, with no
clear stopping point for retrieval and high sensitivity to even one missed
passage. We term these pluri-hop questions and formalize them by three
criteria: recall sensitivity, exhaustiveness, and exactness. To study this
setting, we introduce PluriHopWIND, a diagnostic multilingual dataset of 48
pluri-hop questions built from 191 real-world wind industry reports in German
and English. We show that PluriHopWIND is 8-40% more repetitive than other
common datasets and thus has higher density of distractor documents, better
reflecting practical challenges of recurring report corpora. We test a
traditional RAG pipeline as well as graph-based and multimodal variants, and
find that none of the tested approaches exceed 40% in statement-wise F1 score.
Motivated by this, we propose PluriHopRAG, a RAG architecture that follows a
"check all documents individually, filter cheaply" approach: it (i) decomposes
queries into document-level subquestions and (ii) uses a cross-encoder filter
to discard irrelevant documents before costly LLM reasoning. We find that
PluriHopRAG achieves relative F1 score improvements of 18-52% depending on base
LLM. Despite its modest size, PluriHopWIND exposes the limitations of current
QA systems on repetitive, distractor-rich corpora. PluriHopRAG's performance
highlights the value of exhaustive retrieval and early filtering as a powerful
alternative to top-k methods.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸º"pluri-hop"çš„æ–°å‹é—®ç­”ä»»åŠ¡ï¼Œå…¶ç‰¹ç‚¹æ˜¯éœ€è¦åœ¨å¤§é‡é‡å¤æ€§æ–‡æ¡£ä¸­è¿›è¡Œå…¨é¢æ£€ç´¢å’Œç²¾ç¡®èšåˆã€‚ä½œè€…æ„å»ºäº†ä¸€ä¸ªå¤šè¯­è¨€è¯Šæ–­æ•°æ®é›†PluriHopWINDï¼Œå¹¶æµ‹è¯•äº†å¤šç§RAGæ–¹æ³•ï¼Œå‘ç°ç°æœ‰æ–¹æ³•è¡¨ç°ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œä»–ä»¬æå‡ºäº†PluriHopRAGæ¶æ„ï¼Œé€šè¿‡æŸ¥è¯¢åˆ†è§£å’Œäº¤å‰ç¼–ç å™¨è¿‡æ»¤ç­–ç•¥æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œè®ºè¯äº†å…¨é¢æ£€ç´¢å’Œæ—©æœŸè¿‡æ»¤åœ¨é‡å¤æ€§æ–‡æ¡£é—®ç­”ä¸­çš„ä¼˜åŠ¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.14337v1">Stop-RAG: Value-Based Retrieval Control for Iterative RAG</a></td><td><details><summary>å±•å¼€</summary>Iterative retrieval-augmented generation (RAG) enables large language models
to answer complex multi-hop questions, but each additional loop increases
latency, costs, and the risk of introducing distracting evidence, motivating
the need for an efficient stopping strategy. Existing methods either use a
predetermined number of iterations or rely on confidence proxies that poorly
reflect whether more retrieval will actually help. We cast iterative RAG as a
finite-horizon Markov decision process and introduce Stop-RAG, a value-based
controller that adaptively decides when to stop retrieving. Trained with
full-width forward-view Q($\lambda$) targets from complete trajectories,
Stop-RAG learns effective stopping policies while remaining compatible with
black-box APIs and existing pipelines. On multi-hop question-answering
benchmarks, Stop-RAG consistently outperforms both fixed-iteration baselines
and prompting-based stopping with LLMs. These results highlight adaptive
stopping as a key missing component in current agentic systems, and demonstrate
that value-based control can improve the accuracy of RAG systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºStop-RAGçš„è‡ªé€‚åº”åœæ­¢ç­–ç•¥ï¼Œç”¨äºä¼˜åŒ–è¿­ä»£å¼æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ•ˆç‡ã€‚é€šè¿‡å°†è¿­ä»£RAGå»ºæ¨¡ä¸ºæœ‰é™èŒƒå›´é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ŒStop-RAGåŸºäºä»·å€¼æ§åˆ¶åŠ¨æ€å†³å®šä½•æ—¶åœæ­¢æ£€ç´¢ï¼Œä»è€Œå‡å°‘å»¶è¿Ÿã€æˆæœ¬å’Œæ— å…³è¯æ®çš„å¹²æ‰°ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šè·³é—®ç­”ä»»åŠ¡ä¸­ä¼˜äºå›ºå®šè¿­ä»£æ¬¡æ•°å’ŒåŸºäºæç¤ºçš„åœæ­¢ç­–ç•¥ï¼ŒéªŒè¯äº†è‡ªé€‚åº”æ§åˆ¶åœ¨æå‡RAGç³»ç»Ÿå‡†ç¡®æ€§ä¸­çš„é‡è¦æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.14271v1">Less is More: Denoising Knowledge Graphs For Retrieval Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) systems enable large language models
(LLMs) instant access to relevant information for the generative process,
demonstrating their superior performance in addressing common LLM challenges
such as hallucination, factual inaccuracy, and the knowledge cutoff.
Graph-based RAG further extends this paradigm by incorporating knowledge graphs
(KGs) to leverage rich, structured connections for more precise and inferential
responses. A critical challenge, however, is that most Graph-based RAG systems
rely on LLMs for automated KG construction, often yielding noisy KGs with
redundant entities and unreliable relationships. This noise degrades retrieval
and generation performance while also increasing computational cost. Crucially,
current research does not comprehensively address the denoising problem for
LLM-generated KGs. In this paper, we introduce DEnoised knowledge Graphs for
Retrieval Augmented Generation (DEG-RAG), a framework that addresses these
challenges through: (1) entity resolution, which eliminates redundant entities,
and (2) triple reflection, which removes erroneous relations. Together, these
techniques yield more compact, higher-quality KGs that significantly outperform
their unprocessed counterparts. Beyond the methods, we conduct a systematic
evaluation of entity resolution for LLM-generated KGs, examining different
blocking strategies, embedding choices, similarity metrics, and entity merging
techniques. To the best of our knowledge, this is the first comprehensive
exploration of entity resolution in LLM-generated KGs. Our experiments
demonstrate that this straightforward approach not only drastically reduces
graph size but also consistently improves question answering performance across
diverse popular Graph-based RAG variants.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDEG-RAGçš„æ¡†æ¶ï¼Œé€šè¿‡å®ä½“è§£æå’Œä¸‰å…ƒç»„åå°„æŠ€æœ¯è§£å†³åŸºäºçŸ¥è¯†å›¾è°±çš„RAGç³»ç»Ÿä¸­å› å¤§è¯­è¨€æ¨¡å‹è‡ªåŠ¨æ„å»ºçŸ¥è¯†å›¾è°±è€Œäº§ç”Ÿçš„å™ªå£°é—®é¢˜ï¼Œä»è€Œæé«˜çŸ¥è¯†å›¾è°±çš„è´¨é‡å’Œæ£€ç´¢ç”Ÿæˆæ€§èƒ½ï¼Œå¹¶åœ¨å®éªŒä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.14252v1">MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented Generation Systems</a></td><td><details><summary>å±•å¼€</summary>The traditional RAG paradigm, which typically engages in the comprehension of
relevant text chunks in response to received queries, inherently restricts both
the depth of knowledge internalization and reasoning capabilities. To address
this limitation, our research transforms the text processing in RAG from
passive chunking to proactive understanding, defining this process as document
memory extraction with the objective of simulating human cognitive processes
during reading. Building upon this, we propose the Mixtures of scenario-aware
document Memories (MoM) framework, engineered to efficiently handle documents
from multiple domains and train small language models (SLMs) to acquire the
ability to proactively explore and construct document memories. The MoM
initially instructs large language models (LLMs) to simulate domain experts in
generating document logical outlines, thereby directing structured chunking and
core content extraction. It employs a multi-path sampling and multi-perspective
evaluation mechanism, specifically designing comprehensive metrics that
represent chunk clarity and extraction completeness to select the optimal
document memories. Additionally, to infuse deeper human-like reading abilities
during the training of SLMs, we incorporate a reverse reasoning strategy, which
deduces refined expert thinking paths from high-quality outcomes. Finally,
leveraging diverse forms of content generated by MoM, we develop a three-layer
document memory retrieval mechanism, which is grounded in our theoretical proof
from the perspective of probabilistic modeling. Extensive experimental results
across three distinct domains demonstrate that the MoM framework not only
resolves text chunking challenges in existing RAG systems, providing LLMs with
semantically complete document memories, but also paves the way for SLMs to
achieve human-centric intelligent text processing.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›ä¼ ç»ŸRAGæ¡†æ¶çš„æ–¹æ³•â€”â€”MoMï¼ˆMixtures of scenario-aware document Memoriesï¼‰ï¼Œé€šè¿‡ä¸»åŠ¨ç†è§£æ–‡æ¡£ï¼ˆæ¨¡æ‹Ÿäººç±»è®¤çŸ¥ï¼‰æ›¿ä»£è¢«åŠ¨åˆ†å—å¤„ç†ï¼Œåˆ©ç”¨LLMç”Ÿæˆé€»è¾‘å¤§çº²æŒ‡å¯¼ç»“æ„åŒ–åˆ†å—å’Œæ ¸å¿ƒå†…å®¹æå–ï¼Œå¹¶ç»“åˆå¤šè·¯å¾„é‡‡æ ·ã€å¤šè§†è§’è¯„ä¼°åŠåå‘æ¨ç†ç­–ç•¥ï¼Œæå‡å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰çš„ä¸»åŠ¨æ¢ç´¢ä¸è®°å¿†æ„å»ºèƒ½åŠ›ã€‚å®éªŒè¯æ˜MoMèƒ½è§£å†³ç°æœ‰RAGçš„æ–‡æœ¬åˆ†å—é—®é¢˜ï¼Œæä¾›è¯­ä¹‰å®Œæ•´çš„æ–‡æ¡£è®°å¿†ï¼Œå¹¶æ¨åŠ¨SLMså®ç°æ›´äººæ€§åŒ–çš„æ–‡æœ¬å¤„ç†ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-15
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.13799v1">BRIEF-Pro: Universal Context Compression with Short-to-Long Synthesis for Fast and Accurate Multi-Hop Reasoning</a></td><td><details><summary>å±•å¼€</summary>As retrieval-augmented generation (RAG) tackles complex tasks, increasingly
expanded contexts offer richer information, but at the cost of higher latency
and increased cognitive load on the model. To mitigate this bottleneck,
especially for intricate multi-hop questions, we introduce BRIEF-Pro. It is a
universal, lightweight compressor that distills relevant evidence for a given
query from retrieved documents into a concise summary for seamless integration
into in-context RAG. Using seed data consisting of relatively short contexts
(fewer than 1k words), BRIEF-Pro is trained to perform abstractive compression
of extended contexts exceeding 10k words across a wide range of scenarios.
Furthermore, BRIEF-Pro offers flexible user control over summary length by
allowing users to specify the desired number of sentences. Experiments on four
open-domain multi-hop question-answering datasets show that BRIEF-Pro generates
more concise and relevant summaries, enhancing performance across small, large,
and proprietary language models. With the 70B reader model, 32x compression by
BRIEF-Pro improves QA performance by 4.67% on average over LongLLMLingua's 9x,
while requiring only 23% of its computational overhead.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºBRIEF-Proçš„è½»é‡çº§é€šç”¨å‹ç¼©å™¨ï¼Œæ—¨åœ¨è§£å†³RAGç³»ç»Ÿä¸­å› æ£€ç´¢ä¸Šä¸‹æ–‡è¿‡é•¿å¯¼è‡´çš„å»¶è¿Ÿå’Œæ¨¡å‹è®¤çŸ¥è´Ÿè·é—®é¢˜ã€‚BRIEF-Proé€šè¿‡æŠ½è±¡å‹ç¼©æŠ€æœ¯ä»æ£€ç´¢æ–‡æ¡£ä¸­æå–å…³é”®ä¿¡æ¯ç”Ÿæˆç®€æ´æ‘˜è¦ï¼Œæ”¯æŒç”¨æˆ·è‡ªå®šä¹‰æ‘˜è¦é•¿åº¦ï¼Œå¹¶åœ¨å¤šè·³é—®ç­”ä»»åŠ¡ä¸­æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ï¼ˆå¦‚70Bå‚æ•°æ¨¡å‹ä¸Šå‹ç¼©32å€æ—¶QAæ€§èƒ½å¹³å‡æå‡4.67%ï¼‰ï¼ŒåŒæ—¶é™ä½è®¡ç®—å¼€é”€ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.13750v1">Confidence-Based Response Abstinence: Improving LLM Trustworthiness via Activation-Based Uncertainty Estimation</a></td><td><details><summary>å±•å¼€</summary>We propose a method for confidence estimation in retrieval-augmented
generation (RAG) systems that aligns closely with the correctness of large
language model (LLM) outputs. Confidence estimation is especially critical in
high-stakes domains such as finance and healthcare, where the cost of an
incorrect answer outweighs that of not answering the question. Our approach
extends prior uncertainty quantification methods by leveraging raw feed-forward
network (FFN) activations as auto-regressive signals, avoiding the information
loss inherent in token logits and probabilities after projection and softmax
normalization. We model confidence prediction as a sequence classification
task, and regularize training with a Huber loss term to improve robustness
against noisy supervision. Applied in a real-world financial industry
customer-support setting with complex knowledge bases, our method outperforms
strong baselines and maintains high accuracy under strict latency constraints.
Experiments on Llama 3.1 8B model show that using activations from only the
16th layer preserves accuracy while reducing response latency. Our results
demonstrate that activation-based confidence modeling offers a scalable,
architecture-aware path toward trustworthy RAG deployment.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç”¨äºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿçš„ç½®ä¿¡åº¦ä¼°è®¡æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰çš„åŸå§‹æ¿€æ´»ä¿¡å·æ¥æ›´å‡†ç¡®åœ°é¢„æµ‹æ¨¡å‹è¾“å‡ºçš„æ­£ç¡®æ€§ï¼Œé€‚ç”¨äºé‡‘èç­‰é«˜é£é™©é¢†åŸŸï¼Œå¹¶åœ¨å®é™…åº”ç”¨ä¸­å±•ç¤ºäº†ä¼˜è¶Šæ€§èƒ½å’Œä½å»¶è¿Ÿã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.13590v1">RAG Meets Temporal Graphs: Time-Sensitive Modeling and Retrieval for Evolving Knowledge</a></td><td><details><summary>å±•å¼€</summary>Knowledge is inherently time-sensitive and continuously evolves over time.
Although current Retrieval-Augmented Generation (RAG) systems enrich LLMs with
external knowledge, they largely ignore this temporal nature. This raises two
challenges for RAG. First, current RAG methods lack effective time-aware
representations. Same facts of different time are difficult to distinguish with
vector embeddings or conventional knowledge graphs. Second, most RAG
evaluations assume a static corpus, leaving a blind spot regarding update costs
and retrieval stability as knowledge evolves. To make RAG time-aware, we
propose Temporal GraphRAG (TG-RAG), which models external corpora as a bi-level
temporal graph consisting of a temporal knowledge graph with timestamped
relations and a hierarchical time graph. Multi-granularity temporal summaries
are generated for each time node to capture both key events and broader trends
at that time. The design supports incremental updates by extracting new
temporal facts from the incoming corpus and merging them into the existing
graph. The temporal graph explicitly represents identical facts at different
times as distinct edges to avoid ambiguity, and the time hierarchy graph allows
only generating reports for new leaf time nodes and their ancestors, ensuring
effective and efficient updates. During inference, TG-RAG dynamically retrieves
a subgraph within the temporal and semantic scope of the query, enabling
precise evidence gathering. Moreover, we introduce ECT-QA, a time-sensitive
question-answering dataset featuring both specific and abstract queries, along
with a comprehensive evaluation protocol designed to assess incremental update
capabilities of RAG systems. Extensive experiments show that TG-RAG
significantly outperforms existing baselines, demonstrating the effectiveness
of our method in handling temporal knowledge and incremental updates.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†Temporal GraphRAG (TG-RAG)ï¼Œä¸€ç§æ”¹è¿›çš„RAGç³»ç»Ÿï¼Œé€šè¿‡æ„å»ºåŒå±‚æ—¶åºå›¾ï¼ˆåŒ…å«å¸¦æ—¶é—´æˆ³çš„å…³ç³»å›¾å’Œæ—¶é—´å±‚æ¬¡å›¾ï¼‰æ¥æ˜¾å¼å»ºæ¨¡çŸ¥è¯†çš„æ—¶é—´æ•æ„Ÿæ€§ï¼Œæ”¯æŒåŠ¨æ€æŸ¥è¯¢å’Œå¢é‡æ›´æ–°ï¼Œå¹¶å¼•å…¥ECT-QAæ•°æ®é›†éªŒè¯å…¶åœ¨å¤„ç†æ—¶åºçŸ¥è¯†å’Œæ›´æ–°æ•ˆç‡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.13366v1">Document Intelligence in the Era of Large Language Models: A Survey</a></td><td><details><summary>å±•å¼€</summary>Document AI (DAI) has emerged as a vital application area, and is
significantly transformed by the advent of large language models (LLMs). While
earlier approaches relied on encoder-decoder architectures, decoder-only LLMs
have revolutionized DAI, bringing remarkable advancements in understanding and
generation. This survey provides a comprehensive overview of DAI's evolution,
highlighting current research attempts and future prospects of LLMs in this
field. We explore key advancements and challenges in multimodal, multilingual,
and retrieval-augmented DAI, while also suggesting future research directions,
including agent-based approaches and document-specific foundation models. This
paper aims to provide a structured analysis of the state-of-the-art in DAI and
its implications for both academic and practical applications.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹æ–‡æ¡£äººå·¥æ™ºèƒ½ï¼ˆDAIï¼‰çš„å˜é©æ€§å½±å“ï¼Œç‰¹åˆ«å…³æ³¨äº†åŒ…æ‹¬æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨å†…çš„å¤šæ¨¡æ€ã€å¤šè¯­è¨€æŠ€æœ¯è¿›å±•ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶æ–¹å‘å¦‚åŸºäºä»£ç†çš„æ¡†æ¶å’Œæ–‡æ¡£ä¸“ç”¨åŸºç¡€æ¨¡å‹ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.13363v1">D-SMART: Enhancing LLM Dialogue Consistency via Dynamic Structured Memory And Reasoning Tree</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) often exhibit factual inconsistencies and
logical decay in extended, multi-turn dialogues, a challenge stemming from
their reliance on static, pre-trained knowledge and an inability to reason
adaptively over the dialogue history. Prevailing mitigation strategies, such as
Retrieval-Augmented Generation (RAG) and agentic working memories, improve
information recall but still engage with fundamentally static knowledge sources
and follow pre-defined single reasoning path. This hinders their ability to
preserve factual and logical consistency of their responses in multi-turn
dialogues while the context evolves over time. To address this issue, we
propose D-SMART, a model-agnostic framework designed to maintain multi-turn
dialogue consistency by enabling LLMs to build and reason over a dynamic,
structured representation of the conversational context. This is achieved via
two synergistic components: (1) a Dynamic Structured Memory (DSM), which
incrementally constructs and maintains an authoritative, OWL-compliant
knowledge graph of the conversation; and (2) a Reasoning Tree (RT), which
executes inferences as an explicit and traceable multi-step search over the
graph. As the popular-used quality score (judged by GPT-4) can overlook logical
flaws, we introduce new NLI-based metrics to better measure multi-turn dialogue
consistency. Comprehensive experiments on the MT-Bench-101 benchmark show that
D-SMART significantly outperforms state-of-the-art baselines, elevating the
dialogue consistency score by over 48\% for both proprietary and open-source
models, and notably improves the quality score of the latter by up to 10.1\%.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè½®å¯¹è¯ä¸­å­˜åœ¨çš„é€»è¾‘è¡°å‡å’Œäº‹å®ä¸ä¸€è‡´é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºD-SMARTçš„æ¨¡å‹æ— å…³æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åŠ¨æ€ç»“æ„åŒ–è®°å¿†ï¼ˆDSMï¼‰å’Œæ¨ç†æ ‘ï¼ˆRTï¼‰æ¥å¢å¼ºLLMså¯¹å¯¹è¯ä¸Šä¸‹æ–‡çš„åŠ¨æ€ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†å¤šè½®å¯¹è¯çš„ä¸€è‡´æ€§ã€‚æ–‡ç« æåŠäº†ç°æœ‰æ–¹æ³•ï¼ˆå¦‚RAGï¼‰çš„å±€é™æ€§ï¼Œå¹¶å¼•å…¥äº†æ–°çš„åŸºäºNLIçš„è¯„ä¼°æŒ‡æ ‡ï¼Œå®éªŒç»“æœè¡¨æ˜D-SMARTåœ¨ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.13329v1">Embedding-Based Context-Aware Reranker</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) systems rely on retrieving relevant
evidence from a corpus to support downstream generation. The common practice of
splitting a long document into multiple shorter passages enables finer-grained
and targeted information retrieval. However, it also introduces challenges when
a correct retrieval would require inference across passages, such as resolving
coreference, disambiguating entities, and aggregating evidence scattered across
multiple sources. Many state-of-the-art (SOTA) reranking methods, despite
utilizing powerful large pretrained language models with potentially high
inference costs, still neglect the aforementioned challenges. Therefore, we
propose Embedding-Based Context-Aware Reranker (EBCAR), a lightweight reranking
framework operating directly on embeddings of retrieved passages with enhanced
cross-passage understandings through the structural information of the passages
and a hybrid attention mechanism, which captures both high-level interactions
across documents and low-level relationships within each document. We evaluate
EBCAR against SOTA rerankers on the ConTEB benchmark, demonstrating its
effectiveness for information retrieval requiring cross-passage inference and
its advantages in both accuracy and efficiency.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºEBCARçš„è½»é‡çº§é‡æ’åºæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³RAGç³»ç»Ÿä¸­è·¨æ®µè½æ¨ç†çš„æŒ‘æˆ˜ï¼ˆå¦‚å…±æŒ‡æ¶ˆè§£å’Œè¯æ®èšåˆï¼‰ï¼Œé€šè¿‡ç»“åˆåµŒå…¥ä¿¡æ¯å’Œæ··åˆæ³¨æ„åŠ›æœºåˆ¶æå‡æ£€ç´¢æ€§èƒ½ï¼Œå¹¶åœ¨ConTEBåŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†å…¶é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.13312v1">ChatR1: Reinforcement Learning for Conversational Reasoning and Retrieval Augmented Question Answering</a></td><td><details><summary>å±•å¼€</summary>We present ChatR1, a reasoning framework based on reinforcement learning (RL)
for conversational question answering (CQA). Reasoning plays an important role
in CQA, where user intent evolves across dialogue turns, and utterances are
often underspecified, requiring contextual interpretation, query reformulation,
and dynamic coordination between retrieval and generation. Unlike static
`rewrite, retrieve, and generate' pipelines, ChatR1 interleaves search and
reasoning across turns, enabling exploratory and adaptive behaviors learned
through RL. To address the challenge of sparse and delayed rewards in RL, we
propose an intent-aware reward that provides turn-level feedback by aligning
retrieval and reasoning with evolving user goals. Our proposed ChatR1
demonstrates strong performance on both 3B and 7B model backbones,
outperforming competitive models on five CQA datasets, measured by different
metrics (F1, BERTScore, and LLM-as-judge). We include a diverse set of CQA
datasets to cover topic shifts, evolving intents, mixed-initiative dialogues,
and multi-document grounding, testing ChatR1's performance from various
aspects. Ablation studies confirm the effectiveness of the intent-aware reward.
Our analyses further reveal diverse reasoning trajectories and effective use of
the search tool. ChatR1 also generalizes robustly across domains, demonstrating
that RL-based reasoning enables more flexible and context-sensitive behavior
than static CQA pipelines.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ChatR1ï¼Œä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å¯¹è¯å¼é—®ç­”ï¼ˆCQAï¼‰æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€äº¤ç»‡æœç´¢å’Œæ¨ç†æ¥é€‚åº”ç”¨æˆ·æ„å›¾çš„æ¼”å˜ï¼Œåˆ©ç”¨æ„å›¾æ„ŸçŸ¥å¥–åŠ±ä¼˜åŒ–æ£€ç´¢ä¸ç”Ÿæˆçš„ååŒï¼Œæ˜¾è‘—æå‡äº†å¤šä»»åŠ¡åœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.13272v1">Beyond Correctness: Rewarding Faithful Reasoning in Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Inspired by the success of reinforcement learning (RL) in Large Language
Model (LLM) training for domains like math and code, recent works have begun
exploring how to train LLMs to use search engines more effectively as tools for
retrieval-augmented generation. Although these methods achieve performance
improvement across QA benchmarks, many prioritize final answer correctness
while overlooking the quality of intermediate reasoning steps, which may lead
to chain-of-thought unfaithfulness. In this paper, we first introduce a
comprehensive evaluation framework for evaluating RL-based search agents,
covering three distinct faithfulness metrics: information-think faithfulness,
think-answer faithfulness, and think-search faithfulness. Our evaluations
reveal that a prototypical RL-based search agent, Search-R1, has significant
room for improvement in this regard. To foster faithful reasoning, we introduce
VERITAS (Verifying Entailed Reasoning through Intermediate Traceability in
Agentic Search), a novel framework that integrates fine-grained faithfulness
rewards into the reinforcement learning process. Our experiments show that
models trained with VERITAS not only significantly improve reasoning
faithfulness, but also achieve comparable task performance across seven QA
benchmarks.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨å¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ”¹è¿›å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½¿ç”¨æœç´¢å¼•æ“è¿›è¡Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ—¶çš„æ¨ç†å¿ å®æ€§é—®é¢˜ï¼Œæå‡ºè¯„ä¼°æ¡†æ¶å’ŒVERITASæ–¹æ³•ä»¥æå‡ä¸­é—´æ¨ç†æ­¥éª¤çš„è´¨é‡ï¼Œæœ€ç»ˆåœ¨å¤šä¸ªQAåŸºå‡†æµ‹è¯•ä¸­å®ç°æ›´é«˜å¿ å®æ€§å’Œä»»åŠ¡æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.13193v1">ReMindRAG: Low-Cost LLM-Guided Knowledge Graph Traversal for Efficient RAG</a></td><td><details><summary>å±•å¼€</summary>Knowledge graphs (KGs), with their structured representation capabilities,
offer promising avenue for enhancing Retrieval Augmented Generation (RAG)
systems, leading to the development of KG-RAG systems. Nevertheless, existing
methods often struggle to achieve effective synergy between system
effectiveness and cost efficiency, leading to neither unsatisfying performance
nor excessive LLM prompt tokens and inference time. To this end, this paper
proposes REMINDRAG, which employs an LLM-guided graph traversal featuring node
exploration, node exploitation, and, most notably, memory replay, to improve
both system effectiveness and cost efficiency. Specifically, REMINDRAG
memorizes traversal experience within KG edge embeddings, mirroring the way
LLMs "memorize" world knowledge within their parameters, but in a train-free
manner. We theoretically and experimentally confirm the effectiveness of
REMINDRAG, demonstrating its superiority over existing baselines across various
benchmark datasets and LLM backbones. Our code is available at
https://github.com/kilgrims/ReMindRAG.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºREMINDRAGçš„KG-RAGç³»ç»Ÿï¼Œé€šè¿‡LLMå¼•å¯¼çš„å›¾éå†ï¼ˆåŒ…å«èŠ‚ç‚¹æ¢ç´¢ã€èŠ‚ç‚¹åˆ©ç”¨å’Œè®°å¿†å›æ”¾ï¼‰æ¥æå‡çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ä¸RAGç³»ç»Ÿçš„ååŒæ•ˆæœï¼ŒåŒæ—¶ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½å’Œæˆæœ¬æ•ˆç‡ã€‚å…¶æ ¸å¿ƒåˆ›æ–°æ˜¯å°†éå†ç»éªŒåµŒå…¥KGè¾¹è¡¨ç¤ºä¸­ï¼ˆæ— éœ€è®­ç»ƒï¼‰ï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†å’ŒLLMä¸»å¹²æ¨¡å‹ä¸Šä¼˜äºç°æœ‰åŸºçº¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.13191v1">Grounding Long-Context Reasoning with Contextual Normalization for Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has become an essential approach for
extending the reasoning and knowledge capacity of large language models (LLMs).
While prior research has primarily focused on retrieval quality and prompting
strategies, the influence of how the retrieved documents are framed, i.e.,
context format, remains underexplored. We show that seemingly superficial
choices, such as delimiters or structural markers in key-value extraction, can
induce substantial shifts in accuracy and stability, even when semantic content
is identical. To systematically investigate this effect, we design controlled
experiments that vary context density, delimiter styles, and positional
placement, revealing the underlying factors that govern performance
differences. Building on these insights, we introduce Contextual Normalization,
a lightweight strategy that adaptively standardizes context representations
before generation. Extensive experiments on both controlled and real-world RAG
benchmarks across diverse settings demonstrate that the proposed strategy
consistently improves robustness to order variation and strengthens
long-context utilization. These findings underscore that reliable RAG depends
not only on retrieving the right content, but also on how that content is
presented, offering both new empirical evidence and a practical technique for
better long-context reasoning.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­ä¸Šä¸‹æ–‡æ ¼å¼ï¼ˆå¦‚åˆ†éš”ç¬¦å’Œç»“æ„æ ‡è®°ï¼‰å¯¹æ¨¡å‹æ€§èƒ½å’Œç¨³å®šæ€§çš„å½±å“ï¼Œæå‡ºäº†ä¸€ç§ç§°ä¸ºâ€œä¸Šä¸‹æ–‡å½’ä¸€åŒ–â€çš„è½»é‡çº§ç­–ç•¥æ¥æ ‡å‡†åŒ–ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œå®éªŒè¯æ˜è¯¥ç­–ç•¥å¯æå‡æ¨¡å‹å¯¹é¡ºåºå˜åŒ–çš„é²æ£’æ€§å’Œé•¿ä¸Šä¸‹æ–‡åˆ©ç”¨èƒ½åŠ›ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-14
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.12801v1">DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search</a></td><td><details><summary>å±•å¼€</summary>Multimodal Large Language Models (MLLMs) in real-world applications require
access to external knowledge sources and must remain responsive to the dynamic
and ever-changing real-world information in order to address
information-seeking and knowledge-intensive user queries. Existing approaches,
such as retrieval augmented generation (RAG) methods, search agents, and search
equipped MLLMs, often suffer from rigid pipelines, excessive search calls, and
poorly constructed search queries, which result in inefficiencies and
suboptimal outcomes. To address these limitations, we present DeepMMSearch-R1,
the first multimodal LLM capable of performing on-demand, multi-turn web
searches and dynamically crafting queries for both image and text search tools.
Specifically, DeepMMSearch-R1 can initiate web searches based on relevant crops
of the input image making the image search more effective, and can iteratively
adapt text search queries based on retrieved information, thereby enabling
self-reflection and self-correction. Our approach relies on a two-stage
training pipeline: a cold start supervised finetuning phase followed by an
online reinforcement learning optimization. For training, we introduce
DeepMMSearchVQA, a novel multimodal VQA dataset created through an automated
pipeline intermixed with real-world information from web search tools. This
dataset contains diverse, multi-hop queries that integrate textual and visual
information, teaching the model when to search, what to search for, which
search tool to use and how to reason over the retrieved information. We conduct
extensive experiments across a range of knowledge-intensive benchmarks to
demonstrate the superiority of our approach. Finally, we analyze the results
and provide insights that are valuable for advancing multimodal web-search.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ä»‹ç»äº†DeepMMSearch-R1ï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œæ—¨åœ¨é€šè¿‡åŠ¨æ€ç”Ÿæˆå›¾åƒå’Œæ–‡æœ¬æœç´¢æŸ¥è¯¢ã€å¤šè½®è¿­ä»£æ£€ç´¢åŠè‡ªåæ€ä¼˜åŒ–ï¼Œè§£å†³ç°æœ‰æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•åœ¨ç®¡é“åƒµåŒ–ã€æœç´¢æ•ˆç‡ä½ä¸‹å’ŒæŸ¥è¯¢è´¨é‡ä¸è¶³ç­‰é—®é¢˜ã€‚æ–‡ç« æå‡ºä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼ˆç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼‰å¹¶æ„å»ºäº†æ–°å‹å¤šæ¨¡æ€VQAæ•°æ®é›†DeepMMSearchVQAï¼Œé€šè¿‡å®éªŒéªŒè¯äº†å…¶åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.12668v1">The Role of Parametric Injection-A Systematic Study of Parametric Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
retrieving external documents. As an emerging form of RAG, parametric
retrieval-augmented generation (PRAG) encodes documents as model parameters
(i.e., LoRA modules) and injects these representations into the model during
inference, enabling interaction between the LLM and documents at parametric
level. Compared with directly placing documents in the input context, PRAG is
more efficient and has the potential to offer deeper model-document
interaction. Despite its growing attention, the mechanism underlying parametric
injection remains poorly understood. In this work, we present a systematic
study of PRAG to clarify the role of parametric injection, showing that
parameterized documents capture only partial semantic information of documents,
and relying on them alone yields inferior performance compared to interaction
at text level. However, these parametric representations encode high-level
document information that can enhance the model's understanding of documents
within the input context. When combined parameterized documents with textual
documents, the model can leverage relevant information more effectively and
become more robust to noisy inputs, achieving better performance than either
source alone. We recommend jointly using parameterized and textual documents
and advocate for increasing the information content of parametric
representations to advance PRAG.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä¸€ç§æ–°å…´çš„RAGå½¢å¼â€”â€”å‚æ•°åŒ–æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆPRAGï¼‰ï¼Œå®ƒé€šè¿‡å°†æ–‡æ¡£ç¼–ç ä¸ºæ¨¡å‹å‚æ•°ï¼ˆå¦‚LoRAæ¨¡å—ï¼‰å¹¶åœ¨æ¨ç†æ—¶æ³¨å…¥è¿™äº›è¡¨ç¤ºï¼Œå®ç°äº†LLMä¸æ–‡æ¡£åœ¨å‚æ•°å±‚é¢çš„äº¤äº’ã€‚ç ”ç©¶æŒ‡å‡ºï¼ŒPRAGè™½æ¯”ç›´æ¥å°†æ–‡æ¡£æ”¾å…¥è¾“å…¥ä¸Šä¸‹æ–‡æ›´é«˜æ•ˆï¼Œä½†å•ç‹¬ä½¿ç”¨æ—¶æ€§èƒ½ä¸å¦‚æ–‡æœ¬çº§åˆ«äº¤äº’ï¼›ç„¶è€Œï¼Œå‚æ•°åŒ–è¡¨ç¤ºèƒ½æ•è·é«˜é˜¶æ–‡æ¡£ä¿¡æ¯ï¼Œè‹¥ä¸æ–‡æœ¬ç»“åˆå¯æå‡æ¨¡å‹æ€§èƒ½å¹¶å¢å¼ºæŠ—å™ªèƒ½åŠ›ã€‚ä½œè€…å»ºè®®è”åˆä½¿ç”¨ä¸¤ç§è¡¨ç¤ºå¹¶æå‡å‚æ•°åŒ–ä¿¡æ¯é‡ä»¥ä¼˜åŒ–PRAGã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.12460v1">Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to
enhance the factuality of Large Language Models (LLMs). However, existing RAG
systems often suffer from an unfaithfulness issue, where the model's response
contradicts evidence from the retrieved context. Existing approaches to
improving contextual faithfulness largely rely on external interventions, such
as prompt engineering, decoding constraints, or reward-based fine-tuning. These
works treat the LLM as a black box and overlook a crucial question: how does
the LLM internally integrate retrieved evidence with its parametric memory,
particularly under knowledge conflicts? To address this gap, we conduct a
probing-based analysis of hidden-state representations in LLMs and observe
three findings: knowledge integration occurs hierarchically, conflicts manifest
as latent signals at the sentence level, and irrelevant context is often
amplified when aligned with parametric knowledge. Building on these findings,
we propose CLEAR (Conflict-Localized and Enhanced Attention for RAG), a
framework that (i) decomposes context into fine-grained sentence-level
knowledge, (ii) employs hidden-state probing to localize conflicting knowledge,
and (iii) introduces conflict-aware fine-tuning to guide the model to
accurately integrate retrieved evidence. Extensive experiments across three
benchmarks demonstrate that CLEAR substantially improves both accuracy and
contextual faithfulness, consistently outperforming strong baselines under
diverse conflict conditions. The related resources are available at
https://github.com/LinfengGao/CLEAR.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ç°æœ‰æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­å­˜åœ¨çš„â€œä¸å¿ å®â€é—®é¢˜ï¼ˆæ¨¡å‹å“åº”ä¸æ£€ç´¢è¯æ®çŸ›ç›¾ï¼‰ï¼Œæå‡ºé€šè¿‡åˆ†æå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å†…éƒ¨éšè—çŠ¶æ€è¡¨ç¤ºæ¥ç ”ç©¶çŸ¥è¯†æ•´åˆæœºåˆ¶ï¼Œå¹¶å¼€å‘äº†CLEARæ¡†æ¶ï¼Œé€šè¿‡å¥å­çº§çŸ¥è¯†åˆ†è§£ã€å†²çªå®šä½å’Œå†²çªæ„ŸçŸ¥å¾®è°ƒæ¥æé«˜RAGçš„å‡†ç¡®æ€§å’Œä¸Šä¸‹æ–‡å¿ å®æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.12434v1">PRoH: Dynamic Planning and Reasoning over Knowledge Hypergraphs for Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Knowledge Hypergraphs (KHs) have recently emerged as a knowledge
representation for retrieval-augmented generation (RAG), offering a paradigm to
model multi-entity relations into a structured form. However, existing KH-based
RAG methods suffer from three major limitations: static retrieval planning,
non-adaptive retrieval execution, and superficial use of KH structure and
semantics, which constrain their ability to perform effective multi-hop
question answering. To overcome these limitations, we propose PRoH, a dynamic
Planning and Reasoning over Knowledge Hypergraphs framework. PRoH incorporates
three core innovations: (i) a context-aware planning module that sketches the
local KH neighborhood to guide structurally grounded reasoning plan generation;
(ii) a structured question decomposition process that organizes subquestions as
a dynamically evolving Directed Acyclic Graph (DAG) to enable adaptive,
multi-trajectory exploration; and (iii) an Entity-Weighted Overlap (EWO)-guided
reasoning path retrieval algorithm that prioritizes semantically coherent
hyperedge traversals. Experiments across multiple domains demonstrate that PRoH
achieves state-of-the-art performance, surpassing the prior SOTA model
HyperGraphRAG by an average of 19.73% in F1 and 8.41% in Generation Evaluation
(G-E) score, while maintaining strong robustness in long-range multi-hop
reasoning tasks.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºPRoHæ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è§„åˆ’å’ŒçŸ¥è¯†è¶…å›¾æ¨ç†æ”¹è¿›ç°æœ‰åŸºäºçŸ¥è¯†è¶…å›¾çš„RAGæ–¹æ³•ï¼Œè§£å†³äº†é™æ€æ£€ç´¢è§„åˆ’ã€éé€‚åº”æ€§æ‰§è¡ŒåŠç»“æ„è¯­ä¹‰åˆ©ç”¨ä¸è¶³ä¸‰å¤§é—®é¢˜ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨å¤šè·³é—®ç­”ä»»åŠ¡ä¸­æ€§èƒ½æ˜¾è‘—ä¼˜äºå…ˆå‰æœ€ä¼˜æ¨¡å‹HyperGraphRAGã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.12323v1">RAG-Anything: All-in-One RAG Framework</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm
for expanding Large Language Models beyond their static training limitations.
However, a critical misalignment exists between current RAG capabilities and
real-world information environments. Modern knowledge repositories are
inherently multimodal, containing rich combinations of textual content, visual
elements, structured tables, and mathematical expressions. Yet existing RAG
frameworks are limited to textual content, creating fundamental gaps when
processing multimodal documents. We present RAG-Anything, a unified framework
that enables comprehensive knowledge retrieval across all modalities. Our
approach reconceptualizes multimodal content as interconnected knowledge
entities rather than isolated data types. The framework introduces dual-graph
construction to capture both cross-modal relationships and textual semantics
within a unified representation. We develop cross-modal hybrid retrieval that
combines structural knowledge navigation with semantic matching. This enables
effective reasoning over heterogeneous content where relevant evidence spans
multiple modalities. RAG-Anything demonstrates superior performance on
challenging multimodal benchmarks, achieving significant improvements over
state-of-the-art methods. Performance gains become particularly pronounced on
long documents where traditional approaches fail. Our framework establishes a
new paradigm for multimodal knowledge access, eliminating the architectural
fragmentation that constrains current systems. Our framework is open-sourced
at: https://github.com/HKUDS/RAG-Anything.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†åä¸º"RAG-Anything"çš„ç»Ÿä¸€æ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰RAGæŠ€æœ¯å±€é™äºæ–‡æœ¬æ£€ç´¢çš„é—®é¢˜ï¼Œé€šè¿‡å°†å¤šæ¨¡æ€å†…å®¹é‡æ„ä¸ºç›¸äº’å…³è”çš„çŸ¥è¯†å®ä½“ï¼Œé‡‡ç”¨åŒå›¾ç»“æ„æ•è·è·¨æ¨¡æ€å…³ç³»å’Œæ–‡æœ¬è¯­ä¹‰ï¼Œå®ç°è·¨æ¨¡æ€æ··åˆæ£€ç´¢ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€é•¿æ–‡æ¡£çš„å¤„ç†æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.12316v1">Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation</a></td><td><details><summary>å±•å¼€</summary>Counter-speech generation is at the core of many expert activities, such as
fact-checking and hate speech, to counter harmful content. Yet, existing work
treats counter-speech generation as pure text generation task, mainly based on
Large Language Models or NGO experts. These approaches show severe drawbacks
due to the limited reliability and coherence in the generated countering text,
and in scalability, respectively. To close this gap, we introduce a novel
framework to model counter-speech generation as knowledge-wise text generation
process. Our framework integrates advanced Retrieval-Augmented Generation (RAG)
pipelines to ensure the generation of trustworthy counter-speech for 8 main
target groups identified in the hate speech literature, including women, people
of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons,
and other. We built a knowledge base over the United Nations Digital Library,
EUR-Lex and the EU Agency for Fundamental Rights, comprising a total of 32,792
texts. We use the MultiTarget-CONAN dataset to empirically assess the quality
of the generated counter-speech, both through standard metrics (i.e., JudgeLM)
and a human evaluation. Results show that our framework outperforms standard
LLM baselines and competitive approach, on both assessments. The resulting
framework and the knowledge base pave the way for studying trustworthy and
sound counter-speech generation, in hate speech and beyond.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§æ–°é¢–çš„åä»‡æ¨è¨€è®ºç”Ÿæˆæ¡†æ¶ï¼Œå°†å…¶å»ºæ¨¡ä¸ºåŸºäºçŸ¥è¯†çš„æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ï¼Œé€šè¿‡é›†æˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œä»è”åˆå›½æ•°å­—å›¾ä¹¦é¦†ç­‰æ„å»ºçš„çŸ¥è¯†åº“ä¸­æ£€ç´¢ä¿¡æ¯ï¼Œä»¥ç¡®ä¿ç”Ÿæˆå¯ä¿¡çš„åä»‡æ¨è¨€è®ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ ‡å‡†è¯„ä¼°å’Œäººå·¥è¯„ä¼°ä¸­å‡ä¼˜äºä¼ ç»Ÿå¤§è¯­è¨€æ¨¡å‹åŸºå‡†ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.12171v1">MatSciBench: Benchmarking the Reasoning Ability of Large Language Models in Materials Science</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) have demonstrated remarkable abilities in
scientific reasoning, yet their reasoning capabilities in materials science
remain underexplored. To fill this gap, we introduce MatSciBench, a
comprehensive college-level benchmark comprising 1,340 problems that span the
essential subdisciplines of materials science. MatSciBench features a
structured and fine-grained taxonomy that categorizes materials science
questions into 6 primary fields and 31 sub-fields, and includes a three-tier
difficulty classification based on the reasoning length required to solve each
question. MatSciBench provides detailed reference solutions enabling precise
error analysis and incorporates multimodal reasoning through visual contexts in
numerous questions. Evaluations of leading models reveal that even the
highest-performing model, Gemini-2.5-Pro, achieves under 80% accuracy on
college-level materials science questions, highlighting the complexity of
MatSciBench. Our systematic analysis of different reasoning strategie--basic
chain-of-thought, tool augmentation, and self-correction--demonstrates that no
single method consistently excels across all scenarios. We further analyze
performance by difficulty level, examine trade-offs between efficiency and
accuracy, highlight the challenges inherent in multimodal reasoning tasks,
analyze failure modes across LLMs and reasoning methods, and evaluate the
influence of retrieval-augmented generation. MatSciBench thus establishes a
comprehensive and solid benchmark for assessing and driving improvements in the
scientific reasoning capabilities of LLMs within the materials science domain.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†MatSciBenchï¼Œä¸€ä¸ªé’ˆå¯¹ææ–™ç§‘å­¦é¢†åŸŸçš„å…¨é¢å¤§å­¦çº§åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«1,340ä¸ªé—®é¢˜ï¼Œè¦†ç›–å¤šä¸ªå­é¢†åŸŸå’Œéš¾åº¦å±‚çº§ã€‚æ–‡ç« è¯„ä¼°äº†é¢†å…ˆå¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œåˆ†æäº†ä¸åŒæ¨ç†ç­–ç•¥ï¼ˆåŒ…æ‹¬æ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰çš„æ•ˆæœï¼Œå¹¶æŒ‡å‡ºè¯¥åŸºå‡†åœ¨æå‡ææ–™ç§‘å­¦æ¨ç†èƒ½åŠ›æ–¹é¢çš„ä»·å€¼ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-13
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.11654v1">FinVet: A Collaborative Framework of RAG and External Fact-Checking Agents for Financial Misinformation Detection</a></td><td><details><summary>å±•å¼€</summary>Financial markets face growing threats from misinformation that can trigger
billions in losses in minutes. Most existing approaches lack transparency in
their decision-making and provide limited attribution to credible sources. We
introduce FinVet, a novel multi-agent framework that integrates two
Retrieval-Augmented Generation (RAG) pipelines with external fact-checking
through a confidence-weighted voting mechanism. FinVet employs adaptive
three-tier processing that dynamically adjusts verification strategies based on
retrieval confidence, from direct metadata extraction to hybrid reasoning to
full model-based analysis. Unlike existing methods, FinVet provides
evidence-backed verdicts, source attribution, confidence scores, and explicit
uncertainty flags when evidence is insufficient. Experimental evaluation on the
FinFact dataset shows that FinVet achieves an F1 score of 0.85, which is a
10.4% improvement over the best individual pipeline (fact-check pipeline) and
37% improvement over standalone RAG approaches.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºFinVetçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆä¸¤æ¡RAGç®¡é“å’Œå¤–éƒ¨äº‹å®æ ¸æŸ¥æœºåˆ¶ï¼Œç»“åˆç½®ä¿¡åº¦åŠ æƒæŠ•ç¥¨è¿›è¡Œé‡‘èä¿¡æ¯éªŒè¯ï¼Œå®ç°åŠ¨æ€ä¸‰å±‚çº§å¤„ç†ç­–ç•¥ï¼Œæä¾›è¯æ®æ”¯æŒã€æ¥æºè¿½æº¯åŠä¸ç¡®å®šæ€§æ ‡æ³¨ï¼Œåœ¨FinFactæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜å…¶F1åˆ†æ•°æ˜¾è‘—ä¼˜äºç‹¬ç«‹RAGæ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.11541v1">Query-Specific GNN: A Comprehensive Graph Representation Learning Method for Retrieval Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) has demonstrated its ability to enhance
Large Language Models (LLMs) by integrating external knowledge sources.
However, multi-hop questions, which require the identification of multiple
knowledge targets to form a synthesized answer, raise new challenges for RAG
systems. Under the multi-hop settings, existing methods often struggle to fully
understand the questions with complex semantic structures and are susceptible
to irrelevant noise during the retrieval of multiple information targets. To
address these limitations, we propose a novel graph representation learning
framework for multi-hop question retrieval. We first introduce a
Multi-information Level Knowledge Graph (Multi-L KG) to model various
information levels for a more comprehensive understanding of multi-hop
questions. Based on this, we design a Query-Specific Graph Neural Network
(QSGNN) for representation learning on the Multi-L KG. QSGNN employs
intra/inter-level message passing mechanisms, and in each message passing the
information aggregation is guided by the query, which not only facilitates
multi-granular information aggregation but also significantly reduces the
impact of noise. To enhance its ability to learn robust representations, we
further propose two synthesized data generation strategies for pre-training the
QSGNN. Extensive experimental results demonstrate the effectiveness of our
framework in multi-hop scenarios, especially in high-hop questions the
improvement can reach 33.8\%. The code is available at:
https://github.com/Jerry2398/QSGNN.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡é’ˆå¯¹RAGç³»ç»Ÿåœ¨å¤šè·³é—®é¢˜ï¼ˆéœ€æ£€ç´¢å¤šç›®æ ‡çŸ¥è¯†åˆæˆç­”æ¡ˆï¼‰ä¸­çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å›¾è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ã€‚é€šè¿‡æ„å»ºå¤šå±‚æ¬¡çŸ¥è¯†å›¾è°±ï¼ˆMulti-L KGï¼‰å’Œè®¾è®¡åŸºäºæŸ¥è¯¢çš„å›¾ç¥ç»ç½‘ç»œï¼ˆQSGNNï¼‰ï¼Œåˆ©ç”¨è·¨å±‚çº§ä¿¡æ¯ä¼ é€’å’Œå™ªå£°æŠ‘åˆ¶æœºåˆ¶æå‡å¤šè·³æ£€ç´¢æ€§èƒ½ï¼Œå¹¶ç»“åˆåˆæˆæ•°æ®é¢„è®­ç»ƒç­–ç•¥ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨å¤šè·³åœºæ™¯ï¼ˆå°¤å…¶æ˜¯é«˜è·³é—®é¢˜ï¼‰ä¸­æ•ˆæœæ˜¾è‘—æå‡è¾¾33.8%ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.11483v1">Uncertainty Quantification for Retrieval-Augmented Reasoning</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented reasoning (RAR) is a recent evolution of
retrieval-augmented generation (RAG) that employs multiple reasoning steps for
retrieval and generation. While effective for some complex queries, RAR remains
vulnerable to errors and misleading outputs. Uncertainty quantification (UQ)
offers methods to estimate the confidence of systems' outputs. These methods,
however, often handle simple queries with no retrieval or single-step
retrieval, without properly handling RAR setup. Accurate estimation of UQ for
RAR requires accounting for all sources of uncertainty, including those arising
from retrieval and generation. In this paper, we account for all these sources
and introduce Retrieval-Augmented Reasoning Consistency (R2C)--a novel UQ
method for RAR. The core idea of R2C is to perturb the multi-step reasoning
process by applying various actions to reasoning steps. These perturbations
alter the retriever's input, which shifts its output and consequently modifies
the generator's input at the next step. Through this iterative feedback loop,
the retriever and generator continuously reshape one another's inputs, enabling
us to capture uncertainty arising from both components. Experiments on five
popular RAR systems across diverse QA datasets show that R2C improves AUROC by
over 5% on average compared to the state-of-the-art UQ baselines. Extrinsic
evaluations using R2C as an external signal further confirm its effectiveness
for two downstream tasks: in Abstention, it achieves ~5% gains in both
F1Abstain and AccAbstain; in Model Selection, it improves the exact match by
~7% over single models and ~3% over selection methods.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹æ£€ç´¢å¢å¼ºæ¨ç†ï¼ˆRARï¼ŒRAGçš„å¤šæ­¥æ¨ç†æ‰©å±•ï¼‰çš„æ–°å‹ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•R2Cï¼Œé€šè¿‡æ‰°åŠ¨å¤šæ­¥æ¨ç†è¿‡ç¨‹å¹¶è¿­ä»£åé¦ˆæ£€ç´¢ä¸ç”Ÿæˆç»„ä»¶çš„è¾“å…¥å·®å¼‚ï¼Œæ˜¾è‘—æå‡äº†å¤æ‚æŸ¥è¯¢ä¸‹çš„ä¸ç¡®å®šæ€§ä¼°è®¡æ€§èƒ½ï¼Œå®éªŒè¯æ˜å…¶åœ¨å¤šé¡¹ä¸‹æ¸¸ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰åŸºçº¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.11394v1">VeriCite: Towards Reliable Citations in Retrieval-Augmented Generation via Rigorous Verification</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has emerged as a crucial approach for
enhancing the responses of large language models (LLMs) with external knowledge
sources. Despite the impressive performance in complex question-answering
tasks, RAG still struggles with hallucinations. Attributing RAG-generated
content through in-line citations has demonstrated potential in reducing
hallucinations and facilitating human verification. Existing citation
generation methods primarily rely on either fine-tuning the generator or
employing post-processing approaches for citation matching. However, the former
approach demands substantial annotated data and computational resources, while
the latter often encounters difficulties in managing multiple citations and
frequently produces suboptimal results. In this paper, we introduce a novel
framework, called VeriCite, designed to rigorously validate supporting evidence
and enhance answer attribution. Specifically, VeriCite breaks down into a
three-stage generation: 1) The initial answer generation first generates a
response based on all available contexts and has its claims verified through
the NLI model; 2) the supporting evidence selection assesses the utility of
each document and extracts useful supporting evidences; 3) the final answer
refinement integrates the initial response and collected evidences to produce
the final, refined answer.We conduct experiments across five open-source LLMs
and four datasets, demonstrating that VeriCite can significantly improve
citation quality while maintaining the correctness of the answers.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºVeriCiteçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³RAGæŠ€æœ¯åœ¨ç”Ÿæˆå†…å®¹æ—¶å¯èƒ½å‡ºç°å¹»è§‰çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸‰é˜¶æ®µç”Ÿæˆè¿‡ç¨‹ï¼ˆåˆå§‹ç­”æ¡ˆç”Ÿæˆã€æ”¯æŒè¯æ®é€‰æ‹©ã€æœ€ç»ˆç­”æ¡ˆç²¾ç‚¼ï¼‰æ¥éªŒè¯æ”¯æŒè¯æ®å¹¶å¢å¼ºç­”æ¡ˆçš„å¯è¿½æº¯æ€§ï¼Œå®éªŒè¡¨æ˜VeriCiteèƒ½æ˜¾è‘—æé«˜å¼•ç”¨è´¨é‡å¹¶ä¿æŒç­”æ¡ˆå‡†ç¡®æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.11358v1">LLM-Specific Utility: A New Perspective for Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
incorporating external knowledge. While traditional retrieval focuses on
relevance, RAG's effectiveness depends on the utility of retrieved passages,
i.e., the usefulness in facilitating the generation of an accurate and
comprehensive answer. Existing studies often treat utility as a generic
attribute, ignoring the fact that different LLMs may benefit differently from
the same passage due to variations in internal knowledge and comprehension
ability. In this work, we introduce and systematically investigate the notion
of LLM-specific utility. Through large-scale experiments across multiple
datasets and LLMs, we demonstrate that human-annotated passages are not optimal
for LLMs and that ground-truth utilitarian passages are not transferable across
different LLMs. These findings highlight the necessity of adopting the
LLM-specific utility in RAG research. Our findings indicate that some
human-annotated passages are not ground-truth utilitarian passages for specific
LLMs, partially due to the varying readability of queries and passages for
LLMs, a tendency for which perplexity is a key metric. Based on these findings,
we propose a benchmarking procedure for LLM-specific utility judgments. We
evaluate existing utility judgment methods on six datasets and find that while
verbalized methods using pseudo-answers perform robustly, LLMs struggle to
assess utility effectively-failing to reject all passages for known queries and
to select truly useful ones for unknown queries.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­æ£€ç´¢å†…å®¹çš„æ•ˆç”¨é—®é¢˜ï¼Œæå‡ºå¹¶ç ”ç©¶äº†LLMç‰¹å®šçš„æ•ˆç”¨æ¦‚å¿µã€‚é€šè¿‡å¤§è§„æ¨¡å®éªŒï¼Œä½œè€…å‘ç°äººç±»æ ‡æ³¨çš„æ®µè½å¯¹ä¸åŒLLMå¹¶éæœ€ä¼˜ï¼Œä¸”æ•ˆç”¨æ®µè½åœ¨ä¸åŒLLMé—´ä¸å¯è¿ç§»ï¼Œå¼ºè°ƒäº†åœ¨RAGç ”ç©¶ä¸­è€ƒè™‘LLMç‰¹å®šæ•ˆç”¨çš„å¿…è¦æ€§ã€‚è®ºæ–‡è¿˜æå‡ºäº†åŸºäºLLMç‰¹å®šæ•ˆç”¨çš„åŸºå‡†æµ‹è¯•æµç¨‹ï¼Œå¹¶è¯„ä¼°äº†ç°æœ‰æ•ˆç”¨åˆ¤æ–­æ–¹æ³•çš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.11217v1">Domain-Specific Data Generation Framework for RAG Adaptation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) combines the language understanding and
reasoning power of large language models (LLMs) with external retrieval to
enable domain-grounded responses. Effectively adapting RAG systems to
domain-specific settings requires specialized, context-rich training data
beyond general-purpose question-answering. Here, we propose RAGen, a scalable
and modular framework for generating domain-grounded question-answer-context
(QAC) triples tailored to diverse RAG adaptation approaches. RAGen produces
these QAC triples by identifying key concepts in documents, generating diverse
questions guided by Bloom's Taxonomy-inspired principles, and pairing them with
precise answers extracted from relevant contexts. RAGen supports multiple RAG
adaptation strategies, including the optimization of key components such as the
LLM, retriever, and embedding model, etc. Its modular pipeline features
semantic chunking, hierarchical concept extraction, and multi-chunk retrieval,
along with the introduction of curated distractor contexts to promote robust
reasoning. Designed for scalability, RAGen efficiently handles large and
evolving document corpora without redundant processing, making it especially
suitable for dynamic evolving domains such as scientific research and
enterprise knowledge bases.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†RAGenï¼Œä¸€ä¸ªå¯æ‰©å±•ä¸”æ¨¡å—åŒ–çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé’ˆå¯¹ç‰¹å®šé¢†åŸŸå®šåˆ¶çš„é—®ç­”ä¸Šä¸‹æ–‡ä¸‰å…ƒç»„ï¼ˆQACï¼‰ï¼Œä»¥æ”¯æŒä¸åŒRAGç³»ç»Ÿçš„é€‚åº”æ€§ä¼˜åŒ–ï¼ŒåŒ…æ‹¬æ”¹è¿›å¤§è¯­è¨€æ¨¡å‹ã€æ£€ç´¢å™¨å’ŒåµŒå…¥æ¨¡å‹ç­‰å…³é”®ç»„ä»¶ï¼Œå¹¶é€‚ç”¨äºåŠ¨æ€æ¼”è¿›çš„é¢†åŸŸå¦‚ç§‘ç ”å’Œä¼ä¸šçŸ¥è¯†åº“ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.11195v1">RAG-Pull: Imperceptible Attacks on RAG Systems for Code Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) increases the reliability and
trustworthiness of the LLM response and reduces hallucination by eliminating
the need for model retraining. It does so by adding external data into the
LLM's context. We develop a new class of black-box attack, RAG-Pull, that
inserts hidden UTF characters into queries or external code repositories,
redirecting retrieval toward malicious code, thereby breaking the models'
safety alignment. We observe that query and code perturbations alone can shift
retrieval toward attacker-controlled snippets, while combined query-and-target
perturbations achieve near-perfect success. Once retrieved, these snippets
introduce exploitable vulnerabilities such as remote code execution and SQL
injection. RAG-Pull's minimal perturbations can alter the model's safety
alignment and increase preference towards unsafe code, therefore opening up a
new class of attacks on LLMs.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹RAGçš„æ–°æ”»å‡»æ–¹æ³•RAG-Pullï¼Œé€šè¿‡åœ¨æŸ¥è¯¢æˆ–å¤–éƒ¨ä»£ç åº“ä¸­æ’å…¥éšè—çš„UTFå­—ç¬¦ï¼Œå°†æ£€ç´¢ç»“æœå¯¼å‘æ¶æ„ä»£ç ï¼Œä»è€Œç ´åæ¨¡å‹çš„å®‰å…¨å¯¹é½æ€§ï¼Œå¯¼è‡´è¿œç¨‹ä»£ç æ‰§è¡Œå’ŒSQLæ³¨å…¥ç­‰æ¼æ´è¢«åˆ©ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.11122v1">DyKnow-RAG: Dynamic Knowledge Utilization Reinforcement Framework for Noisy Retrieval-Augmented Generation in E-commerce Search Relevance</a></td><td><details><summary>å±•å¼€</summary>Accurately modeling query-item relevance drives e-commerce ranking, yet
long-tail, knowledge-heavy, and fast-evolving queries exceed parametric LLM
coverage. External context (reviews, attribute encyclopedias, UGC) can help but
is noisy, and single-pass latency and cost forbid any clean-then-summarize
step. The model must, per query, judge relevance and decide whether to use,
partially use, or ignore the context. DyKnow-RAG is a dynamic noisy-RAG
framework built on Group Relative Policy Optimization. It trains two rollout
groups (no external context vs a single retrieved chunk) and applies
posterior-driven inter-group advantage scaling that adaptively reweights their
contributions by the per-query correctness gap. This teaches when to trust
retrieval versus fall back to parametric knowledge, without process labels,
value networks, or extra inference passes, preserving single-pass, single-chunk
deployment under production latency. Training combines: (1) supervised
initialization with a structured rationale that explicitly records the
context-usage decision; (2) an RL pool prioritized by SFT uncertainty to focus
where context choice is most consequential; and (3) an optional lightweight DPO
warm start to stabilize with-context calibration. Under a unified
retrieval/index and fixed latency budget, DyKnow-RAG outperforms SFT, DPO, and
vanilla GRPO in offline tests, and delivers consistent lifts on GSB, Query
Goodrate, and Item Goodrate in Taobao A/B testing. It is deployed in Taobao's
production relevance system, serving live traffic. To our knowledge, it is
among the first single-pass RAG solutions for e-commerce relevance, turning
noisy external signals into reliable gains without added online complexity.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†DyKnow-RAGæ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€å™ªå£°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆDynamic Noisy-RAGï¼‰æŠ€æœ¯ä¼˜åŒ–ç”µå•†æ’åºä¸­çš„æŸ¥è¯¢-å•†å“ç›¸å…³æ€§å»ºæ¨¡ã€‚è¯¥ç³»ç»ŸåŸºäºGroup Relative Policy Optimizationï¼ˆGRPOï¼‰ï¼Œç»“åˆç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ï¼ŒåŠ¨æ€å†³å®šä½•æ—¶åˆ©ç”¨æ£€ç´¢çš„å¤–éƒ¨ä¸Šä¸‹æ–‡ï¼ˆå¦‚è¯„è®ºã€ç™¾ç§‘ç­‰ï¼‰ï¼Œè§£å†³é•¿å°¾ã€çŸ¥è¯†å¯†é›†åŠå¿«é€Ÿå˜åŒ–æŸ¥è¯¢çš„è¦†ç›–é—®é¢˜ã€‚DyKnow-RAGåœ¨è®­ç»ƒä¸­é€šè¿‡è‡ªé€‚åº”æƒé‡è°ƒæ•´å’Œå•æ¬¡æ¨ç†éƒ¨ç½²ï¼Œå®ç°äº†æ£€ç´¢å¯ä¿¡åº¦åˆ¤æ–­ä¸å‚æ•°åŒ–çŸ¥è¯†çš„äº’è¡¥ï¼Œæœ€ç»ˆåœ¨æ·˜å®ç”Ÿäº§ç¯å¢ƒä¸­æ˜¾è‘—æå‡äº†ç›¸å…³æ€§æŒ‡æ ‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.10931v1">PoU: Proof-of-Use to Counter Tool-Call Hacking in DeepResearch Agents</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) agents, such as recent
DeepResearch-style systems, extend large language models (LLMs) with autonomous
information-seeking capabilities through external tools. While reinforcement
learning (RL) has enabled impressive multi-step reasoning, we identify a
previously overlooked failure mode, Tool-Call Hacking, where agents inflate
reward signals by issuing superficially correct tool calls without genuinely
leveraging the retrieved evidence. This results in (i) mode collapse into
repetitive reliance on a single source and (ii) spurious grounding, where
answers are only weakly supported by cited content.
  To address this, we propose Proof-of-Use (PoU), an evidence-grounded RL
framework that enforces verifiable causal links between retrieved evidence,
reasoning traces, and final answers. PoU operationalizes this through a unified
step-wise contract combining syntactic citation validation, perturbation-based
sensitivity rewards, and answer-evidence alignment objectives, ensuring that
tool usage remains both interpretable and functionally grounded.
  Across seven QA benchmarks spanning in-domain, out-of-domain, and
out-of-tool-distribution settings, PoU consistently outperforms strong
DeepResearch baselines in factual accuracy, evidence faithfulness, and
tool-routing balance. These findings highlight the necessity of grounding
RL-trained agents not merely in task outcomes but in the causal use of
retrieved information, offering a principled path toward trustworthy
retrieval-augmented reasoning.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æ¢è®¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ™ºèƒ½ä½“ä¸­çš„"å·¥å…·è°ƒç”¨é»‘å®¢"é—®é¢˜ï¼Œå³æ¨¡å‹é€šè¿‡è¡¨é¢æ­£ç¡®çš„å·¥å…·è°ƒç”¨è€Œéå®é™…åˆ©ç”¨æ£€ç´¢è¯æ®æ¥æå‡å¥–åŠ±ä¿¡å·ï¼Œå¯¼è‡´æ¨¡å¼å´©æºƒå’Œè™šå‡å¼•ç”¨ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸º"ä½¿ç”¨è¯æ˜"ï¼ˆPoUï¼‰çš„æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆè¯­æ³•å¼•ç”¨éªŒè¯ã€åŸºäºæ‰°åŠ¨çš„æ•æ„Ÿæ€§å¥–åŠ±å’Œç­”æ¡ˆ-è¯æ®å¯¹é½ç›®æ ‡ï¼Œç¡®ä¿å·¥å…·ä½¿ç”¨çš„å¯è§£é‡Šæ€§å’ŒåŠŸèƒ½æ€§åŸºç¡€ï¼Œåœ¨å¤šä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-12
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.10828v1">VeritasFi: An Adaptable, Multi-tiered RAG Framework for Multi-modal Financial Question Answering</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) is becoming increasingly essential for
Question Answering (QA) in the financial sector, where accurate and
contextually grounded insights from complex public disclosures are crucial.
However, existing financial RAG systems face two significant challenges: (1)
they struggle to process heterogeneous data formats, such as text, tables, and
figures; and (2) they encounter difficulties in balancing general-domain
applicability with company-specific adaptation. To overcome these challenges,
we present VeritasFi, an innovative hybrid RAG framework that incorporates a
multi-modal preprocessing pipeline alongside a cutting-edge two-stage training
strategy for its re-ranking component. VeritasFi enhances financial QA through
three key innovations: (1) A multi-modal preprocessing pipeline that seamlessly
transforms heterogeneous data into a coherent, machine-readable format. (2) A
tripartite hybrid retrieval engine that operates in parallel, combining deep
multi-path retrieval over a semantically indexed document corpus, real-time
data acquisition through tool utilization, and an expert-curated memory bank
for high-frequency questions, ensuring comprehensive scope, accuracy, and
efficiency. (3) A two-stage training strategy for the document re-ranker, which
initially constructs a general, domain-specific model using anonymized data,
followed by rapid fine-tuning on company-specific data for targeted
applications. By integrating our proposed designs, VeritasFi presents a
groundbreaking framework that greatly enhances the adaptability and robustness
of financial RAG systems, providing a scalable solution for both general-domain
and company-specific QA tasks. Code accompanying this work is available at
https://github.com/simplew4y/VeritasFi.git.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºVeritasFiçš„æ··åˆRAGæ¡†æ¶ï¼Œé’ˆå¯¹é‡‘èé¢†åŸŸé—®ç­”ç³»ç»Ÿä¸­çš„å¼‚æ„æ•°æ®å¤„ç†å’Œé€šç”¨æ€§ä¸å…¬å¸ç‰¹å®šé€‚åº”æ€§å¹³è¡¡é—®é¢˜ï¼Œæå‡ºäº†å¤šæ¨¡æ€é¢„å¤„ç†æµæ°´çº¿ã€ä¸‰é‡æ··åˆæ£€ç´¢å¼•æ“åŠä¸¤é˜¶æ®µæ–‡æ¡£é‡æ’åºè®­ç»ƒç­–ç•¥ï¼Œä»¥æå‡é‡‘èRAGç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œé€‚åº”æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.10824v1">Agentic RAG for Software Testing with Hybrid Vector-Graph and Multi-Agent Orchestration</a></td><td><details><summary>å±•å¼€</summary>We present an approach to software testing automation using Agentic
Retrieval-Augmented Generation (RAG) systems for Quality Engineering (QE)
artifact creation. We combine autonomous AI agents with hybrid vector-graph
knowledge systems to automate test plan, case, and QE metric generation. Our
approach addresses traditional software testing limitations by leveraging LLMs
such as Gemini and Mistral, multi-agent orchestration, and enhanced
contextualization. The system achieves remarkable accuracy improvements from
65% to 94.8% while ensuring comprehensive document traceability throughout the
quality engineering lifecycle. Experimental validation of enterprise Corporate
Systems Engineering and SAP migration projects demonstrates an 85% reduction in
testing timeline, an 85% improvement in test suite efficiency, and projected
35% cost savings, resulting in a 2-month acceleration of go-live.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨åŸºäºä»£ç†çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆAgentic RAGï¼‰ç³»ç»Ÿè‡ªåŠ¨åŒ–ç”Ÿæˆè´¨é‡å·¥ç¨‹ï¼ˆQEï¼‰å·¥ä»¶çš„æ–¹æ³•ï¼Œç»“åˆè‡ªä¸»AIä»£ç†ä¸æ··åˆå‘é‡-å›¾çŸ¥è¯†ç³»ç»Ÿï¼Œæ˜¾è‘—æå‡äº†æµ‹è¯•è®¡åˆ’ã€ç”¨ä¾‹å’ŒæŒ‡æ ‡çš„ç”Ÿæˆå‡†ç¡®æ€§ï¼ˆ65%â†’94.8%ï¼‰ï¼Œå¹¶åœ¨ä¼ä¸šçº§é¡¹ç›®ä¸­éªŒè¯äº†85%çš„æµ‹è¯•æ—¶é—´ç¼©å‡å’Œ35%æˆæœ¬èŠ‚çº¦æ•ˆæœã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.10815v1">DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems</a></td><td><details><summary>å±•å¼€</summary>Automating the formalization of mathematical statements for theorem proving
remains a major challenge for Large Language Models (LLMs). LLMs struggle to
identify and utilize the prerequisite mathematical knowledge and its
corresponding formal representation in languages like Lean. Current
retrieval-augmented autoformalization methods query external libraries using
the informal statement directly, but overlook a fundamental limitation:
informal mathematical statements are often complex and offer limited context on
the underlying math concepts. To address this, we introduce DRIFT, a novel
framework that enables LLMs to decompose informal mathematical statements into
smaller, more tractable ''sub-components''. This facilitates targeted retrieval
of premises from mathematical libraries such as Mathlib. Additionally, DRIFT
retrieves illustrative theorems to help models use premises more effectively in
formalization tasks. We evaluate DRIFT across diverse benchmarks (ProofNet,
ConNF, and MiniF2F-test) and find that it consistently improves premise
retrieval, nearly doubling the F1 score compared to the DPR baseline on
ProofNet. Notably, DRIFT demonstrates strong performance on the
out-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14% and
42.25% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that
retrieval effectiveness in mathematical autoformalization depends heavily on
model-specific knowledge boundaries, highlighting the need for adaptive
retrieval strategies aligned with each model's capabilities.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†DRIFTæ¡†æ¶ï¼Œé€šè¿‡å°†éæ­£å¼æ•°å­¦é™ˆè¿°åˆ†è§£ä¸ºæ›´å°çš„å­ç»„ä»¶æ¥æ”¹è¿›æ£€ç´¢å¢å¼ºçš„è‡ªåŠ¨å½¢å¼åŒ–æ–¹æ³•ï¼Œå¹¶åˆ©ç”¨é’ˆå¯¹æ€§æ£€ç´¢æ•°å­¦åº“ä¸­çš„å‰æå’Œç¤ºä¾‹å®šç†ï¼Œæ˜¾è‘—æå‡äº†LLMsåœ¨æ•°å­¦å®šç†è¯æ˜ä¸­çš„å½¢å¼åŒ–èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.10806v1">Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based Structures</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) are adept at generating responses based on
information within their context. While this ability is useful for interacting
with structured data like code files, another popular method,
Retrieval-Augmented Generation (RAG), retrieves relevant documents to augment
the model's in-context learning. However, it is not well-explored how to best
represent this retrieved knowledge for generating responses on structured data,
particularly hierarchical structures like trees. In this work, we propose a
novel bottom-up method to linearize knowledge from tree-like structures (like a
GitHub repository) by generating implicit, aggregated summaries at each
hierarchical level. This approach enables the knowledge to be stored in a
knowledge base and used directly with RAG. We then compare our method to using
RAG on raw, unstructured code, evaluating the accuracy and quality of the
generated responses. Our results show that while response quality is comparable
across both methods, our approach generates over 68% fewer documents in the
retriever, a significant gain in efficiency. This finding suggests that
leveraging implicit, linearized knowledge may be a highly effective and
scalable strategy for handling complex, hierarchical data structures.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•ä¼˜åŒ–RAGåœ¨å¤„ç†ç»“æ„åŒ–æ•°æ®ï¼ˆå¦‚æ ‘å½¢ç»“æ„çš„GitHubä»“åº“ï¼‰æ—¶çš„çŸ¥è¯†è¡¨ç¤ºæ–¹æ³•ï¼Œæå‡ºäº†ä¸€ç§è‡ªåº•å‘ä¸Šç”Ÿæˆéšå¼èšåˆæ‘˜è¦çš„æ–°æ–¹æ³•ï¼Œæ˜¾è‘—å‡å°‘äº†æ£€ç´¢æ–‡æ¡£æ•°é‡ï¼ŒåŒæ—¶ä¿æŒäº†å›ç­”è´¨é‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.10787v1">Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG</a></td><td><details><summary>å±•å¼€</summary>The performance gains of LLMs have historically been driven by scaling up
model size and training data. However, the rapidly diminishing availability of
high-quality training data is introducing a fundamental bottleneck, shifting
the focus of research toward inference-time scaling. This paradigm uses
additional computation at the time of deployment to substantially improve LLM
performance on downstream tasks without costly model re-training. This review
systematically surveys the diverse techniques contributing to this new era of
inference-time scaling, organizing the rapidly evolving field into two
comprehensive perspectives: Output-focused and Input-focused methods.
Output-focused techniques encompass complex, multi-step generation strategies,
including reasoning (e.g., CoT, ToT, ReAct), various search and decoding
methods (e.g., MCTS, beam search), training for long CoT (e.g., RLVR, GRPO),
and model ensemble methods. Input-focused techniques are primarily categorized
by few-shot and RAG, with RAG as the central focus. The RAG section is further
detailed through a structured examination of query expansion, data, retrieval
and reranker, LLM generation methods, and multi-modal RAG.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç³»ç»Ÿåœ°è°ƒç ”äº†æ¨ç†æ—¶æ‰©å±•ï¼ˆinference-time scalingï¼‰çš„å¤šæ ·åŒ–æŠ€æœ¯ï¼Œé‡ç‚¹å…³æ³¨è¾“å…¥å¯¼å‘æ–¹æ³•ä¸­çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ï¼Œå¹¶è¯¦ç»†æ¢è®¨äº†RAGçš„æŸ¥è¯¢æ‰©å±•ã€æ•°æ®æ£€ç´¢ã€é‡æ’åºã€ç”Ÿæˆæ–¹æ³•åŠå¤šæ¨¡æ€åº”ç”¨ï¼Œå°†å…¶è§†ä¸ºæå‡å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½çš„å…³é”®æ–¹å‘ä¹‹ä¸€ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.10549v1">ELAIPBench: A Benchmark for Expert-Level Artificial Intelligence Paper Understanding</a></td><td><details><summary>å±•å¼€</summary>While large language models (LLMs) excel at many domain-specific tasks, their
ability to deeply comprehend and reason about full-length academic papers
remains underexplored. Existing benchmarks often fall short of capturing such
depth, either due to surface-level question design or unreliable evaluation
metrics. To address this gap, we introduce ELAIPBench, a benchmark curated by
domain experts to evaluate LLMs' comprehension of artificial intelligence (AI)
research papers. Developed through an incentive-driven, adversarial annotation
process, ELAIPBench features 403 multiple-choice questions from 137 papers. It
spans three difficulty levels and emphasizes non-trivial reasoning rather than
shallow retrieval. Our experiments show that the best-performing LLM achieves
an accuracy of only 39.95%, far below human performance. Moreover, we observe
that frontier LLMs equipped with a thinking mode or a retrieval-augmented
generation (RAG) system fail to improve final results-even harming accuracy due
to overthinking or noisy retrieval. These findings underscore the significant
gap between current LLM capabilities and genuine comprehension of academic
papers.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ELAIPBenchï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹äººå·¥æ™ºèƒ½ç ”ç©¶è®ºæ–‡ç†è§£èƒ½åŠ›çš„ä¸“å®¶æ ‡æ³¨åŸºå‡†ï¼Œç ”ç©¶å‘ç°å³ä¾¿é…å¤‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿçš„å…ˆè¿›LLMsä¹Ÿæ— æ³•æå‡æ€§èƒ½ï¼Œç”šè‡³å› æ£€ç´¢å™ªå£°è€Œé™ä½å‡†ç¡®æ€§ï¼Œæ­ç¤ºäº†LLMsåœ¨æ·±åº¦ç†è§£å­¦æœ¯è®ºæ–‡æ–¹é¢çš„ä¸è¶³ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.10480v1">Latent Retrieval Augmented Generation of Cross-Domain Protein Binders</a></td><td><details><summary>å±•å¼€</summary>Designing protein binders targeting specific sites, which requires to
generate realistic and functional interaction patterns, is a fundamental
challenge in drug discovery. Current structure-based generative models are
limited in generating nterfaces with sufficient rationality and
interpretability. In this paper, we propose Retrieval-Augmented Diffusion for
Aligned interface (RADiAnce), a new framework that leverages known interfaces
to guide the design of novel binders. By unifying retrieval and generation in a
shared contrastive latent space, our model efficiently identifies relevant
interfaces for a given binding site and seamlessly integrates them through a
conditional latent diffusion generator, enabling cross-domain interface
transfer. Extensive exeriments show that RADiAnce significantly outperforms
baseline models across multiple metrics, including binding affinity and
recovery of geometries and interactions. Additional experimental results
validate cross-domain generalization, demonstrating that retrieving interfaces
from diverse domains, such as peptides, antibodies, and protein fragments,
enhances the generation performance of binders for other domains. Our work
establishes a new paradigm for protein binder design that successfully bridges
retrieval-based knowledge and generative AI, opening new possibilities for drug
discovery.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºRADiAnceçš„æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆæ£€ç´¢å·²çŸ¥è›‹ç™½è´¨æ¥å£å’Œç”Ÿæˆæ–°ç»‘å®šå‰‚çš„æ–¹æ³•ï¼Œåˆ©ç”¨å…±äº«å¯¹æ¯”æ½œç©ºé—´å’Œæ¡ä»¶æ½œæ‰©æ•£ç”Ÿæˆå™¨ï¼Œå®ç°äº†è·¨é¢†åŸŸæ¥å£è½¬ç§»ï¼Œæ˜¾è‘—æå‡äº†è›‹ç™½è´¨ç»‘å®šå‰‚è®¾è®¡çš„æ€§èƒ½ï¼Œä¸ºè¯ç‰©å‘ç°æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.10452v1">Steering Over-refusals Towards Safety in Retrieval Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Safety alignment in large language models (LLMs) induces over-refusals --
where LLMs decline benign requests due to aggressive safety filters. We analyze
this phenomenon in retrieval-augmented generation (RAG), where both the query
intent and retrieved context properties influence refusal behavior. We
construct RagRefuse, a domain-stratified benchmark spanning medical, chemical,
and open domains, pairing benign and harmful queries with controlled context
contamination patterns and sizes. Our analysis shows that context arrangement /
contamination, domain of query and context, and harmful-text density trigger
refusals even on benign queries, with effects depending on model-specific
alignment choices. To mitigate over-refusals, we introduce
\textsc{SafeRAG-Steering}, a model-centric embedding intervention that steers
the embedding regions towards the confirmed safe, non-refusing output regions
at inference time. This reduces over-refusals in contaminated RAG pipelines
while preserving legitimate refusals.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­å› å®‰å…¨å¯¹é½å¯¼è‡´çš„è¿‡åº¦æ‹’ç»é—®é¢˜ï¼Œæ„å»ºäº†å¤šé¢†åŸŸåŸºå‡†æµ‹è¯•RagRefuseåˆ†æå½±å“å› ç´ ï¼Œå¹¶æå‡ºäº†SafeRAG-Steeringæ–¹æ³•ï¼Œé€šè¿‡åµŒå…¥å¹²é¢„å‡å°‘æ— å®³æŸ¥è¯¢çš„è¯¯æ‹’ï¼ŒåŒæ—¶ä¿æŒå¯¹æœ‰å®³è¯·æ±‚çš„åˆç†æ‹’ç»ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.10448v1">RECON: Reasoning with Condensation for Efficient Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) systems trained using reinforcement
learning (RL) with reasoning are hampered by inefficient context management,
where long, noisy retrieved documents increase costs and degrade performance.
We introduce RECON (REasoning with CONdensation), a framework that integrates
an explicit summarization module to compress evidence within the reasoning
loop. Our summarizer is trained via a two-stage process: relevance pretraining
on QA datasets, followed by multi-aspect distillation from proprietary LLMs to
ensure factuality and clarity. Integrated into the Search-R1 pipeline, RECON
reduces total context length by 35\%, leading to improved training speed and
inference latency, while simultaneously improving RAG performance on downstream
QA benchmarks. Notably, it boosts the average EM score of the 3B model by
14.5\% and the 7B model by 3.0\%, showing particular strength in multi-hop QA.
RECON demonstrates that learned context compression is essential for building
practical, scalable, and performant RAG systems. Our code implementation is
made available at https://github.com/allfornancy/RECON.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºRECONçš„æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥æ˜¾å¼æ‘˜è¦æ¨¡å—åœ¨æ¨ç†å¾ªç¯ä¸­å‹ç¼©æ£€ç´¢è¯æ®ï¼Œè§£å†³äº†RAGç³»ç»Ÿä¸­é•¿ä¸”å™ªå£°æ–‡æ¡£å¯¼è‡´çš„æ•ˆç‡ä½ä¸‹é—®é¢˜ï¼Œå¹¶ç»“åˆå¼ºåŒ–å­¦ä¹ å’Œå¤šé˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œæ˜¾è‘—å‡å°‘äº†ä¸Šä¸‹æ–‡é•¿åº¦å¹¶æå‡äº†QAä»»åŠ¡æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.10426v1">Taming a Retrieval Framework to Read Images in Humanlike Manner for Augmenting Generation of MLLMs</a></td><td><details><summary>å±•å¼€</summary>Multimodal large language models (MLLMs) often fail in fine-grained visual
question answering, producing hallucinations about object identities,
positions, and relations because textual queries are not explicitly anchored to
visual referents. Retrieval-augmented generation (RAG) alleviates some errors,
but it fails to align with human-like processing at both the retrieval and
augmentation levels. Specifically, it focuses only on global-level image
information but lacks local detail and limits reasoning about fine-grained
interactions. To overcome this limitation, we present Human-Like
Retrieval-Augmented Generation (HuLiRAG), a framework that stages multimodal
reasoning as a ``what--where--reweight'' cascade. Queries are first anchored to
candidate referents via open-vocabulary detection (what), then spatially
resolved with SAM-derived masks to recover fine-grained precision (where), and
adaptively prioritized through the trade-off between local and global alignment
(reweight). Mask-guided fine-tuning further injects spatial evidence into the
generation process, transforming grounding from a passive bias into an explicit
constraint on answer formulation. Extensive experiments demonstrate that this
human-like cascade improves grounding fidelity and factual consistency while
reducing hallucinations, advancing multimodal question answering toward
trustworthy reasoning.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºHuLiRAGçš„æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡"What-Where-Reweight"å¤šçº§æ¨ç†æœºåˆ¶æ”¹è¿›ä¼ ç»ŸRAGåœ¨ç»†ç²’åº¦è§†è§‰é—®ç­”ä¸­çš„ä¸è¶³ã€‚è¯¥æ¡†æ¶ç»“åˆå¼€æ”¾è¯æ±‡æ£€æµ‹ï¼ˆå®šä½ç‰©ä½“ï¼‰ã€SAMåˆ†å‰²ï¼ˆç²¾ç¡®å®šä½ç©ºé—´å…³ç³»ï¼‰å’Œè‡ªé€‚åº”æƒé‡è°ƒæ•´ï¼Œå¹¶å¼•å…¥æ©ç å¯¼å‘å¾®è°ƒï¼Œæ˜¾è‘—å‡å°‘äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç‰©ä½“è¯†åˆ«ã€ä½ç½®å’Œå…³ç³»æ¨ç†ä¸­çš„å¹»è§‰é—®é¢˜ï¼Œæå‡äº†å›ç­”çš„å¯é æ€§å’Œäº‹å®ä¸€è‡´æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.10390v1">RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models</a></td><td><details><summary>å±•å¼€</summary>The ability of language models in RAG systems to selectively refuse to answer
based on flawed context is critical for safety, yet remains a significant
failure point. Our large-scale study reveals that even frontier models struggle
in this setting, with refusal accuracy dropping below 50% on multi-document
tasks, while exhibiting either dangerous overconfidence or overcaution. Static
benchmarks fail to reliably evaluate this capability, as models exploit
dataset-specific artifacts and memorize test instances. We introduce
RefusalBench, a generative methodology that programmatically creates diagnostic
test cases through controlled linguistic perturbation. Our framework employs
176 distinct perturbation strategies across six categories of informational
uncertainty and three intensity levels. Evaluation of over 30 models uncovers
systematic failure patterns: refusal comprises separable detection and
categorization skills, and neither scale nor extended reasoning improves
performance. We find that selective refusal is a trainable, alignment-sensitive
capability, offering a clear path for improvement. We release two benchmarks --
RefusalBench-NQ (single document) and RefusalBench-GaRAGe (multi-document) --
and our complete generation framework to enable continued, dynamic evaluation
of this critical capability.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†RAGç³»ç»Ÿä¸­è¯­è¨€æ¨¡å‹åŸºäºæœ‰ç¼ºé™·çš„ä¸Šä¸‹æ–‡é€‰æ‹©æ‹’ç»å›ç­”çš„èƒ½åŠ›ï¼Œå‘ç°å‰æ²¿æ¨¡å‹åœ¨è¯¥ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œå¹¶æå‡ºRefusalBenchè¿™ä¸€é€šè¿‡ç¨‹åºåŒ–ç”Ÿæˆè¯Šæ–­æµ‹è¯•æ¡ˆä¾‹çš„æ–¹æ³•è®ºæ¡†æ¶ï¼ŒåŒæ—¶å‘å¸ƒäº†ä¸¤ä¸ªåŸºå‡†æµ‹è¯•é›†ä»¥æ”¹è¿›æ¨¡å‹çš„å…³é”®æ‹’ç»èƒ½åŠ›ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-11
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.10129v1">CacheClip: Accelerating RAG with Effective KV Cache Reuse</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) systems suffer from severe
time-to-first-token (TTFT) bottlenecks due to long input sequences. Existing KV
cache reuse methods face a fundamental trade-off: prefix caching requires
identical prefixes that rarely occur in RAG scenarios, while direct
precomputation sacrifices quality due to missing inter-chunk attention and
repeated attention sinks. Recent methods like APE and CacheBlend partially
address these issues but remain inadequate for robust RAG applications. This
paper presents CacheClip, a novel framework that achieves both fast TTFT and
high generation quality. Our key insight is that small auxiliary LLMs exhibit
similar last-layer attention distributions to primary LLMs (the target model
for generation), enabling efficient identification of tokens critical for
restoring inter-chunk attention, thereby significantly improving response
quality on cross-chunk reasoning tasks. CacheClip integrates three techniques:
(1) auxiliary-model-guided token selection for selective KV cache
recomputation, where the auxiliary model is finetuned to improve selection
accuracy, (2) shared prefixes to eliminate redundant attention sinks, and (3)
grouping strategy to maintain local coherence during partial KV cache updates.
Experiments show CacheClip retains up to 94.8% and 85.0% of full-attention
performance on NIAH and LongBench, outperforming APE and CacheBlend by 25.2%
and 35.1% on NIAH (with reomp% = 20%). Meanwhile, CacheClip accelerates LLM
inference by up to 1.92x in prefill time, providing a practical solution to the
efficiency-quality trade-off in RAG systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºCacheClipçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³RAGç³»ç»Ÿä¸­å› é•¿è¾“å…¥åºåˆ—å¯¼è‡´çš„é¦–ä¸ªä»¤ç‰Œç”Ÿæˆæ—¶é—´ï¼ˆTTFTï¼‰ç“¶é¢ˆé—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨è¾…åŠ©å°å‹LLMè¯†åˆ«å…³é”®ä»¤ç‰Œä»¥æ¢å¤è·¨å—æ³¨æ„åŠ›ï¼Œå¹¶ç»“åˆé€‰æ‹©æ€§KVç¼“å­˜é‡è®¡ç®—ã€å…±äº«å‰ç¼€å’Œåˆ†ç»„ç­–ç•¥ï¼ŒCacheClipåœ¨ä¿è¯ç”Ÿæˆè´¨é‡çš„åŒæ—¶æ˜¾è‘—æå‡äº†æ¨ç†æ•ˆç‡ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨å¤šé¡¹ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.10114v1">LinearRAG: Linear Graph Retrieval Augmented Generation on Large-scale Corpora</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) is widely used to mitigate
hallucinations of Large Language Models (LLMs) by leveraging external
knowledge. While effective for simple queries, traditional RAG systems struggle
with large-scale, unstructured corpora where information is fragmented. Recent
advances incorporate knowledge graphs to capture relational structures,
enabling more comprehensive retrieval for complex, multi-hop reasoning tasks.
However, existing graph-based RAG (GraphRAG) methods rely on unstable and
costly relation extraction for graph construction, often producing noisy graphs
with incorrect or inconsistent relations that degrade retrieval quality. In
this paper, we revisit the pipeline of existing GraphRAG systems and propose
LinearRAG (Linear Graph-based Retrieval-Augmented Generation), an efficient
framework that enables reliable graph construction and precise passage
retrieval. Specifically, LinearRAG constructs a relation-free hierarchical
graph, termed Tri-Graph, using only lightweight entity extraction and semantic
linking, avoiding unstable relation modeling. This new paradigm of graph
construction scales linearly with corpus size and incurs no extra token
consumption, providing an economical and reliable indexing of the original
passages. For retrieval, LinearRAG adopts a two-stage strategy: (i) relevant
entity activation via local semantic bridging, followed by (ii) passage
retrieval through global importance aggregation. Extensive experiments on four
datasets demonstrate that LinearRAG significantly outperforms baseline models.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºLinearRAGçš„é«˜æ•ˆæ¡†æ¶ï¼Œé€šè¿‡æ„å»ºæ— å±‚çº§å…³ç³»çš„å…³ç³»æ— å…³å±‚æ¬¡å›¾ï¼ˆTri-Graphï¼‰æ¥è§£å†³ä¼ ç»ŸåŸºäºçŸ¥è¯†å›¾è°±çš„RAGï¼ˆGraphRAGï¼‰æ–¹æ³•ä¸­å…³ç³»æŠ½å–ä¸ç¨³å®šå’Œé«˜æˆæœ¬çš„é—®é¢˜ï¼Œä»è€Œå®ç°å¯é çš„å›¾æ„å»ºå’Œç²¾ç¡®çš„æ®µè½æ£€ç´¢ï¼Œæ˜¾è‘—æå‡äº†å¤§è§„æ¨¡éç»“æ„åŒ–è¯­æ–™åº“ä¸Šçš„æ£€ç´¢æ•ˆæœã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.10008v1">RIPRAG: Hack a Black-box Retrieval-Augmented Generation Question-Answering System with Reinforcement Learning</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) systems based on Large Language Models
(LLMs) have become a core technology for tasks such as question-answering (QA)
and content generation. However, by injecting poisoned documents into the
database of RAG systems, attackers can manipulate LLMs to generate text that
aligns with their intended preferences. Existing research has primarily focused
on white-box attacks against simplified RAG architectures. In this paper, we
investigate a more complex and realistic scenario: the attacker lacks knowledge
of the RAG system's internal composition and implementation details, and the
RAG system comprises components beyond a mere retriever. Specifically, we
propose the RIPRAG attack framework, an end-to-end attack pipeline that treats
the target RAG system as a black box, where the only information accessible to
the attacker is whether the poisoning succeeds. Our method leverages
Reinforcement Learning (RL) to optimize the generation model for poisoned
documents, ensuring that the generated poisoned document aligns with the target
RAG system's preferences. Experimental results demonstrate that this method can
effectively execute poisoning attacks against most complex RAG systems,
achieving an attack success rate (ASR) improvement of up to 0.72 compared to
baseline methods. This highlights prevalent deficiencies in current defensive
methods and provides critical insights for LLM security research.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ç ”ç©¶äº†RAGç³»ç»Ÿåœ¨ç¼ºä¹å†…éƒ¨çŸ¥è¯†æƒ…å†µä¸‹çš„é»‘ç›’æ”»å‡»åœºæ™¯ï¼Œæå‡ºRIPRAGæ”»å‡»æ¡†æ¶ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç”Ÿæˆä¼˜åŒ–åçš„æŠ•æ¯’æ–‡æ¡£ä»¥æ“çºµRAGç³»ç»Ÿè¾“å‡ºï¼Œå®éªŒæ˜¾ç¤ºè¯¥æ–¹æ³•æ˜¾è‘—æå‡æ”»å‡»æˆåŠŸç‡ï¼Œæ­ç¤ºäº†å½“å‰é˜²å¾¡æœºåˆ¶çš„ç¼ºé™·ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-10
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.09355v1">NL2GenSym: Natural Language to Generative Symbolic Rules for SOAR Cognitive Architecture via Large Language Models</a></td><td><details><summary>å±•å¼€</summary>SOAR, a classic symbol-based cognitive architecture, has been fostering the
development of general, human-like intelligent agents. Nevertheless, its
practical adoption is hindered by the laborious manual rule coding. Emerging
Large Language Models (LLMs) present the immense potential for efficient rules
generation. However, there is a critical gap that current research
predominantly focuses on conceptual frameworks and lacks robust experimental
validation. To bridge this gap, we propose \textit{N}atural \textit{L}anguage
to \textit{Gen}erative \textit{Sym}bolic Rules (NL2GenSym), a novel framework
that integrates LLMs with SOAR to autonomously produce generative symbolic
rules from natural language. Specifically, our framework introduces a novel
Execution-Grounded Generator-Critic mechanism. The LLM-based Generator, guided
by a Retrieval-Augmented Generation-accessed self-evolving domain knowledge
base, proposes rules from natural language. Subsequently, these rules are
immediately executed within the SOAR environment to rigorously validate their
correctness. Based on this execution-grounded feedback, a reflective LLM-based
Critic drives the iterative refinement of these rules. Experiments on our
specialized Water Jug Problem (WJP) dataset, utilizing both Gemini and Qwen
series models, validate the efficacy of our framework. It achieves a success
rate over 86\% in generating rules from natural language. Crucially, the
framework also generates novel heuristic rules, reducing average decision
cycles for solving the WJP to 1.98 times the optimal solution and 1/1000 of
baseline methods. Additionally, our initial experiments show that NL2GenSym
enables smaller-parameter models to achieve better performance than larger
counterparts.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºNL2GenSymçš„æ–°æ¡†æ¶ï¼Œå°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸SOARè®¤çŸ¥æ¶æ„ç»“åˆï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è®¿é—®è‡ªæ¼”åŒ–çš„é¢†åŸŸçŸ¥è¯†åº“ï¼Œä»è‡ªç„¶è¯­è¨€ä¸­è‡ªåŠ¨ç”Ÿæˆç¬¦å·è§„åˆ™ï¼Œå¹¶é€šè¿‡æ‰§è¡ŒéªŒè¯å’Œè¿­ä»£ä¼˜åŒ–æœºåˆ¶æå‡è§„åˆ™çš„æ­£ç¡®æ€§å’Œæ•ˆç‡ã€‚å®éªŒéªŒè¯äº†è¯¥æ¡†æ¶åœ¨ç”Ÿæˆè§„åˆ™å’Œå¯å‘å¼è§„åˆ™æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—æå‡äº†é—®é¢˜è§£å†³æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.09266v1">CFVBench: A Comprehensive Video Benchmark for Fine-grained Multimodal Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Multimodal Retrieval-Augmented Generation (MRAG) enables Multimodal Large
Language Models (MLLMs) to generate responses with external multimodal
evidence, and numerous video-based MRAG benchmarks have been proposed to
evaluate model capabilities across retrieval and generation stages. However,
existing benchmarks remain limited in modality coverage and format diversity,
often focusing on single- or limited-modality tasks, or coarse-grained scene
understanding. To address these gaps, we introduce CFVBench, a large-scale,
manually verified benchmark constructed from 599 publicly available videos,
yielding 5,360 open-ended QA pairs. CFVBench spans high-density formats and
domains such as chart-heavy reports, news broadcasts, and software tutorials,
requiring models to retrieve and reason over long temporal video spans while
maintaining fine-grained multimodal information. Using CFVBench, we
systematically evaluate 7 retrieval methods and 14 widely-used MLLMs, revealing
a critical bottleneck: current models (even GPT5 or Gemini) struggle to capture
transient yet essential fine-grained multimodal details. To mitigate this, we
propose Adaptive Visual Refinement (AVR), a simple yet effective framework that
adaptively increases frame sampling density and selectively invokes external
tools when necessary. Experiments show that AVR consistently enhances
fine-grained multimodal comprehension and improves performance across all
evaluated MLLMs</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºCFVBenchçš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆMRAGï¼‰åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ£€ç´¢å’Œç”Ÿæˆé˜¶æ®µçš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŸºäº599ä¸ªå…¬å¼€è§†é¢‘æ„å»ºï¼ŒåŒ…å«5,360ä¸ªå¼€æ”¾å¼é—®ç­”å¯¹ï¼Œè¦†ç›–é«˜å¯†åº¦æ ¼å¼å’Œå¤šæ ·é¢†åŸŸï¼Œè¦æ±‚æ¨¡å‹åœ¨é•¿æ—¶è§†é¢‘ä¸­æ£€ç´¢å¹¶æ¨ç†ç»†ç²’åº¦å¤šæ¨¡æ€ä¿¡æ¯ã€‚ç ”ç©¶å‘ç°å½“å‰æ¨¡å‹åœ¨æ•æ‰å…³é”®ç»†èŠ‚æ–¹é¢å­˜åœ¨ç“¶é¢ˆï¼Œå¹¶æå‡ºäº†è‡ªé€‚åº”è§†è§‰ä¼˜åŒ–ï¼ˆAVRï¼‰æ¡†æ¶ä»¥å¢å¼ºç»†ç²’åº¦å¤šæ¨¡æ€ç†è§£ï¼Œå®éªŒè¯æ˜AVRèƒ½æœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.09156v1">Agentic-KGR: Co-evolutionary Knowledge Graph Construction through Multi-Agent Reinforcement Learning</a></td><td><details><summary>å±•å¼€</summary>Current knowledge-enhanced large language models (LLMs) rely on static,
pre-constructed knowledge bases that suffer from coverage gaps and temporal
obsolescence, limiting their effectiveness in dynamic information environments.
We present Agentic-KGR, a novel framework enabling co-evolution between LLMs
and knowledge graphs (KGs) through multi-round reinforcement learning (RL). Our
approach introduces three key innovations: (1) a dynamic schema expansion
mechanism that systematically extends graph ontologies beyond pre-defined
boundaries during training; (2) a retrieval-augmented memory system enabling
synergistic co-evolution between model parameters and knowledge structures
through continuous optimization; (3) a learnable multi-scale prompt compression
approach that preserves critical information while reducing computational
complexity through adaptive sequence optimization. Experimental results
demonstrate substantial improvements over supervised baselines and single-round
RL approaches in knowledge extraction tasks. When integrated with GraphRAG, our
method achieves superior performance in downstream QA tasks, with significant
gains in both accuracy and knowledge coverage compared to existing methods.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºAgentic-KGRçš„æ–°æ¡†æ¶ï¼Œé€šè¿‡å¤šè½®å¼ºåŒ–å­¦ä¹ å®ç°å¤§è¯­è¨€æ¨¡å‹ä¸çŸ¥è¯†å›¾è°±çš„ååŒè¿›åŒ–ï¼ŒåŒ…å«åŠ¨æ€æ¨¡å¼æ‰©å±•ã€æ£€ç´¢å¢å¼ºè®°å¿†ç³»ç»Ÿå’Œå¯å­¦ä¹ çš„å¤šå°ºåº¦æç¤ºå‹ç¼©ç­‰åˆ›æ–°ï¼Œæ˜¾è‘—æå‡äº†çŸ¥è¯†æå–å’Œé—®ç­”ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¹¶ä¸GraphRAGç»“åˆå±•ç¤ºäº†ä¼˜è¶Šæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.09106v1">When Retrieval Succeeds and Fails: Rethinking Retrieval-Augmented Generation for LLMs</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) have enabled a wide range of applications
through their powerful capabilities in language understanding and generation.
However, as LLMs are trained on static corpora, they face difficulties in
addressing rapidly evolving information or domain-specific queries.
Retrieval-Augmented Generation (RAG) was developed to overcome this limitation
by integrating LLMs with external retrieval mechanisms, allowing them to access
up-to-date and contextually relevant knowledge. However, as LLMs themselves
continue to advance in scale and capability, the relative advantages of
traditional RAG frameworks have become less pronounced and necessary. Here, we
present a comprehensive review of RAG, beginning with its overarching
objectives and core components. We then analyze the key challenges within RAG,
highlighting critical weakness that may limit its effectiveness. Finally, we
showcase applications where LLMs alone perform inadequately, but where RAG,
when combined with LLMs, can substantially enhance their effectiveness. We hope
this work will encourage researchers to reconsider the role of RAG and inspire
the development of next-generation RAG systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ˜¯ä¸€ç¯‡å…³äºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„ç»¼è¿°æ€§æ–‡ç« ï¼Œæ¢è®¨äº†RAGå¦‚ä½•é€šè¿‡ç»“åˆå¤–éƒ¨æ£€ç´¢æœºåˆ¶å¼¥è¡¥å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŠ¨æ€ä¿¡æ¯å’Œé¢†åŸŸç‰¹å®šæŸ¥è¯¢ä¸Šçš„ä¸è¶³ï¼Œåˆ†æäº†RAGçš„æ ¸å¿ƒç»„ä»¶ã€å…³é”®æŒ‘æˆ˜åŠå…¶å±€é™æ€§ï¼Œå¹¶å±•ç¤ºäº†RAGä¸LLMsç»“åˆçš„åº”ç”¨åœºæ™¯ï¼Œæ—¨åœ¨æ¨åŠ¨ä¸‹ä¸€ä»£RAGç³»ç»Ÿçš„å‘å±•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.09093v1">Exploiting Web Search Tools of AI Agents for Data Exfiltration</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) are now routinely used to autonomously execute
complex tasks, from natural language processing to dynamic workflows like web
searches. The usage of tool-calling and Retrieval Augmented Generation (RAG)
allows LLMs to process and retrieve sensitive corporate data, amplifying both
their functionality and vulnerability to abuse. As LLMs increasingly interact
with external data sources, indirect prompt injection emerges as a critical and
evolving attack vector, enabling adversaries to exploit models through
manipulated inputs. Through a systematic evaluation of indirect prompt
injection attacks across diverse models, we analyze how susceptible current
LLMs are to such attacks, which parameters, including model size and
manufacturer, specific implementations, shape their vulnerability, and which
attack methods remain most effective. Our results reveal that even well-known
attack patterns continue to succeed, exposing persistent weaknesses in model
defenses. To address these vulnerabilities, we emphasize the need for
strengthened training procedures to enhance inherent resilience, a centralized
database of known attack vectors to enable proactive defense, and a unified
testing framework to ensure continuous security validation. These steps are
essential to push developers toward integrating security into the core design
of LLMs, as our findings show that current models still fail to mitigate
long-standing threats.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç»“åˆå·¥å…·è°ƒç”¨å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯å¤„ç†æ•æ„Ÿæ•°æ®æ—¶é¢ä¸´çš„é—´æ¥æç¤ºæ³¨å…¥æ”»å‡»é£é™©ï¼Œé€šè¿‡ç³»ç»Ÿè¯„ä¼°ä¸åŒæ¨¡å‹çš„è„†å¼±æ€§ã€å½±å“å› ç´ åŠæ”»å‡»æ–¹æ³•ï¼Œæ­ç¤ºäº†å½“å‰é˜²å¾¡çš„ä¸è¶³ï¼Œå¹¶æå‡ºäº†åŠ å¼ºè®­ç»ƒã€å»ºç«‹æ”»å‡»å‘é‡æ•°æ®åº“å’Œç»Ÿä¸€æµ‹è¯•æ¡†æ¶ç­‰å®‰å…¨æ”¹è¿›æªæ–½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.08981v1">SEER: Sustainability Enhanced Engineering of Software Requirements</a></td><td><details><summary>å±•å¼€</summary>The rapid expansion of software development has significant environmental,
technical, social, and economic impacts. Achieving the United Nations
Sustainable Development Goals by 2030 compels developers to adopt sustainable
practices. Existing methods mostly offer high-level guidelines, which are
time-consuming to implement and rely on team adaptability. Moreover, they focus
on design or implementation, while sustainability assessment should start at
the requirements engineering phase. In this paper, we introduce SEER, a
framework which addresses sustainability concerns in the early software
development phase. The framework operates in three stages: (i) it identifies
sustainability requirements (SRs) relevant to a specific software product from
a general taxonomy; (ii) it evaluates how sustainable system requirements are
based on the identified SRs; and (iii) it optimizes system requirements that
fail to satisfy any SR. The framework is implemented using the reasoning
capabilities of large language models and the agentic RAG (Retrieval Augmented
Generation) approach. SEER has been experimented on four software projects from
different domains. Results generated using Gemini 2.5 reasoning model
demonstrate the effectiveness of the proposed approach in accurately
identifying a broad range of sustainability concerns across diverse domains.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†SEERæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’ŒRAGæŠ€æœ¯ï¼Œåœ¨è½¯ä»¶å¼€å‘æ—©æœŸé˜¶æ®µè¯†åˆ«ã€è¯„ä¼°å’Œä¼˜åŒ–å¯æŒç»­æ€§éœ€æ±‚ï¼Œä»¥åº”å¯¹ä¸åŒé¢†åŸŸçš„å¯æŒç»­æ€§é—®é¢˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.08976v1">Hierarchical Scheduling for Multi-Vector Image Retrieval</a></td><td><details><summary>å±•å¼€</summary>To effectively leverage user-specific data, retrieval augmented generation
(RAG) is employed in multimodal large language model (MLLM) applications.
However, conventional retrieval approaches often suffer from limited retrieval
accuracy. Recent advances in multi-vector retrieval (MVR) improve accuracy by
decomposing queries and matching against segmented images. They still suffer
from sub-optimal accuracy and efficiency, overlooking alignment between the
query and varying image objects and redundant fine-grained image segments. In
this work, we present an efficient scheduling framework for image retrieval -
HiMIR. First, we introduce a novel hierarchical paradigm, employing multiple
intermediate granularities for varying image objects to enhance alignment.
Second, we minimize redundancy in retrieval by leveraging cross-hierarchy
similarity consistency and hierarchy sparsity to minimize unnecessary matching
computation. Furthermore, we configure parameters for each dataset
automatically for practicality across diverse scenarios. Our empirical study
shows that, HiMIR not only achieves substantial accuracy improvements but also
reduces computation by up to 3.5 times over the existing MVR system.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºHiMIRçš„é«˜æ•ˆå›¾åƒæ£€ç´¢è°ƒåº¦æ¡†æ¶ï¼Œé’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸­æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„å±€é™æ€§è¿›è¡Œä¼˜åŒ–ã€‚é€šè¿‡åˆ†å±‚å¤šç²’åº¦å¯¹é½ç­–ç•¥å‡å°‘å†—ä½™è®¡ç®—ï¼Œæå‡æ£€ç´¢å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå®éªŒæ˜¾ç¤ºå…¶æ€§èƒ½ä¼˜äºç°æœ‰å¤šå‘é‡æ£€ç´¢ï¼ˆMVRï¼‰ç³»ç»Ÿã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.08958v1">EcphoryRAG: Re-Imagining Knowledge-Graph RAG via Human Associative Memory</a></td><td><details><summary>å±•å¼€</summary>Cognitive neuroscience research indicates that humans leverage cues to
activate entity-centered memory traces (engrams) for complex, multi-hop
recollection. Inspired by this mechanism, we introduce EcphoryRAG, an
entity-centric knowledge graph RAG framework. During indexing, EcphoryRAG
extracts and stores only core entities with corresponding metadata, a
lightweight approach that reduces token consumption by up to 94\% compared to
other structured RAG systems. For retrieval, the system first extracts cue
entities from queries, then performs a scalable multi-hop associative search
across the knowledge graph. Crucially, EcphoryRAG dynamically infers implicit
relations between entities to populate context, enabling deep reasoning without
exhaustive pre-enumeration of relationships. Extensive evaluations on the
2WikiMultiHop, HotpotQA, and MuSiQue benchmarks demonstrate that EcphoryRAG
sets a new state-of-the-art, improving the average Exact Match (EM) score from
0.392 to 0.474 over strong KG-RAG methods like HippoRAG. These results validate
the efficacy of the entity-cue-multi-hop retrieval paradigm for complex
question answering.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†EcphoryRAGï¼Œä¸€ç§åŸºäºå®ä½“ä¸­å¿ƒçŸ¥è¯†å›¾è°±çš„RAGæ¡†æ¶ï¼Œé€šè¿‡æå–å’Œå­˜å‚¨æ ¸å¿ƒå®ä½“åŠå…ƒæ•°æ®å‡å°‘tokenæ¶ˆè€—ï¼Œå¹¶åˆ©ç”¨å¤šè·³å…³è”æ£€ç´¢å’ŒåŠ¨æ€å…³ç³»æ¨ç†æå‡å¤æ‚é—®ç­”æ€§èƒ½ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.08945v1">FATHOMS-RAG: A Framework for the Assessment of Thinking and Observation in Multimodal Systems that use Retrieval Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) has emerged as a promising paradigm for
improving factual accuracy in large language models (LLMs). We introduce a
benchmark designed to evaluate RAG pipelines as a whole, evaluating a
pipeline's ability to ingest, retrieve, and reason about several modalities of
information, differentiating it from existing benchmarks that focus on
particular aspects such as retrieval. We present (1) a small, human-created
dataset of 93 questions designed to evaluate a pipeline's ability to ingest
textual data, tables, images, and data spread across these modalities in one or
more documents; (2) a phrase-level recall metric for correctness; (3) a
nearest-neighbor embedding classifier to identify potential pipeline
hallucinations; (4) a comparative evaluation of 2 pipelines built with
open-source retrieval mechanisms and 4 closed-source foundation models; and (5)
a third-party human evaluation of the alignment of our correctness and
hallucination metrics. We find that closed-source pipelines significantly
outperform open-source pipelines in both correctness and hallucination metrics,
with wider performance gaps in questions relying on multimodal and
cross-document information. Human evaluation of our metrics showed average
agreement of 4.62 for correctness and 4.53 for hallucination detection on a 1-5
Likert scale (5 indicating "strongly agree").</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç®¡é“çš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬åˆ›å»ºåŒ…å«æ–‡æœ¬ã€è¡¨æ ¼å’Œå›¾åƒçš„å¤šæ¨¡æ€æ•°æ®é›†ã€æå‡ºæ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆçŸ­è¯­çº§å‡†ç¡®ç‡å’Œæœ€è¿‘é‚»åµŒå…¥åˆ†ç±»å™¨ç”¨äºæ£€æµ‹å¹»è§‰ï¼‰ï¼Œå¹¶å¯¹å¼€æºä¸é—­æºRAGç®¡é“è¿›è¡Œæ€§èƒ½æ¯”è¾ƒï¼Œç»“æœæ˜¾ç¤ºé—­æºæ¨¡å‹åœ¨å¤šæ¨¡æ€å’Œè·¨æ–‡æ¡£ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä¼˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.08935v1">Personalize Before Retrieve: LLM-based Personalized Query Expansion for User-Centric Retrieval</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) critically depends on effective query
expansion to retrieve relevant information. However, existing expansion methods
adopt uniform strategies that overlook user-specific semantics, ignoring
individual expression styles, preferences, and historical context. In practice,
identical queries in text can express vastly different intentions across users.
This representational rigidity limits the ability of current RAG systems to
generalize effectively in personalized settings. Specifically, we identify two
core challenges for personalization: 1) user expression styles are inherently
diverse, making it difficult for standard expansions to preserve personalized
intent. 2) user corpora induce heterogeneous semantic structures-varying in
topical focus and lexical organization-which hinders the effective anchoring of
expanded queries within the user's corpora space. To address these challenges,
we propose Personalize Before Retrieve (PBR), a framework that incorporates
user-specific signals into query expansion prior to retrieval. PBR consists of
two components: P-PRF, which generates stylistically aligned pseudo feedback
using user history for simulating user expression style, and P-Anchor, which
performs graph-based structure alignment over user corpora to capture its
structure. Together, they produce personalized query representations tailored
for retrieval. Experiments on two personalized benchmarks show that PBR
consistently outperforms strong baselines, with up to 10% gains on PersonaBench
across retrievers. Our findings demonstrate the value of modeling
personalization before retrieval to close the semantic gap in user-adaptive RAG
systems. Our code is available at https://github.com/Zhang-Yingyi/PBR-code.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºPBRï¼ˆPersonalize Before Retrieveï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³RAGç³»ç»Ÿä¸­æŸ¥è¯¢æ‰©å±•å¿½ç•¥ç”¨æˆ·ä¸ªæ€§åŒ–è¯­ä¹‰ï¼ˆå¦‚è¡¨è¾¾é£æ ¼ã€åå¥½å’Œå†å²ä¸Šä¸‹æ–‡ï¼‰çš„é—®é¢˜ã€‚PBRé€šè¿‡æ•´åˆç”¨æˆ·ç‰¹å®šä¿¡å·ï¼ˆP-PRFæ¨¡æ‹Ÿç”¨æˆ·è¡¨è¾¾é£æ ¼ï¼ŒP-Anchorå¯¹é½ç”¨æˆ·è¯­æ–™ç»“æ„ï¼‰ç”Ÿæˆä¸ªæ€§åŒ–æŸ¥è¯¢è¡¨ç¤ºï¼Œå®éªŒè¡¨æ˜å…¶åœ¨ä¸ªæ€§åŒ–åŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œæå‡äº†10%çš„æ£€ç´¢æ•ˆæœã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-09
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.08383v1">QAgent: A modular Search Agent with Interactive Query Understanding</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) excel at natural language tasks but are limited
by their static parametric knowledge, especially in knowledge-intensive task.
Retrieval-augmented generation (RAG) mitigates this by integrating external
information. However, (1) traditional RAG struggles with complex query
understanding, and (2) even search agents trained with reinforcement learning
(RL), despite their promise, still face generalization and deployment
challenges. To address these limitations, we propose QAgent, a unified agentic
RAG framework that employs a search agent for adaptive retrieval. This agent
optimizes its understanding of the query through interactive reasoning and
retrieval. To facilitate real-world application, we focus on modular search
agent for query understanding that are plug-and-play in complex systems.
Secifically, the agent follows a multi-step decision process trained with RL to
maximize retrieval quality and support accurate downstream answers. We further
analyze the strengths and weaknesses of end-to-end RL and propose a strategy
that focuses on effective retrieval, thereby enhancing generalization in LLM
applications. Experiments show QAgent excels at QA and serves as a
plug-and-play module for real-world deployment.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºQAgentçš„æ–°å‹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒæœç´¢ä»£ç†ä»¥ä¼˜åŒ–å¤æ‚æŸ¥è¯¢çš„ç†è§£å’Œè‡ªé€‚åº”æ£€ç´¢ï¼Œè§£å†³ä¼ ç»ŸRAGåœ¨æŸ¥è¯¢ç†è§£å’Œæ³›åŒ–èƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œå¹¶å®ç°å³æ’å³ç”¨çš„æ¨¡å—åŒ–éƒ¨ç½²ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.08149v1">AI Knowledge Assist: An Automated Approach for the Creation of Knowledge Bases for Conversational AI Agents</a></td><td><details><summary>å±•å¼€</summary>The utilization of conversational AI systems by leveraging Retrieval
Augmented Generation (RAG) techniques to solve customer problems has been on
the rise with the rapid progress of Large Language Models (LLMs). However, the
absence of a company-specific dedicated knowledge base is a major barrier to
the integration of conversational AI systems in contact centers. To this end,
we introduce AI Knowledge Assist, a system that extracts knowledge in the form
of question-answer (QA) pairs from historical customer-agent conversations to
automatically build a knowledge base. Fine-tuning a lightweight LLM on internal
data demonstrates state-of-the-art performance, outperforming larger
closed-source LLMs. More specifically, empirical evaluation on 20 companies
demonstrates that the proposed AI Knowledge Assist system that leverages the
LLaMA-3.1-8B model eliminates the cold-start gap in contact centers by
achieving above 90% accuracy in answering information-seeking questions. This
enables immediate deployment of RAG-powered chatbots.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ä»‹ç»äº†AI Knowledge Assistç³»ç»Ÿï¼Œé€šè¿‡ä»å†å²å®¢æˆ·-ä»£ç†å¯¹è¯ä¸­æå–é—®ç­”å¯¹è‡ªåŠ¨æ„å»ºçŸ¥è¯†åº“ï¼Œå¹¶åˆ©ç”¨è½»é‡çº§LLMï¼ˆå¦‚LLaMA-3.1-8Bï¼‰å¾®è°ƒå†…éƒ¨æ•°æ®ï¼Œåœ¨è§£å†³è”ç³»ä¸­å¿ƒå†·å¯åŠ¨é—®é¢˜ä¸­å®ç°è¶…è¿‡90%çš„å‡†ç¡®ç‡ï¼Œæ”¯æŒåŸºäºRAGæŠ€æœ¯çš„èŠå¤©æœºå™¨äººå¿«é€Ÿéƒ¨ç½²ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.08109v1">VersionRAG: Version-Aware Retrieval-Augmented Generation for Evolving Documents</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) systems fail when documents evolve
through versioning-a ubiquitous characteristic of technical documentation.
Existing approaches achieve only 58-64% accuracy on version-sensitive
questions, retrieving semantically similar content without temporal validity
checks. We present VersionRAG, a version-aware RAG framework that explicitly
models document evolution through a hierarchical graph structure capturing
version sequences, content boundaries, and changes between document states.
During retrieval, VersionRAG routes queries through specialized paths based on
intent classification, enabling precise version-aware filtering and change
tracking. On our VersionQA benchmark-100 manually curated questions across 34
versioned technical documents-VersionRAG achieves 90% accuracy, outperforming
naive RAG (58%) and GraphRAG (64%). VersionRAG reaches 60% accuracy on implicit
change detection where baselines fail (0-10%), demonstrating its ability to
track undocumented modifications. Additionally, VersionRAG requires 97% fewer
tokens during indexing than GraphRAG, making it practical for large-scale
deployment. Our work establishes versioned document QA as a distinct task and
provides both a solution and benchmark for future research.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†VersionRAGï¼Œä¸€ä¸ªé’ˆå¯¹ç‰ˆæœ¬åŒ–æ–‡æ¡£çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚å›¾ç»“æ„æ˜¾å¼å»ºæ¨¡æ–‡æ¡£æ¼”å˜è¿‡ç¨‹ï¼Œè§£å†³äº†ä¼ ç»ŸRAGåœ¨æ–‡æ¡£ç‰ˆæœ¬æ›´æ–°æ—¶å‡†ç¡®æ€§ä¸è¶³çš„é—®é¢˜ã€‚VersionRAGåœ¨ç‰ˆæœ¬æ•æ„Ÿé—®é¢˜ä¸Šè¾¾åˆ°90%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶å¤§å¹…é™ä½ç´¢å¼•å¼€é”€ï¼Œä¸ºç‰ˆæœ¬åŒ–æ–‡æ¡£é—®ç­”ä»»åŠ¡æä¾›äº†è§£å†³æ–¹æ¡ˆå’ŒåŸºå‡†ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.07925v1">Enabling Personalized Long-term Interactions in LLM-based Agents through Persistent Memory and User Profiles</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) increasingly serve as the central control unit
of AI agents, yet current approaches remain limited in their ability to deliver
personalized interactions. While Retrieval Augmented Generation enhances LLM
capabilities by improving context-awareness, it lacks mechanisms to combine
contextual information with user-specific data. Although personalization has
been studied in fields such as human-computer interaction or cognitive science,
existing perspectives largely remain conceptual, with limited focus on
technical implementation. To address these gaps, we build on a unified
definition of personalization as a conceptual foundation to derive technical
requirements for adaptive, user-centered LLM-based agents. Combined with
established agentic AI patterns such as multi-agent collaboration or
multi-source retrieval, we present a framework that integrates persistent
memory, dynamic coordination, self-validation, and evolving user profiles to
enable personalized long-term interactions. We evaluate our approach on three
public datasets using metrics such as retrieval accuracy, response correctness,
or BertScore. We complement these results with a five-day pilot user study
providing initial insights into user feedback on perceived personalization. The
study provides early indications that guide future work and highlights the
potential of integrating persistent memory and user profiles to improve the
adaptivity and perceived personalization of LLM-based agents.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸ç”¨æˆ·ç‰¹å®šæ•°æ®ï¼Œæå‡åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„AIä»£ç†çš„ä¸ªæ€§åŒ–äº¤äº’èƒ½åŠ›ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œæ•´åˆæŒä¹…è®°å¿†ã€åŠ¨æ€åè°ƒã€è‡ªæˆ‘éªŒè¯å’ŒåŠ¨æ€ç”¨æˆ·ç”»åƒç­‰æŠ€æœ¯ï¼Œä»¥å®ç°é•¿æœŸä¸ªæ€§åŒ–äº’åŠ¨ï¼Œå¹¶é€šè¿‡å®éªŒå’Œç”¨æˆ·ç ”ç©¶éªŒè¯äº†å…¶æ•ˆæœã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.07923v1">STEPER: Step-wise Knowledge Distillation for Enhancing Reasoning Ability in Multi-Step Retrieval-Augmented Language Models</a></td><td><details><summary>å±•å¼€</summary>Answering complex real-world questions requires step-by-step retrieval and
integration of relevant information to generate well-grounded responses.
However, existing knowledge distillation methods overlook the need for
different reasoning abilities at different steps, hindering transfer in
multi-step retrieval-augmented frameworks. To address this, we propose Stepwise
Knowledge Distillation for Enhancing Reasoning Ability in Multi-Step
Retrieval-Augmented Language Models (StepER). StepER employs step-wise
supervision to align with evolving information and reasoning demands across
stages. Additionally, it incorporates difficulty-aware training to
progressively optimize learning by prioritizing suitable steps. Our method is
adaptable to various multi-step retrieval-augmented language models, including
those that use retrieval queries for reasoning paths or decomposed questions.
Extensive experiments show that StepER outperforms prior methods on multi-hop
QA benchmarks, with an 8B model achieving performance comparable to a 70B
teacher model.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºStepERçš„é€æ­¥çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œæ—¨åœ¨æå‡å¤šæ­¥æ£€ç´¢å¢å¼ºè¯­è¨€æ¨¡å‹ï¼ˆRAGï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡åˆ†æ­¥ç›‘ç£å’Œéš¾åº¦æ„ŸçŸ¥è®­ç»ƒä¼˜åŒ–ä¸åŒæ­¥éª¤çš„ä¿¡æ¯æ•´åˆä¸æ¨ç†éœ€æ±‚ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨å¤šè·³é—®ç­”ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.07920v1">Profit Mirage: Revisiting Information Leakage in LLM-based Financial Agents</a></td><td><details><summary>å±•å¼€</summary>LLM-based financial agents have attracted widespread excitement for their
ability to trade like human experts. However, most systems exhibit a "profit
mirage": dazzling back-tested returns evaporate once the model's knowledge
window ends, because of the inherent information leakage in LLMs. In this
paper, we systematically quantify this leakage issue across four dimensions and
release FinLake-Bench, a leakage-robust evaluation benchmark. Furthermore, to
mitigate this issue, we introduce FactFin, a framework that applies
counterfactual perturbations to compel LLM-based agents to learn causal drivers
instead of memorized outcomes. FactFin integrates four core components:
Strategy Code Generator, Retrieval-Augmented Generation, Monte Carlo Tree
Search, and Counterfactual Simulator. Extensive experiments show that our
method surpasses all baselines in out-of-sample generalization, delivering
superior risk-adjusted performance.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åŸºäºLLMçš„é‡‘èä»£ç†å› ä¿¡æ¯æ³„éœ²å¯¼è‡´çš„"åˆ©æ¶¦å¹»è±¡"é—®é¢˜ï¼Œæå‡ºäº†æ³„æ¼é²æ£’æ€§åŸºå‡†FinLake-Benchå’Œè§£å†³æ–¹æ¡ˆæ¡†æ¶FactFinã€‚FactFiné€šè¿‡åäº‹å®æ‰°åŠ¨ä½¿æ¨¡å‹å­¦ä¹ å› æœé©±åŠ¨è€Œéè®°å¿†ç»“æœï¼Œå…¶æ ¸å¿ƒåŒ…å«æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­‰ç»„ä»¶ï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨æ ·æœ¬å¤–æ³›åŒ–ä¸­ä¼˜äºåŸºçº¿å¹¶æå‡é£é™©è°ƒæ•´åè¡¨ç°ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.07794v1">HiPRAG: Hierarchical Process Rewards for Efficient Agentic Retrieval Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Agentic RAG is a powerful technique for incorporating external information
that LLMs lack, enabling better problem solving and question answering.
However, suboptimal search behaviors exist widely, such as over-search
(retrieving information already known) and under-search (failing to search when
necessary), which leads to unnecessary overhead and unreliable outputs. Current
training methods, which typically rely on outcome-based rewards in a RL
framework, lack the fine-grained control needed to address these
inefficiencies. To overcome this, we introduce Hierarchical Process Rewards for
Efficient agentic RAG (HiPRAG), a training methodology that incorporates a
fine-grained, knowledge-grounded process reward into the RL training. Our
approach evaluates the necessity of each search decision on-the-fly by
decomposing the agent's reasoning trajectory into discrete, parsable steps. We
then apply a hierarchical reward function that provides an additional bonus
based on the proportion of optimal search and non-search steps, on top of
commonly used outcome and format rewards. Experiments on the Qwen2.5 and
Llama-3.2 models across seven diverse QA benchmarks show that our method
achieves average accuracies of 65.4% (3B) and 67.2% (7B). This is accomplished
while improving search efficiency, reducing the over-search rate to just 2.3%
and concurrently lowering the under-search rate. These results demonstrate the
efficacy of optimizing the reasoning process itself, not just the final
outcome. Further experiments and analysis demonstrate that HiPRAG shows good
generalizability across a wide range of RL algorithms, model families, sizes,
and types. This work demonstrates the importance and potential of fine-grained
control through RL, for improving the efficiency and optimality of reasoning
for search agents.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºHiPRAGçš„è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡åˆ†å±‚è¿‡ç¨‹å¥–åŠ±ä¼˜åŒ–RAGä¸­çš„æœç´¢è¡Œä¸ºï¼Œå‡å°‘è¿‡åº¦æœç´¢å’Œä¸è¶³æœç´¢é—®é¢˜ï¼Œæé«˜æœç´¢æ•ˆç‡å’Œå›ç­”å‡†ç¡®æ€§ï¼Œå¹¶åœ¨å¤šä¸ªQAåŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.07748v1">Haibu Mathematical-Medical Intelligent Agent:Enhancing Large Language Model Reliability in Medical Tasks via Verifiable Reasoning Chains</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) show promise in medicine but are prone to
factual and logical errors, which is unacceptable in this high-stakes field. To
address this, we introduce the "Haibu Mathematical-Medical Intelligent Agent"
(MMIA), an LLM-driven architecture that ensures reliability through a formally
verifiable reasoning process. MMIA recursively breaks down complex medical
tasks into atomic, evidence-based steps. This entire reasoning chain is then
automatically audited for logical coherence and evidence traceability, similar
to theorem proving. A key innovation is MMIA's "bootstrapping" mode, which
stores validated reasoning chains as "theorems." Subsequent tasks can then be
efficiently solved using Retrieval-Augmented Generation (RAG), shifting from
costly first-principles reasoning to a low-cost verification model. We
validated MMIA across four healthcare administration domains, including DRG/DIP
audits and medical insurance adjudication, using expert-validated benchmarks.
Results showed MMIA achieved an error detection rate exceeding 98% with a false
positive rate below 1%, significantly outperforming baseline LLMs. Furthermore,
the RAG matching mode is projected to reduce average processing costs by
approximately 85% as the knowledge base matures. In conclusion, MMIA's
verifiable reasoning framework is a significant step toward creating
trustworthy, transparent, and cost-effective AI systems, making LLM technology
viable for critical applications in medicine.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ä»‹ç»äº†â€œHaibu Mathematical-Medical Intelligent Agent (MMIA)â€ï¼Œä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¶æ„ï¼Œé€šè¿‡å¯éªŒè¯çš„æ¨ç†è¿‡ç¨‹ç¡®ä¿åŒ»å­¦ä»»åŠ¡çš„å¯é æ€§ã€‚MMIAå°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºåŸºäºè¯æ®çš„åŸå­æ­¥éª¤ï¼Œå¹¶åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯å­˜å‚¨å·²éªŒè¯çš„æ¨ç†é“¾ä½œä¸ºâ€œå®šç†â€ï¼Œä»è€Œé™ä½å¤„ç†æˆæœ¬ã€‚å®éªŒè¡¨æ˜ï¼ŒMMIAåœ¨åŒ»ç–—ç®¡ç†é¢†åŸŸæ˜¾è‘—ä¼˜äºåŸºçº¿LLMï¼Œé”™è¯¯æ£€æµ‹ç‡è¾¾98%ä»¥ä¸Šï¼Œä¸”RAGæ¨¡å¼é¢„è®¡å¯é™ä½85%çš„æˆæœ¬ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.07728v1">Who Stole Your Data? A Method for Detecting Unauthorized RAG Theft</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) enhances Large Language Models (LLMs) by
mitigating hallucinations and outdated information issues, yet simultaneously
facilitates unauthorized data appropriation at scale. This paper addresses this
challenge through two key contributions. First, we introduce RPD, a novel
dataset specifically designed for RAG plagiarism detection that encompasses
diverse professional domains and writing styles, overcoming limitations in
existing resources. Second, we develop a dual-layered watermarking system that
embeds protection at both semantic and lexical levels, complemented by an
interrogator-detective framework that employs statistical hypothesis testing on
accumulated evidence. Extensive experimentation demonstrates our approach's
effectiveness across varying query volumes, defense prompts, and retrieval
parameters, while maintaining resilience against adversarial evasion
techniques. This work establishes a foundational framework for intellectual
property protection in retrieval-augmented AI systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡é’ˆå¯¹RAGæŠ€æœ¯å¯èƒ½å¯¼è‡´çš„æœªç»æˆæƒæ•°æ®æ»¥ç”¨é—®é¢˜ï¼Œæå‡ºäº†ä¸¤ä¸ªè§£å†³æ–¹æ¡ˆï¼šä¸€æ˜¯æ„å»ºä¸“é—¨ç”¨äºæ£€æµ‹RAGæŠ„è¢­çš„å¤šé¢†åŸŸæ•°æ®é›†RPDï¼ŒäºŒæ˜¯è®¾è®¡ä¸€ç§ç»“åˆè¯­ä¹‰å’Œè¯æ±‡åŒå±‚æ¬¡æ°´å°çš„ä¿æŠ¤ç³»ç»Ÿï¼Œå¹¶é€šè¿‡ç»Ÿè®¡å‡è®¾æ£€éªŒæ¡†æ¶éªŒè¯å…¶æœ‰æ•ˆæ€§ï¼Œæ—¨åœ¨ä¿æŠ¤æ£€ç´¢å¢å¼ºAIç³»ç»Ÿä¸­çš„çŸ¥è¯†äº§æƒã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.07718v1">SUBQRAG: sub-question driven dynamic graph rag</a></td><td><details><summary>å±•å¼€</summary>Graph Retrieval-Augmented Generation (Graph RAG) effectively builds a
knowledge graph (KG) to connect disparate facts across a large document corpus.
However, this broad-view approach often lacks the deep structured reasoning
needed for complex multi-hop question answering (QA), leading to incomplete
evidence and error accumulation. To address these limitations, we propose
SubQRAG, a sub-question-driven framework that enhances reasoning depth. SubQRAG
decomposes a complex question into an ordered chain of verifiable
sub-questions. For each sub-question, it retrieves relevant triples from the
graph. When the existing graph is insufficient, the system dynamically expands
it by extracting new triples from source documents in real time. All triples
used in the reasoning process are aggregated into a "graph memory," forming a
structured and traceable evidence path for final answer generation. Experiments
on three multi-hop QA benchmarks demonstrate that SubQRAG achieves consistent
and significant improvements, especially in Exact Match scores.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSubQRAGçš„å­é—®é¢˜é©±åŠ¨çš„å›¾æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå¯éªŒè¯çš„å­é—®é¢˜é“¾ï¼ŒåŠ¨æ€æ£€ç´¢å’Œæ‰©å±•çŸ¥è¯†å›¾è°±ä¸­çš„ä¸‰å…ƒç»„ï¼Œå¹¶æ„å»ºå¯è¿½æº¯çš„"å›¾è®°å¿†"è·¯å¾„ï¼Œæ˜¾è‘—æå‡äº†å¤šè·³é—®ç­”ä»»åŠ¡çš„æ€§èƒ½ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-08
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.07233v1">LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document Understanding</a></td><td><details><summary>å±•å¼€</summary>Question answering over visually rich documents (VRDs) requires reasoning not
only over isolated content but also over documents' structural organization and
cross-page dependencies. However, conventional retrieval-augmented generation
(RAG) methods encode content in isolated chunks during ingestion, losing
structural and cross-page dependencies, and retrieve a fixed number of pages at
inference, regardless of the specific demands of the question or context. This
often results in incomplete evidence retrieval and degraded answer quality for
multi-page reasoning tasks. To address these limitations, we propose LAD-RAG, a
novel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs
a symbolic document graph that captures layout structure and cross-page
dependencies, adding it alongside standard neural embeddings to yield a more
holistic representation of the document. During inference, an LLM agent
dynamically interacts with the neural and symbolic indices to adaptively
retrieve the necessary evidence based on the query. Experiments on
MMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG
improves retrieval, achieving over 90% perfect recall on average without any
top-k tuning, and outperforming baseline retrievers by up to 20% in recall at
comparable noise levels, yielding higher QA accuracy with minimal latency.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLAD-RAGçš„æ–°å‹å¸ƒå±€æ„ŸçŸ¥åŠ¨æ€RAGæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸRAGæ–¹æ³•åœ¨å¤„ç†è§†è§‰ä¸°å¯Œæ–‡æ¡£ï¼ˆVRDsï¼‰æ—¶å› å¿½ç•¥æ–‡æ¡£ç»“æ„å’Œè·¨é¡µä¾èµ–å…³ç³»è€Œå¯¼è‡´çš„è¯æ®ä¸å®Œæ•´å’Œç­”æ¡ˆè´¨é‡ä¸‹é™é—®é¢˜ã€‚LAD-RAGé€šè¿‡æ„å»ºç¬¦å·æ–‡æ¡£å›¾æ¥æ•æ‰å¸ƒå±€ç»“æ„å’Œè·¨é¡µä¾èµ–ï¼Œå¹¶ç»“åˆç¥ç»åµŒå…¥æŠ€æœ¯ï¼Œåœ¨æ¨ç†é˜¶æ®µç”±LLMæ™ºèƒ½ä½“åŠ¨æ€äº¤äº’ä»¥è‡ªé€‚åº”æ£€ç´¢è¯æ®ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æ£€ç´¢æ•ˆæœå’Œé—®ç­”å‡†ç¡®æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.07096v1">Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided Sarcastic Speech Synthesis</a></td><td><details><summary>å±•å¼€</summary>Sarcasm is a subtle form of non-literal language that poses significant
challenges for speech synthesis due to its reliance on nuanced semantic,
contextual, and prosodic cues. While existing speech synthesis research has
focused primarily on broad emotional categories, sarcasm remains largely
unexplored. In this paper, we propose a Large Language Model (LLM)-enhanced
Retrieval-Augmented framework for sarcasm-aware speech synthesis. Our approach
combines (1) semantic embeddings from a LoRA-fine-tuned LLaMA 3, which capture
pragmatic incongruity and discourse-level cues of sarcasm, and (2) prosodic
exemplars retrieved via a Retrieval Augmented Generation (RAG) module, which
provide expressive reference patterns of sarcastic delivery. Integrated within
a VITS backbone, this dual conditioning enables more natural and contextually
appropriate sarcastic speech. Experiments demonstrate that our method
outperforms baselines in both objective measures and subjective evaluations,
yielding improvements in speech naturalness, sarcastic expressivity, and
downstream sarcasm detection.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºLLMå¢å¼ºçš„æ£€ç´¢å¢å¼ºæ¡†æ¶ï¼Œç”¨äºå…·æœ‰è®½åˆºæ„ŸçŸ¥çš„è¯­éŸ³åˆæˆï¼Œç»“åˆäº†å¾®è°ƒLLaMA 3çš„è¯­ä¹‰åµŒå…¥å’Œé€šè¿‡RAGæ¨¡å—æ£€ç´¢çš„éŸµå¾‹èŒƒä¾‹ï¼Œä»¥ç”Ÿæˆæ›´è‡ªç„¶å’Œç¬¦åˆä¸Šä¸‹æ–‡çš„è®½åˆºè¯­éŸ³ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.06999v1">Towards Reliable Retrieval in RAG Systems for Large Legal Datasets</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) is a promising approach to mitigate
hallucinations in Large Language Models (LLMs) for legal applications, but its
reliability is critically dependent on the accuracy of the retrieval step. This
is particularly challenging in the legal domain, where large databases of
structurally similar documents often cause retrieval systems to fail. In this
paper, we address this challenge by first identifying and quantifying a
critical failure mode we term Document-Level Retrieval Mismatch (DRM), where
the retriever selects information from entirely incorrect source documents. To
mitigate DRM, we investigate a simple and computationally efficient technique
which we refer to as Summary-Augmented Chunking (SAC). This method enhances
each text chunk with a document-level synthetic summary, thereby injecting
crucial global context that would otherwise be lost during a standard chunking
process. Our experiments on a diverse set of legal information retrieval tasks
show that SAC greatly reduces DRM and, consequently, also improves text-level
retrieval precision and recall. Interestingly, we find that a generic
summarization strategy outperforms an approach that incorporates legal expert
domain knowledge to target specific legal elements. Our work provides evidence
that this practical, scalable, and easily integrable technique enhances the
reliability of RAG systems when applied to large-scale legal document datasets.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡é’ˆå¯¹æ³•å¾‹é¢†åŸŸä¸­æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿçš„æ£€ç´¢æ­¥éª¤å‡†ç¡®æ€§ä¸è¶³é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºâ€œæ‘˜è¦å¢å¼ºåˆ†å—â€ï¼ˆSACï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡ä¸ºæ–‡æœ¬å—æ·»åŠ æ–‡æ¡£çº§åˆæˆæ‘˜è¦æ¥å‡å°‘æ–‡æ¡£çº§æ£€ç´¢ä¸åŒ¹é…ï¼ˆDRMï¼‰ï¼Œä»è€Œæå‡æ£€ç´¢ç²¾åº¦å’Œå¬å›ç‡ï¼Œå¢å¼ºRAGç³»ç»Ÿåœ¨æ³•å¾‹æ–‡æ¡£æ•°æ®é›†ä¸Šçš„å¯é æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.06888v1">M3Retrieve: Benchmarking Multimodal Retrieval for Medicine</a></td><td><details><summary>å±•å¼€</summary>With the increasing use of RetrievalAugmented Generation (RAG), strong
retrieval models have become more important than ever. In healthcare,
multimodal retrieval models that combine information from both text and images
offer major advantages for many downstream tasks such as question answering,
cross-modal retrieval, and multimodal summarization, since medical data often
includes both formats. However, there is currently no standard benchmark to
evaluate how well these models perform in medical settings. To address this
gap, we introduce M3Retrieve, a Multimodal Medical Retrieval Benchmark.
M3Retrieve, spans 5 domains,16 medical fields, and 4 distinct tasks, with over
1.2 Million text documents and 164K multimodal queries, all collected under
approved licenses. We evaluate leading multimodal retrieval models on this
benchmark to explore the challenges specific to different medical specialities
and to understand their impact on retrieval performance. By releasing
M3Retrieve, we aim to enable systematic evaluation, foster model innovation,
and accelerate research toward building more capable and reliable multimodal
retrieval systems for medical applications. The dataset and the baselines code
are available in this github page https://github.com/AkashGhosh/M3Retrieve.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†M3Retrieveï¼Œä¸€ä¸ªå¤šæ¨¡æ€åŒ»å­¦æ£€ç´¢åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°ç»“åˆæ–‡æœ¬å’Œå›¾åƒçš„æ£€ç´¢æ¨¡å‹åœ¨åŒ»ç–—é¢†åŸŸçš„æ€§èƒ½ï¼Œä»¥æ”¯æŒRAGç­‰ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¹¶ä¿ƒè¿›åŒ»ç–—åº”ç”¨ä¸­æ›´å¯é çš„å¤šæ¨¡æ€æ£€ç´¢ç³»ç»Ÿçš„ç ”ç©¶ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.06719v1">Differentially Private Synthetic Text Generation for Retrieval-Augmented Generation (RAG)</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
grounding them in external knowledge. However, its application in sensitive
domains is limited by privacy risks. Existing private RAG methods typically
rely on query-time differential privacy (DP), which requires repeated noise
injection and leads to accumulated privacy loss. To address this issue, we
propose DP-SynRAG, a framework that uses LLMs to generate differentially
private synthetic RAG databases. Unlike prior methods, the synthetic text can
be reused once created, thereby avoiding repeated noise injection and
additional privacy costs. To preserve essential information for downstream RAG
tasks, DP-SynRAG extends private prediction, which instructs LLMs to generate
text that mimics subsampled database records in a DP manner. Experiments show
that DP-SynRAG achieves superior performanec to the state-of-the-art private
RAG systems while maintaining a fixed privacy budget, offering a scalable
solution for privacy-preserving RAG.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDP-SynRAGçš„éšç§ä¿æŠ¤æ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆå·®åˆ†éšç§çš„åˆæˆRAGæ•°æ®åº“æ¥è§£å†³ä¼ ç»ŸRAGåœ¨æ•æ„Ÿé¢†åŸŸåº”ç”¨æ—¶çš„éšç§é£é™©é—®é¢˜ï¼Œé¿å…äº†é‡å¤å™ªå£°æ³¨å…¥å’Œéšç§æŸå¤±ç´¯ç§¯ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨å›ºå®šéšç§é¢„ç®—ä¸‹æ€§èƒ½ä¼˜äºç°æœ‰éšç§RAGç³»ç»Ÿã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-07
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.06002v1">Deterministic Legal Retrieval: An Action API for Querying the SAT-Graph RAG</a></td><td><details><summary>å±•å¼€</summary>The Structure-Aware Temporal Graph RAG (SAT-Graph RAG) addresses core
limitations of standard Retrieval-Augmented Generation in the legal domain by
providing a verifiable knowledge graph that models hierarchical structure,
temporal evolution, and causal events of legal norms. However, a critical gap
remains: how to reliably query this structured knowledge without sacrificing
its deterministic properties. This paper introduces the SAT-Graph API, a formal
query execution layer centered on canonical actions-atomic, composable, and
auditable primitives that isolate probabilistic discovery from deterministic
retrieval. These actions enable: (i) high-precision hybrid search; (ii) robust
reference resolution; (iii) point-in-time version retrieval; and (iv) auditable
causal tracing. We demonstrate how planner-guided agents can decompose complex
queries into Directed Acyclic Graphs (DAGs) of these actions. This two-layer
architecture transforms retrieval from an opaque black box to a transparent,
auditable process, directly addressing Explainable AI (XAI) requirements for
high-stakes domains.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†SAT-Graph RAGï¼Œä¸€ç§æ”¹è¿›æ ‡å‡†RAGåœ¨å¸æ³•é¢†åŸŸåº”ç”¨çš„æŠ€æœ¯ï¼Œé€šè¿‡æ„å»ºå¯éªŒè¯çš„çŸ¥è¯†å›¾è°±æ¥å»ºæ¨¡æ³•å¾‹è§„èŒƒçš„ç»“æ„ã€æ—¶é—´å’Œå› æœå…³ç³»ã€‚ä¸ºäº†è§£å†³å¦‚ä½•åœ¨ä¸ç‰ºç‰²å…¶ç¡®å®šæ€§å±æ€§çš„å‰æä¸‹å¯é æŸ¥è¯¢ç»“æ„åŒ–çŸ¥è¯†çš„é—®é¢˜ï¼Œè®ºæ–‡å¼•å…¥äº†SAT-Graph APIï¼Œä¸€ä¸ªåŸºäºè§„èŒƒåŒ–æ“ä½œçš„æŸ¥è¯¢æ‰§è¡Œå±‚ï¼Œæ”¯æŒé«˜ç²¾åº¦æ··åˆæœç´¢ã€ç‰ˆæœ¬æ£€ç´¢å’Œå¯å®¡è®¡çš„å› æœè¿½è¸ªç­‰åŠŸèƒ½ï¼Œå¹¶é€šè¿‡åŒå±‚æ¶æ„å¢å¼ºäº†æ£€ç´¢è¿‡ç¨‹çš„é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.05691v1">DecEx-RAG: Boosting Agentic Retrieval-Augmented Generation with Decision and Execution Optimization via Process Supervision</a></td><td><details><summary>å±•å¼€</summary>Agentic Retrieval-Augmented Generation (Agentic RAG) enhances the processing
capability for complex tasks through dynamic retrieval and adaptive workflows.
Recent advances (e.g., Search-R1) have shown that outcome-supervised
reinforcement learning demonstrate strong performance. However, this approach
still suffers from inefficient exploration, sparse reward signals, and
ambiguous global reward feedback. To address these challenges, we propose
DecEx-RAG, which models RAG as a Markov Decision Process (MDP) incorporating
decision-making and execution, while introducing an efficient pruning strategy
to optimize data expansion. Through comprehensive process-level policy
optimization, DecEx-RAG significantly enhances the autonomous task
decomposition, dynamic retrieval, and high-quality answer generation
capabilities of large language models (LLMs). Experiments show that DecEx-RAG
achieves an average absolute performance improvement of $6.2\%$ across six
datasets, significantly outperforming existing baselines. Moreover, the pruning
strategy improves data construction efficiency by nearly $6 \times$, providing
an efficient solution for process-supervised RAG training. The code is
available at https://github.com/sdsxdxl/DecEx-RAG.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDecEx-RAGçš„æ”¹è¿›æ–¹æ³•ï¼Œé€šè¿‡å°†RAGå»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰å¹¶å¼•å…¥é«˜æ•ˆå‰ªæç­–ç•¥ï¼Œè§£å†³äº†ä¼ ç»ŸRAGåœ¨æ¢ç´¢æ•ˆç‡ã€ç¨€ç–å¥–åŠ±å’Œå…¨å±€åé¦ˆæ¨¡ç³Šæ€§ä¸Šçš„é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†ä»»åŠ¡åˆ†è§£ã€åŠ¨æ€æ£€ç´¢å’Œç­”æ¡ˆç”Ÿæˆèƒ½åŠ›ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ€§èƒ½æå‡6.2%ï¼Œæ•°æ®æ„å»ºæ•ˆç‡æé«˜è¿‘6å€ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.05524v1">KEO: Knowledge Extraction on OMIn via Knowledge Graphs and RAG for Safety-Critical Aviation Maintenance</a></td><td><details><summary>å±•å¼€</summary>We present Knowledge Extraction on OMIn (KEO), a domain-specific knowledge
extraction and reasoning framework with large language models (LLMs) in
safety-critical contexts. Using the Operations and Maintenance Intelligence
(OMIn) dataset, we construct a QA benchmark spanning global sensemaking and
actionable maintenance tasks. KEO builds a structured Knowledge Graph (KG) and
integrates it into a retrieval-augmented generation (RAG) pipeline, enabling
more coherent, dataset-wide reasoning than traditional text-chunk RAG. We
evaluate locally deployable LLMs (Gemma-3, Phi-4, Mistral-Nemo) and employ
stronger models (GPT-4o, Llama-3.3) as judges. Experiments show that KEO
markedly improves global sensemaking by revealing patterns and system-level
insights, while text-chunk RAG remains effective for fine-grained procedural
tasks requiring localized retrieval. These findings underscore the promise of
KG-augmented LLMs for secure, domain-specific QA and their potential in
high-stakes reasoning.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†KEOæ¡†æ¶ï¼Œé€šè¿‡æ„å»ºç»“æ„åŒ–çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰å¹¶å°†å…¶é›†æˆåˆ°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æµç¨‹ä¸­ï¼Œæå‡å¤§è¯­è¨€æ¨¡å‹åœ¨å®‰å…¨å…³é”®é¢†åŸŸï¼ˆåŸºäºOMInæ•°æ®é›†ï¼‰çš„å…¨å±€æ¨ç†èƒ½åŠ›ï¼Œå®éªŒè¡¨æ˜KGå¢å¼ºçš„RAGåœ¨ç³»ç»Ÿçº§åˆ†æä¸Šä¼˜äºä¼ ç»Ÿæ–‡æœ¬ç‰‡æ®µæ£€ç´¢ï¼ŒåŒæ—¶ä¿ç•™äº†ç»†ç²’åº¦ä»»åŠ¡çš„å¤„ç†ä¼˜åŠ¿ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-06
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.04905v1">Retrieval-Augmented Code Generation: A Survey with Focus on Repository-Level Approaches</a></td><td><details><summary>å±•å¼€</summary>Recent advancements in large language models (LLMs) have substantially
improved automated code generation. While function-level and file-level
generation have achieved promising results, real-world software development
typically requires reasoning across entire repositories. This gives rise to the
challenging task of Repository-Level Code Generation (RLCG), where models must
capture long-range dependencies, ensure global semantic consistency, and
generate coherent code spanning multiple files or modules. To address these
challenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful
paradigm that integrates external retrieval mechanisms with LLMs, enhancing
context-awareness and scalability. In this survey, we provide a comprehensive
review of research on Retrieval-Augmented Code Generation (RACG), with an
emphasis on repository-level approaches. We categorize existing work along
several dimensions, including generation strategies, retrieval modalities,
model architectures, training paradigms, and evaluation protocols. Furthermore,
we summarize widely used datasets and benchmarks, analyze current limitations,
and outline key challenges and opportunities for future research. Our goal is
to establish a unified analytical framework for understanding this rapidly
evolving field and to inspire continued progress in AI-powered software
engineering.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒŒæ™¯ä¸‹ï¼Œåˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯è§£å†³**ä»“åº“çº§ä»£ç ç”Ÿæˆï¼ˆRLCGï¼‰**æŒ‘æˆ˜çš„ç ”ç©¶è¿›å±•ï¼Œç³»ç»Ÿç»¼è¿°äº†æ£€ç´¢å¢å¼ºä»£ç ç”Ÿæˆï¼ˆRACGï¼‰çš„æ–¹æ³•ã€åˆ†ç±»ï¼ˆå¦‚ç”Ÿæˆç­–ç•¥ã€æ£€ç´¢æ¨¡æ€ç­‰ï¼‰ã€æ•°æ®é›†åŠæœªæ¥æ–¹å‘ï¼Œæ—¨åœ¨æ„å»ºç»Ÿä¸€çš„åˆ†ææ¡†æ¶å¹¶æ¨åŠ¨AIé©±åŠ¨çš„è½¯ä»¶å·¥ç¨‹å‘å±•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.04757v1">ModernBERT + ColBERT: Enhancing biomedical RAG through an advanced re-ranking retriever</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) is a powerful technique for enriching
Large Language Models (LLMs) with external knowledge, allowing for factually
grounded responses, a critical requirement in high-stakes domains such as
healthcare. However, the efficacy of RAG systems is fundamentally restricted by
the performance of their retrieval module, since irrelevant or semantically
misaligned documents directly compromise the accuracy of the final generated
response. General-purpose dense retrievers can struggle with the nuanced
language of specialised domains, while the high accuracy of in-domain models is
often achieved at prohibitive computational costs. In this work, we aim to
address this trade-off by developing and evaluating a two-stage retrieval
architecture that combines a lightweight ModernBERT bidirectional encoder for
efficient initial candidate retrieval with a ColBERTv2 late-interaction model
for fine-grained re-ranking. We conduct comprehensive evaluations of our
retriever module performance and RAG system performance in the biomedical
context, fine-tuning the IR module using 10k question-passage pairs from
PubMedQA. Our analysis of the retriever module confirmed the positive impact of
the ColBERT re-ranker, which improved Recall@3 by up to 4.2 percentage points
compared to its retrieve-only counterpart. When integrated into the biomedical
RAG, our IR module leads to a state-of-the-art average accuracy of 0.4448 on
the five tasks of the MIRAGE question-answering benchmark, outperforming strong
baselines such as MedCPT (0.4436). Our ablation studies reveal that this
performance is critically dependent on a joint fine-tuning process that aligns
the retriever and re-ranker; otherwise, the re-ranker might degrade the
performance.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆè½»é‡çº§ModernBERTå’ŒColBERTv2çš„ä¸¤é˜¶æ®µæ£€ç´¢æ¶æ„ï¼Œä»¥æå‡ç”Ÿç‰©åŒ»å­¦é¢†åŸŸRAGç³»ç»Ÿçš„æ£€ç´¢æ€§èƒ½ï¼Œé€šè¿‡åœ¨PubMedQAæ•°æ®é›†ä¸Šçš„å¾®è°ƒå’Œå®éªŒéªŒè¯ï¼Œæ˜¾è‘—æé«˜äº†å¬å›ç‡å’Œé—®ç­”å‡†ç¡®æ€§ï¼Œå¹¶åœ¨MIRAGEåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€ä¼˜æ°´å¹³ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.04536v1">3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs Using MCP and RAG</a></td><td><details><summary>å±•å¼€</summary>This paper proposes "3Dify," a procedural 3D computer graphics (3D-CG)
generation framework utilizing Large Language Models (LLMs). The framework
enables users to generate 3D-CG content solely through natural language
instructions. 3Dify is built upon Dify, an open-source platform for AI
application development, and incorporates several state-of-the-art LLM-related
technologies such as the Model Context Protocol (MCP) and Retrieval-Augmented
Generation (RAG). For 3D-CG generation support, 3Dify automates the operation
of various Digital Content Creation (DCC) tools via MCP. When DCC tools do not
support MCP-based interaction, the framework employs the Computer-Using Agent
(CUA) method to automate Graphical User Interface (GUI) operations. Moreover,
to enhance image generation quality, 3Dify allows users to provide feedback by
selecting preferred images from multiple candidates. The LLM then learns
variable patterns from these selections and applies them to subsequent
generations. Furthermore, 3Dify supports the integration of locally deployed
LLMs, enabling users to utilize custom-developed models and to reduce both time
and monetary costs associated with external API calls by leveraging their own
computational resources.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†â€œ3Difyâ€ï¼Œä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ç¨‹åºåŒ–3Dè®¡ç®—æœºå›¾å½¢ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆ3Då†…å®¹ã€‚å®ƒæ•´åˆäº†åŒ…æ‹¬æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨å†…çš„å…ˆè¿›LLMæŠ€æœ¯ï¼Œå¹¶åˆ©ç”¨Model Context Protocolï¼ˆMCPï¼‰å’ŒComputer-Using Agentï¼ˆCUAï¼‰æ–¹æ³•è‡ªåŠ¨åŒ–æ•°å­—å†…å®¹åˆ›å»ºå·¥å…·çš„æ“ä½œï¼ŒåŒæ—¶æ”¯æŒç”¨æˆ·åé¦ˆå’Œæœ¬åœ°LLMéƒ¨ç½²ä»¥ä¼˜åŒ–ç”Ÿæˆè´¨é‡å’Œé™ä½æˆæœ¬ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.04488v1">Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning</a></td><td><details><summary>å±•å¼€</summary>Multi-agent debate often wastes compute by using a fixed adversarial stance,
aggregating without deliberation, or stopping on heuristics. We introduce MACI,
an active controller with two independent dials that decouple information from
behavior: an information dial that gates evidence by quality, and a behavior
dial that schedules contentiousness from exploration to consolidation. A
moderator tracks disagreement, overlap, evidence quality, and argument quality,
and halts when gains plateau. We provide theory-lite guarantees for
nonincreasing dispersion and provable termination, with a budget-feasible
scheduler. Across clinical diagnosis and news-bias tasks, MACI improves
accuracy and calibration while reducing tokens, and converts residual
uncertainty into precision RAG plans that specify what to retrieve next. We use
a cross-family LLM judge (CRIT) as a conservative soft weight and stop signal,
validated for order invariance and judge-swap stability; stability depends on
using high-capability judges. MACI turns debate into a budget-aware,
measurable, and provably terminating controller.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†MACIï¼Œä¸€ç§å¤šæ™ºèƒ½ä½“è¾©è®ºæ§åˆ¶å™¨ï¼Œé€šè¿‡ä¿¡æ¯è´¨é‡ç­›é€‰å’Œè¡Œä¸ºè°ƒåº¦ä¼˜åŒ–è¾©è®ºè¿‡ç¨‹ï¼Œå¹¶åœ¨æ®‹ä½™ä¸ç¡®å®šæ€§æ—¶ç”Ÿæˆç²¾ç¡®çš„RAGè®¡åˆ’ä»¥æŒ‡å¯¼åç»­æ£€ç´¢ï¼Œä»è€Œæå‡ä»»åŠ¡å‡†ç¡®æ€§å’Œæ ¡å‡†åº¦ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-05
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.04392v1">Improving Consistency in Retrieval-Augmented Systems with Group Similarity Rewards</a></td><td><details><summary>å±•å¼€</summary>RAG systems are increasingly deployed in high-stakes domains where users
expect outputs to be consistent across semantically equivalent queries.
However, existing systems often exhibit significant inconsistencies due to
variability in both the retriever and generator (LLM), undermining trust and
reliability. In this work, we focus on information consistency, i.e., the
requirement that outputs convey the same core content across semantically
equivalent inputs. We introduce a principled evaluation framework that
decomposes RAG consistency into retriever-level, generator-level, and
end-to-end components, helping identify inconsistency sources. To improve
consistency, we propose Paraphrased Set Group Relative Policy Optimization
(PS-GRPO), an RL approach that leverages multiple rollouts across paraphrased
set to assign group similarity rewards. We leverage PS-GRPO to achieve
Information Consistent RAG (Con-RAG), training the generator to produce
consistent outputs across paraphrased queries and remain robust to
retrieval-induced variability. Because exact reward computation over paraphrase
sets is computationally expensive, we also introduce a scalable approximation
method that retains effectiveness while enabling efficient, large-scale
training. Empirical evaluations across short-form, multi-hop, and long-form QA
benchmarks demonstrate that Con-RAG significantly improves both consistency and
accuracy over strong baselines, even in the absence of explicit ground-truth
supervision. Our work provides practical solutions for evaluating and building
reliable RAG systems for safety-critical deployments.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡é’ˆå¯¹RAGç³»ç»Ÿåœ¨è¯­ä¹‰ç­‰æ•ˆæŸ¥è¯¢ä¸‹è¾“å‡ºä¸ä¸€è‡´çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è¯„ä¼°æ¡†æ¶ï¼ˆåˆ†è§£æ£€ç´¢å™¨ã€ç”Ÿæˆå™¨å’Œç«¯åˆ°ç«¯ä¸ä¸€è‡´æ€§ï¼‰å’Œæ”¹è¿›æ–¹æ³•PS-GRPOï¼ˆåŸºäºå¼ºåŒ–å­¦ä¹ çš„ç»„ç›¸ä¼¼æ€§å¥–åŠ±ï¼‰ï¼Œæœ€ç»ˆå®ç°äº†ä¿¡æ¯ä¸€è‡´æ€§æ›´å¼ºçš„Con-RAGç³»ç»Ÿï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å…¶åœ¨å¤šä»»åŠ¡ä¸­æå‡ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§çš„æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.04293v1">Equipping Retrieval-Augmented Large Language Models with Document Structure Awareness</a></td><td><details><summary>å±•å¼€</summary>While large language models (LLMs) demonstrate impressive capabilities, their
reliance on parametric knowledge often leads to factual inaccuracies.
Retrieval-Augmented Generation (RAG) mitigates this by leveraging external
documents, yet existing approaches treat retrieved passages as isolated chunks,
ignoring valuable structure that is crucial for document organization.
Motivated by this gap, we propose Retrieve-DocumentRoute-Read (RDR2), a novel
framework that explicitly incorporates structural information throughout the
RAG process. RDR2 employs an LLM-based router to dynamically navigate document
structure trees, jointly evaluating content relevance and hierarchical
relationships to assemble optimal evidence. Our key innovation lies in
formulating document routing as a trainable task, with automatic action
curation and structure-aware passage selection inspired by human reading
strategies. Through comprehensive evaluation on five challenging datasets, RDR2
achieves state-of-the-art performance, demonstrating that explicit structural
awareness significantly enhances RAG systems' ability to acquire and utilize
knowledge, particularly in complex scenarios requiring multi-document
synthesis.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºRDR2çš„æ–°å‹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ï¼Œé€šè¿‡æ˜¾å¼åˆ©ç”¨æ–‡æ¡£ç»“æ„ä¿¡æ¯æ”¹è¿›ä¼ ç»ŸRAGæ–¹æ³•ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŸºäºLLMçš„è·¯ç”±å™¨åŠ¨æ€å¯¼èˆªæ–‡æ¡£ç»“æ„æ ‘ï¼Œç»“åˆå†…å®¹ç›¸å…³æ€§å’Œå±‚æ¬¡å…³ç³»é€‰æ‹©æœ€ä¼˜è¯æ®ï¼Œå¹¶åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†ç»“æ„æ„ŸçŸ¥èƒ½æ˜¾è‘—æå‡RAGç³»ç»Ÿåœ¨å¤æ‚å¤šæ–‡æ¡£åœºæ™¯ä¸‹çš„çŸ¥è¯†è·å–ä¸åˆ©ç”¨èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.04226v3">Epistemic Diversity and Knowledge Collapse in Large Language Models</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) tend to generate lexically, semantically, and
stylistically homogenous texts. This poses a risk of knowledge collapse, where
homogenous LLMs mediate a shrinking in the range of accessible information over
time. Existing works on homogenization are limited by a focus on closed-ended
multiple-choice setups or fuzzy semantic features, and do not look at trends
across time and cultural contexts. To overcome this, we present a new
methodology to measure epistemic diversity, i.e., variation in real-world
claims in LLM outputs, which we use to perform a broad empirical study of LLM
knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200
prompt variations sourced from real user chats. For the topics in our study, we
show that while newer models tend to generate more diverse claims, nearly all
models are less epistemically diverse than a basic web search. We find that
model size has a negative impact on epistemic diversity, while
retrieval-augmented generation (RAG) has a positive impact, though the
improvement from RAG varies by the cultural context. Finally, compared to a
traditional knowledge source (Wikipedia), we find that country-specific claims
reflect the English language more than the local one, highlighting a gap in
epistemic representation</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆæ–‡æœ¬çš„åŒè´¨åŒ–é—®é¢˜åŠå…¶å¯¼è‡´çš„â€œçŸ¥è¯†å´©æºƒâ€é£é™©ï¼Œæå‡ºäº†ä¸€ç§è¡¡é‡è®¤çŸ¥å¤šæ ·æ€§ï¼ˆepistemic diversityï¼‰çš„æ–°æ–¹æ³•ï¼Œå¹¶é€šè¿‡å®éªŒå‘ç°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯èƒ½æ˜¾è‘—æå‡æ¨¡å‹è¾“å‡ºçš„å¤šæ ·æ€§ï¼Œä½†å…¶æ•ˆæœå—æ–‡åŒ–èƒŒæ™¯å½±å“ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.04145v1">Automating construction safety inspections using a multi-modal vision-language RAG framework</a></td><td><details><summary>å±•å¼€</summary>Conventional construction safety inspection methods are often inefficient as
they require navigating through large volume of information. Recent advances in
large vision-language models (LVLMs) provide opportunities to automate safety
inspections through enhanced visual and linguistic understanding. However,
existing applications face limitations including irrelevant or unspecific
responses, restricted modal inputs and hallucinations. Utilisation of Large
Language Models (LLMs) for this purpose is constrained by availability of
training data and frequently lack real-time adaptability. This study introduces
SiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG)
framework for automating construction safety inspection reports by integrating
visual and audio inputs. Using real-world data, SiteShield outperformed
unimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04,
precision of 0.76, and recall of 0.96. The findings indicate that SiteShield
offers a novel pathway to enhance information retrieval and efficiency in
generating safety reports.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€å¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶SiteShieldï¼Œç”¨äºè‡ªåŠ¨åŒ–ç”Ÿæˆå»ºç­‘å®‰å…¨æ£€æŸ¥æŠ¥å‘Šï¼Œé€šè¿‡æ•´åˆè§†è§‰å’ŒéŸ³é¢‘è¾“å…¥æå‡æ£€ç´¢å’Œç”Ÿæˆæ•ˆç‡ï¼Œå®éªŒè¡¨æ˜å…¶æ€§èƒ½ä¼˜äºå•æ¨¡æ€LLMæ¨¡å‹ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-04
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.03847v1">Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs</a></td><td><details><summary>å±•å¼€</summary>Small language models (SLMs; 1-12B params, sometimes up to 20B) are
sufficient and often superior for agentic workloads where the objective is
schema- and API-constrained accuracy rather than open-ended generation. We
synthesize recent evidence across open and proprietary SLMs (Phi-4-Mini,
Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B,
DeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4,
StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with
guided decoding libraries (XGrammar, Outlines). We formalize SLM-default,
LLM-fallback systems with uncertainty-aware routing and verifier cascades, and
propose engineering metrics that reflect real production goals: cost per
successful task (CPS), schema validity rate, executable call rate, p50/p95
latency, and energy per request. Guided decoding, strict JSON Schema outputs,
and validator-first tool execution close much of the capability gap with larger
models and often let SLMs match or surpass LLMs on tool use, function calling,
and RAG at 10x-100x lower token cost with materially better latency and energy.
We provide design patterns for agent stacks that prioritize SLMs: schema-first
prompting, type-safe function registries, confidence scoring with verifier
rollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits
where fallback remains valuable (open-domain reasoning and some long-horizon
planning). The result is a practical blueprint for building fast, inexpensive,
and reliable agents that default to SLMs while preserving headroom with
targeted LLM assistance.
  Keywords: small language models, agents, function calling, structured
outputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency,
edge inference</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰åœ¨ä»£ç†ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»“æ„åŒ–è¾“å‡ºå’ŒAPIè°ƒç”¨ç­‰å—é™åœºæ™¯ä¸‹çš„é«˜æ•ˆè¡¨ç°ï¼Œå¹¶æå‡ºäº†ç»“åˆä¸ç¡®å®šæ€§æ„ŸçŸ¥è·¯ç”±å’ŒéªŒè¯å™¨çº§è”çš„SLM-defaultç³»ç»Ÿã€‚è™½ç„¶ä¸»è¦èšç„¦äºå·¥å…·ä½¿ç”¨å’Œå‡½æ•°è°ƒç”¨ï¼Œä½†æ˜ç¡®æåˆ°SLMsåœ¨RAGä»»åŠ¡ä¸­èƒ½ä»¥æ›´ä½æˆæœ¬åŒ¹é…æˆ–è¶…è¶Šå¤§å‹æ¨¡å‹ï¼ŒåŒæ—¶æä¾›äº†ä¼˜åŒ–RAGæ€§èƒ½çš„è®¾è®¡æ¨¡å¼ï¼ˆå¦‚schema-firstæç¤ºå’Œè½»é‡çº§é€‚é…æŠ€æœ¯ï¼‰ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.03687v1">MedReflect: Teaching Medical LLMs to Self-Improve via Reflective Correction</a></td><td><details><summary>å±•å¼€</summary>Medical problem solving demands expert knowledge and intricate reasoning.
Recent studies of large language models (LLMs) attempt to ease this complexity
by introducing external knowledge verification through retrieval-augmented
generation or by training on reasoning datasets. However, these approaches
suffer from drawbacks such as retrieval overhead and high annotation costs, and
they heavily rely on substituted external assistants to reach limited
performance in medical field. In this paper, we introduce MedReflect, a
generalizable framework designed to inspire LLMs with a physician-like
reflective thinking mode. MedReflect generates a single-pass reflection chain
that includes initial hypothesis generation, self-questioning, self-answering
and decision refinement. This self-verified and self-reflective nature releases
large language model's latent capability in medical problem-solving without
external retrieval or heavy annotation. We demonstrate that MedReflect enables
cost-efficient medical dataset construction: with merely 2,000 randomly sampled
training examples and a light fine-tuning, this approach achieves notable
absolute accuracy improvements across a series of medical benchmarks while
cutting annotation requirements. Our results provide evidence that LLMs can
learn to solve specialized medical problems via self-reflection and
self-improve, reducing reliance on external supervision and extensive
task-specific fine-tuning data.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åœ¨åŒ»å­¦é—®é¢˜è§£å†³ä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡è‡ªæˆ‘åæ€æ¨¡å¼ï¼ˆMedReflectæ¡†æ¶ï¼‰æå‡æ€§èƒ½çš„æ–¹æ³•ï¼ŒåŒæ—¶å¯¹æ¯”äº†ä¼ ç»Ÿæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„å±€é™æ€§ï¼ˆå¦‚æ£€ç´¢å¼€é”€å’Œä¾èµ–å¤–éƒ¨çŸ¥è¯†ï¼‰ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§æ— éœ€å¤–éƒ¨æ£€ç´¢æˆ–å¤§é‡æ ‡æ³¨çš„è‡ªéªŒè¯åæ€é“¾æœºåˆ¶ï¼Œæ˜¾è‘—é™ä½äº†æ•°æ®éœ€æ±‚å¹¶æé«˜äº†æ¨¡å‹åœ¨åŒ»å­¦ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.03663v2">UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG</a></td><td><details><summary>å±•å¼€</summary>Multimodal retrieval-augmented generation (MM-RAG) is a key approach for
applying large language models (LLMs) and agents to real-world knowledge bases,
yet current evaluations are fragmented, focusing on either text or images in
isolation or on simplified multimodal setups that fail to capture
document-centric multimodal use cases. In this paper, we introduce
UniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from
70k real-world PDF pages across eight domains. Our pipeline extracts and links
evidence from text, tables, and figures, then generates 1,600 multimodal QA
pairs spanning factual retrieval, comparison, summarization, and logical
reasoning queries. To ensure reliability, 20% of QA pairs are validated by
multiple annotators and expert adjudication. UniDoc-Bench supports
apples-to-apples comparison across four paradigms: (1) text-only, (2)
image-only, (3) multimodal text-image fusion, and (4) multimodal joint
retrieval -- under a unified protocol with standardized candidate pools,
prompts, and evaluation metrics. Our experiments show that multimodal
text-image fusion RAG systems consistently outperform both unimodal and jointly
multimodal embedding-based retrieval, indicating that neither text nor images
alone are sufficient and that current multimodal embeddings remain inadequate.
Beyond benchmarking, our analysis reveals when and how visual context
complements textual evidence, uncovers systematic failure modes, and offers
actionable guidance for developing more robust MM-RAG pipelines.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†UniDoc-Benchï¼Œä¸€ä¸ªé’ˆå¯¹å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆMM-RAGï¼‰çš„å¤§è§„æ¨¡çœŸå®åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡ä»8ä¸ªé¢†åŸŸçš„7ä¸‡é¡µPDFä¸­æå–æ–‡æœ¬ã€è¡¨æ ¼å’Œå›¾åƒè¯æ®å¹¶ç”Ÿæˆ1,600ä¸ªå¤šæ¨¡æ€QAå¯¹ï¼Œè¯„ä¼°äº†å››ç§æ£€ç´¢èŒƒå¼ï¼ˆçº¯æ–‡æœ¬ã€çº¯å›¾åƒã€å¤šæ¨¡æ€èåˆç­‰ï¼‰ï¼Œæ­ç¤ºäº†å¤šæ¨¡æ€èåˆç³»ç»Ÿçš„ä¼˜åŠ¿åŠå½“å‰åµŒå…¥æ–¹æ³•çš„ä¸è¶³ï¼Œä¸ºMM-RAGç³»ç»Ÿå¼€å‘æä¾›äº†å®è·µæŒ‡å¯¼ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-03
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.03521v1">Identifying Financial Risk Information Using RAG with a Contrastive Insight</a></td><td><details><summary>å±•å¼€</summary>In specialized domains, humans often compare new problems against similar
examples, highlight nuances, and draw conclusions instead of analyzing
information in isolation. When applying reasoning in specialized contexts with
LLMs on top of a RAG, the pipeline can capture contextually relevant
information, but it is not designed to retrieve comparable cases or related
problems.
  While RAG is effective at extracting factual information, its outputs in
specialized reasoning tasks often remain generic, reflecting broad facts rather
than context-specific insights. In finance, it results in generic risks that
are true for the majority of companies. To address this limitation, we propose
a peer-aware comparative inference layer on top of RAG.
  Our contrastive approach outperforms baseline RAG in text generation metrics
such as ROUGE and BERTScore in comparison with human-generated equity research
and risk.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºåœ¨RAGåŸºç¡€ä¸Šå¢åŠ ä¸€ä¸ªåŒè¡Œæ„ŸçŸ¥çš„æ¯”è¾ƒæ¨ç†å±‚ï¼Œä»¥è§£å†³RAGåœ¨ä¸“ä¸šé¢†åŸŸæ¨ç†ä»»åŠ¡ä¸­è¾“å‡ºè¿‡äºé€šç”¨çš„é—®é¢˜ï¼Œå¹¶åœ¨é‡‘èé¢†åŸŸé€šè¿‡å¯¹æ¯”å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡æŒ‡æ ‡ï¼ˆå¦‚ROUGEå’ŒBERTScoreï¼‰ä¸Šä¼˜äºåŸºçº¿RAGæ¨¡å‹ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.03458v1">Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image, Audio, and Video</a></td><td><details><summary>å±•å¼€</summary>We present Omni-Embed-Nemotron, a unified multimodal retrieval embedding
model developed to handle the increasing complexity of real-world information
needs. While Retrieval-Augmented Generation (RAG) has significantly advanced
language models by incorporating external knowledge, existing text-based
retrievers rely on clean, structured input and struggle with the visually and
semantically rich content found in real-world documents such as PDFs, slides,
or videos. Recent work such as ColPali has shown that preserving document
layout using image-based representations can improve retrieval quality.
Building on this, and inspired by the capabilities of recent multimodal models
such as Qwen2.5-Omni, we extend retrieval beyond text and images to also
support audio and video modalities. Omni-Embed-Nemotron enables both
cross-modal (e.g., text - video) and joint-modal (e.g., text - video+audio)
retrieval using a single model. We describe the architecture, training setup,
and evaluation results of Omni-Embed-Nemotron, and demonstrate its
effectiveness in text, image, and video retrieval.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†Omni-Embed-Nemotronï¼Œä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€æ£€ç´¢åµŒå…¥æ¨¡å‹ï¼Œæ—¨åœ¨å¤„ç†ç°å®ä¸–ç•Œä¸­å¤æ‚çš„ä¿¡æ¯éœ€æ±‚ã€‚æ–‡ç« æŒ‡å‡ºï¼Œå°½ç®¡RAGæŠ€æœ¯é€šè¿‡æ•´åˆå¤–éƒ¨çŸ¥è¯†æ˜¾è‘—æå‡äº†è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œä½†ç°æœ‰çš„åŸºäºæ–‡æœ¬çš„æ£€ç´¢å™¨åœ¨å¤„ç†PDFã€å¹»ç¯ç‰‡æˆ–è§†é¢‘ç­‰è§†è§‰å’Œè¯­ä¹‰ä¸°å¯Œçš„å†…å®¹æ—¶å­˜åœ¨å±€é™ã€‚Omni-Embed-Nemotronæ‰©å±•äº†æ£€ç´¢èŒƒå›´ï¼Œæ”¯æŒæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘çš„å¤šæ¨¡æ€æ£€ç´¢ï¼ŒåŒ…æ‹¬è·¨æ¨¡æ€å’Œè”åˆæ¨¡æ€æ£€ç´¢ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘æ£€ç´¢ä¸­çš„æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.03418v1">ContraGen: A Multi-Agent Generation Framework for Enterprise Contradictions Detection</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) integrates LLMs with external sources,
offering advanced capabilities for information access and decision-making.
However, contradictions in retrieved evidence can result in inconsistent or
untrustworthy outputs, which is especially problematic in enterprise settings
where compliance, governance, and accountability are critical. Existing
benchmarks for contradiction detection are limited to sentence-level analysis
and do not capture the complexity of enterprise documents such as contracts,
financial filings, compliance reports, or policy manuals. To address this
limitation, we propose ContraGen, a contradiction-aware benchmark framework
tailored to enterprise domain. The framework generates synthetic
enterprise-style documents with embedded contradictions, enabling systematic
evaluation of both intra-document and cross-document consistency. Automated
contradiction mining is combined with human-in-the-loop validation to ensure
high accuracy. Our contributions include generating realistic enterprise
documents, modeling a taxonomy of contradiction types common in business
processes, enabling controlled creation of self- and pairwise contradictions,
developing a contradiction-aware retrieval evaluation pipeline and embedding
human oversight to reflect domain-specific judgment complexity. This work
establishes a foundation for more trustworthy and accountable RAG systems in
enterprise information-seeking applications, where detecting and resolving
contradictions is essential for reducing risk and ensuring compliance.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡é’ˆå¯¹RAGç³»ç»Ÿä¸­æ£€ç´¢è¯æ®çŸ›ç›¾å¯¼è‡´è¾“å‡ºä¸å¯ä¿¡çš„é—®é¢˜ï¼Œæå‡ºé¢å‘ä¼ä¸šé¢†åŸŸçš„ContraGenåŸºå‡†æ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆå«çŸ›ç›¾çš„ä¼ä¸šæ–‡æ¡£ã€æ„å»ºçŸ›ç›¾åˆ†ç±»ä½“ç³»åŠè¯„ä¼°æµç¨‹ï¼Œæå‡RAGåœ¨ä¼ä¸šåˆè§„åœºæ™¯ä¸‹çš„å¯é æ€§ä¸çŸ›ç›¾æ£€æµ‹èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.02967v1">Grounding Large Language Models in Clinical Evidence: A Retrieval-Augmented Generation System for Querying UK NICE Clinical Guidelines</a></td><td><details><summary>å±•å¼€</summary>This paper presents the development and evaluation of a Retrieval-Augmented
Generation (RAG) system for querying the United Kingdom's National Institute
for Health and Care Excellence (NICE) clinical guidelines using Large Language
Models (LLMs). The extensive length and volume of these guidelines can impede
their utilisation within a time-constrained healthcare system, a challenge this
project addresses through the creation of a system capable of providing users
with precisely matched information in response to natural language queries. The
system's retrieval architecture, composed of a hybrid embedding mechanism, was
evaluated against a database of 10,195 text chunks derived from three hundred
guidelines. It demonstrates high performance, with a Mean Reciprocal Rank (MRR)
of 0.814, a Recall of 81% at the first chunk and of 99.1% within the top ten
retrieved chunks, when evaluated on 7901 queries.
  The most significant impact of the RAG system was observed during the
generation phase. When evaluated on a manually curated dataset of seventy
question-answer pairs, RAG-enhanced models showed substantial gains in
performance. Faithfulness, the measure of whether an answer is supported by the
source text, was increased by 64.7 percentage points to 99.5% for the
RAG-enhanced O4-Mini model and significantly outperformed the medical-focused
Meditron3-8B LLM, which scored 43%. This, combined with a perfect Context
Precision score of 1 for all RAG-enhanced models, confirms the system's ability
to prevent information fabrication by grounding its answers in relevant source
material. This study thus establishes RAG as an effective, reliable, and
scalable approach for applying generative AI in healthcare, enabling
cost-effective access to medical guidelines.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡å¼€å‘å¹¶è¯„ä¼°äº†ä¸€ä¸ªåŸºäºRAGçš„ç³»ç»Ÿï¼Œç”¨äºé€šè¿‡å¤§è¯­è¨€æ¨¡å‹æŸ¥è¯¢è‹±å›½NICEä¸´åºŠæŒ‡å—ï¼Œé€šè¿‡æ··åˆåµŒå…¥æ£€ç´¢æ¶æ„ä»å¤§é‡æŒ‡å—æ–‡æœ¬ä¸­ç²¾å‡†åŒ¹é…ä¿¡æ¯ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆç­”æ¡ˆçš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼ˆå¦‚å¿ å®åº¦æå‡è‡³99.5%ï¼‰ï¼ŒéªŒè¯äº†RAGåœ¨åŒ»ç–—é¢†åŸŸçš„é«˜æ•ˆåº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.02936v1">RAxSS: Retrieval-Augmented Sparse Sampling for Explainable Variable-Length Medical Time Series Classification</a></td><td><details><summary>å±•å¼€</summary>Medical time series analysis is challenging due to data sparsity, noise, and
highly variable recording lengths. Prior work has shown that stochastic sparse
sampling effectively handles variable-length signals, while retrieval-augmented
approaches improve explainability and robustness to noise and weak temporal
correlations. In this study, we generalize the stochastic sparse sampling
framework for retrieval-informed classification. Specifically, we weight window
predictions by within-channel similarity and aggregate them in probability
space, yielding convex series-level scores and an explicit evidence trail for
explainability. Our method achieves competitive iEEG classification performance
and provides practitioners with greater transparency and explainability. We
evaluate our method in iEEG recordings collected in four medical centers,
demonstrating its potential for reliable and explainable clinical
variable-length time series classification.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆéšæœºç¨€ç–é‡‡æ ·å’Œæ£€ç´¢å¢å¼ºæ–¹æ³•çš„æ¡†æ¶ï¼Œç”¨äºå¤„ç†åŒ»ç–—æ—¶é—´åºåˆ—åˆ†ç±»é—®é¢˜ï¼Œé€šè¿‡åŸºäºé€šé“å†…ç›¸ä¼¼æ€§åŠ æƒçš„çª—å£é¢„æµ‹å’Œæ¦‚ç‡ç©ºé—´èšåˆï¼Œæé«˜äº†åˆ†ç±»æ€§èƒ½ã€å¯è§£é‡Šæ€§åŠå¯¹å™ªå£°çš„é²æ£’æ€§ï¼Œå¹¶åœ¨å¤šä¸­å¿ƒiEEGæ•°æ®ä¸ŠéªŒè¯äº†å…¶å¯é æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.02827v1">StepChain GraphRAG: Reasoning Over Knowledge Graphs for Multi-Hop Question Answering</a></td><td><details><summary>å±•å¼€</summary>Recent progress in retrieval-augmented generation (RAG) has led to more
accurate and interpretable multi-hop question answering (QA). Yet, challenges
persist in integrating iterative reasoning steps with external knowledge
retrieval. To address this, we introduce StepChain GraphRAG, a framework that
unites question decomposition with a Breadth-First Search (BFS) Reasoning Flow
for enhanced multi-hop QA. Our approach first builds a global index over the
corpus; at inference time, only retrieved passages are parsed on-the-fly into a
knowledge graph, and the complex query is split into sub-questions. For each
sub-question, a BFS-based traversal dynamically expands along relevant edges,
assembling explicit evidence chains without overwhelming the language model
with superfluous context. Experiments on MuSiQue, 2WikiMultiHopQA, and HotpotQA
show that StepChain GraphRAG achieves state-of-the-art Exact Match and F1
scores. StepChain GraphRAG lifts average EM by 2.57% and F1 by 2.13% over the
SOTA method, achieving the largest gain on HotpotQA (+4.70% EM, +3.44% F1).
StepChain GraphRAG also fosters enhanced explainability by preserving the
chain-of-thought across intermediate retrieval steps. We conclude by discussing
how future work can mitigate the computational overhead and address potential
hallucinations from large language models to refine efficiency and reliability
in multi-hop QA.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºStepChain GraphRAGçš„æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆé—®é¢˜åˆ†è§£å’Œå¹¿åº¦ä¼˜å…ˆæœç´¢ï¼ˆBFSï¼‰æ¨ç†æµç¨‹ï¼Œæ”¹è¿›äº†å¤šè·³é—®ç­”ï¼ˆQAï¼‰ä»»åŠ¡ã€‚è¯¥æ–¹æ³•åœ¨æ£€ç´¢æ—¶åŠ¨æ€æ„å»ºçŸ¥è¯†å›¾ï¼Œå°†å¤æ‚æŸ¥è¯¢æ‹†åˆ†ä¸ºå­é—®é¢˜ï¼Œå¹¶é€šè¿‡BFSéå†æ‰©å±•ç›¸å…³è¯æ®é“¾ï¼Œä»è€Œæå‡å‡†ç¡®æ€§ï¼ˆåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°SOTAæ€§èƒ½ï¼‰å’Œå¯è§£é‡Šæ€§ï¼ŒåŒæ—¶è®¨è®ºäº†æœªæ¥ä¼˜åŒ–è®¡ç®—æ•ˆç‡å’Œå‡å°‘å¤§æ¨¡å‹å¹»è§‰çš„æ–¹å‘ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.02803v1">Work Zones challenge VLM Trajectory Planning: Toward Mitigation and Robust Autonomous Driving</a></td><td><details><summary>å±•å¼€</summary>Visual Language Models (VLMs), with powerful multimodal reasoning
capabilities, are gradually integrated into autonomous driving by several
automobile manufacturers to enhance planning capability in challenging
environments. However, the trajectory planning capability of VLMs in work
zones, which often include irregular layouts, temporary traffic control, and
dynamically changing geometric structures, is still unexplored. To bridge this
gap, we conduct the \textit{first} systematic study of VLMs for work zone
trajectory planning, revealing that mainstream VLMs fail to generate correct
trajectories in $68.0%$ of cases. To better understand these failures, we first
identify candidate patterns via subgraph mining and clustering analysis, and
then confirm the validity of $8$ common failure patterns through human
verification. Building on these findings, we propose REACT-Drive, a trajectory
planning framework that integrates VLMs with Retrieval-Augmented Generation
(RAG). Specifically, REACT-Drive leverages VLMs to convert prior failure cases
into constraint rules and executable trajectory planning code, while RAG
retrieves similar patterns in new scenarios to guide trajectory generation.
Experimental results on the ROADWork dataset show that REACT-Drive yields a
reduction of around $3\times$ in average displacement error relative to VLM
baselines under evaluation with Qwen2.5-VL. In addition, REACT-Drive yields the
lowest inference time ($0.58$s) compared with other methods such as fine-tuning
($17.90$s). We further conduct experiments using a real vehicle in 15 work zone
scenarios in the physical world, demonstrating the strong practicality of
REACT-Drive.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è‡ªä¸»é©¾é©¶å·¥ä½œåŒºè½¨è¿¹è§„åˆ’ä¸­çš„å±€é™æ€§å’Œæ”¹è¿›æ–¹æ³•ï¼Œæå‡ºäº†ä¸€ç§åä¸ºREACT-Driveçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œåˆ©ç”¨æ£€ç´¢åˆ°çš„ç›¸ä¼¼å¤±è´¥æ¨¡å¼æŒ‡å¯¼è½¨è¿¹è§„åˆ’ï¼Œæ˜¾è‘—æå‡äº†è½¨è¿¹ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚å®éªŒè¯æ˜REACT-Driveåœ¨å‡å°‘å¹³å‡ä½ç§»è¯¯å·®å’Œæ¨ç†æ—¶é—´æ–¹é¢ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶åœ¨çœŸå®åœºæ™¯ä¸­éªŒè¯äº†å…¶å®ç”¨æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.02668v1">AgenticRAG: Tool-Augmented Foundation Models for Zero-Shot Explainable Recommender Systems</a></td><td><details><summary>å±•å¼€</summary>Foundation models have revolutionized artificial intelligence, yet their
application in recommender systems remains limited by reasoning opacity and
knowledge constraints. This paper introduces AgenticRAG, a novel framework that
combines tool-augmented foundation models with retrieval-augmented generation
for zero-shot explainable recommendations. Our approach integrates external
tool invocation, knowledge retrieval, and chain-of-thought reasoning to create
autonomous recommendation agents capable of transparent decision-making without
task-specific training. Experimental results on three real-world datasets
demonstrate that AgenticRAG achieves consistent improvements over
state-of-the-art baselines, with NDCG@10 improvements of 0.4\% on Amazon
Electronics, 0.8\% on MovieLens-1M, and 1.6\% on Yelp datasets. The framework
exhibits superior explainability while maintaining computational efficiency
comparable to traditional methods.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºAgenticRAGçš„æ–°å‹æ¡†æ¶ï¼Œå°†å·¥å…·å¢å¼ºçš„åŸºç¡€æ¨¡å‹ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç›¸ç»“åˆï¼Œç”¨äºé›¶æ ·æœ¬å¯è§£é‡Šæ¨èã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆå¤–éƒ¨å·¥å…·è°ƒç”¨ã€çŸ¥è¯†æ£€ç´¢å’Œæ€ç»´é“¾æ¨ç†ï¼Œåˆ›å»ºäº†èƒ½å¤Ÿé€æ˜å†³ç­–çš„è‡ªä¸»æ¨èä»£ç†ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒã€‚å®éªŒè¡¨æ˜å…¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒå¥½çš„è§£é‡Šæ€§å’Œè®¡ç®—æ•ˆç‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.02657v2">Less LLM, More Documents: Searching for Improved RAG</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) couples document retrieval with large
language models (LLMs). While scaling generators improves accuracy, it also
raises cost and limits deployability. We explore an orthogonal axis: enlarging
the retriever's corpus to reduce reliance on large LLMs. Experimental results
show that corpus scaling consistently strengthens RAG and can often serve as a
substitute for increasing model size, though with diminishing returns at larger
scales. Small- and mid-sized generators paired with larger corpora often rival
much larger models with smaller corpora; mid-sized models tend to gain the
most, while tiny and large models benefit less. Our analysis shows that
improvements arise primarily from increased coverage of answer-bearing
passages, while utilization efficiency remains largely unchanged. These
findings establish a principled corpus-generator trade-off: investing in larger
corpora offers an effective path to stronger RAG, often comparable to enlarging
the LLM itself.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æ¢è®¨äº†é€šè¿‡æ‰©å¤§æ£€ç´¢å™¨çš„è¯­æ–™åº“æ¥å‡å°‘å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¾èµ–çš„æ–¹æ³•ï¼Œå®éªŒè¡¨æ˜è¯­æ–™åº“æ‰©å±•èƒ½æœ‰æ•ˆå¢å¼ºRAGæ€§èƒ½ï¼Œå¯ä½œä¸ºå¢å¤§æ¨¡å‹è§„æ¨¡çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå°¤å…¶å¯¹ä¸­å°å‹ç”Ÿæˆå™¨æ•ˆæœæ˜¾è‘—ï¼Œå¹¶æ­ç¤ºäº†è¯­æ–™è¦†ç›–èŒƒå›´ä¸æ¨¡å‹æ•ˆç‡ä¹‹é—´çš„æƒè¡¡å…³ç³»ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.02653v1">Geolog-IA: Conversational System for Academic Theses</a></td><td><details><summary>å±•å¼€</summary>This study presents the development of Geolog-IA, a novel conversational
system based on artificial intelligence that responds naturally to questions
about geology theses from the Central University of Ecuador. Our proposal uses
the Llama 3.1 and Gemini 2.5 language models, which are complemented by a
Retrieval Augmented Generation (RAG) architecture and an SQLite database. This
strategy allows us to overcome problems such as hallucinations and outdated
knowledge. The evaluation of Geolog-IA's performance with the BLEU metric
reaches an average of 0.87, indicating high consistency and accuracy in the
responses generated. The system offers an intuitive, web-based interface that
facilitates interaction and information retrieval for directors, teachers,
students, and administrative staff at the institution. This tool can be a key
support in education, training, and research and establishes a basis for future
applications in other disciplines.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ä»‹ç»äº†Geolog-IAï¼Œä¸€ä¸ªåŸºäºäººå·¥æ™ºèƒ½çš„å¯¹è¯ç³»ç»Ÿï¼Œåˆ©ç”¨Llama 3.1å’ŒGemini 2.5è¯­è¨€æ¨¡å‹ï¼Œç»“åˆRAGæ¶æ„å’ŒSQLiteæ•°æ®åº“ï¼Œä»¥è§£å†³å¹»è§‰å’ŒçŸ¥è¯†è¿‡æ—¶é—®é¢˜ï¼Œä¸ºå„ç“œå¤šå°”ä¸­å¤®å¤§å­¦çš„åœ°è´¨å­¦è®ºæ–‡æä¾›é«˜å‡†ç¡®æ€§çš„è‡ªç„¶è¯­è¨€å›ç­”ï¼Œå¹¶é€šè¿‡BLEUæŒ‡æ ‡è¯„ä¼°æ˜¾ç¤ºå…¶é«˜ä¸€è‡´æ€§ï¼ˆå¹³å‡0.87ï¼‰ã€‚ç³»ç»Ÿæä¾›åŸºäºç½‘é¡µçš„ç›´è§‚ç•Œé¢ï¼Œæ”¯æŒæ•™è‚²ã€åŸ¹è®­å’Œç ”ç©¶ï¼Œå¹¶é€‚ç”¨äºå…¶ä»–å­¦ç§‘ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.02634v1">Automatic Building Code Review: A Case Study</a></td><td><details><summary>å±•å¼€</summary>Building officials, particularly those in resource-constrained or rural
jurisdictions, face labor-intensive, error-prone, and costly manual reviews of
design documents as projects increase in size and complexity. The growing
adoption of Building Information Modeling (BIM) and Large Language Models
(LLMs) presents opportunities for automated code review (ACR) solutions. This
study introduces a novel agent-driven framework that integrates BIM-based data
extraction with automated verification using both retrieval-augmented
generation (RAG) and Model Context Protocol (MCP) agent pipelines. The
framework employs LLM-enabled agents to extract geometry, schedules, and system
attributes from heterogeneous file types, which are then processed for building
code checking through two complementary mechanisms: (1) direct API calls to the
US Department of Energy COMcheck engine, providing deterministic and
audit-ready outputs, and (2) RAG-based reasoning over rule provisions, enabling
flexible interpretation where coverage is incomplete or ambiguous.
  The framework was evaluated through case demonstrations, including automated
extraction of geometric attributes (such as surface area, tilt, and insulation
values), parsing of operational schedules, and validation of lighting
allowances under ASHRAE Standard 90.1-2022. Comparative performance tests
across multiple LLMs showed that GPT-4o achieved the best balance of efficiency
and stability, while smaller models exhibited inconsistencies or failures.
Results confirm that MCP agent pipelines outperform RAG reasoning pipelines in
rigor and reliability. This work advances ACR research by demonstrating a
scalable, interoperable, and production-ready approach that bridges BIM with
authoritative code review tools.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºBIMå’ŒLLMçš„è‡ªåŠ¨åŒ–å»ºç­‘è§„èŒƒå®¡æŸ¥æ¡†æ¶ï¼Œæ•´åˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰ä»£ç†æµç¨‹ï¼Œé€šè¿‡ä¸¤ç§æœºåˆ¶éªŒè¯å»ºç­‘è§„èŒƒï¼šç›´æ¥è°ƒç”¨COMcheckå¼•æ“å’ŒRAGå¯¹è§„åˆ™æ¡æ¬¾è¿›è¡Œæ¨ç†ï¼Œæ¡ˆä¾‹æµ‹è¯•è¡¨æ˜MCPåœ¨ä¸¥è°¨æ€§å’Œå¯é æ€§ä¸Šä¼˜äºRAGï¼Œä½†RAGåœ¨è§„åˆ™ä¸æ˜ç¡®æ—¶æä¾›äº†çµæ´»è§£é‡Šèƒ½åŠ›ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-02
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.02243v1">AccurateRAG: A Framework for Building Accurate Retrieval-Augmented Question-Answering Applications</a></td><td><details><summary>å±•å¼€</summary>We introduce AccurateRAG -- a novel framework for constructing
high-performance question-answering applications based on retrieval-augmented
generation (RAG). Our framework offers a pipeline for development efficiency
with tools for raw dataset processing, fine-tuning data generation, text
embedding & LLM fine-tuning, output evaluation, and building RAG systems
locally. Experimental results show that our framework outperforms previous
strong baselines and obtains new state-of-the-art question-answering
performance on benchmark datasets.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†åä¸ºAccurateRAGçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ„å»ºé«˜æ€§èƒ½é—®ç­”åº”ç”¨ã€‚è¯¥æ¡†æ¶æä¾›äº†ä¸€å¥—å¼€å‘æµç¨‹å·¥å…·ï¼ŒåŒ…æ‹¬åŸå§‹æ•°æ®é›†å¤„ç†ã€å¾®è°ƒæ•°æ®ç”Ÿæˆã€æ–‡æœ¬åµŒå…¥ä¸å¤§æ¨¡å‹å¾®è°ƒã€è¾“å‡ºè¯„ä¼°åŠæœ¬åœ°RAGç³»ç»Ÿæ„å»ºï¼Œå¹¶åœ¨å®éªŒä¸­è¶…è¶Šç°æœ‰åŸºçº¿ï¼Œå®ç°äº†åŸºå‡†æ•°æ®é›†ä¸Šçš„æœ€æ–°æœ€å…ˆè¿›é—®ç­”æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.02044v1">Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage</a></td><td><details><summary>å±•å¼€</summary>End-to-end speech-in speech-out dialogue systems are emerging as a powerful
alternative to traditional ASR-LLM-TTS pipelines, generating more natural,
expressive responses with significantly lower latency. However, these systems
remain prone to hallucinations due to limited factual grounding. While
text-based dialogue systems address this challenge by integrating tools such as
web search and knowledge graph APIs, we introduce the first approach to extend
tool use directly into speech-in speech-out systems. A key challenge is that
tool integration substantially increases response latency, disrupting
conversational flow. To mitigate this, we propose Streaming Retrieval-Augmented
Generation (Streaming RAG), a novel framework that reduces user-perceived
latency by predicting tool queries in parallel with user speech, even before
the user finishes speaking. Specifically, we develop a post-training pipeline
that teaches the model when to issue tool calls during ongoing speech and how
to generate spoken summaries that fuse audio queries with retrieved text
results, thereby improving both accuracy and responsiveness. To evaluate our
approach, we construct AudioCRAG, a benchmark created by converting queries
from the publicly available CRAG dataset into speech form. Experimental results
demonstrate that our streaming RAG approach increases QA accuracy by up to 200%
relative (from 11.1% to 34.2% absolute) and further enhances user experience by
reducing tool use latency by 20%. Importantly, our streaming RAG approach is
modality-agnostic and can be applied equally to typed input, paving the way for
more agentic, real-time AI assistants.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸º"Streaming Retrieval-Augmented Generation (Streaming RAG)"çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç«¯åˆ°ç«¯è¯­éŸ³å¯¹è¯ç³»ç»Ÿä¸­å­˜åœ¨çš„äº‹å®åŸºç¡€ä¸è¶³å’Œå»¶è¿Ÿé—®é¢˜ã€‚é€šè¿‡å¹¶è¡Œé¢„æµ‹å·¥å…·æŸ¥è¯¢å¹¶ä¸ç”¨æˆ·è¯­éŸ³åŒæ­¥å¤„ç†ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†é—®ç­”å‡†ç¡®æ€§ï¼ˆç›¸å¯¹æå‡200%ï¼‰å¹¶é™ä½20%çš„å·¥å…·ä½¿ç”¨å»¶è¿Ÿï¼ŒåŒæ—¶æ„å»ºäº†ä¸“é—¨çš„è¯­éŸ³è¯„æµ‹åŸºå‡†AudioCRAGã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.01910v1">Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under Deficiencies with Iterative Refinement</a></td><td><details><summary>å±•å¼€</summary>Graph Neural Networks (GNNs) are widely adopted in Web-related applications,
serving as a core technique for learning from graph-structured data, such as
text-attributed graphs. Yet in real-world scenarios, such graphs exhibit
deficiencies that substantially undermine GNN performance. While prior
GNN-based augmentation studies have explored robustness against individual
imperfections, a systematic understanding of how graph-native and Large
Language Models (LLMs) enhanced methods behave under compound deficiencies is
still missing. Specifically, there has been no comprehensive investigation
comparing conventional approaches and recent LLM-on-graph frameworks, leaving
their merits unclear. To fill this gap, we conduct the first empirical study
that benchmarks these two lines of methods across diverse graph deficiencies,
revealing overlooked vulnerabilities and challenging the assumption that LLM
augmentation is consistently superior. Building on empirical findings, we
propose Robust Graph Learning via Retrieval-Augmented Contrastive Refinement
(RoGRAD) framework. Unlike prior one-shot LLM-as-Enhancer designs, RoGRAD is
the first iterative paradigm that leverages Retrieval-Augmented Generation
(RAG) to inject retrieval-grounded augmentations by supplying class-consistent,
diverse augmentations and enforcing discriminative representations through
iterative graph contrastive learning. It transforms LLM augmentation for graphs
from static signal injection into dynamic refinement. Extensive experiments
demonstrate RoGRAD's superiority over both conventional GNN- and LLM-enhanced
baselines, achieving up to 82.43% average improvement.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRoGRADçš„æ–°å‹å›¾å­¦ä¹ æ¡†æ¶ï¼Œé¦–æ¬¡å°†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯è¿­ä»£åº”ç”¨äºå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰å¢å¼ºä»»åŠ¡ï¼Œé€šè¿‡åŠ¨æ€å¯¹æ¯”å­¦ä¹ æ³¨å…¥æ£€ç´¢åˆ°çš„ç±»åˆ«ä¸€è‡´æ€§æ•°æ®ï¼Œè§£å†³äº†ä¼ ç»ŸLLMé™æ€å¢å¼ºå’Œå¤åˆå›¾ç¼ºé™·ä¸‹çš„æ€§èƒ½ç“¶é¢ˆï¼Œå®éªŒæ˜¾ç¤ºå…¶æ•ˆæœæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.01800v1">REBot: From RAG to CatRAG with Semantic Enrichment and Graph Routing</a></td><td><details><summary>å±•å¼€</summary>Academic regulation advising is essential for helping students interpret and
comply with institutional policies, yet building effective systems requires
domain specific regulatory resources. To address this challenge, we propose
REBot, an LLM enhanced advisory chatbot powered by CatRAG, a hybrid retrieval
reasoning framework that integrates retrieval augmented generation with graph
based reasoning. CatRAG unifies dense retrieval and graph reasoning, supported
by a hierarchical, category labeled knowledge graph enriched with semantic
features for domain alignment. A lightweight intent classifier routes queries
to the appropriate retrieval modules, ensuring both factual accuracy and
contextual depth. We construct a regulation specific dataset and evaluate REBot
on classification and question answering tasks, achieving state of the art
performance with an F1 score of 98.89%. Finally, we implement a web application
that demonstrates the practical value of REBot in real world academic advising
scenarios.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†REBotï¼Œä¸€ç§åŸºäºCatRAGï¼ˆç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆä¸å›¾æ¨ç†çš„æ··åˆæ¡†æ¶ï¼‰çš„å­¦æœ¯æ³•è§„å’¨è¯¢èŠå¤©æœºå™¨äººã€‚CatRAGé€šè¿‡åˆ†å±‚æ ‡è®°çš„çŸ¥è¯†å›¾è°±å’Œè¯­ä¹‰ç‰¹å¾æ•´åˆå¯†é›†æ£€ç´¢ä¸å›¾æ¨ç†ï¼Œè½»é‡çº§æ„å›¾åˆ†ç±»å™¨ç¡®ä¿æŸ¥è¯¢çš„å‡†ç¡®æ€§å’Œä¸Šä¸‹æ–‡æ·±åº¦ã€‚å®éªŒè¡¨æ˜REBotåœ¨åˆ†ç±»å’Œé—®ç­”ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ˆF1åˆ†æ•°98.89%ï¼‰ï¼Œå¹¶é€šè¿‡ç½‘é¡µåº”ç”¨éªŒè¯äº†å…¶å®é™…åº”ç”¨ä»·å€¼ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.01622v1">LLM4Rec: Large Language Models for Multimodal Generative Recommendation with Causal Debiasing</a></td><td><details><summary>å±•å¼€</summary>Contemporary generative recommendation systems face significant challenges in
handling multimodal data, eliminating algorithmic biases, and providing
transparent decision-making processes. This paper introduces an enhanced
generative recommendation framework that addresses these limitations through
five key innovations: multimodal fusion architecture, retrieval-augmented
generation mechanisms, causal inference-based debiasing, explainable
recommendation generation, and real-time adaptive learning capabilities. Our
framework leverages advanced large language models as the backbone while
incorporating specialized modules for cross-modal understanding, contextual
knowledge integration, bias mitigation, explanation synthesis, and continuous
model adaptation. Extensive experiments on three benchmark datasets
(MovieLens-25M, Amazon-Electronics, Yelp-2023) demonstrate consistent
improvements in recommendation accuracy, fairness, and diversity compared to
existing approaches. The proposed framework achieves up to 2.3% improvement in
NDCG@10 and 1.4% enhancement in diversity metrics while maintaining
computational efficiency through optimized inference strategies.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ”¹è¿›çš„ç”Ÿæˆå¼æ¨èæ¡†æ¶ï¼Œé€šè¿‡äº”é¡¹å…³é”®åˆ›æ–°è§£å†³å¤šæ¨¡æ€æ•°æ®å¤„ç†ã€ç®—æ³•åå·®æ¶ˆé™¤å’Œå†³ç­–é€æ˜åº¦ç­‰é—®é¢˜ï¼Œå…¶ä¸­åŒ…æ‹¬æ£€ç´¢å¢å¼ºç”Ÿæˆæœºåˆ¶ï¼ˆRAGï¼‰ã€‚è¯¥æ¡†æ¶ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ä¸å¤šæ¨¡æ€èåˆã€å»åå› æœæ¨ç†ç­‰æŠ€æœ¯ï¼Œåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶åœ¨æ¨èå‡†ç¡®æ€§ã€å…¬å¹³æ€§å’Œå¤šæ ·æ€§æ–¹é¢çš„æå‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.01612v1">RAG-BioQA Retrieval-Augmented Generation for Long-Form Biomedical Question Answering</a></td><td><details><summary>å±•å¼€</summary>The exponential growth of biomedical literature creates significant
challenges for accessing precise medical information. Current biomedical
question-answering systems primarily focus on short-form answers, failing to
provide the comprehensive explanations necessary for clinical decision-making.
We present RAG-BioQA, a novel framework combining retrieval-augmented
generation with domain-specific fine-tuning to produce evidence-based,
long-form biomedical answers. Our approach integrates BioBERT embeddings with
FAISS indexing and compares various re-ranking strategies (BM25, ColBERT,
MonoT5) to optimize context selection before synthesizing evidence through a
fine-tuned T5 model. Experimental results on the PubMedQA dataset show
significant improvements over baselines, with our best model achieving
substantial gains across BLEU, ROUGE, and METEOR metrics, advancing the state
of accessible, evidence-based biomedical knowledge retrieval.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†RAG-BioQAæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œé¢†åŸŸç‰¹å®šå¾®è°ƒï¼Œç”ŸæˆåŸºäºè¯æ®çš„é•¿ç¯‡ç”Ÿç‰©åŒ»å­¦ç­”æ¡ˆï¼Œä¼˜åŒ–äº†ä¸Šä¸‹æ–‡æ£€ç´¢ä¸åˆæˆï¼Œå¹¶åœ¨PubMedQAæ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¼˜äºåŸºçº¿çš„æ€§èƒ½æå‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.01600v1">A Comparison of Independent and Joint Fine-tuning Strategies for Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>A Comparison of Independent and Joint Fine-tuning Strategies for
Retrieval-Augmented Generation Download PDF Neal Gregory Lawton, Alfy Samuel,
Anoop Kumar, Daben Liu Published: 20 Aug 2025, Last Modified: 17 Sept 2025EMNLP
2025 FindingsConference, Publication Chairs, AuthorsRevisionsBibTeXCC BY 4.0
Keywords: Retrieval-Augmented Generation (RAG), Large Language Models (LLMs),
Fine-tuning, Question Answering, Joint fine-tuning TL;DR: We evaluate and
compare strategies for fine-tuning Retrieval Augmented Generation (RAG)
pipelines, including independent fine-tuning, joint fine-tuning, and two-phase
fine-tuning. Abstract: Retrieval augmented generation (RAG) is a popular
framework for question answering that is powered by two large language models
(LLMs): an embedding model that retrieves context documents from a database
that are relevant to a given question, and a generator model that uses the
retrieved context to generate an answer to the question. Both the embedding and
generator models can be fine-tuned to increase performance of a RAG pipeline on
a new task, but multiple fine-tuning strategies exist with different costs and
benefits. In this paper, we evaluate and compare several RAG fine-tuning
strategies, including independent, joint, and two-phase fine-tuning. In our
experiments, we observe that all of these strategies achieve about equal
improvement in EM and F1 generation quality metrics, although they have
significantly different computational costs. We conclude the optimal
fine-tuning strategy to use depends on whether the training dataset includes
context labels and whether a grid search over the learning rates for the
embedding and generator models is required.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¯”è¾ƒäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­ä¸åŒå¾®è°ƒç­–ç•¥ï¼ˆç‹¬ç«‹ã€è”åˆå’Œä¸¤é˜¶æ®µå¾®è°ƒï¼‰çš„æ€§èƒ½å’Œè®¡ç®—æˆæœ¬ï¼Œå‘ç°åœ¨ç”Ÿæˆè´¨é‡ä¸Šè¡¨ç°ç›¸è¿‘ä½†è®¡ç®—ä»£ä»·å·®å¼‚æ˜¾è‘—ï¼Œå¹¶æŒ‡å‡ºæœ€ä¼˜ç­–ç•¥å–å†³äºè®­ç»ƒæ•°æ®æ˜¯å¦åŒ…å«ä¸Šä¸‹æ–‡æ ‡ç­¾åŠæ˜¯å¦éœ€è¦å­¦ä¹ ç‡è°ƒä¼˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.01558v1">CardioRAG: A Retrieval-Augmented Generation Framework for Multimodal Chagas Disease Detection</a></td><td><details><summary>å±•å¼€</summary>Chagas disease affects nearly 6 million people worldwide, with Chagas
cardiomyopathy representing its most severe complication. In regions where
serological testing capacity is limited, AI-enhanced electrocardiogram (ECG)
screening provides a critical diagnostic alternative. However, existing machine
learning approaches face challenges such as limited accuracy, reliance on large
labeled datasets, and more importantly, weak integration with evidence-based
clinical diagnostic indicators. We propose a retrieval-augmented generation
framework, CardioRAG, integrating large language models with interpretable
ECG-based clinical features, including right bundle branch block, left anterior
fascicular block, and heart rate variability metrics. The framework uses
variational autoencoder-learned representations for semantic case retrieval,
providing contextual cases to guide clinical reasoning. Evaluation demonstrated
high recall performance of 89.80%, with a maximum F1 score of 0.68 for
effective identification of positive cases requiring prioritized serological
testing. CardioRAG provides an interpretable, clinical evidence-based approach
particularly valuable for resource-limited settings, demonstrating a pathway
for embedding clinical indicators into trustworthy medical AI systems.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºCardioRAGçš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œç»“åˆå¤§è¯­è¨€æ¨¡å‹å’Œå¯è§£é‡Šçš„å¿ƒç”µå›¾ä¸´åºŠç‰¹å¾ï¼ˆå¦‚å³æŸæ”¯ä¼ å¯¼é˜»æ»ç­‰ï¼‰ï¼Œé€šè¿‡æ£€ç´¢ç›¸å…³ç—…ä¾‹æä¾›ä¸´åºŠæ¨ç†æŒ‡å¯¼ï¼Œæ˜¾è‘—æå‡äº†æ°åŠ æ–¯ç—…å¿ƒè‚Œç—…ç­›æŸ¥çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ï¼Œé€‚ç”¨äºèµ„æºæœ‰é™åœ°åŒºã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.01553v1">IoDResearch: Deep Research on Private Heterogeneous Data via the Internet of Data</a></td><td><details><summary>å±•å¼€</summary>The rapid growth of multi-source, heterogeneous, and multimodal scientific
data has increasingly exposed the limitations of traditional data management.
Most existing DeepResearch (DR) efforts focus primarily on web search while
overlooking local private data. Consequently, these frameworks exhibit low
retrieval efficiency for private data and fail to comply with the FAIR
principles, ultimately resulting in inefficiency and limited reusability. To
this end, we propose IoDResearch (Internet of Data Research), a private
data-centric Deep Research framework that operationalizes the Internet of Data
paradigm. IoDResearch encapsulates heterogeneous resources as FAIR-compliant
digital objects, and further refines them into atomic knowledge units and
knowledge graphs, forming a heterogeneous graph index for multi-granularity
retrieval. On top of this representation, a multi-agent system supports both
reliable question answering and structured scientific report generation.
Furthermore, we establish the IoD DeepResearch Benchmark to systematically
evaluate both data representation and Deep Research capabilities in IoD
scenarios. Experimental results on retrieval, QA, and report-writing tasks show
that IoDResearch consistently surpasses representative RAG and Deep Research
baselines. Overall, IoDResearch demonstrates the feasibility of
private-data-centric Deep Research under the IoD paradigm, paving the way
toward more trustworthy, reusable, and automated scientific discovery.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºIoDResearchæ¡†æ¶ï¼Œé€šè¿‡å°†å¼‚æ„æ•°æ®å°è£…ä¸ºFAIRåˆè§„çš„æ•°å­—å¯¹è±¡å¹¶æ„å»ºå¤šç²’åº¦æ£€ç´¢çš„å¼‚æ„å›¾ç´¢å¼•ï¼Œç»“åˆå¤šæ™ºèƒ½ä½“ç³»ç»Ÿå®ç°å¯é é—®ç­”å’Œç»“æ„åŒ–æŠ¥å‘Šç”Ÿæˆï¼Œå®éªŒè¡¨æ˜å…¶åœ¨æ£€ç´¢å’Œç”Ÿæˆä»»åŠ¡ä¸Šä¼˜äºRAGåŸºçº¿ï¼Œå±äºRAGæŠ€æœ¯åœ¨ç§æœ‰æ•°æ®åœºæ™¯ä¸‹çš„ä¼˜åŒ–åº”ç”¨ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-10-01
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2510.01523v1">MetaSynth: Multi-Agent Metadata Generation from Implicit Feedback in Black-Box Systems</a></td><td><details><summary>å±•å¼€</summary>Meta titles and descriptions strongly shape engagement in search and
recommendation platforms, yet optimizing them remains challenging. Search
engine ranking models are black box environments, explicit labels are
unavailable, and feedback such as click-through rate (CTR) arrives only
post-deployment. Existing template, LLM, and retrieval-augmented approaches
either lack diversity, hallucinate attributes, or ignore whether candidate
phrasing has historically succeeded in ranking. This leaves a gap in directly
leveraging implicit signals from observable outcomes. We introduce MetaSynth, a
multi-agent retrieval-augmented generation framework that learns from implicit
search feedback. MetaSynth builds an exemplar library from top-ranked results,
generates candidate snippets conditioned on both product content and exemplars,
and iteratively refines outputs via evaluator-generator loops that enforce
relevance, promotional strength, and compliance. On both proprietary e-commerce
data and the Amazon Reviews corpus, MetaSynth outperforms strong baselines
across NDCG, MRR, and rank metrics. Large-scale A/B tests further demonstrate
10.26% CTR and 7.51% clicks. Beyond metadata, this work contributes a general
paradigm for optimizing content in black-box systems using implicit signals.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†MetaSynthï¼Œä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºä¼˜åŒ–æœç´¢å¼•æ“çš„å…ƒæ ‡é¢˜å’Œæè¿°ã€‚å®ƒé€šè¿‡ä»æ’åé å‰çš„æœç´¢ç»“æœä¸­æ„å»ºç¤ºä¾‹åº“ï¼Œç»“åˆäº§å“å†…å®¹å’Œå†å²æˆåŠŸæ¡ˆä¾‹ç”Ÿæˆå€™é€‰ç‰‡æ®µï¼Œå¹¶é€šè¿‡è¯„ä¼°-ç”Ÿæˆå¾ªç¯è¿­ä»£ä¼˜åŒ–è¾“å‡ºï¼Œä»¥æé«˜ç›¸å…³æ€§ã€æ¨å¹¿å¼ºåº¦å’Œåˆè§„æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒMetaSynthåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰åŸºçº¿ï¼Œå¹¶æ˜¾è‘—æå‡äº†ç‚¹å‡»ç‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.01409v1">OntoLogX: Ontology-Guided Knowledge Graph Extraction from Cybersecurity Logs with Large Language Models</a></td><td><details><summary>å±•å¼€</summary>System logs represent a valuable source of Cyber Threat Intelligence (CTI),
capturing attacker behaviors, exploited vulnerabilities, and traces of
malicious activity. Yet their utility is often limited by lack of structure,
semantic inconsistency, and fragmentation across devices and sessions.
Extracting actionable CTI from logs therefore requires approaches that can
reconcile noisy, heterogeneous data into coherent and interoperable
representations. We introduce OntoLogX, an autonomous Artificial Intelligence
(AI) agent that leverages Large Language Models (LLMs) to transform raw logs
into ontology-grounded Knowledge Graphs (KGs). OntoLogX integrates a
lightweight log ontology with Retrieval Augmented Generation (RAG) and
iterative correction steps, ensuring that generated KGs are syntactically and
semantically valid. Beyond event-level analysis, the system aggregates KGs into
sessions and employs a LLM to predict MITRE ATT&CK tactics, linking low-level
log evidence to higher-level adversarial objectives. We evaluate OntoLogX on
both logs from a public benchmark and a real-world honeypot dataset,
demonstrating robust KG generation across multiple KGs backends and accurate
mapping of adversarial activity to ATT&CK tactics. Results highlight the
benefits of retrieval and correction for precision and recall, the
effectiveness of code-oriented models in structured log analysis, and the value
of ontology-grounded representations for actionable CTI extraction.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†OntoLogXï¼Œä¸€ä¸ªåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å°†åŸå§‹æ—¥å¿—è½¬åŒ–ä¸ºåŸºäºæœ¬ä½“çš„çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰çš„è‡ªä¸»AIä»£ç†ã€‚å®ƒç»“åˆäº†è½»é‡çº§æ—¥å¿—æœ¬ä½“ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œé€šè¿‡è¿­ä»£æ ¡æ­£æ­¥éª¤ç¡®ä¿ç”Ÿæˆçš„KGsåœ¨è¯­æ³•å’Œè¯­ä¹‰ä¸Šæœ‰æ•ˆï¼Œå¹¶å°†æ—¥å¿—äº‹ä»¶å…³è”åˆ°MITRE ATT&CKæ¡†æ¶ä¸­çš„é«˜çº§æ”»å‡»ç­–ç•¥ï¼Œä»è€Œæå‡ç½‘ç»œå¨èƒæƒ…æŠ¥ï¼ˆCTIï¼‰çš„å¯æ“ä½œæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.01375v1">Fine-tuning with RAG for Improving LLM Learning of New Skills</a></td><td><details><summary>å±•å¼€</summary>Large language model (LLM) agents deployed for multi-step tasks frequently
fail in predictable ways: attempting actions with unmet preconditions, issuing
redundant commands, or mishandling environment constraints. While
retrieval-augmented generation (RAG) can improve performance by providing
runtime guidance, it requires maintaining external knowledge databases and adds
computational overhead at every deployment. We propose a simple pipeline that
converts inference-time retrieval into learned competence through distillation.
Our approach: (1) extracts compact, reusable hints from agent failures, (2)
uses these hints to generate improved teacher trajectories via one-shot
retrieval at episode start, and (3) trains student models on these trajectories
with hint strings removed, forcing internalization rather than memorization.
Across two interactive benchmarks, ALFWorld (household tasks) and WebShop
(online shopping), distilled students consistently outperform baseline agents,
achieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving
WebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens
than retrieval-augmented teachers depending on the environment. The approach
generalizes across model scales (7B/14B parameters) and agent architectures
(ReAct/StateAct), demonstrating that retrieval benefits can be effectively
internalized through targeted fine-tuning without permanent runtime
dependencies.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§é€šè¿‡çŸ¥è¯†è’¸é¦å°†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„è¿è¡Œæ—¶æ£€ç´¢è½¬åŒ–ä¸ºæ¨¡å‹å†…éƒ¨èƒ½åŠ›çš„æ–¹æ³•ï¼Œä»¥å‡å°‘å¯¹å¤–éƒ¨çŸ¥è¯†åº“çš„ä¾èµ–å’Œè®¡ç®—å¼€é”€ã€‚è¯¥æ–¹æ³•ä»æ™ºèƒ½ä½“å¤±è´¥ä¸­æå–ç´§å‡‘çš„æç¤ºï¼Œç”Ÿæˆæ”¹è¿›çš„æ•™å¸ˆè½¨è¿¹ï¼Œå¹¶è®­ç»ƒå­¦ç”Ÿæ¨¡å‹å†…éƒ¨åŒ–è¿™äº›çŸ¥è¯†ï¼Œåœ¨ALFWorldå’ŒWebShopåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.01363v1">Retrieval-Augmented Framework for LLM-Based Clinical Decision Support</a></td><td><details><summary>å±•å¼€</summary>The increasing complexity of clinical decision-making, alongside the rapid
expansion of electronic health records (EHR), presents both opportunities and
challenges for delivering data-informed care. This paper proposes a clinical
decision support system powered by Large Language Models (LLMs) to assist
prescribing clinicians. The system generates therapeutic suggestions by
analyzing historical EHR data, including patient demographics, presenting
complaints, clinical symptoms, diagnostic information, and treatment histories.
The framework integrates natural language processing with structured clinical
inputs to produce contextually relevant recommendations. Rather than replacing
clinician judgment, it is designed to augment decision-making by retrieving and
synthesizing precedent cases with comparable characteristics, drawing on local
datasets or federated sources where applicable. At its core, the system employs
a retrieval-augmented generation (RAG) pipeline that harmonizes unstructured
narratives and codified data to support LLM-based inference. We outline the
system's technical components, including representation representation
alignment and generation strategies. Preliminary evaluations, conducted with
de-identified and synthetic clinical datasets, examine the clinical
plausibility and consistency of the model's outputs. Early findings suggest
that LLM-based tools may provide valuable decision support in prescribing
workflows when appropriately constrained and rigorously validated. This work
represents an initial step toward integration of generative AI into real-world
clinical decision-making with an emphasis on transparency, safety, and
alignment with established practices.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯æ•´åˆç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰ä¸­çš„ç»“æ„åŒ–å’Œéç»“æ„åŒ–æ•°æ®ï¼Œç”Ÿæˆæ²»ç–—å»ºè®®ã€‚ç³»ç»Ÿåˆ©ç”¨å†å²ç—…ä¾‹æ•°æ®æ£€ç´¢ç›¸ä¼¼æ¡ˆä¾‹ï¼Œè¾…åŠ©ä¸´åºŠåŒ»ç”Ÿå†³ç­–ï¼Œå¹¶å¼ºè°ƒé€æ˜åº¦ã€å®‰å…¨æ€§ä¸ä¸´åºŠéªŒè¯ã€‚åˆæ­¥è¯„ä¼°è¡¨æ˜å…¶åœ¨å¤„æ–¹å·¥ä½œæµä¸­å…·æœ‰æ½œåœ¨ä»·å€¼ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.01165v1">GRAD: Generative Retrieval-Aligned Demonstration Sampler for Efficient Few-Shot Reasoning</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) achieve strong performance across diverse tasks,
but their effectiveness often depends on the quality of the provided context.
Retrieval-Augmented Generation (RAG) enriches prompts with external
information, but its reliance on static databases constrains adaptability and
can result in irrelevant demonstrations. In this work, we propose a Generative
Retrieval-Aligned Demonstrator (GRAD), a dynamic demonstration-based approach
where an LLM model is trained to generate input-specific concise
demonstrations. By tailoring demonstrations to each input, our method offers
better contextual support than traditional RAG approaches. We demonstrate the
superiority of GRAD under budget constraints, where we limit both the number of
tokens used per demonstration and the number of tokens used for the final
output. Trained solely on a math dataset, GRAD consistently outperforms strong
baselines on Qwen2.5-14B across mathematical reasoning and advanced STEM
questions, highlighting GRAD's robust generalization to out-of-distribution
(OOD) domains such as physics, chemistry, and computer science. Furthermore, we
show that demonstrations generated by trained smaller models can effectively
guide larger target models, reducing training costs while maintaining
competitive accuracy. Overall, this work introduces a scalable demonstration
generator model presenting the first step toward a dynamic few-shot learning
paradigm in resource-constrained settings. We release the code used for the
project.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºGRADï¼ˆGenerative Retrieval-Aligned Demonstratorï¼‰çš„åŠ¨æ€æ¼”ç¤ºç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ä¸ºæ¯ä¸ªè¾“å…¥ç”Ÿæˆç‰¹å®šä¸”ç®€æ´çš„æ¼”ç¤ºï¼Œä»¥æä¾›æ¯”ä¼ ç»ŸRAGæ›´ç²¾å‡†çš„ä¸Šä¸‹æ–‡æ”¯æŒã€‚å®éªŒè¡¨æ˜ï¼ŒGRADåœ¨æ•°å­¦æ¨ç†å’ŒSTEMé—®é¢˜ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶èƒ½æ³›åŒ–åˆ°ç‰©ç†ã€åŒ–å­¦ç­‰OODé¢†åŸŸï¼ŒåŒæ—¶å°æ¨¡å‹ç”Ÿæˆçš„æ¼”ç¤ºå¯æœ‰æ•ˆæŒ‡å¯¼å¤§æ¨¡å‹ï¼Œé™ä½è®­ç»ƒæˆæœ¬ã€‚è¯¥ç ”ç©¶ä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„åŠ¨æ€å°æ ·æœ¬å­¦ä¹ æä¾›äº†æ–°æ€è·¯ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.01115v1">Exploring Network-Knowledge Graph Duality: A Case Study in Agentic Supply Chain Risk Analysis</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) struggle with the complex, multi-modal, and
network-native data underlying financial risk. Standard Retrieval-Augmented
Generation (RAG) oversimplifies relationships, while specialist models are
costly and static. We address this gap with an LLM-centric agent framework for
supply chain risk analysis. Our core contribution is to exploit the inherent
duality between networks and knowledge graphs (KG). We treat the supply chain
network as a KG, allowing us to use structural network science principles for
retrieval. A graph traverser, guided by network centrality scores, efficiently
extracts the most economically salient risk paths. An agentic architecture
orchestrates this graph retrieval alongside data from numerical factor tables
and news streams. Crucially, it employs novel ``context shells'' -- descriptive
templates that embed raw figures in natural language -- to make quantitative
data fully intelligible to the LLM. This lightweight approach enables the model
to generate concise, explainable, and context-rich risk narratives in real-time
without costly fine-tuning or a dedicated graph database.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºLLMçš„ä¾›åº”é“¾é£é™©åˆ†ææ¡†æ¶ï¼Œé€šè¿‡å°†ä¾›åº”é“¾ç½‘ç»œè§†ä¸ºçŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ï¼Œåˆ©ç”¨ç½‘ç»œä¸­å¿ƒæ€§è¯„åˆ†æŒ‡å¯¼æ£€ç´¢ï¼Œå¹¶ç»“åˆæ•°å€¼å› å­è¡¨å’Œæ–°é—»æµæ•°æ®ï¼Œé‡‡ç”¨åˆ›æ–°çš„"context shells"æŠ€æœ¯ä½¿å®šé‡æ•°æ®æ›´æ˜“è¢«LLMç†è§£ï¼Œä»è€Œç”Ÿæˆå®æ—¶ã€å¯è§£é‡Šä¸”ä¸Šä¸‹æ–‡çš„é£é™©åˆ†ææŠ¥å‘Šï¼Œæ”¹è¿›äº†ä¼ ç»ŸRAGæ–¹æ³•åœ¨é‡‘èé£é™©é¢†åŸŸçš„å±€é™æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.00919v2">Benchmarking Foundation Models with Retrieval-Augmented Generation in Olympic-Level Physics Problem Solving</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) with foundation models has achieved
strong performance across diverse tasks, but their capacity for expert-level
reasoning-such as solving Olympiad-level physics problems-remains largely
unexplored. Inspired by the way students prepare for competitions by reviewing
past problems, we investigate the potential of RAG to enhance physics reasoning
in foundation models. We introduce PhoPile, a high-quality multimodal dataset
specifically designed for Olympiad-level physics, enabling systematic study of
retrieval-based reasoning. PhoPile includes diagrams, graphs, and equations,
capturing the inherently multimodal nature of physics problem solving. Using
PhoPile, we benchmark RAG-augmented foundation models, covering both large
language models (LLMs) and large multimodal models (LMMs) with multiple
retrievers. Our results demonstrate that integrating retrieval with physics
corpora can improve model performance, while also highlighting challenges that
motivate further research in retrieval-augmented physics reasoning.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†RAGæŠ€æœ¯åœ¨åŸºç¡€æ¨¡å‹ä¸­å¢å¼ºç‰©ç†æ¨ç†èƒ½åŠ›çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å¥¥æ—åŒ¹å…‹çº§ç‰©ç†é—®é¢˜çš„è§£å†³ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„å¤šæ¨¡æ€æ•°æ®é›†PhoPileï¼Œç”¨äºç³»ç»Ÿç ”ç©¶åŸºäºæ£€ç´¢çš„æ¨ç†ï¼Œå¹¶è¯„ä¼°äº†ä¸åŒæ£€ç´¢å™¨å’ŒåŸºç¡€æ¨¡å‹ï¼ˆåŒ…æ‹¬LLMså’ŒLMMsï¼‰çš„æ€§èƒ½ï¼Œç»“æœè¡¨æ˜æ£€ç´¢ç‰©ç†è¯­æ–™åº“èƒ½æå‡æ¨¡å‹è¡¨ç°ï¼Œä½†ä¹ŸæŒ‡å‡ºäº†æœªæ¥ç ”ç©¶çš„æŒ‘æˆ˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.00880v1">HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate Hallucinations in Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) excel in many NLP tasks but remain prone to
hallucinations, limiting trust in real-world applications. We present
HalluGuard, a 4B-parameter Small Reasoning Model (SRM) for mitigating
hallucinations in Retrieval-Augmented Generation (RAG). HalluGuard classifies
document-claim pairs as grounded or hallucinated and produces evidence-grounded
justifications for transparency. Our approach combines (i) a domain-agnostic
synthetic dataset derived from FineWeb and refined through multi-stage curation
and data reformation, (ii) synthetic grounded and hallucinated claims, and
(iii) preference-based fine-tuning with Odds Ratio Preference Optimization to
distill large-model reasoning into a smaller backbone. On the RAGTruth subset
of the LLM-AggreFact benchmark, HalluGuard achieves 84.0% balanced accuracy
(BAcc), rivaling specialized models, MiniCheck (7B; 84.0%) and Granite Guardian
3.3 (8B; 82.2%) while using roughly half their parameters. Over the full
benchmark it reaches 75.7% BAcc, matching larger general-purpose LLMs such as
GPT-4o (75.9%). We will release HalluGuard and datasets under Apache 2.0 upon
acceptance.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†HalluGuardï¼Œä¸€ä¸ª4Bå‚æ•°çš„å°å‹æ¨ç†æ¨¡å‹ï¼ˆSRMï¼‰ï¼Œç”¨äºç¼“è§£æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­çš„å¹»è§‰é—®é¢˜ã€‚HalluGuardé€šè¿‡åˆ†ç±»æ–‡æ¡£-å£°æ˜å¯¹æ˜¯å¦åŸºäºäº‹å®ï¼Œå¹¶ç”Ÿæˆè¯æ®æ”¯æŒçš„åˆç†è§£é‡Šï¼Œç»“åˆäº†é¢†åŸŸæ— å…³çš„æ•°æ®é›†ã€åˆæˆæ•°æ®åŠåå¥½å¾®è°ƒæŠ€æœ¯ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ€§èƒ½æ¥è¿‘æˆ–è¶…è¶Šæ›´å¤§è§„æ¨¡çš„æ¨¡å‹ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.00829v1">Exposing the Cracks: Vulnerabilities of Retrieval-Augmented LLM-based Machine Translation</a></td><td><details><summary>å±•å¼€</summary>\textbf{RE}trieval-\textbf{A}ugmented \textbf{L}LM-based \textbf{M}achine
\textbf{T}ranslation (REAL-MT) shows promise for knowledge-intensive tasks like
idiomatic translation, but its reliability under noisy retrieval contexts
remains poorly understood despite this being a common challenge in real-world
deployment. To address this gap, we propose a noise synthesis framework and new
metrics to evaluate the robustness of REAL-MT systematically. Using this
framework, we instantiate REAL-MT with Qwen-series models, including standard
LLMs and large reasoning models (LRMs) with enhanced reasoning, and evaluate
their performance on idiomatic translation across high-, medium-, and
low-resource language pairs under synthesized noise. Our results show that
low-resource language pairs, which rely more heavily on retrieved context,
degrade more severely under noise than high-resource ones and often produce
nonsensical translations. Although LRMs possess enhanced reasoning
capabilities, they show no improvement in error correction and are even more
susceptible to noise, tending to rationalize incorrect contexts. We find that
this stems from an attention shift away from the source idiom to noisy content,
while confidence increases despite declining accuracy, indicating poor
calibration. To mitigate these issues, we investigate training-free and
fine-tuning strategies, which improve robustness at the cost of performance in
clean contexts, revealing a fundamental trade-off. Our findings highlight the
limitations of current approaches, underscoring the need for self-verifying
integration mechanisms.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ç ”ç©¶äº†åŸºäºæ£€ç´¢å¢å¼ºçš„å¤§è¯­è¨€æ¨¡å‹æœºå™¨ç¿»è¯‘ï¼ˆREAL-MTï¼‰åœ¨å™ªå£°æ£€ç´¢ç¯å¢ƒä¸‹çš„é²æ£’æ€§ï¼Œæå‡ºå™ªå£°åˆæˆæ¡†æ¶å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œå‘ç°ä½èµ„æºè¯­è¨€å¯¹å’Œå¢å¼ºæ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰æ˜“å—å™ªå£°å¹²æ‰°ï¼Œå¹¶æ¢è®¨äº†æ— è®­ç»ƒå’Œå¾®è°ƒç­–ç•¥çš„æ”¹è¿›æ–¹æ³•ï¼Œæ­ç¤ºäº†æ€§èƒ½ä¸é²æ£’æ€§çš„æƒè¡¡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.00662v1">Facilitating Cognitive Accessibility with LLMs: A Multi-Task Approach to Easy-to-Read Text Generation</a></td><td><details><summary>å±•å¼€</summary>Simplifying complex texts is essential for ensuring equitable access to
information, especially for individuals with cognitive impairments. The
Easy-to-Read (ETR) initiative offers a framework for making content accessible
to the neurodivergent population, but the manual creation of such texts remains
time-consuming and resource-intensive. In this work, we investigate the
potential of large language models (LLMs) to automate the generation of ETR
content. To address the scarcity of aligned corpora and the specificity of ETR
constraints, we propose a multi-task learning (MTL) approach that trains models
jointly on text summarization, text simplification, and ETR generation. We
explore two different strategies: multi-task retrieval-augmented generation
(RAG) for in-context learning, and MTL-LoRA for parameter-efficient
fine-tuning. Our experiments with Mistral-7B and LLaMA-3-8B, based on ETR-fr, a
new high-quality dataset, demonstrate the benefits of multi-task setups over
single-task baselines across all configurations. Moreover, results show that
the RAG-based strategy enables generalization in out-of-domain settings, while
MTL-LoRA outperforms all learning strategies within in-domain configurations.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ç ”ç©¶äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªåŠ¨ç”Ÿæˆæ˜“è¯»æ–‡æœ¬ï¼ˆETRï¼‰çš„æ–¹æ³•ï¼Œæå‡ºäº†ä¸€ç§å¤šä»»åŠ¡å­¦ä¹ ï¼ˆMTLï¼‰æ¡†æ¶ï¼Œç»“åˆæ–‡æœ¬æ‘˜è¦ã€æ–‡æœ¬ç®€åŒ–å’ŒETRç”Ÿæˆä»»åŠ¡ï¼Œå¹¶æ¢ç´¢äº†åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„ä¸Šä¸‹æ–‡å­¦ä¹ ç­–ç•¥å’Œå‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼ˆMTL-LoRAï¼‰ã€‚å®éªŒè¡¨æ˜ï¼Œå¤šä»»åŠ¡è®¾ç½®ä¼˜äºå•ä»»åŠ¡åŸºçº¿ï¼ŒRAGç­–ç•¥åœ¨è·¨é¢†åŸŸåœºæ™¯ä¸­è¡¨ç°è‰¯å¥½ï¼Œè€ŒMTL-LoRAåœ¨é¢†åŸŸå†…é…ç½®ä¸­æ•ˆæœæœ€ä½³ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.05137v1">Demystifying deep search: a holistic evaluation with hint-free multi-hop questions and factorised metrics</a></td><td><details><summary>å±•å¼€</summary>RAG (Retrieval-Augmented Generation) systems and web agents are increasingly
evaluated on multi-hop deep search tasks, yet current practice suffers from two
major limitations. First, most benchmarks leak the reasoning path in the
question text, allowing models to follow surface cues rather than discover
reasoning chains autonomously. Second, evaluation is typically reduced to a
single pass rate, which collapses diverse behaviours into one score and
obscures whether failures stem from inadequate search, poor knowledge use, or
inappropriate refusal. To address these issues, we present WebDetective, a
benchmark of hint-free multi-hop questions paired with a controlled Wikipedia
sandbox that ensures full traceability of model actions, and a holistic
evaluation framework that separates search sufficiency, knowledge utilisation,
and refusal behaviour. Our evaluation of 25 state-of-the-art models reveals
systematic weaknesses across all architectures: models struggle with knowledge
utilisation despite having sufficient evidence and demonstrate near-absent
appropriate refusal when evidence is lacking. These patterns expose a
fundamental gap: today's systems excel at executing given reasoning paths but
fail when required to discover them. We develop an agentic workflow,
EvidenceLoop, that explicitly targets the challenges our benchmark identifies,
incorporating verification loops and systematic evidence tracking that improve
both search and synthesis capabilities. This baseline demonstrates that
WebDetective's diagnostic framework can guide concrete architectural
improvements, establishing our benchmark as a critical tool for developing
genuinely autonomous reasoning systems rather than pattern-following agents.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†WebDetectiveï¼Œä¸€ä¸ªé’ˆå¯¹RAGç³»ç»Ÿå’Œç½‘é¡µä»£ç†çš„å¤šè·³æ·±åº¦æœç´¢ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è¯„ä¼°æ–¹æ³•ä¸­æ¨ç†è·¯å¾„æ³„éœ²å’Œå•ä¸€è¯„åˆ†çš„é—®é¢˜ï¼Œå¹¶æ­ç¤ºæ¨¡å‹åœ¨çŸ¥è¯†åˆ©ç”¨å’Œæ‹’ç»è¡Œä¸ºæ–¹é¢çš„ç³»ç»Ÿæ€§å¼±ç‚¹ï¼Œæœ€ç»ˆæå‡ºäº†ä¸€ä¸ªæ”¹è¿›çš„ä»£ç†å·¥ä½œæµç¨‹EvidenceLoopæ¥æå‡æœç´¢å’Œåˆæˆèƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.00586v1">Eyes-on-Me: Scalable RAG Poisoning through Transferable Attention-Steering Attractors</a></td><td><details><summary>å±•å¼€</summary>Existing data poisoning attacks on retrieval-augmented generation (RAG)
systems scale poorly because they require costly optimization of poisoned
documents for each target phrase. We introduce Eyes-on-Me, a modular attack
that decomposes an adversarial document into reusable Attention Attractors and
Focus Regions. Attractors are optimized to direct attention to the Focus
Region. Attackers can then insert semantic baits for the retriever or malicious
instructions for the generator, adapting to new targets at near zero cost. This
is achieved by steering a small subset of attention heads that we empirically
identify as strongly correlated with attack success. Across 18 end-to-end RAG
settings (3 datasets $\times$ 2 retrievers $\times$ 3 generators), Eyes-on-Me
raises average attack success rates from 21.9 to 57.8 (+35.9 points,
2.6$\times$ over prior work). A single optimized attractor transfers to unseen
black box retrievers and generators without retraining. Our findings establish
a scalable paradigm for RAG data poisoning and show that modular, reusable
components pose a practical threat to modern AI systems. They also reveal a
strong link between attention concentration and model outputs, informing
interpretability research.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºEyes-on-Meçš„æ–°å‹æ•°æ®æŠ•æ¯’æ”»å‡»æ–¹æ³•ï¼Œé’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿã€‚è¯¥æ–¹æ³•é€šè¿‡å°†å¯¹æŠ—æ€§æ–‡æ¡£åˆ†è§£ä¸ºå¯é‡ç”¨çš„â€œæ³¨æ„åŠ›å¸å¼•å™¨â€å’Œâ€œç„¦ç‚¹åŒºåŸŸâ€ï¼Œæ˜¾è‘—æé«˜äº†æ”»å‡»æ•ˆç‡ï¼Œæ— éœ€é’ˆå¯¹æ¯ä¸ªç›®æ ‡çŸ­è¯­è¿›è¡Œæ˜‚è´µçš„ä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§RAGè®¾ç½®ä¸‹å°†æ”»å‡»æˆåŠŸç‡ä»21.9%æå‡è‡³57.8%ï¼Œå¹¶æ­ç¤ºäº†æ³¨æ„åŠ›é›†ä¸­ä¸æ¨¡å‹è¾“å‡ºä¹‹é—´çš„å¼ºå…³è”ï¼Œä¸ºRAGç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯è§£é‡Šæ€§ç ”ç©¶æä¾›äº†æ–°è§è§£ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.00566v2">Panorama: Fast-Track Nearest Neighbors</a></td><td><details><summary>å±•å¼€</summary>Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose
embeddings are close to that of a given query in a high-dimensional space,
aiming to balance accuracy with speed. Used in recommendation systems, image
and video retrieval, natural language processing, and retrieval-augmented
generation (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT
utilize graph, tree, clustering, and quantization techniques to navigate large
vector spaces. Despite this progress, ANNS systems spend up to 99\% of query
time to compute distances in their final refinement phase. In this paper, we
present PANORAMA, a machine learning-driven approach that tackles the ANNS
verification bottleneck through data-adaptive learned orthogonal transforms
that facilitate the accretive refinement of distance bounds. Such transforms
compact over 90\% of signal energy into the first half of dimensions, enabling
early candidate pruning with partial distance computations. We integrate
PANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and
Annoy, without index modification, using level-major memory layouts,
SIMD-vectorized partial distance computations, and cache-aware access patterns.
Experiments across diverse datasets -- from image-based CIFAR-10 and GIST to
modern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate
that PANORAMA affords a 2--30$\times$ end-to-end speedup with no recall loss.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºPANORAMAçš„æœºå™¨å­¦ä¹ é©±åŠ¨æ–¹æ³•ï¼Œé€šè¿‡æ•°æ®è‡ªé€‚åº”çš„å­¦ä¹ æ­£äº¤å˜æ¢ä¼˜åŒ–è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ï¼ˆANNSï¼‰çš„éªŒè¯ç“¶é¢ˆï¼Œæ˜¾è‘—æå‡æ£€ç´¢æ•ˆç‡ï¼Œå¹¶ç‰¹åˆ«æåˆ°ANNSåœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚è¯¥æ–¹æ³•åœ¨ä¸ä¿®æ”¹ç´¢å¼•çš„æƒ…å†µä¸‹é›†æˆåˆ°ç°æœ‰ANNSç®—æ³•ä¸­ï¼Œå®ç°äº†2-30å€çš„ç«¯åˆ°ç«¯åŠ é€Ÿä¸”ä¸æŸå¤±å¬å›ç‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.00552v1">Data Quality Challenges in Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Organizations increasingly adopt Retrieval-Augmented Generation (RAG) to
enhance Large Language Models with enterprise-specific knowledge. However,
current data quality (DQ) frameworks have been primarily developed for static
datasets, and only inadequately address the dynamic, multi-stage nature of RAG
systems. This study aims to develop DQ dimensions for this new type of AI-based
systems. We conduct 16 semi-structured interviews with practitioners of leading
IT service companies. Through a qualitative content analysis, we inductively
derive 15 distinct DQ dimensions across the four processing stages of RAG
systems: data extraction, data transformation, prompt & search, and generation.
Our findings reveal that (1) new dimensions have to be added to traditional DQ
frameworks to also cover RAG contexts; (2) these new dimensions are
concentrated in early RAG steps, suggesting the need for front-loaded quality
management strategies, and (3) DQ issues transform and propagate through the
RAG pipeline, necessitating a dynamic, step-aware approach to quality
management.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­æ•°æ®è´¨é‡ï¼ˆDQï¼‰ç»´åº¦çš„å¼€å‘ï¼Œé€šè¿‡è®¿è°ˆITæœåŠ¡å…¬å¸çš„å®è·µè€…ï¼Œå½’çº³å‡º15ä¸ªDQç»´åº¦ï¼Œè¦†ç›–RAGç³»ç»Ÿçš„å››ä¸ªå¤„ç†é˜¶æ®µï¼Œå¹¶æŒ‡å‡ºéœ€è¦æ›´æ–°ä¼ ç»ŸDQæ¡†æ¶ä»¥é€‚åº”RAGçš„åŠ¨æ€ç‰¹æ€§ï¼Œå¼ºè°ƒæ—©æœŸé˜¶æ®µçš„è´¨é‡ç®¡ç†é‡è¦æ€§åŠé—®é¢˜åœ¨æµç¨‹ä¸­çš„ä¼ é€’æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.00508v1">Copy-Paste to Mitigate Large Language Model Hallucinations</a></td><td><details><summary>å±•å¼€</summary>While Retrieval-Augmented Generation (RAG) enables large language models
(LLMs) to generate contextually grounded responses, contextual faithfulness
remains challenging as LLMs may not consistently trust provided context,
leading to hallucinations that undermine reliability. We observe an inverse
correlation between response copying degree and context-unfaithful
hallucinations on RAGTruth, suggesting that higher copying degrees reduce
hallucinations by fostering genuine contextual belief. We propose CopyPasteLLM,
obtained through two-stage high-copying response preference training. We design
three prompting methods to enhance copying degree, demonstrating that
high-copying responses achieve superior contextual faithfulness and
hallucination control. These approaches enable a fully automated pipeline that
transforms generated responses into high-copying preference data for training
CopyPasteLLM. On FaithEval, ConFiQA and PubMedQA, CopyPasteLLM achieves best
performance in both counterfactual and original contexts, remarkably with 12.2%
to 24.5% accuracy improvements on FaithEval over the best baseline, while
requiring only 365 training samples -- 1/50th of baseline data. To elucidate
CopyPasteLLM's effectiveness, we propose the Context-Parameter Copying
Capturing algorithm. Interestingly, this reveals that CopyPasteLLM recalibrates
reliance on internal parametric knowledge rather than external knowledge during
generation. All codes are available at
https://github.com/longyongchao/CopyPasteLLM</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡é’ˆå¯¹RAGä¸­LLMså¯¹æ£€ç´¢å†…å®¹ä¿¡ä»»ä¸è¶³å¯¼è‡´çš„å¹»è§‰é—®é¢˜ï¼Œæå‡ºCopyPasteLLMæ¨¡å‹ï¼Œé€šè¿‡é«˜å¤åˆ¶åº¦å“åº”è®­ç»ƒå’Œæç¤ºæ–¹æ³•å¢å¼ºä¸Šä¸‹æ–‡å¿ å®åº¦ï¼Œæ˜¾è‘—é™ä½å¹»è§‰å¹¶æå‡å‡†ç¡®æ€§ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2510.00482v1">Agent Fine-tuning through Distillation for Domain-specific LLMs in Microdomains</a></td><td><details><summary>å±•å¼€</summary>Agentic large language models (LLMs) have become prominent for autonomously
interacting with external environments and performing multi-step reasoning
tasks. Most approaches leverage these capabilities via in-context learning with
few-shot prompts, but this often results in lengthy inputs and higher
computational costs. Agent fine-tuning offers an alternative by enabling LLMs
to internalize procedural reasoning and domain-specific knowledge through
training on relevant data and demonstration trajectories. While prior studies
have focused on general domains, their effectiveness in specialized technical
microdomains remains unclear. This paper explores agent fine-tuning for domain
adaptation within Hitachi's JP1 middleware, a microdomain for specialized IT
operations. We fine-tuned LLMs using JP1-specific datasets derived from domain
manuals and distilled reasoning trajectories generated by LLMs themselves,
enhancing decision making accuracy and search efficiency. During inference, we
used an agentic prompt with retrieval-augmented generation and introduced a
context-answer extractor to improve information relevance. On JP1 certification
exam questions, our method achieved a 14% performance improvement over the base
model, demonstrating the potential of agent fine-tuning for domain-specific
reasoning in complex microdomains.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æ¢è®¨äº†åœ¨æ—¥ç«‹JP1ä¸­é—´ä»¶è¿™ä¸€ç‰¹å®šæŠ€æœ¯å¾®é¢†åŸŸä¸­ï¼Œé€šè¿‡å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»¥æå‡é¢†åŸŸé€‚åº”æ€§çš„æ–¹æ³•ï¼Œå…¶ä¸­åœ¨æ¨ç†é˜¶æ®µé‡‡ç”¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯å’Œä¸Šä¸‹æ–‡-ç­”æ¡ˆæå–å™¨æ¥æé«˜ä¿¡æ¯çš„ç›¸å…³æ€§ï¼Œæœ€ç»ˆåœ¨JP1è®¤è¯è€ƒè¯•é—®é¢˜ä¸Šå®ç°äº†14%çš„æ€§èƒ½æå‡ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-09-30
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2509.26584v1">Fairness Testing in Retrieval-Augmented Generation: How Small Perturbations Reveal Bias in Small Language Models</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) are widely used across multiple domains but
continue to raise concerns regarding security and fairness. Beyond known attack
vectors such as data poisoning and prompt injection, LLMs are also vulnerable
to fairness bugs. These refer to unintended behaviors influenced by sensitive
demographic cues (e.g., race or sexual orientation) that should not affect
outcomes. Another key issue is hallucination, where models generate plausible
yet false information. Retrieval-Augmented Generation (RAG) has emerged as a
strategy to mitigate hallucinations by combining external retrieval with text
generation. However, its adoption raises new fairness concerns, as the
retrieved content itself may surface or amplify bias. This study conducts
fairness testing through metamorphic testing (MT), introducing controlled
demographic perturbations in prompts to assess fairness in sentiment analysis
performed by three Small Language Models (SLMs) hosted on HuggingFace
(Llama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.3, and Llama-3.1-Nemotron-8B),
each integrated into a RAG pipeline. Results show that minor demographic
variations can break up to one third of metamorphic relations (MRs). A detailed
analysis of these failures reveals a consistent bias hierarchy, with
perturbations involving racial cues being the predominant cause of the
violations. In addition to offering a comparative evaluation, this work
reinforces that the retrieval component in RAG must be carefully curated to
prevent bias amplification. The findings serve as a practical alert for
developers, testers and small organizations aiming to adopt accessible SLMs
without compromising fairness or reliability.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†RAGæŠ€æœ¯ä¸­çš„å…¬å¹³æ€§é—®é¢˜ï¼Œé€šè¿‡èœ•å˜æµ‹è¯•è¯„ä¼°å°å‹è¯­è¨€æ¨¡å‹åœ¨RAGæµç¨‹ä¸­å¯¹æ•æ„Ÿäººå£ç»Ÿè®¡çº¿ç´¢çš„åå·®è¡¨ç°ï¼Œå¹¶æ­ç¤ºæ£€ç´¢å†…å®¹å¯èƒ½åŠ å‰§åè§çš„ç°è±¡ï¼Œæå‡ºéœ€è°¨æ…å¤„ç†æ£€ç´¢ç»„ä»¶ä»¥é¿å…åè§æ”¾å¤§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.26383v1">Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning</a></td><td><details><summary>å±•å¼€</summary>Knowledge-graph retrieval-augmented generation (KG-RAG) couples large
language models (LLMs) with structured, verifiable knowledge graphs (KGs) to
reduce hallucinations and expose reasoning traces. However, many KG-RAG systems
compose multiple LLM modules (e.g planning, reasoning, and responding),
inflating inference cost and binding behavior to a specific target KG. To
address this, we introduce KG-R1, an agentic KG retrieval-augmented generation
(KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single
agent that interacts with KGs as its environment, learning to retrieve at each
step and incorporating the retrieved information into its reasoning and
generation. The process is optimized through end-to-end RL. In controlled
experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our
method demonstrates both efficiency and transferability: Using Qwen-2.5-3B,
KG-R1 improves answer accuracy with fewer generation tokens than prior
multi-module workflow methods that use larger foundation or fine-tuned models.
Furthermore, KG-R1 enables plug and play: after training, it maintains strong
accuracy on new KGs without modification. These properties make KG-R1 a
promising KG-RAG framework for real-world deployment. Our code is publicly
available at https://github.com/Jinyeop3110/KG-R1.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„çŸ¥è¯†å›¾è°±æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶KG-R1ï¼Œé€šè¿‡å•æ™ºèƒ½ä½“ä¸çŸ¥è¯†å›¾è°±äº¤äº’ï¼Œä¼˜åŒ–æ£€ç´¢å’Œç”Ÿæˆè¿‡ç¨‹ï¼Œåœ¨é™ä½æ¨ç†æˆæœ¬çš„åŒæ—¶æé«˜å‡†ç¡®æ€§å’Œå¯è¿ç§»æ€§ï¼Œå¹¶åœ¨KGQAåŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†å…¶é«˜æ•ˆæ€§å’Œå³æ’å³ç”¨èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.26205v1">Human-Centered Evaluation of RAG outputs: a framework and questionnaire for human-AI collaboration</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) systems are increasingly deployed in
user-facing applications, yet systematic, human-centered evaluation of their
outputs remains underexplored. Building on Gienapp's utility-dimension
framework, we designed a human-centred questionnaire that assesses RAG outputs
across 12 dimensions. We iteratively refined the questionnaire through several
rounds of ratings on a set of query-output pairs and semantic discussions.
Ultimately, we incorporated feedback from both a human rater and a human-LLM
pair. Results indicate that while large language models (LLMs) reliably focus
on metric descriptions and scale labels, they exhibit weaknesses in detecting
textual format variations. Humans struggled to focus strictly on metric
descriptions and labels. LLM ratings and explanations were viewed as a helpful
support, but numeric LLM and human ratings lacked agreement. The final
questionnaire extends the initial framework by focusing on user intent, text
structuring, and information verifiability.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æ¢è®¨äº†ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„RAGç³»ç»Ÿè¯„ä¼°æ–¹æ³•ï¼Œé€šè¿‡è®¾è®¡åŒ…å«12ä¸ªç»´åº¦çš„é—®å·ï¼Œç»“åˆäººç±»ä¸LLMçš„åé¦ˆè¿­ä»£ä¼˜åŒ–ï¼Œå‘ç°LLMåœ¨æ–‡æœ¬æ ¼å¼è¯†åˆ«ä¸Šçš„ä¸è¶³åŠäººæœºè¯„åˆ†å·®å¼‚ï¼Œæœ€ç»ˆæ‰©å±•äº†è¯„ä¼°æ¡†æ¶ä»¥é‡ç‚¹å…³æ³¨ç”¨æˆ·æ„å›¾ã€æ–‡æœ¬ç»“æ„å’Œä¿¡æ¯å¯éªŒè¯æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.26184v1">Auto-ARGUE: LLM-Based Report Generation Evaluation</a></td><td><details><summary>å±•å¼€</summary>Generation of long-form, citation-backed reports is a primary use case for
retrieval augmented generation (RAG) systems. While open-source evaluation
tools exist for various RAG tasks, ones tailored to report generation are
lacking. Accordingly, we introduce Auto-ARGUE, a robust LLM-based
implementation of the recent ARGUE framework for report generation evaluation.
We present analysis of Auto-ARGUE on the report generation pilot task from the
TREC 2024 NeuCLIR track, showing good system-level correlations with human
judgments. We further release a web app for visualization of Auto-ARGUE
outputs.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†Auto-ARGUEï¼Œä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å·¥å…·ï¼Œç”¨äºè¯„ä¼°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿåœ¨ç”Ÿæˆå¸¦å¼•ç”¨çš„é•¿ç¯‡æŠ¥å‘Šä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨TREC 2024 NeuCLIRä»»åŠ¡ä¸Šä¸äººç±»è¯„ä»·çš„è‰¯å¥½ç›¸å…³æ€§ï¼ŒåŒæ—¶å‘å¸ƒäº†å¯è§†åŒ–è¾“å‡ºçš„ç½‘é¡µåº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.26136v1">CliniBench: A Clinical Outcome Prediction Benchmark for Generative and Encoder-Based Language Models</a></td><td><details><summary>å±•å¼€</summary>With their growing capabilities, generative large language models (LLMs) are
being increasingly investigated for complex medical tasks. However, their
effectiveness in real-world clinical applications remains underexplored. To
address this, we present CliniBench, the first benchmark that enables
comparability of well-studied encoder-based classifiers and generative LLMs for
discharge diagnosis prediction from admission notes in MIMIC-IV dataset. Our
extensive study compares 12 generative LLMs and 3 encoder-based classifiers and
demonstrates that encoder-based classifiers consistently outperform generative
models in diagnosis prediction. We assess several retrieval augmentation
strategies for in-context learning from similar patients and find that they
provide notable performance improvements for generative LLMs.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†CliniBenchï¼Œä¸€ä¸ªç”¨äºæ¯”è¾ƒåŸºäºç¼–ç å™¨çš„åˆ†ç±»å™¨å’Œç”Ÿæˆå¼å¤§è¯­è¨€æ¨¡å‹åœ¨MIMIC-IVæ•°æ®é›†å‡ºé™¢è¯Šæ–­é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°çš„åŸºå‡†æµ‹è¯•ï¼Œç ”ç©¶å‘ç°ç¼–ç å™¨æ¨¡å‹è¡¨ç°æ›´ä¼˜ï¼Œå¹¶é€šè¿‡æ£€ç´¢å¢å¼ºç­–ç•¥æå‡äº†ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.26011v1">RAGferee: Building Contextual Reward Models for Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Existing Reward Models (RMs), typically trained on general preference data,
struggle in Retrieval Augmented Generation (RAG) settings, which require
judging responses for faithfulness to retrieved context, relevance to the user
query, appropriate refusals when context is insufficient, completeness and
conciseness of information. To address the lack of publicly available
RAG-centric preference datasets and specialised RMs, we introduce RAGferee, a
methodology that repurposes question-answering (QA) datasets into preference
pairs that prioritise groundedness over stylistic features, enabling the
training of contextual RMs better suited to judging RAG responses. Using
RAGferee, we curate a small preference dataset of 4K samples and fine-tune RMs
ranging from 7B to 24B parameters. Our RAG-centric RMs achieve state-of-the-art
performance on ContextualJudgeBench, surpassing existing 70B+ RMs trained on
much larger (up to 2.4M samples) general corpora, with an absolute improvement
of +15.5%.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡é’ˆå¯¹ç°æœ‰å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰åœ¨RAGåœºæ™¯ä¸­çš„ä¸è¶³ï¼ˆå¦‚å¯¹æ£€ç´¢å†…å®¹å¿ å®åº¦ã€æŸ¥è¯¢ç›¸å…³æ€§ã€ä¿¡æ¯å®Œæ•´æ€§çš„è¯„ä¼°ï¼‰ï¼Œæå‡ºRAGfereeæ–¹æ³•ï¼Œå°†é—®ç­”æ•°æ®é›†è½¬æ¢ä¸ºä¼˜å…ˆè€ƒè™‘äº‹å®å‡†ç¡®æ€§çš„åå¥½æ•°æ®ï¼Œå¹¶è®­ç»ƒå‡ºä¸“ç”¨äºRAGå“åº”çš„RMï¼Œå…¶æ€§èƒ½åœ¨ContextualJudgeBenchä¸Šè¶…è¶Šé€šç”¨å¤§å‹RMï¼ˆ+15.5%ï¼‰ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.25973v1">Scalable and Robust LLM Unlearning by Correcting Responses with Retrieved Exclusions</a></td><td><details><summary>å±•å¼€</summary>Language models trained on web-scale corpora risk memorizing and exposing
sensitive information, prompting the need for effective machine unlearning.
Prior methods mainly focus on input queries to suppress sensitive outputs, yet
this often fails to eliminate the underlying knowledge and limits scalability.
To address this, we propose Corrective Unlearning with Retrieved Exclusions
(CURE), a novel unlearning framework that verifies model outputs for leakage
and revises them into safe responses. Specifically, CURE employs a lightweight
corrector that is applied to the original model to verify whether outputs
contain target knowledge and to rewrite them if any leakage is detected. To
efficiently handle large-scale unlearning requests, CURE retrieves unlearning
targets that are relevant to the initial response and provides them as
in-context references to the corrector for detection and conditional revision.
By leveraging this retrieval augmentation, the corrector can adapt to new
unlearning requests without additional training. Extensive evaluations
demonstrate that CURE substantially reduces information leakage, even from
indirect queries where prior works fall short, while maintaining response
quality and general utility. Moreover, it demonstrates robustness under
continual unlearning scenarios, making it practical for real-world
applications.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºCUREçš„æœºå™¨é—å¿˜æ¡†æ¶ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯æ£€æµ‹å’Œä¿®æ­£æ¨¡å‹è¾“å‡ºä¸­çš„æ•æ„Ÿä¿¡æ¯æ³„æ¼ã€‚å®ƒåˆ©ç”¨è½»é‡çº§æ ¡æ­£å™¨ç»“åˆæ£€ç´¢åˆ°çš„ç›¸å…³é—å¿˜ç›®æ ‡ï¼ŒåŠ¨æ€è°ƒæ•´è¾“å‡ºä»¥ç¡®ä¿å®‰å…¨ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½å’Œæ‰©å±•æ€§ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡æŒç»­é—å¿˜åœºæ™¯ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.25839v1">RAE: A Neural Network Dimensionality Reduction Method for Nearest Neighbors Preservation in Vector Search</a></td><td><details><summary>å±•å¼€</summary>While high-dimensional embedding vectors are being increasingly employed in
various tasks like Retrieval-Augmented Generation and Recommendation Systems,
popular dimensionality reduction (DR) methods such as PCA and UMAP have rarely
been adopted for accelerating the retrieval process due to their inability of
preserving the nearest neighbor (NN) relationship among vectors. Empowered by
neural networks' optimization capability and the bounding effect of Rayleigh
quotient, we propose a Regularized Auto-Encoder (RAE) for k-NN preserving
dimensionality reduction. RAE constrains the network parameter variation
through regularization terms, adjusting singular values to control embedding
magnitude changes during reduction, thus preserving k-NN relationships. We
provide a rigorous mathematical analysis demonstrating that regularization
establishes an upper bound on the norm distortion rate of transformed vectors,
thereby offering provable guarantees for k-NN preservation. With modest
training overhead, RAE achieves superior k-NN recall compared to existing DR
approaches while maintaining fast retrieval efficiency.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§ç”¨äºä¿æŒkæœ€è¿‘é‚»ï¼ˆk-NNï¼‰å…³ç³»çš„æ­£åˆ™åŒ–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆRAEï¼‰ï¼Œæ—¨åœ¨è§£å†³é«˜ç»´åµŒå…¥å‘é‡åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­‰ä»»åŠ¡ä¸­å› ç»´åº¦ç¼©å‡å¯¼è‡´çš„æœ€è¿‘é‚»å…³ç³»ç ´åé—®é¢˜ï¼Œä»è€Œæå‡æ£€ç´¢æ•ˆç‡å¹¶ä¿è¯å‡†ç¡®æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.25736v1">Think Less, Label Better: Multi-Stage Domain-Grounded Synthetic Data Generation for Fine-Tuning Large Language Models in Telecommunications</a></td><td><details><summary>å±•å¼€</summary>The success of large language models (LLMs) depends heavily on large-scale,
high-quality instruction-following and reinforcement datasets. However,
generating such data through human annotation is prohibitively time-consuming
particularly for domain-specific tasks like telecom network troubleshooting,
where accurate responses require deep technical expertise and contextual
understanding. In this paper, we present a fully automated, retrieval-augmented
pipeline for generating synthetic question-answer (QA) pairs grounded in
structured domain knowledge. Our multi-stage framework integrates a retriever,
base generator, and refinement model to synthesize and enhance QA pairs using
documents retrieved from a domain-specific knowledge graph. To ensure data
quality, we employ customized RAGAS-based scoring to filter low-quality
samples, producing a high-quality dataset suitable for reinforcement
fine-tuning (RFT). We demonstrate our approach in a real-world telecom scenario
focused on radio access network (RAN) troubleshooting. The resulting pipeline
generates complex, context-rich troubleshooting solution plans without human
intervention. This work offers a scalable solution for building instruction and
reinforcement datasets in specialized domains, significantly reducing
dependence on manual labeling while maintaining high technical fidelity.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨è‡ªåŠ¨ã€æ£€ç´¢å¢å¼ºçš„æµç¨‹ï¼Œç”¨äºç”ŸæˆåŸºäºç»“æ„åŒ–é¢†åŸŸçŸ¥è¯†çš„åˆæˆé—®ç­”å¯¹ï¼ˆQAï¼‰ï¼Œé€šè¿‡ç»“åˆæ£€ç´¢å™¨ã€åŸºç¡€ç”Ÿæˆå™¨å’Œç²¾ç‚¼æ¨¡å‹çš„å¤šé˜¶æ®µæ¡†æ¶ï¼Œä»é¢†åŸŸç‰¹å®šçŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢æ–‡æ¡£å¹¶ç”Ÿæˆé«˜è´¨é‡QAæ•°æ®é›†ï¼Œåº”ç”¨äºç”µä¿¡ç½‘ç»œæ•…éšœæ’é™¤ç­‰ä¸“ä¸šé¢†åŸŸã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.25716v1">DeepCodeSeek: Real-Time API Retrieval for Context-Aware Code Generation</a></td><td><details><summary>å±•å¼€</summary>Current search techniques are limited to standard RAG query-document
applications. In this paper, we propose a novel technique to expand the code
and index for predicting the required APIs, directly enabling high-quality,
end-to-end code generation for auto-completion and agentic AI applications. We
address the problem of API leaks in current code-to-code benchmark datasets by
introducing a new dataset built from real-world ServiceNow Script Includes that
capture the challenge of unclear API usage intent in the code. Our evaluation
metrics show that this method achieves 87.86% top-40 retrieval accuracy,
allowing the critical context with APIs needed for successful downstream code
generation. To enable real-time predictions, we develop a comprehensive
post-training pipeline that optimizes a compact 0.6B reranker through synthetic
dataset generation, supervised fine-tuning, and reinforcement learning. This
approach enables our compact reranker to outperform a much larger 8B model
while maintaining 2.5x reduced latency, effectively addressing the nuances of
enterprise-specific code without the computational overhead of larger models.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ‰©å±•RAGæŠ€æœ¯å’Œç´¢å¼•çš„æ–°æ–¹æ³•ï¼Œä¸“æ³¨äºé€šè¿‡æ£€ç´¢é¢„æµ‹æ‰€éœ€APIä»¥å®ç°é«˜è´¨é‡çš„ç«¯åˆ°ç«¯ä»£ç ç”Ÿæˆï¼Œè§£å†³äº†å½“å‰ä»£ç åŸºå‡†æ•°æ®é›†ä¸­APIæ³„éœ²é—®é¢˜ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–çš„åè®­ç»ƒæµç¨‹æå‡å®æ—¶é¢„æµ‹æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.25669v1">GroundSight: Augmenting Vision-Language Models with Grounding Information and De-hallucination</a></td><td><details><summary>å±•å¼€</summary>We propose a method to improve Visual Question Answering (VQA) with
Retrieval-Augmented Generation (RAG) by introducing text-grounded object
localization. Rather than retrieving information based on the entire image, our
approach enables the model to generate a bounding box around the object most
relevant to the question, allowing for targeted image cropping and focused
retrieval. This reduces background noise, improves alignment between visual and
textual cues, and helps mitigate hallucinations. Our RAG method enhances
context-aware VQA responses increased the accuracy from 22.19% to 25.64%, with
an absolute increase of 3.45 percentage points, compared to the baseline
Llama-3.2-Vision-11B agent. We also proposed a de-hallucination method based on
question type which can effectively reduce the hallucination rate from 65.79%
to 13.88% and improves the truthfulness score.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›è§†è§‰é—®ç­”ï¼ˆVQAï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆRAGæŠ€æœ¯å’ŒåŸºäºæ–‡æœ¬çš„ç‰©ä½“å®šä½ï¼Œæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆä¸é—®é¢˜æœ€ç›¸å…³ç‰©ä½“çš„è¾¹ç•Œæ¡†ï¼Œä»è€Œè¿›è¡Œé’ˆå¯¹æ€§å›¾åƒè£å‰ªå’Œèšç„¦æ£€ç´¢ã€‚æ­¤æ–¹æ³•å‡å°‘äº†èƒŒæ™¯å™ªå£°ï¼Œæå‡äº†è§†è§‰ä¸æ–‡æœ¬çº¿ç´¢çš„å¯¹é½ï¼Œå¹¶é™ä½äº†å¹»è§‰ç°è±¡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥RAGæ–¹æ³•å°†VQAå‡†ç¡®ç‡ä»22.19%æå‡è‡³25.64%ï¼Œå¹¶æå‡ºäº†åŸºäºé—®é¢˜ç±»å‹çš„å»å¹»è§‰æ–¹æ³•ï¼Œå°†å¹»è§‰ç‡ä»65.79%é™ä½è‡³13.88%ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-09-29
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2509.25143v1">TemMed-Bench: Evaluating Temporal Medical Image Reasoning in Vision-Language Models</a></td><td><details><summary>å±•å¼€</summary>Existing medical reasoning benchmarks for vision-language models primarily
focus on analyzing a patient's condition based on an image from a single visit.
However, this setting deviates significantly from real-world clinical practice,
where doctors typically refer to a patient's historical conditions to provide a
comprehensive assessment by tracking their changes over time. In this paper, we
introduce TemMed-Bench, the first benchmark designed for analyzing changes in
patients' conditions between different clinical visits, which challenges large
vision-language models (LVLMs) to reason over temporal medical images.
TemMed-Bench consists of a test set comprising three tasks - visual
question-answering (VQA), report generation, and image-pair selection - and a
supplementary knowledge corpus of over 17,000 instances. With TemMed-Bench, we
conduct an evaluation of six proprietary and six open-source LVLMs. Our results
show that most LVLMs lack the ability to analyze patients' condition changes
over temporal medical images, and a large proportion perform only at a
random-guessing level in the closed-book setting. In contrast, GPT o3, o4-mini
and Claude 3.5 Sonnet demonstrate comparatively decent performance, though they
have yet to reach the desired level. Furthermore, we explore augmenting the
input with both retrieved visual and textual modalities in the medical domain.
We also show that multi-modal retrieval augmentation yields notably higher
performance gains than no retrieval and textual retrieval alone across most
models on our benchmark, with the VQA task showing an average improvement of
2.59%. Overall, we compose a benchmark grounded on real-world clinical
practice, and it reveals LVLMs' limitations in temporal medical image
reasoning, as well as highlighting the use of multi-modal retrieval
augmentation as a potentially promising direction worth exploring to address
this challenge.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†TemMed-Benchï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨æ—¶é—´æ€§åŒ»å­¦å›¾åƒæ¨ç†ä¸­åˆ†ææ‚£è€…ç—…æƒ…å˜åŒ–èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶æ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸Šçš„å±€é™æ€§ï¼Œå¹¶æ¢è®¨äº†é€šè¿‡å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºï¼ˆç»“åˆè§†è§‰å’Œæ–‡æœ¬æ£€ç´¢ï¼‰æå‡æ¨¡å‹æ€§èƒ½çš„æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨è§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼ˆå¹³å‡æå‡2.59%ï¼‰ï¼Œè¡¨æ˜å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºæ˜¯è§£å†³è¿™ä¸€æŒ‘æˆ˜çš„æ½œåœ¨æ–¹å‘ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.24869v1">Retro*: Optimizing LLMs for Reasoning-Intensive Document Retrieval</a></td><td><details><summary>å±•å¼€</summary>With the growing popularity of LLM agents and RAG, it has become increasingly
important to retrieve documents that are essential for solving a task, even
when their connection to the task is indirect or implicit. Addressing this
problem requires fine-grained reasoning to accurately assess the relevance
between the task and each candidate document. This capability, however, poses a
significant challenge for existing IR techniques. Despite recent progress in
reasoning-enhanced IR, existing approaches still face significant challenges in
applicability, scalability, and efficiency. In this work, we propose Retro*, a
novel approach for reasoning-intensive document retrieval. Our method
introduces a rubric-based relevance scoring mechanism, enabling the model to
reason about the relationship between a task and a document based on explicitly
defined criteria, whereby producing a fine-grained, interpretable relevance
score. Retro* also supports test-time scaling by combining multiple reasoning
trajectories via score integration, which produces more reliable relevance
estimates. To optimize Retro*'s reasoning capabilities, we introduce a novel
reinforcement learning algorithm tailored for its relevance scoring mechanism,
which employs two composite rewards to fully exploit the trajectories of each
training sample. Our experiments show that Retro* outperforms existing document
retrieval methods with notable advantages, leading to state-of-the-art
performance on the BRIGHT benchmark.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†Retro*æ–¹æ³•ï¼Œé€šè¿‡ç»†ç²’åº¦æ¨ç†å’ŒåŸºäºæ ‡å‡†çš„è¯„åˆ†æœºåˆ¶æ¥æ”¹è¿›æ–‡æ¡£æ£€ç´¢ï¼Œè§£å†³RAGä¸­å› ä»»åŠ¡ä¸æ–‡æ¡£é—´æ¥å…³è”å¯¼è‡´çš„æ£€ç´¢éš¾é¢˜ï¼Œå¹¶åœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.24866v1">Metaphor identification using large language models: A comparison of RAG, prompt engineering, and fine-tuning</a></td><td><details><summary>å±•å¼€</summary>Metaphor is a pervasive feature of discourse and a powerful lens for
examining cognition, emotion, and ideology. Large-scale analysis, however, has
been constrained by the need for manual annotation due to the context-sensitive
nature of metaphor. This study investigates the potential of large language
models (LLMs) to automate metaphor identification in full texts. We compare
three methods: (i) retrieval-augmented generation (RAG), where the model is
provided with a codebook and instructed to annotate texts based on its rules
and examples; (ii) prompt engineering, where we design task-specific verbal
instructions; and (iii) fine-tuning, where the model is trained on hand-coded
texts to optimize performance. Within prompt engineering, we test zero-shot,
few-shot, and chain-of-thought strategies. Our results show that
state-of-the-art closed-source LLMs can achieve high accuracy, with fine-tuning
yielding a median F1 score of 0.79. A comparison of human and LLM outputs
reveals that most discrepancies are systematic, reflecting well-known grey
areas and conceptual challenges in metaphor theory. We propose that LLMs can be
used to at least partly automate metaphor identification and can serve as a
testbed for developing and refining metaphor identification protocols and the
theory that underpins them.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ç ”ç©¶äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªåŠ¨åŒ–è¯†åˆ«æ–‡æœ¬ä¸­éšå–»çš„ä¸‰ç§æ–¹æ³•ï¼ŒåŒ…æ‹¬æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€æç¤ºå·¥ç¨‹å’Œå¾®è°ƒï¼Œå¹¶å‘ç°RAGç»“åˆä»£ç ä¹¦è§„åˆ™ä¸ç¤ºä¾‹çš„æ–¹æ³•èƒ½æœ‰æ•ˆæå‡éšå–»æ ‡æ³¨çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶æ­ç¤ºäº†æ¨¡å‹ä¸äººç±»æ ‡æ³¨å·®å¼‚çš„ç³»ç»Ÿæ€§ç†è®ºæ ¹æºã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.24276v1">G-reasoner: Foundation Models for Unified Reasoning over Graph-structured Knowledge</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) excel at complex reasoning but remain limited by
static and incomplete parametric knowledge. Retrieval-augmented generation
(RAG) mitigates this by incorporating external knowledge, yet existing RAGs
struggle with knowledge-intensive tasks due to fragmented information and weak
modeling of knowledge structure. Graphs offer a natural way to model
relationships within knowledge, but LLMs are inherently unstructured and cannot
effectively reason over graph-structured data. Recent graph-enhanced RAG
(GraphRAG) attempts to bridge this gap by constructing tailored graphs and
enabling LLMs to reason on them. However, these methods often depend on ad-hoc
graph designs, heuristic search, or costly agent pipelines, which hinder
scalability and generalization. To address these challenges, we present
G-reasoner, a unified framework that integrates graph and language foundation
models for reasoning over diverse graph-structured knowledge. Central to our
approach is QuadGraph, a standardized four-layer abstraction that unifies
heterogeneous knowledge sources into a common graph representation. Building on
this, we introduce a 34M-parameter graph foundation model (GFM) that jointly
captures graph topology and textual semantics, and is integrated with LLMs to
enhance reasoning in downstream applications. To ensure scalability and
efficiency, mixed-precision training and distributed message-passing are
implemented to scale GFM with more GPUs. Extensive experiments on six
benchmarks show that G-reasoner consistently outperforms state-of-the-art
baselines, significantly enhances LLM reasoning, and achieves strong efficiency
and cross-graph generalization.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºG-reasonerçš„ç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå›¾ç»“æ„å’Œè¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆå¦‚QuadGraphæ ‡å‡†åŒ–æŠ½è±¡å±‚å’Œå›¾åŸºç¡€æ¨¡å‹GFMï¼‰ï¼Œæ”¹è¿›ç°æœ‰RAGåœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼ˆå¦‚ä¿¡æ¯ç¢ç‰‡åŒ–å’ŒçŸ¥è¯†ç»“æ„å»ºæ¨¡è–„å¼±ï¼‰ï¼Œå¹¶å®éªŒè¯æ˜äº†å…¶åœ¨å¢å¼ºå¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ã€æ•ˆç‡åŠè·¨å›¾æ³›åŒ–æ€§æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.24253v1">MRAG-Suite: A Diagnostic Evaluation Platform for Visual Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Multimodal Retrieval-Augmented Generation (Visual RAG) significantly advances
question answering by integrating visual and textual evidence. Yet, current
evaluations fail to systematically account for query difficulty and ambiguity.
We propose MRAG-Suite, a diagnostic evaluation platform integrating diverse
multimodal benchmarks (WebQA, Chart-RAG, Visual-RAG, MRAG-Bench). We introduce
difficulty-based and ambiguity-aware filtering strategies, alongside
MM-RAGChecker, a claim-level diagnostic tool. Our results demonstrate
substantial accuracy reductions under difficult and ambiguous queries,
highlighting prevalent hallucinations. MM-RAGChecker effectively diagnoses
these issues, guiding future improvements in Visual RAG systems.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†MRAG-Suiteï¼Œä¸€ä¸ªé’ˆå¯¹å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆVisual RAGï¼‰çš„è¯Šæ–­è¯„ä¼°å¹³å°ï¼Œé€šè¿‡æ•´åˆå¤šç§å¤šæ¨¡æ€åŸºå‡†å’Œå¼•å…¥åŸºäºéš¾åº¦åŠæ¨¡ç³Šæ€§çš„è¿‡æ»¤ç­–ç•¥ï¼Œæ­ç¤ºäº†ç°æœ‰ç³»ç»Ÿåœ¨é¢å¯¹å›°éš¾å’Œæ¨¡ç³ŠæŸ¥è¯¢æ—¶çš„å‡†ç¡®ç‡ä¸‹é™é—®é¢˜ï¼Œå¹¶æä¾›äº†è¯Šæ–­å·¥å…·MM-RAGCheckerä»¥æŒ‡å¯¼æœªæ¥æ”¹è¿›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.24212v1">ScenarioBench: Trace-Grounded Compliance Evaluation for Text-to-SQL and RAG</a></td><td><details><summary>å±•å¼€</summary>ScenarioBench is a policy-grounded, trace-aware benchmark for evaluating
Text-to-SQL and retrieval-augmented generation in compliance contexts. Each
YAML scenario includes a no-peek gold-standard package with the expected
decision, a minimal witness trace, the governing clause set, and the canonical
SQL, enabling end-to-end scoring of both what a system decides and why. Systems
must justify outputs using clause IDs from the same policy canon, making
explanations falsifiable and audit-ready. The evaluator reports decision
accuracy, trace quality (completeness, correctness, order), retrieval
effectiveness, SQL correctness via result-set equivalence, policy coverage,
latency, and an explanation-hallucination rate. A normalized Scenario
Difficulty Index (SDI) and a budgeted variant (SDI-R) aggregate results while
accounting for retrieval difficulty and time. Compared with prior Text-to-SQL
or KILT/RAG benchmarks, ScenarioBench ties each decision to clause-level
evidence under strict grounding and no-peek rules, shifting gains toward
justification quality under explicit time budgets.</details></td><td><details><summary>å±•å¼€</summary>ScenarioBenchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°æ–‡æœ¬åˆ°SQLï¼ˆText-to-SQLï¼‰å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨åˆè§„åœºæ™¯ä¸‹çš„åŸºå‡†æµ‹è¯•å·¥å…·ï¼Œå®ƒé€šè¿‡YAMLåœºæ™¯æ•´åˆäº†å†³ç­–ä¾æ®ã€è¿½è¸ªä¿¡æ¯ã€æ¡æ¬¾é›†å’Œæ ‡å‡†SQLï¼Œæ”¯æŒç«¯åˆ°ç«¯è¯„åˆ†ï¼Œå¹¶å¼ºè°ƒè¾“å‡ºçš„å¯éªŒè¯æ€§ä¸å®¡è®¡å°±ç»ªæ€§ï¼ŒåŒæ—¶æä¾›å¤šç»´åº¦è¯„ä¼°æŒ‡æ ‡å’Œéš¾åº¦æŒ‡æ•°ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.24183v1">Retrieval-augmented GUI Agents with Generative Guidelines</a></td><td><details><summary>å±•å¼€</summary>GUI agents powered by vision-language models (VLMs) show promise in
automating complex digital tasks. However, their effectiveness in real-world
applications is often limited by scarce training data and the inherent
complexity of these tasks, which frequently require long-tailed knowledge
covering rare, unseen scenarios. We propose RAG-GUI , a lightweight VLM that
leverages web tutorials at inference time. RAG-GUI is first warm-started via
supervised finetuning (SFT) and further refined through self-guided rejection
sampling finetuning (RSF). Designed to be model-agnostic, RAG-GUI functions as
a generic plug-in that enhances any VLM-based agent. Evaluated across three
distinct tasks, it consistently outperforms baseline agents and surpasses other
inference baselines by 2.6% to 13.3% across two model sizes, demonstrating
strong generalization and practical plug-and-play capabilities in real-world
scenarios.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†RAG-GUIï¼Œä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è½»é‡çº§GUIä»£ç†ï¼Œé€šè¿‡åˆ©ç”¨ç½‘é¡µæ•™ç¨‹ä½œä¸ºæ£€ç´¢å¢å¼ºçš„æ¨ç†èµ„æºæ¥è§£å†³å¤æ‚æ•°å­—ä»»åŠ¡ä¸­è®­ç»ƒæ•°æ®ç¨€ç¼ºå’Œé•¿å°¾çŸ¥è¯†ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡ç›‘ç£å¾®è°ƒå’Œè‡ªå¼•å¯¼æ‹’ç»é‡‡æ ·å¾®è°ƒä¼˜åŒ–æ¨¡å‹ï¼Œå±•ç°å‡ºå¼ºæ³›åŒ–èƒ½åŠ›å’Œå³æ’å³ç”¨ç‰¹æ€§ï¼Œåœ¨å¤šé¡¹ä»»åŠ¡ä¸­ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-09-28
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2509.23874v1">Multi-Value-Product Retrieval-Augmented Generation for Industrial Product Attribute Value Identification</a></td><td><details><summary>å±•å¼€</summary>Identifying attribute values from product profiles is a key task for
improving product search, recommendation, and business analytics on e-commerce
platforms, which we called Product Attribute Value Identification (PAVI) .
However, existing PAVI methods face critical challenges, such as cascading
errors, inability to handle out-of-distribution (OOD) attribute values, and
lack of generalization capability. To address these limitations, we introduce
Multi-Value-Product Retrieval-Augmented Generation (MVP-RAG), combining the
strengths of retrieval, generation, and classification paradigms. MVP-RAG
defines PAVI as a retrieval-generation task, where the product title
description serves as the query, and products and attribute values act as the
corpus. It first retrieves similar products of the same category and candidate
attribute values, and then generates the standardized attribute values. The key
advantages of this work are: (1) the proposal of a multi-level retrieval
scheme, with products and attribute values as distinct hierarchical levels in
PAVI domain (2) attribute value generation of large language model to
significantly alleviate the OOD problem and (3) its successful deployment in a
real-world industrial environment. Extensive experimental results demonstrate
that MVP-RAG performs better than the state-of-the-art baselines.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºMVP-RAGï¼ˆMulti-Value-Product Retrieval-Augmented Generationï¼‰çš„æ–¹æ³•ï¼Œç”¨äºè§£å†³ç”µå­å•†åŠ¡å¹³å°ä¸­çš„äº§å“å±æ€§å€¼è¯†åˆ«ï¼ˆPAVIï¼‰é—®é¢˜ã€‚MVP-RAGç»“åˆäº†æ£€ç´¢ã€ç”Ÿæˆå’Œåˆ†ç±»èŒƒå¼ï¼Œé€šè¿‡å¤šçº§æ£€ç´¢æ–¹æ¡ˆï¼ˆäº§å“å±‚çº§å’Œå±æ€§å€¼å±‚çº§ï¼‰æ£€ç´¢ç›¸ä¼¼äº§å“å’Œå€™é€‰å±æ€§å€¼ï¼Œç„¶ååˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆæ ‡å‡†åŒ–çš„å±æ€§å€¼ï¼Œæ˜¾è‘—ç¼“è§£äº†åˆ†å¸ƒå¤–ï¼ˆOODï¼‰é—®é¢˜ï¼Œå¹¶å·²åœ¨å·¥ä¸šç¯å¢ƒä¸­æˆåŠŸéƒ¨ç½²ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶æ€§èƒ½ä¼˜äºç°æœ‰æœ€ä¼˜åŸºçº¿æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.23793v1">Transformer Tafsir at QIAS 2025 Shared Task: Hybrid Retrieval-Augmented Generation for Islamic Knowledge Question Answering</a></td><td><details><summary>å±•å¼€</summary>This paper presents our submission to the QIAS 2025 shared task on Islamic
knowledge understanding and reasoning. We developed a hybrid
retrieval-augmented generation (RAG) system that combines sparse and dense
retrieval methods with cross-encoder reranking to improve large language model
(LLM) performance. Our three-stage pipeline incorporates BM25 for initial
retrieval, a dense embedding retrieval model for semantic matching, and
cross-encoder reranking for precise content retrieval. We evaluate our approach
on both subtasks using two LLMs, Fanar and Mistral, demonstrating that the
proposed RAG pipeline enhances performance across both, with accuracy
improvements up to 25%, depending on the task and model configuration. Our best
configuration is achieved with Fanar, yielding accuracy scores of 45% in
Subtask 1 and 80% in Subtask 2.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªç”¨äºä¼Šæ–¯å…°çŸ¥è¯†ç†è§£å’Œæ¨ç†çš„æ··åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿï¼Œç»“åˆäº†ç¨€ç–ä¸å¯†é›†æ£€ç´¢æ–¹æ³•åŠäº¤å‰ç¼–ç å™¨é‡æ’åºï¼Œé€šè¿‡ä¸‰é˜¶æ®µæµç¨‹ï¼ˆBM25åˆæ£€ã€å¯†é›†åµŒå…¥è¯­ä¹‰åŒ¹é…ã€äº¤å‰ç¼–ç å™¨ç²¾æ£€ï¼‰æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆFanarå’ŒMistralï¼‰æ€§èƒ½ï¼Œå®éªŒæ˜¾ç¤ºæœ€é«˜å¯å°†å‡†ç¡®ç‡æå‡25%ï¼Œå…¶ä¸­Fanaræ¨¡å‹åœ¨ä¸¤é¡¹å­ä»»åŠ¡ä¸­åˆ†åˆ«è¾¾åˆ°45%å’Œ80%çš„å‡†ç¡®ç‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.23659v1">Aligning LLMs for Multilingual Consistency in Enterprise Applications</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) remain unreliable for global enterprise
applications due to substantial performance gaps between high-resource and
mid/low-resource languages, driven by English-centric pretraining and internal
reasoning biases. This inconsistency undermines customer experience and
operational reliability in multilingual settings such as customer support,
content moderation, and information retrieval. Even with advanced
Retrieval-Augmented Generation (RAG) systems, we observe up to an 29% accuracy
drop in non-English languages compared to English.
  We propose a practical, batch-wise alignment strategy for fine-tuning LLMs,
leveraging semantically equivalent multilingual data in each training batch to
directly align model outputs across languages. This approach improves
non-English accuracy by up to 23.9\% without compromising English performance,
model reasoning, or retrieval quality. Our method is simple to implement,
scalable, and integrates seamlessly with existing LLM training \& deployment
pipelines, enabling more robust and equitable multilingual AI solutions in
industry.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹ï¼ˆå°¤å…¶æ˜¯ä¸­ä½èµ„æºè¯­è¨€ï¼‰æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼ŒæŒ‡å‡ºå³ä½¿é‡‡ç”¨RAGç³»ç»Ÿï¼Œéè‹±è¯­è¯­è¨€çš„å‡†ç¡®ç‡ä»æ˜¾è‘—ä½äºè‹±è¯­ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŸºäºæ‰¹é‡å¯¹é½çš„å¾®è°ƒç­–ç•¥ï¼Œåˆ©ç”¨å¤šè¯­è¨€è¯­ä¹‰ç­‰æ•ˆæ•°æ®ç›´æ¥å¯¹é½æ¨¡å‹è¾“å‡ºï¼Œä»è€Œæå‡éè‹±è¯­è¯­è¨€çš„å‡†ç¡®ç‡ï¼ˆæœ€é«˜23.9%ï¼‰ï¼ŒåŒæ—¶ä¸å½±å“è‹±è¯­æ€§èƒ½æˆ–æ£€ç´¢è´¨é‡ã€‚ç ”ç©¶æ—¨åœ¨å¢å¼ºRAGåœ¨å¤šè¯­è¨€å·¥ä¸šåœºæ™¯ï¼ˆå¦‚å®¢æœã€å†…å®¹å®¡æ ¸ç­‰ï¼‰ä¸­çš„å…¬å¹³æ€§å’Œå¯é æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.23630v1">Game-Oriented ASR Error Correction via RAG-Enhanced LLM</a></td><td><details><summary>å±•å¼€</summary>With the rise of multiplayer online games, real-time voice communication is
essential for team coordination. However, general ASR systems struggle with
gaming-specific challenges like short phrases, rapid speech, jargon, and noise,
leading to frequent errors. To address this, we propose the GO-AEC framework,
which integrates large language models, Retrieval-Augmented Generation (RAG),
and a data augmentation strategy using LLMs and TTS. GO-AEC includes data
augmentation, N-best hypothesis-based correction, and a dynamic game knowledge
base. Experiments show GO-AEC reduces character error rate by 6.22% and
sentence error rate by 29.71%, significantly improving ASR accuracy in gaming
scenarios.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†GO-AECæ¡†æ¶ï¼Œç»“åˆå¤§è¯­è¨€æ¨¡å‹ã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä»¥åŠæ•°æ®å¢å¼ºç­–ç•¥ï¼Œé’ˆå¯¹æ¸¸æˆåœºæ™¯ä¸­çš„å®æ—¶è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æŒ‘æˆ˜ï¼ˆå¦‚çŸ­çŸ­è¯­ã€å¿«é€Ÿè¯­éŸ³ã€æœ¯è¯­å’Œå™ªå£°ï¼‰è¿›è¡Œä¼˜åŒ–ï¼Œé€šè¿‡åŠ¨æ€æ¸¸æˆçŸ¥è¯†åº“å’ŒN-bestå‡è®¾æ ¡æ­£æ˜¾è‘—é™ä½äº†é”™è¯¯ç‡ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-09-27
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2509.23519v1">ReliabilityRAG: Effective and Provably Robust Defense for RAG-based Web-Search</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) enhances Large Language Models by
grounding their outputs in external documents. These systems, however, remain
vulnerable to attacks on the retrieval corpus, such as prompt injection.
RAG-based search systems (e.g., Google's Search AI Overview) present an
interesting setting for studying and protecting against such threats, as
defense algorithms can benefit from built-in reliability signals -- like
document ranking -- and represent a non-LLM challenge for the adversary due to
decades of work to thwart SEO.
  Motivated by, but not limited to, this scenario, this work introduces
ReliabilityRAG, a framework for adversarial robustness that explicitly
leverages reliability information of retrieved documents.
  Our first contribution adopts a graph-theoretic perspective to identify a
"consistent majority" among retrieved documents to filter out malicious ones.
We introduce a novel algorithm based on finding a Maximum Independent Set (MIS)
on a document graph where edges encode contradiction. Our MIS variant
explicitly prioritizes higher-reliability documents and provides provable
robustness guarantees against bounded adversarial corruption under natural
assumptions. Recognizing the computational cost of exact MIS for large
retrieval sets, our second contribution is a scalable weighted sample and
aggregate framework. It explicitly utilizes reliability information, preserving
some robustness guarantees while efficiently handling many documents.
  We present empirical results showing ReliabilityRAG provides superior
robustness against adversarial attacks compared to prior methods, maintains
high benign accuracy, and excels in long-form generation tasks where prior
robustness-focused methods struggled. Our work is a significant step towards
more effective, provably robust defenses against retrieved corpus corruption in
RAG.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ReliabilityRAGæ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºRAGç³»ç»Ÿå¯¹æŠ—æ£€ç´¢æ–‡æ¡£åº“ä¸­æ¶æ„æ”»å‡»ï¼ˆå¦‚æç¤ºæ³¨å…¥ï¼‰çš„é²æ£’æ€§ã€‚é€šè¿‡å›¾è®ºæ–¹æ³•è¯†åˆ«æ–‡æ¡£é—´çš„çŸ›ç›¾å…³ç³»å¹¶ä¼˜å…ˆé€‰æ‹©é«˜å¯é æ€§æ–‡æ¡£ï¼Œç»“åˆå¯æ‰©å±•çš„åŠ æƒé‡‡æ ·èšåˆæŠ€æœ¯ï¼Œè¯¥æ¡†æ¶åœ¨ä¿è¯é«˜æ•ˆå¤„ç†å¤§è§„æ¨¡æ£€ç´¢é›†çš„åŒæ—¶ï¼Œæä¾›äº†ç†è®ºä¸Šçš„å¯¹æŠ—æ”»å‡»é˜²å¾¡ä¿è¯ï¼Œå¹¶åœ¨å®éªŒä¸­å±•ç°å‡ºä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.23233v1">Detecting Corpus-Level Knowledge Inconsistencies in Wikipedia with Large Language Models</a></td><td><details><summary>å±•å¼€</summary>Wikipedia is the largest open knowledge corpus, widely used worldwide and
serving as a key resource for training large language models (LLMs) and
retrieval-augmented generation (RAG) systems. Ensuring its accuracy is
therefore critical. But how accurate is Wikipedia, and how can we improve it?
  We focus on inconsistencies, a specific type of factual inaccuracy, and
introduce the task of corpus-level inconsistency detection. We present CLAIRE,
an agentic system that combines LLM reasoning with retrieval to surface
potentially inconsistent claims along with contextual evidence for human
review. In a user study with experienced Wikipedia editors, 87.5% reported
higher confidence when using CLAIRE, and participants identified 64.7% more
inconsistencies in the same amount of time.
  Combining CLAIRE with human annotation, we contribute WIKICOLLIDE, the first
benchmark of real Wikipedia inconsistencies. Using random sampling with
CLAIRE-assisted analysis, we find that at least 3.3% of English Wikipedia facts
contradict another fact, with inconsistencies propagating into 7.3% of FEVEROUS
and 4.0% of AmbigQA examples. Benchmarking strong baselines on this dataset
reveals substantial headroom: the best fully automated system achieves an AUROC
of only 75.1%.
  Our results show that contradictions are a measurable component of Wikipedia
and that LLM-based systems like CLAIRE can provide a practical tool to help
editors improve knowledge consistency at scale.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡èšç„¦äºç»´åŸºç™¾ç§‘ä¸­çš„äº‹å®ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸æ£€ç´¢æŠ€æœ¯çš„æ™ºèƒ½ç³»ç»ŸCLAIREï¼Œç”¨äºæ£€æµ‹è¯­æ–™åº“çº§åˆ«çš„ä¸ä¸€è‡´ä¸»å¼ å¹¶æä¾›ä¸Šä¸‹æ–‡è¯æ®ï¼Œæœ€ç»ˆæ„å»ºäº†é¦–ä¸ªçœŸå®ç»´åŸºç™¾ç§‘ä¸ä¸€è‡´æ€§åŸºå‡†WIKICOLLIDEã€‚ç ”ç©¶è¯å®LLMé©±åŠ¨çš„ç³»ç»Ÿï¼ˆå¦‚CLAIREï¼‰å¯è¾…åŠ©ç¼–è¾‘é«˜æ•ˆæå‡çŸ¥è¯†ä¸€è‡´æ€§ï¼ŒåŒæ—¶æ­ç¤ºäº†æ­¤ç±»ä¸ä¸€è‡´åœ¨ç°æœ‰æ•°æ®é›†ï¼ˆå¦‚FEVEROUSã€AmbigQAï¼‰ä¸­çš„æ¸—é€æƒ…å†µï¼Œå‡¸æ˜¾äº†è‡ªåŠ¨åŒ–ç³»ç»Ÿçš„æ”¹è¿›ç©ºé—´ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.23071v1">From Evidence to Trajectory: Abductive Reasoning Path Synthesis for Training Retrieval-Augmented Generation Agents</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation agents development is hindered by the lack of
process-level supervision to effectively guide agentic capabilities like task
decomposition, retriever invocation, and stepwise decision-making. While
reinforcement learning offers a potential solution, it suffers from sparse
rewards and the limited reasoning capabilities of large language models (LLMs).
Meanwhile, existing data synthesis methods only produce chain-of-thought
rationales and fail to model environmental interactions. In this paper, we
propose EviPath, an evidence-anchored reasoning path synthesis paradigm for RAG
agent development. EviPath comprises: (i) Abductive Subtask Planning, which
decomposes the problem into sub-questions and iteratively plans an optimal
solution path based on the dependencies between them; (ii) Faithful
Sub-question Answering, which uses supporting evidence to construct a proxy
environment to generate reasoning thoughts and answers for each sub-question;
and (iii) Conversational Fine-Tuning, which formats the complete
agent-environment interaction trajectory into a dialogue format suitable for
Supervised Fine-Tuning. EviPath allows LLMs to learn complex reasoning and
tool-use capabilities directly from synthesized data. Extensive experiments on
widely-used question-answering benchmarks show that an 8B parameter model
trained with EviPath-synthesized data significantly and consistently
outperforms state-of-the-art baselines with a double-digit absolute EM gain of
14.7% in open-domain question answering.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºEviPathçš„è¯æ®é”šå®šæ¨ç†è·¯å¾„åˆæˆèŒƒå¼ï¼Œç”¨äºè§£å†³RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ä»£ç†å¼€å‘ä¸­è¿‡ç¨‹çº§ç›‘ç£ä¸è¶³çš„é—®é¢˜ã€‚é€šè¿‡å°†é—®é¢˜åˆ†è§£ä¸ºå­ä»»åŠ¡ã€åˆ©ç”¨æ”¯æŒè¯æ®æ„å»ºä»£ç†ç¯å¢ƒç”Ÿæˆå­é—®é¢˜ç­”æ¡ˆï¼Œå¹¶å°†äº¤äº’è½¨è¿¹æ ¼å¼åŒ–ä¸ºå¯¹è¯æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒï¼ŒEviPathæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¼€æ”¾åŸŸé—®ç­”ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼ˆEMå¢ç›Šè¾¾14.7%ï¼‰ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-09-26
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2509.22565v1">Retrieval-Augmented Guardrails for AI-Drafted Patient-Portal Messages: Error Taxonomy Construction and Large-Scale Evaluation</a></td><td><details><summary>å±•å¼€</summary>Asynchronous patient-clinician messaging via EHR portals is a growing source
of clinician workload, prompting interest in large language models (LLMs) to
assist with draft responses. However, LLM outputs may contain clinical
inaccuracies, omissions, or tone mismatches, making robust evaluation
essential. Our contributions are threefold: (1) we introduce a clinically
grounded error ontology comprising 5 domains and 59 granular error codes,
developed through inductive coding and expert adjudication; (2) we develop a
retrieval-augmented evaluation pipeline (RAEC) that leverages semantically
similar historical message-response pairs to improve judgment quality; and (3)
we provide a two-stage prompting architecture using DSPy to enable scalable,
interpretable, and hierarchical error detection. Our approach assesses the
quality of drafts both in isolation and with reference to similar past
message-response pairs retrieved from institutional archives. Using a two-stage
DSPy pipeline, we compared baseline and reference-enhanced evaluations on over
1,500 patient messages. Retrieval context improved error identification in
domains such as clinical completeness and workflow appropriateness. Human
validation on 100 messages demonstrated superior agreement (concordance = 50%
vs. 33%) and performance (F1 = 0.500 vs. 0.256) of context-enhanced labels vs.
baseline, supporting the use of our RAEC pipeline as AI guardrails for patient
messaging.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ£€ç´¢å¢å¼ºçš„è¯„ä¼°ç®¡é“ï¼ˆRAECï¼‰ï¼Œåˆ©ç”¨è¯­ä¹‰ç›¸ä¼¼çš„å†å²æ¶ˆæ¯-å“åº”å¯¹æ¥æ”¹è¿›å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆçš„ä¸´åºŠå›å¤è‰æ¡ˆçš„è´¨é‡è¯„ä¼°ï¼Œå¹¶é€šè¿‡ä¸¤é˜¶æ®µæç¤ºæ¶æ„å®ç°å¯æ‰©å±•å’Œåˆ†å±‚æ¬¡çš„é”™è¯¯æ£€æµ‹ï¼ŒéªŒè¯äº†æ£€ç´¢ä¸Šä¸‹æ–‡åœ¨æå‡ä¸´åºŠå®Œæ•´æ€§å’Œå·¥ä½œæµé€‚å½“æ€§ç­‰é¢†åŸŸçš„é”™è¯¯è¯†åˆ«æ•ˆæœã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.22516v1">TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent and Explainable Digital Assessments</a></td><td><details><summary>å±•å¼€</summary>This paper introduces TrueGradeAI, an AI-driven digital examination framework
designed to overcome the shortcomings of traditional paper-based assessments,
including excessive paper usage, logistical complexity, grading delays, and
evaluator bias. The system preserves natural handwriting by capturing stylus
input on secure tablets and applying transformer-based optical character
recognition for transcription. Evaluation is conducted through a
retrieval-augmented pipeline that integrates faculty solutions, cache layers,
and external references, enabling a large language model to assign scores with
explicit, evidence-linked reasoning. Unlike prior tablet-based exam systems
that primarily digitize responses, TrueGradeAI advances the field by
incorporating explainable automation, bias mitigation, and auditable grading
trails. By uniting handwriting preservation with scalable and transparent
evaluation, the framework reduces environmental costs, accelerates feedback
cycles, and progressively builds a reusable knowledge base, while actively
working to mitigate grading bias and ensure fairness in assessment.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºTrueGradeAIæ¡†æ¶ï¼Œé€šè¿‡åŸºäºè§¦æ§ç¬”è¾“å…¥çš„æ•°å­—åŒ–è€ƒè¯•ç³»ç»Ÿç»“åˆæ£€ç´¢å¢å¼ºæµç¨‹ï¼ˆé›†æˆæ•™å¸ˆç­”æ¡ˆã€ç¼“å­˜å±‚å’Œå¤–éƒ¨å‚è€ƒï¼‰ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¯è§£é‡Šã€è¯æ®å…³è”çš„è¯„åˆ†ï¼Œè§£å†³ä¼ ç»Ÿè€ƒè¯•å¼Šç«¯å¹¶æå‡é€æ˜åº¦å’Œå…¬å¹³æ€§ï¼Œå±äºRAGåœ¨è‡ªåŠ¨åŒ–è¯„ä¼°é¢†åŸŸçš„åº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.22490v1">JGU Mainz's Submission to the WMT25 Shared Task on LLMs with Limited Resources for Slavic Languages: MT and QA</a></td><td><details><summary>å±•å¼€</summary>This paper presents the JGU Mainz submission to the WMT25 Shared Task on LLMs
with Limited Resources for Slavic Languages: Machine Translation and Question
Answering, focusing on Ukrainian, Upper Sorbian, and Lower Sorbian. For each
language, we jointly fine-tune a Qwen2.5-3B-Instruct model for both tasks with
parameter-efficient finetuning. Our pipeline integrates additional translation
and multiple-choice question answering (QA) data. For Ukrainian QA, we further
use retrieval-augmented generation. We also apply ensembling for QA in Upper
and Lower Sorbian. Experiments show that our models outperform the baseline on
both tasks.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ä»‹ç»äº†JGU Mainzå›¢é˜Ÿé’ˆå¯¹ä½èµ„æºæ–¯æ‹‰å¤«è¯­ï¼ˆä¹Œå…‹å…°è¯­ã€ä¸Šç´¢å¸ƒè¯­å’Œä¸‹ç´¢å¸ƒè¯­ï¼‰çš„æœºå™¨ç¿»è¯‘å’Œé—®ç­”ä»»åŠ¡ï¼Œä½¿ç”¨Qwen2.5-3B-Instructæ¨¡å‹è¿›è¡Œè”åˆå¾®è°ƒï¼Œå¹¶åœ¨ä¹Œå…‹å…°è¯­é—®ç­”ä¸­é‡‡ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œå®éªŒè¡¨æ˜æ¨¡å‹æ€§èƒ½ä¼˜äºåŸºçº¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.22486v1">Your RAG is Unfair: Exposing Fairness Vulnerabilities in Retrieval-Augmented Generation via Backdoor Attacks</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation (RAG) enhances factual grounding by
integrating retrieval mechanisms with generative models but introduces new
attack surfaces, particularly through backdoor attacks. While prior research
has largely focused on disinformation threats, fairness vulnerabilities remain
underexplored. Unlike conventional backdoors that rely on direct
trigger-to-target mappings, fairness-driven attacks exploit the interaction
between retrieval and generation models, manipulating semantic relationships
between target groups and social biases to establish a persistent and covert
influence on content generation.
  This paper introduces BiasRAG, a systematic framework that exposes fairness
vulnerabilities in RAG through a two-phase backdoor attack. During the
pre-training phase, the query encoder is compromised to align the target group
with the intended social bias, ensuring long-term persistence. In the
post-deployment phase, adversarial documents are injected into knowledge bases
to reinforce the backdoor, subtly influencing retrieved content while remaining
undetectable under standard fairness evaluations. Together, BiasRAG ensures
precise target alignment over sensitive attributes, stealthy execution, and
resilience. Empirical evaluations demonstrate that BiasRAG achieves high attack
success rates while preserving contextual relevance and utility, establishing a
persistent and evolving threat to fairness in RAG.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†RAGæŠ€æœ¯åœ¨å…¬å¹³æ€§æ–¹é¢çš„æ½œåœ¨æ¼æ´ï¼Œæå‡ºäº†ä¸€ç§åä¸ºBiasRAGçš„ä¸¤é˜¶æ®µåé—¨æ”»å‡»æ¡†æ¶ã€‚è¯¥æ”»å‡»é€šè¿‡åœ¨é¢„è®­ç»ƒé˜¶æ®µæ“çºµæŸ¥è¯¢ç¼–ç å™¨ä½¿å…¶ä¸ç‰¹å®šç¤¾ä¼šåè§å¯¹é½ï¼Œå¹¶åœ¨éƒ¨ç½²åé˜¶æ®µå‘çŸ¥è¯†åº“æ³¨å…¥å¯¹æŠ—æ€§æ–‡æ¡£ï¼Œä»è€Œåœ¨ä¿æŒéšè”½æ€§çš„åŒæ—¶æŒç»­å½±å“ç”Ÿæˆå†…å®¹ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒBiasRAGä¸ä»…èƒ½é«˜æ•ˆå®æ–½æ”»å‡»ï¼Œè¿˜æ­ç¤ºäº†ç°æœ‰å…¬å¹³æ€§è¯„ä¼°çš„å±€é™æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.22378v1">Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach</a></td><td><details><summary>å±•å¼€</summary>Recently, Image-to-Music (I2M) generation has garnered significant attention,
with potential applications in fields such as gaming, advertising, and
multi-modal art creation. However, due to the ambiguous and subjective nature
of I2M tasks, most end-to-end methods lack interpretability, leaving users
puzzled about the generation results. Even methods based on emotion mapping
face controversy, as emotion represents only a singular aspect of art.
Additionally, most learning-based methods require substantial computational
resources and large datasets for training, hindering accessibility for common
users. To address these challenges, we propose the first Vision Language Model
(VLM)-based I2M framework that offers high interpretability and low
computational cost. Specifically, we utilize ABC notation to bridge the text
and music modalities, enabling the VLM to generate music using natural
language. We then apply multi-modal Retrieval-Augmented Generation (RAG) and
self-refinement techniques to allow the VLM to produce high-quality music
without external training. Furthermore, we leverage the generated motivations
in text and the attention maps from the VLM to provide explanations for the
generated results in both text and image modalities. To validate our method, we
conduct both human studies and machine evaluations, where our method
outperforms others in terms of music quality and music-image consistency,
indicating promising results. Our code is available at
https://github.com/RS2002/Image2Music .</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„Image-to-Musicï¼ˆI2Mï¼‰ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œè‡ªä¼˜åŒ–æŠ€æœ¯ï¼Œæ— éœ€å¤–éƒ¨è®­ç»ƒå³å¯ç”Ÿæˆé«˜è´¨é‡éŸ³ä¹ï¼Œå¹¶åˆ©ç”¨æ–‡æœ¬åŠ¨æœºå’Œæ³¨æ„åŠ›å›¾æä¾›è·¨æ¨¡æ€è§£é‡Šï¼Œåœ¨éŸ³ä¹è´¨é‡ä¸å›¾æ–‡ä¸€è‡´æ€§ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.22325v1">Can Synthetic Query Rewrites Capture User Intent Better than Humans in Retrieval-Augmented Generation?</a></td><td><details><summary>å±•å¼€</summary>Multi-turn RAG systems often face queries with colloquial omissions and
ambiguous references, posing significant challenges for effective retrieval and
generation. Traditional query rewriting relies on human annotators to clarify
queries, but due to limitations in annotators' expressive ability and depth of
understanding, manually rewritten queries often diverge from those needed in
real-world RAG systems, resulting in a gap between user intent and system
response. We observe that high-quality synthetic queries can better bridge this
gap, achieving superior performance in both retrieval and generation compared
to human rewrites. This raises an interesting question: Can rewriting models
trained on synthetic queries better capture user intent than human annotators?
In this paper, we propose SynRewrite, a synthetic data-driven query rewriting
model to generate high-quality synthetic rewrites more aligned with user
intent. To construct training data, we prompt GPT-4o with dialogue history,
current queries, positive documents, and answers to synthesize high-quality
rewrites. A Flan-T5 model is then finetuned on this dataset to map dialogue
history and queries to synthetic rewrites. Finally, we further enhance the
rewriter using the generator's feedback through the DPO algorithm to boost
end-task performance. Experiments on TopiOCQA and QRECC datasets show that
SynRewrite consistently outperforms human rewrites in both retrieval and
generation tasks. Our results demonstrate that synthetic rewrites can serve as
a scalable and effective alternative to human annotations.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤šè½®RAGç³»ç»Ÿä¸­é¢å¯¹å£è¯­åŒ–çœç•¥å’Œæ¨¡ç³ŠæŒ‡ä»£æŸ¥è¯¢æ—¶çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåˆæˆæ•°æ®çš„æŸ¥è¯¢é‡å†™æ¨¡å‹SynRewriteã€‚è¯¥æ–¹æ³•åˆ©ç”¨GPT-4oç”Ÿæˆé«˜è´¨é‡çš„é‡å†™æŸ¥è¯¢è®­ç»ƒæ•°æ®ï¼Œå¹¶å¾®è°ƒFlan-T5æ¨¡å‹ï¼Œå†é€šè¿‡DPOç®—æ³•ç»“åˆç”Ÿæˆå™¨åé¦ˆä¼˜åŒ–æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒSynRewriteåœ¨æ£€ç´¢å’Œç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºäººå·¥é‡å†™ï¼Œè¯æ˜åˆæˆæ•°æ®èƒ½æœ‰æ•ˆæ›¿ä»£äººå·¥æ ‡æ³¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.22009v1">GraphSearch: An Agentic Deep Searching Workflow for Graph Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Graph Retrieval-Augmented Generation (GraphRAG) enhances factual reasoning in
LLMs by structurally modeling knowledge through graph-based representations.
However, existing GraphRAG approaches face two core limitations: shallow
retrieval that fails to surface all critical evidence, and inefficient
utilization of pre-constructed structural graph data, which hinders effective
reasoning from complex queries. To address these challenges, we propose
\textsc{GraphSearch}, a novel agentic deep searching workflow with dual-channel
retrieval for GraphRAG. \textsc{GraphSearch} organizes the retrieval process
into a modular framework comprising six modules, enabling multi-turn
interactions and iterative reasoning. Furthermore, \textsc{GraphSearch} adopts
a dual-channel retrieval strategy that issues semantic queries over chunk-based
text data and relational queries over structural graph data, enabling
comprehensive utilization of both modalities and their complementary strengths.
Experimental results across six multi-hop RAG benchmarks demonstrate that
\textsc{GraphSearch} consistently improves answer accuracy and generation
quality over the traditional strategy, confirming \textsc{GraphSearch} as a
promising direction for advancing graph retrieval-augmented generation.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºGraphSearchçš„æ–°å‹å›¾æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆGraphRAGï¼‰æ–¹æ³•ï¼Œé€šè¿‡åŒé€šé“æ£€ç´¢ç­–ç•¥ï¼ˆè¯­ä¹‰æŸ¥è¯¢å’Œå…³ç³»æŸ¥è¯¢ï¼‰åŠæ¨¡å—åŒ–å·¥ä½œæµè§£å†³äº†ä¼ ç»ŸGraphRAGæ–¹æ³•ä¸­æ£€ç´¢æµ…å±‚åŒ–å’Œå›¾æ•°æ®åˆ©ç”¨æ•ˆç‡ä½çš„é—®é¢˜ï¼Œå®éªŒè¯æ˜å…¶åœ¨å¤šè·³RAGåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†ç­”æ¡ˆå‡†ç¡®æ€§å’Œç”Ÿæˆè´¨é‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.21875v1">LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge Signals</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) aims to mitigate hallucinations in large
language models (LLMs) by grounding responses in retrieved documents. Yet,
RAG-based LLMs still hallucinate even when provided with correct and sufficient
context. A growing line of work suggests that this stems from an imbalance
between how models use external context and their internal knowledge, and
several approaches have attempted to quantify these signals for hallucination
detection. However, existing methods require extensive hyperparameter tuning,
limiting their generalizability. We propose LUMINA, a novel framework that
detects hallucinations in RAG systems through context-knowledge signals:
external context utilization is quantified via distributional distance, while
internal knowledge utilization is measured by tracking how predicted tokens
evolve across transformer layers. We further introduce a framework for
statistically validating these measurements. Experiments on common RAG
hallucination benchmarks and four open-source LLMs show that LUMINA achieves
consistently high AUROC and AUPRC scores, outperforming prior utilization-based
methods by up to +13% AUROC on HalluRAG. Moreover, LUMINA remains robust under
relaxed assumptions about retrieval quality and model matching, offering both
effectiveness and practicality.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†LUMINAæ¡†æ¶ï¼Œä¸“é—¨ç”¨äºæ£€æµ‹RAGç³»ç»Ÿä¸­å› ä¸Šä¸‹æ–‡ä¸å†…éƒ¨çŸ¥è¯†åˆ©ç”¨ä¸å¹³è¡¡å¯¼è‡´çš„å¹»è§‰é—®é¢˜ã€‚é€šè¿‡é‡åŒ–å¤–éƒ¨ä¸Šä¸‹æ–‡åˆ†å¸ƒè·ç¦»å’Œå†…éƒ¨çŸ¥è¯†åœ¨Transformerå±‚ä¸­çš„æ¼”åŒ–ï¼Œç»“åˆç»Ÿè®¡éªŒè¯æ–¹æ³•ï¼ŒLUMINAåœ¨å¤šä¸ªRAGåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ˆå¦‚AUROCæå‡13%ï¼‰ï¼Œä¸”å¯¹æ£€ç´¢è´¨é‡å’Œæ¨¡å‹é€‚é…å…·æœ‰æ›´å¼ºé²æ£’æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.21865v1">Beyond RAG vs. Long-Context: Learning Distraction-Aware Retrieval for Efficient Knowledge Grounding</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) is a framework for grounding Large
Language Models (LLMs) in external, up-to-date information. However, recent
advancements in context window size allow LLMs to process inputs of up to 128K
tokens or more, offering an alternative strategy: supplying the full document
context directly to the model, rather than relying on RAG to retrieve a subset
of contexts. Nevertheless, this emerging alternative strategy has notable
limitations: (i) it is token-inefficient to handle large and potentially
redundant contexts; (ii) it exacerbates the `lost in the middle' phenomenon;
and (iii) under limited model capacity, it amplifies distraction, ultimately
degrading LLM output quality. In this paper, we propose LDAR (Learning
Distraction-Aware Retrieval), an adaptive retriever that learns to retrieve
contexts in a way that mitigates interference from distracting passages,
thereby achieving significantly higher performance with reduced token usage
compared to long-context approaches. Extensive experiments across diverse LLM
architectures and six knowledge-intensive benchmarks demonstrate the
effectiveness and robustness of our approach, highlighting the importance of
balancing the trade-off between information coverage and distraction.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸Šä¸‹æ–‡çª—å£å¢å¤§çš„èƒŒæ™¯ä¸‹ï¼ŒRAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰é¢ä¸´çš„æŒ‘æˆ˜ä¸æ”¹è¿›æ–¹æ³•ï¼Œæå‡ºäº†LDARï¼ˆLearning Distraction-Aware Retrievalï¼‰ç®—æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”æ£€ç´¢å‡å°‘å¹²æ‰°æ€§æ®µè½çš„å½±å“ï¼Œä»¥æé«˜æ€§èƒ½å¹¶é™ä½tokenä½¿ç”¨é‡ï¼Œè¯æ˜äº†åœ¨ä¿¡æ¯è¦†ç›–ä¸å¹²æ‰°é—´å¹³è¡¡çš„é‡è¦æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.21856v1">KnowMT-Bench: Benchmarking Knowledge-Intensive Long-Form Question Answering in Multi-Turn Dialogues</a></td><td><details><summary>å±•å¼€</summary>Multi-Turn Long-Form Question Answering (MT-LFQA) is a key application
paradigm of Large Language Models (LLMs) in knowledge-intensive domains.
However, existing benchmarks are limited to single-turn dialogue, while
multi-turn dialogue benchmarks typically assess other orthogonal capabilities
rather than knowledge-intensive factuality. To bridge this critical gap, we
introduce \textbf{KnowMT-Bench}, the \textit{first-ever} benchmark designed to
systematically evaluate MT-LFQA for LLMs across knowledge-intensive fields,
including medicine, finance, and law. To faithfully assess the model's
real-world performance, KnowMT-Bench employs a dynamic evaluation setting where
models generate their own multi-turn dialogue histories given logically
progressive question sequences. The factual capability and information delivery
efficiency of the \textit{final-turn} answer are then evaluated using a
human-validated automated pipeline. Our experiments reveal that multi-turn
contexts degrade performance: factual capability declines due to the contextual
noise from self-generated histories, while information efficiency drops as
models become more verbose with increasing dialogue length. We then investigate
mitigation strategies, demonstrating that retrieval-augmented generation (RAG)
can effectively alleviate and even reverse this factual degradation. These
findings underscore the importance of our benchmark in evaluating and enhancing
the conversational factual capabilities of LLMs in real-world
knowledge-intensive applications. Code is available at
\href{https://github.com/hardenyu21/KnowMT-Bench}{\textcolor{cyan}{\texttt{KnowMT-Bench}}}.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†KnowMT-Benchï¼Œé¦–ä¸ªç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šè½®é•¿å½¢å¼é—®ç­”ï¼ˆMT-LFQAï¼‰ä¸­çŸ¥è¯†å¯†é›†å‹é¢†åŸŸæ€§èƒ½çš„åŸºå‡†æµ‹è¯•ï¼Œç ”ç©¶å‘ç°å¤šè½®å¯¹è¯ä¼šé™ä½æ¨¡å‹çš„äº‹å®æ€§è¡¨ç°ï¼Œä½†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰èƒ½æœ‰æ•ˆç¼“è§£è¿™ä¸€é€€åŒ–ï¼Œå¼ºè°ƒäº†RAGåœ¨æå‡æ¨¡å‹å¯¹è¯äº‹å®æ€§èƒ½åŠ›ä¸­çš„é‡è¦æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.21848v1">Graph of Agents: Principled Long Context Modeling by Emergent Multi-Agent Collaboration</a></td><td><details><summary>å±•å¼€</summary>As a model-agnostic approach to long context modeling, multi-agent systems
can process inputs longer than a large language model's context window without
retraining or architectural modifications. However, their performance often
heavily relies on hand-crafted multi-agent collaboration strategies and prompt
engineering, which limit generalizability. In this work, we introduce a
principled framework that formalizes the model-agnostic long context modeling
problem as a compression problem, yielding an information-theoretic compression
objective. Building on this framework, we propose Graph of Agents (GoA), which
dynamically constructs an input-dependent collaboration structure that
maximizes this objective. For Llama 3.1 8B and Qwen3 8B across six document
question answering benchmarks, GoA improves the average $F_1$ score of
retrieval-augmented generation by 5.7\% and a strong multi-agent baseline using
a fixed collaboration structure by 16.35\%, respectively. Even with only a 2K
context window, GoA surpasses the 128K context window Llama 3.1 8B on
LongBench, showing a dramatic increase in effective context length. Our source
code is available at https://github.com/tjoo512/graph-of-agents.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º"Graph of Agents (GoA)"çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡å°†é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡é—®é¢˜å½¢å¼åŒ–ä¸ºå‹ç¼©é—®é¢˜ï¼Œå¹¶åŠ¨æ€æ„å»ºè¾“å…¥ç›¸å…³çš„åä½œç»“æ„æ¥ä¼˜åŒ–æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒGoAåœ¨å…­ä¸ªæ–‡æ¡£é—®ç­”åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†RAGçš„F1åˆ†æ•°ï¼ˆ5.7%ï¼‰å’Œå¤šæ™ºèƒ½ä½“åŸºçº¿æ€§èƒ½ï¼ˆ16.35%ï¼‰ï¼Œä¸”åœ¨ä»…æœ‰2Kä¸Šä¸‹æ–‡çª—å£æ—¶è¶…è¶Š128Kçª—å£çš„Llama 3.1æ¨¡å‹ï¼Œå¤§å¹…æå‡äº†æœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.21730v1">ProPerSim: Developing Proactive and Personalized AI Assistants through User-Assistant Simulation</a></td><td><details><summary>å±•å¼€</summary>As large language models (LLMs) become increasingly integrated into daily
life, there is growing demand for AI assistants that are not only reactive but
also proactive and personalized. While recent advances have pushed forward
proactivity and personalization individually, their combination remains
underexplored. To bridge this gap, we introduce ProPerSim, a new task and
simulation framework for developing assistants capable of making timely,
personalized recommendations in realistic home scenarios. In our simulation
environment, a user agent with a rich persona interacts with the assistant,
providing ratings on how well each suggestion aligns with its preferences and
context. The assistant's goal is to use these ratings to learn and adapt to
achieve higher scores over time. Built on ProPerSim, we propose
ProPerAssistant, a retrieval-augmented, preference-aligned assistant that
continually learns and adapts through user feedback. Experiments across 32
diverse personas show that ProPerAssistant adapts its strategy and steadily
improves user satisfaction, highlighting the promise of uniting proactivity and
personalization.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºProPerSimçš„ä»»åŠ¡å’Œä»¿çœŸæ¡†æ¶ï¼Œæ—¨åœ¨å¼€å‘èƒ½å¤Ÿåœ¨ç°å®å®¶åº­åœºæ™¯ä¸­æä¾›ä¸»åŠ¨ä¸”ä¸ªæ€§åŒ–æ¨èçš„AIåŠ©æ‰‹ã€‚è®ºæ–‡ä»‹ç»äº†ProPerAssistantï¼Œä¸€ç§åŸºäºæ£€ç´¢å¢å¼ºï¼ˆretrieval-augmentedï¼‰ã€åå¥½å¯¹é½çš„åŠ©æ‰‹ï¼Œé€šè¿‡ç”¨æˆ·åé¦ˆæŒç»­å­¦ä¹ å’Œé€‚åº”ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨32ç§ä¸åŒç”¨æˆ·è§’è‰²ä¸­èƒ½é€æ­¥æå‡ç”¨æˆ·æ»¡æ„åº¦ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.21710v1">Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the
important paradigm for enhancing Large Language Models (LLMs) with external
knowledge. However, existing approaches face a fundamental trade-off. While
graph-based methods are inherently dependent on high-quality graph structures,
they face significant practical constraints: manually constructed knowledge
graphs are prohibitively expensive to scale, while automatically extracted
graphs from corpora are limited by the performance of the underlying LLM
extractors, especially when using smaller, local-deployed models. This paper
presents Think-on-Graph 3.0 (ToG-3), a novel framework that introduces
Multi-Agent Context Evolution and Retrieval (MACER) mechanism to overcome these
limitations. Our core innovation is the dynamic construction and refinement of
a Chunk-Triplets-Community heterogeneous graph index, which pioneeringly
incorporates a dual-evolution mechanism of Evolving Query and Evolving
Sub-Graph for precise evidence retrieval. This approach addresses a critical
limitation of prior Graph-based RAG methods, which typically construct a static
graph index in a single pass without adapting to the actual query. A
multi-agent system, comprising Constructor, Retriever, Reflector, and Responser
agents, collaboratively engages in an iterative process of evidence retrieval,
answer generation, sufficiency reflection, and, crucially, evolving query and
subgraph. This dual-evolving multi-agent system allows ToG-3 to adaptively
build a targeted graph index during reasoning, mitigating the inherent
drawbacks of static, one-time graph construction and enabling deep, precise
reasoning even with lightweight LLMs. Extensive experiments demonstrate that
ToG-3 outperforms compared baselines on both deep and broad reasoning
benchmarks, and ablation studies confirm the efficacy of the components of
MACER framework.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºThink-on-Graph 3.0ï¼ˆToG-3ï¼‰çš„æ–°å‹RAGæ¡†æ¶ï¼Œé€šè¿‡åˆ›æ–°çš„å¤šä»£ç†ä¸Šä¸‹æ–‡æ¼”åŒ–ä¸æ£€ç´¢ï¼ˆMACERï¼‰æœºåˆ¶å’ŒåŠ¨æ€æ„å»ºçš„Chunk-Triplets-Communityå¼‚æ„å›¾ç´¢å¼•ï¼Œæ”¹è¿›äº†ä¼ ç»ŸåŸºäºå›¾çš„RAGæ–¹æ³•ä¸­é™æ€å›¾ç´¢å¼•çš„å±€é™æ€§ï¼Œå®ç°äº†æŸ¥è¯¢å’Œå­å›¾çš„åŒé‡æ¼”åŒ–ï¼Œä»è€Œåœ¨è½»é‡çº§å¤§è¯­è¨€æ¨¡å‹ä¸Šå®ç°äº†æ›´æ·±æ›´ç²¾å‡†çš„æ¨ç†ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-09-25
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2509.21237v1">Query-Centric Graph Retrieval Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Graph-based retrieval-augmented generation (RAG) enriches large language
models (LLMs) with external knowledge for long-context understanding and
multi-hop reasoning, but existing methods face a granularity dilemma:
fine-grained entity-level graphs incur high token costs and lose context, while
coarse document-level graphs fail to capture nuanced relations. We introduce
QCG-RAG, a query-centric graph RAG framework that enables query-granular
indexing and multi-hop chunk retrieval. Our query-centric approach leverages
Doc2Query and Doc2Query{-}{-} to construct query-centric graphs with
controllable granularity, improving graph quality and interpretability. A
tailored multi-hop retrieval mechanism then selects relevant chunks via the
generated queries. Experiments on LiHuaWorld and MultiHop-RAG show that QCG-RAG
consistently outperforms prior chunk-based and graph-based RAG methods in
question answering accuracy, establishing a new paradigm for multi-hop
reasoning.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†QCG-RAGæ¡†æ¶ï¼Œé€šè¿‡æ„å»ºæŸ¥è¯¢ä¸ºä¸­å¿ƒçš„å›¾ç»“æ„è§£å†³ç°æœ‰åŸºäºå›¾çš„RAGæ–¹æ³•ä¸­ç²’åº¦å›°å¢ƒï¼ˆç»†ç²’åº¦å¯¼è‡´é«˜å¼€é”€ï¼Œç²—ç²’åº¦ä¸¢å¤±ç»†èŠ‚å…³ç³»ï¼‰ï¼Œç»“åˆå¯æ§ç²’åº¦ç´¢å¼•å’Œå¤šè·³åˆ†å—æ£€ç´¢æœºåˆ¶ï¼Œåœ¨é—®ç­”ä»»åŠ¡ä¸­è¶…è¶Šä¼ ç»Ÿåˆ†å—å’Œå›¾åŸºæ–¹æ³•ï¼Œæå‡äº†å¤šè·³æ¨ç†æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.21208v1">CLaw: Benchmarking Chinese Legal Knowledge in Large Language Models - A Fine-grained Corpus and Reasoning Analysis</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) are increasingly tasked with analyzing legal
texts and citing relevant statutes, yet their reliability is often compromised
by general pre-training that ingests legal texts without specialized focus,
obscuring the true depth of their legal knowledge. This paper introduces CLaw,
a novel benchmark specifically engineered to meticulously evaluate LLMs on
Chinese legal knowledge and its application in reasoning. CLaw comprises two
key components: (1) a comprehensive, fine-grained corpus of all 306 Chinese
national statutes, segmented to the subparagraph level and incorporating
precise historical revision timesteps for rigorous recall evaluation (64,849
entries), and (2) a challenging set of 254 case-based reasoning instances
derived from China Supreme Court curated materials to assess the practical
application of legal knowledge. Our empirical evaluation reveals that most
contemporary LLMs significantly struggle to faithfully reproduce legal
provisions. As accurate retrieval and citation of legal provisions form the
basis of legal reasoning, this deficiency critically undermines the reliability
of their responses. We contend that achieving trustworthy legal reasoning in
LLMs requires a robust synergy of accurate knowledge retrieval--potentially
enhanced through supervised fine-tuning (SFT) or retrieval-augmented generation
(RAG)--and strong general reasoning capabilities. This work provides an
essential benchmark and critical insights for advancing domain-specific LLM
reasoning, particularly within the complex legal sphere.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†CLawåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸­å›½æ³•å¾‹çŸ¥è¯†åŠå…¶æ¨ç†åº”ç”¨ä¸­çš„è¡¨ç°ï¼Œå‘ç°ç°æœ‰æ¨¡å‹åœ¨å‡†ç¡®æ£€ç´¢å’Œå¼•ç”¨æ³•å¾‹æ¡æ–‡æ–¹é¢å­˜åœ¨é‡å¤§ç¼ºé™·ï¼Œå¹¶æŒ‡å‡ºé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­‰æŠ€æœ¯æ”¹è¿›çŸ¥è¯†æ£€ç´¢èƒ½åŠ›æ˜¯å®ç°å¯é æ³•å¾‹æ¨ç†çš„å…³é”®ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.21193v1">Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) have recently shown strong progress on
scientific reasoning, yet two major bottlenecks remain. First, explicit
retrieval fragments reasoning, imposing a hidden "tool tax" of extra tokens and
steps. Second, multi-agent pipelines often dilute strong solutions by averaging
across all candidates. We address these challenges with a unified framework
that combines implicit retrieval and structured collaboration. At its
foundation, a Monitor-based retrieval module operates at the token level,
integrating external knowledge with minimal disruption to reasoning. On top of
this substrate, Hierarchical Solution Refinement (HSR) iteratively designates
each candidate as an anchor to be repaired by its peers, while Quality-Aware
Iterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's
Last Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\% accuracy -- the
highest reported to date, surpassing the strongest agent baseline by 13.4
points and leading frontier LLMs by up to 18.1 points, while simultaneously
reducing token usage by 53.5\% and agent steps by 43.7\%. Results on SuperGPQA
and TRQA confirm robustness across domains. Error analysis shows that reasoning
failures and knowledge gaps co-occur in over 85\% of cases, while diversity
analysis reveals a clear dichotomy: retrieval tasks benefit from solution
variety, whereas reasoning tasks favor consensus. Together, these findings
demonstrate how implicit augmentation and structured refinement overcome the
inefficiencies of explicit tool use and uniform aggregation. Code is available
at: https://github.com/tangxiangru/Eigen-1.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆéšå¼æ£€ç´¢å’Œç»“æ„åŒ–åä½œçš„ç»Ÿä¸€æ¡†æ¶æ¥è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦æ¨ç†ä¸­çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åŸºäºMonitorçš„æ£€ç´¢æ¨¡å—åœ¨tokençº§åˆ«é›†æˆå¤–éƒ¨çŸ¥è¯†ï¼Œå‡å°‘æ¨ç†ä¸­æ–­ï¼Œå¹¶é‡‡ç”¨åˆ†å±‚è§£å†³æ–¹æ¡ˆç²¾ç‚¼ï¼ˆHSRï¼‰å’Œè´¨é‡æ„ŸçŸ¥è¿­ä»£æ¨ç†ï¼ˆQAIRï¼‰æ¥ä¼˜åŒ–ç»“æœã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šé¡¹ä»»åŠ¡ä¸­å®ç°äº†æœ€é«˜å‡†ç¡®ç‡ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†tokenå’Œè®¡ç®—æ­¥éª¤çš„æ¶ˆè€—ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.21188v1">Adoption, usability and perceived clinical value of a UK AI clinical reference platform (iatroX): a mixed-methods formative evaluation of real-world usage and a 1,223-respondent user survey</a></td><td><details><summary>å±•å¼€</summary>Clinicians face growing information overload from biomedical literature and
guidelines, hindering evidence-based care. Retrieval-augmented generation (RAG)
with large language models may provide fast, provenance-linked answers, but
requires real-world evaluation. We describe iatroX, a UK-centred RAG-based
clinical reference platform, and report early adoption, usability, and
perceived clinical value from a formative implementation evaluation. Methods
comprised a retrospective analysis of usage across web, iOS, and Android over
16 weeks (8 April-31 July 2025) and an in-product intercept survey. Usage
metrics were drawn from web and app analytics with bot filtering. A client-side
script randomized single-item prompts to approx. 10% of web sessions from a
predefined battery assessing usefulness, reliability, and adoption intent.
Proportions were summarized with Wilson 95% confidence intervals; free-text
comments underwent thematic content analysis. iatroX reached 19,269 unique web
users, 202,660 engagement events, and approx. 40,000 clinical queries. Mobile
uptake included 1,960 iOS downloads and Android growth (peak >750 daily active
users). The survey yielded 1,223 item-level responses: perceived usefulness
86.2% (95% CI 74.8-93.9%; 50/58); would use again 93.3% (95% CI 68.1-99.8%;
14/15); recommend to a colleague 88.4% (95% CI 75.1-95.9%; 38/43); perceived
accuracy 75.0% (95% CI 58.8-87.3%; 30/40); reliability 79.4% (95% CI
62.1-91.3%; 27/34). Themes highlighted speed, guideline-linked answers, and UK
specificity. Early real-world use suggests iatroX can mitigate information
overload and support timely answers for UK clinicians. Limitations include
small per-item samples and early-adopter bias; future work will include
accuracy audits and prospective studies on workflow and care quality.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†åŸºäºRAGæŠ€æœ¯çš„ä¸´åºŠå‚è€ƒå¹³å°iatroXï¼Œæ—¨åœ¨è§£å†³ä¸´åºŠåŒ»ç”Ÿé¢ä¸´çš„ä¿¡æ¯è¿‡è½½é—®é¢˜ã€‚è¯¥å¹³å°é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆæä¾›å¿«é€Ÿã€å¯æº¯æºçš„åŒ»ç–—ç­”æ¡ˆï¼Œå¹¶åœ¨è‹±å›½è¿›è¡Œå®é™…åº”ç”¨è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºæ—©æœŸç”¨æˆ·å¯¹å…¶æœ‰ç”¨æ€§ã€å‡†ç¡®æ€§å’Œå¯é æ€§æŒç§¯æè¯„ä»·ã€‚ç ”ç©¶è¿˜åˆ†æäº†å¹³å°çš„ä½¿ç”¨æ•°æ®ã€ç”¨æˆ·åé¦ˆåŠå±€é™æ€§ï¼Œå¹¶å±•æœ›äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.21035v1">CLAUSE: Agentic Neuro-Symbolic Knowledge Graph Reasoning via Dynamic Learnable Context Engineering</a></td><td><details><summary>å±•å¼€</summary>Knowledge graphs provide structured context for multi-hop question answering,
but deployed systems must balance answer accuracy with strict latency and cost
targets while preserving provenance. Static k-hop expansions and "think-longer"
prompting often over-retrieve, inflate context, and yield unpredictable
runtime. We introduce CLAUSE, an agentic three-agent neuro-symbolic framework
that treats context construction as a sequential decision process over
knowledge graphs, deciding what to expand, which paths to follow or backtrack,
what evidence to keep, and when to stop. Latency (interaction steps) and prompt
cost (selected tokens) are exposed as user-specified budgets or prices,
allowing per-query adaptation to trade-offs among accuracy, latency, and cost
without retraining. CLAUSE employs the proposed Lagrangian-Constrained
Multi-Agent Proximal Policy Optimization (LC-MAPPO) algorithm to coordinate
three agents: Subgraph Architect, Path Navigator, and Context Curator, so that
subgraph construction, reasoning-path discovery, and evidence selection are
jointly optimized under per-query resource budgets on edge edits, interaction
steps, and selected tokens. Across HotpotQA, MetaQA, and FactKG, CLAUSE yields
higher EM@1 while reducing subgraph growth and end-to-end latency at equal or
lower token budgets. On MetaQA-2-hop, relative to the strongest RAG baseline
(GraphRAG), CLAUSE achieves +39.3 EM@1 with 18.6% lower latency and 40.9% lower
edge growth. The resulting contexts are compact, provenance-preserving, and
deliver predictable performance under deployment constraints.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†CLAUSEï¼Œä¸€ç§åŸºäºæ™ºèƒ½ç¥ç»ç¬¦å·æ¡†æ¶çš„å¤šä»£ç†ç³»ç»Ÿï¼Œç”¨äºä¼˜åŒ–çŸ¥è¯†å›¾è°±ä¸Šçš„ä¸Šä¸‹æ–‡æ„å»ºè¿‡ç¨‹ï¼Œé€šè¿‡åŠ¨æ€å†³ç­–åœ¨å‡†ç¡®æ€§ã€å»¶è¿Ÿå’Œæˆæœ¬ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚CLAUSEåˆ©ç”¨LC-MAPPOç®—æ³•åè°ƒä¸‰ä¸ªä»£ç†ï¼ˆå­å›¾æ„å»ºã€è·¯å¾„å¯¼èˆªå’Œä¸Šä¸‹æ–‡ç®¡ç†ï¼‰ï¼Œåœ¨èµ„æºé™åˆ¶ä¸‹æå‡å¤šè·³é—®ç­”çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºä¼ ç»ŸRAGæ–¹æ³•ï¼ˆå¦‚GraphRAGï¼‰ï¼Œå®ƒåœ¨å‡å°‘å­å›¾å¢é•¿å’Œå»¶è¿Ÿçš„åŒæ—¶æ˜¾è‘—æé«˜äº†å‡†ç¡®ç‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.20953v1">Beyond Stars: Bridging the Gap Between Ratings and Review Sentiment with LLM</a></td><td><details><summary>å±•å¼€</summary>We present an advanced approach to mobile app review analysis aimed at
addressing limitations inherent in traditional star-rating systems. Star
ratings, although intuitive and popular among users, often fail to capture the
nuanced feedback present in detailed review texts. Traditional NLP techniques
-- such as lexicon-based methods and classical machine learning classifiers --
struggle to interpret contextual nuances, domain-specific terminology, and
subtle linguistic features like sarcasm. To overcome these limitations, we
propose a modular framework leveraging large language models (LLMs) enhanced by
structured prompting techniques. Our method quantifies discrepancies between
numerical ratings and textual sentiment, extracts detailed, feature-level
insights, and supports interactive exploration of reviews through
retrieval-augmented conversational question answering (RAG-QA). Comprehensive
experiments conducted on three diverse datasets (AWARE, Google Play, and
Spotify) demonstrate that our LLM-driven approach significantly surpasses
baseline methods, yielding improved accuracy, robustness, and actionable
insights in challenging and context-rich review scenarios.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œç»“æ„åŒ–æç¤ºæŠ€æœ¯çš„æ¨¡å—åŒ–æ¡†æ¶ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºçš„å¯¹è¯é—®ç­”ï¼ˆRAG-QAï¼‰æ¥åˆ†æç§»åŠ¨åº”ç”¨è¯„è®ºï¼Œä»¥å…‹æœä¼ ç»Ÿæ˜Ÿçº§è¯„åˆ†å’Œéç»“æ„åŒ–NLPæ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶åœ¨å¤šæ•°æ®é›†å®éªŒä¸­å±•ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.20859v1">Concise and Sufficient Sub-Sentence Citations for Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>In retrieval-augmented generation (RAG) question answering systems,
generating citations for large language model (LLM) outputs enhances
verifiability and helps users identify potential hallucinations. However, we
observe two problems in the citations produced by existing attribution methods.
First, the citations are typically provided at the sentence or even paragraph
level. Long sentences or paragraphs may include a substantial amount of
irrelevant content. Second, sentence-level citations may omit information that
is essential for verifying the output, forcing users to read the surrounding
context. In this paper, we propose generating sub-sentence citations that are
both concise and sufficient, thereby reducing the effort required by users to
confirm the correctness of the generated output. To this end, we first develop
annotation guidelines for such citations and construct a corresponding dataset.
Then, we propose an attribution framework for generating citations that adhere
to our standards. This framework leverages LLMs to automatically generate
fine-tuning data for our task and employs a credit model to filter out
low-quality examples. Our experiments on the constructed dataset demonstrate
that the propose approach can generate high-quality and more readable
citations.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æ¢è®¨äº†åœ¨RAGé—®ç­”ç³»ç»Ÿä¸­ä¸ºLLMè¾“å‡ºç”Ÿæˆæ›´ç²¾ç¡®çš„å­å¥çº§åˆ«å¼•ç”¨ï¼ˆè€Œéä¼ ç»Ÿå¥å­æˆ–æ®µè½çº§ï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨æå‡å¼•ç”¨ä¿¡æ¯çš„ç®€æ´æ€§å’Œå……åˆ†æ€§ï¼Œå‡å°‘ç”¨æˆ·éªŒè¯æˆæœ¬ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç»“åˆè‡ªåŠ¨æ ‡æ³¨å’Œæ•°æ®è¿‡æ»¤çš„å½’å› æ¡†æ¶ï¼Œé€šè¿‡å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.20769v1">Provenance Analysis of Archaeological Artifacts via Multimodal RAG Systems</a></td><td><details><summary>å±•å¼€</summary>In this work, we present a retrieval-augmented generation (RAG)-based system
for provenance analysis of archaeological artifacts, designed to support expert
reasoning by integrating multimodal retrieval and large vision-language models
(VLMs). The system constructs a dual-modal knowledge base from reference texts
and images, enabling raw visual, edge-enhanced, and semantic retrieval to
identify stylistically similar objects. Retrieved candidates are synthesized by
the VLM to generate structured inferences, including chronological,
geographical, and cultural attributions, alongside interpretive justifications.
We evaluate the system on a set of Eastern Eurasian Bronze Age artifacts from
the British Museum. Expert evaluation demonstrates that the system produces
meaningful and interpretable outputs, offering scholars concrete starting
points for analysis and significantly alleviating the cognitive burden of
navigating vast comparative corpora.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„ç³»ç»Ÿï¼Œç”¨äºè€ƒå¤æ–‡ç‰©æ¥æºåˆ†æï¼Œé€šè¿‡æ•´åˆå¤šæ¨¡æ€æ£€ç´¢å’Œå¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œæ„å»ºåŒæ¨¡æ€çŸ¥è¯†åº“ä»¥æ£€ç´¢é£æ ¼ç›¸ä¼¼çš„æ–‡ç‰©ï¼Œå¹¶ç”Ÿæˆç»“æ„åŒ–æ¨æ–­ï¼ˆå¦‚å¹´ä»£ã€åœ°ç†å’Œæ–‡åŒ–å±æ€§ï¼‰åŠè§£é‡Šæ€§ç†ç”±ï¼Œç»å¤§è‹±åšç‰©é¦†çš„æ¬§äºšé’é“œå™¨æ–‡ç‰©éªŒè¯ï¼Œä¸“å®¶è¯„ä¼°è¡¨æ˜ç³»ç»Ÿèƒ½æœ‰æ•ˆæ”¯æŒå­¦æœ¯åˆ†æå¹¶å‡è½»è®¤çŸ¥è´Ÿæ‹…ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.20707v1">An Automated Retrieval-Augmented Generation LLaMA-4 109B-based System for Evaluating Radiotherapy Treatment Plans</a></td><td><details><summary>å±•å¼€</summary>Purpose: To develop a retrieval-augmented generation (RAG) system powered by
LLaMA-4 109B for automated, protocol-aware, and interpretable evaluation of
radiotherapy treatment plans.
  Methods and Materials: We curated a multi-protocol dataset of 614
radiotherapy plans across four disease sites and constructed a knowledge base
containing normalized dose metrics and protocol-defined constraints. The RAG
system integrates three core modules: a retrieval engine optimized across five
SentenceTransformer backbones, a percentile prediction component based on
cohort similarity, and a clinical constraint checker. These tools are directed
by a large language model (LLM) using a multi-step prompt-driven reasoning
pipeline to produce concise, grounded evaluations.
  Results: Retrieval hyperparameters were optimized using Gaussian Process on a
scalarized loss function combining root mean squared error (RMSE), mean
absolute error (MAE), and clinically motivated accuracy thresholds. The best
configuration, based on all-MiniLM-L6-v2, achieved perfect nearest-neighbor
accuracy within a 5-percentile-point margin and a sub-2pt MAE. When tested
end-to-end, the RAG system achieved 100% agreement with the computed values by
standalone retrieval and constraint-checking modules on both percentile
estimates and constraint identification, confirming reliable execution of all
retrieval, prediction and checking steps.
  Conclusion: Our findings highlight the feasibility of combining structured
population-based scoring with modular tool-augmented reasoning for transparent,
scalable plan evaluation in radiation therapy. The system offers traceable
outputs, minimizes hallucination, and demonstrates robustness across protocols.
Future directions include clinician-led validation, and improved domain-adapted
retrieval models to enhance real-world integration.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºLLaMA-4 109Bçš„RAGç³»ç»Ÿï¼Œç”¨äºæ”¾å°„æ²»ç–—è®¡åˆ’çš„è‡ªåŠ¨åŒ–ã€åè®®æ„ŸçŸ¥å’Œå¯è§£é‡Šæ€§è¯„ä¼°ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ•´åˆæ£€ç´¢å¼•æ“ã€ç™¾åˆ†ä½æ•°é¢„æµ‹ç»„ä»¶å’Œä¸´åºŠçº¦æŸæ£€æŸ¥å™¨ï¼Œåˆ©ç”¨å¤šæ­¥æç¤ºé©±åŠ¨çš„æ¨ç†æµç¨‹ç”Ÿæˆç²¾ç¡®è¯„ä¼°ï¼Œå¹¶åœ¨å®éªŒä¸­å±•ç°äº†é«˜å‡†ç¡®æ€§å’Œå¯é æ€§ï¼ŒåŒæ—¶å‡å°‘äº†å¹»è§‰è¾“å‡ºã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-09-24
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2509.20324v1">RAG Security and Privacy: Formalizing the Threat Model and Attack Surface</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) is an emerging approach in natural
language processing that combines large language models (LLMs) with external
document retrieval to produce more accurate and grounded responses. While RAG
has shown strong potential in reducing hallucinations and improving factual
consistency, it also introduces new privacy and security challenges that differ
from those faced by traditional LLMs. Existing research has demonstrated that
LLMs can leak sensitive information through training data memorization or
adversarial prompts, and RAG systems inherit many of these vulnerabilities. At
the same time, reliance of RAG on an external knowledge base opens new attack
surfaces, including the potential for leaking information about the presence or
content of retrieved documents, or for injecting malicious content to
manipulate model behavior. Despite these risks, there is currently no formal
framework that defines the threat landscape for RAG systems. In this paper, we
address a critical gap in the literature by proposing, to the best of our
knowledge, the first formal threat model for retrieval-RAG systems. We
introduce a structured taxonomy of adversary types based on their access to
model components and data, and we formally define key threat vectors such as
document-level membership inference and data poisoning, which pose serious
privacy and integrity risks in real-world deployments. By establishing formal
definitions and attack models, our work lays the foundation for a more rigorous
and principled understanding of privacy and security in RAG systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿåœ¨éšç§å’Œå®‰å…¨æ–¹é¢çš„æ–°æŒ‘æˆ˜ï¼Œæå‡ºäº†é¦–ä¸ªé’ˆå¯¹RAGç³»ç»Ÿçš„æ­£å¼å¨èƒæ¨¡å‹ï¼Œå¹¶å®šä¹‰äº†åŒ…æ‹¬æ–‡æ¡£çº§æˆå‘˜æ¨æ–­å’Œæ•°æ®æŠ•æ¯’åœ¨å†…çš„å…³é”®å¨èƒå‘é‡ï¼Œä¸ºç†è§£å’Œåº”å¯¹RAGç³»ç»Ÿçš„å®‰å…¨é£é™©æä¾›äº†ç†è®ºåŸºç¡€ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.20190v1">STAF: Leveraging LLMs for Automated Attack Tree-Based Security Test Generation</a></td><td><details><summary>å±•å¼€</summary>In modern automotive development, security testing is critical for
safeguarding systems against increasingly advanced threats. Attack trees are
widely used to systematically represent potential attack vectors, but
generating comprehensive test cases from these trees remains a labor-intensive,
error-prone task that has seen limited automation in the context of testing
vehicular systems. This paper introduces STAF (Security Test Automation
Framework), a novel approach to automating security test case generation.
Leveraging Large Language Models (LLMs) and a four-step self-corrective
Retrieval-Augmented Generation (RAG) framework, STAF automates the generation
of executable security test cases from attack trees, providing an end-to-end
solution that encompasses the entire attack surface. We particularly show the
elements and processes needed to provide an LLM to actually produce sensible
and executable automotive security test suites, along with the integration with
an automated testing framework. We further compare our tailored approach with
general purpose (vanilla) LLMs and the performance of different LLMs (namely
GPT-4.1 and DeepSeek) using our approach. We also demonstrate the method of our
operation step-by-step in a concrete case study. Our results show significant
improvements in efficiency, accuracy, scalability, and easy integration in any
workflow, marking a substantial advancement in automating automotive security
testing methodologies. Using TARAs as an input for verfication tests, we create
synergies by connecting two vital elements of a secure automotive development
process.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†STAFæ¡†æ¶ï¼Œåˆ©ç”¨LLMå’Œå››æ­¥è‡ªæ ¡æ­£RAGæŠ€æœ¯ï¼Œè‡ªåŠ¨åŒ–ç”Ÿæˆæ±½è½¦å®‰å…¨æµ‹è¯•ç”¨ä¾‹ï¼Œæ˜¾è‘—æå‡äº†æµ‹è¯•æ•ˆç‡ã€å‡†ç¡®æ€§åŠå¯æ‰©å±•æ€§ï¼Œå¹¶å¯¹æ¯”äº†ä¸åŒLLMï¼ˆå¦‚GPT-4.1å’ŒDeepSeekï¼‰çš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.19980v1">RAD: Towards Trustworthy Retrieval-Augmented Multi-modal Clinical Diagnosis</a></td><td><details><summary>å±•å¼€</summary>Clinical diagnosis is a highly specialized discipline requiring both domain
expertise and strict adherence to rigorous guidelines. While current AI-driven
medical research predominantly focuses on knowledge graphs or natural text
pretraining paradigms to incorporate medical knowledge, these approaches
primarily rely on implicitly encoded knowledge within model parameters,
neglecting task-specific knowledge required by diverse downstream tasks. To
address this limitation, we propose Retrieval-Augmented Diagnosis (RAD), a
novel framework that explicitly injects external knowledge into multimodal
models directly on downstream tasks. Specifically, RAD operates through three
key mechanisms: retrieval and refinement of disease-centered knowledge from
multiple medical sources, a guideline-enhanced contrastive loss that constrains
the latent distance between multi-modal features and guideline knowledge, and
the dual transformer decoder that employs guidelines as queries to steer
cross-modal fusion, aligning the models with clinical diagnostic workflows from
guideline acquisition to feature extraction and decision-making. Moreover,
recognizing the lack of quantitative evaluation of interpretability for
multimodal diagnostic models, we introduce a set of criteria to assess the
interpretability from both image and text perspectives. Extensive evaluations
across four datasets with different anatomies demonstrate RAD's
generalizability, achieving state-of-the-art performance. Furthermore, RAD
enables the model to concentrate more precisely on abnormal regions and
critical indicators, ensuring evidence-based, trustworthy diagnosis. Our code
is available at https://github.com/tdlhl/RAD.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œRetrieval-Augmented Diagnosis (RAD)â€çš„æ–°æ¡†æ¶ï¼Œé€šè¿‡æ£€ç´¢å’Œæ•´åˆå¤šæºåŒ»å­¦çŸ¥è¯†ï¼ˆå¦‚ç–¾ç—…æŒ‡å—ï¼‰ï¼Œç»“åˆå¯¹æ¯”æŸå¤±å’ŒåŒTransformerè§£ç å™¨ç­‰æœºåˆ¶ï¼Œæ˜¾å¼åœ°å°†å¤–éƒ¨çŸ¥è¯†æ³¨å…¥å¤šæ¨¡æ€æ¨¡å‹ï¼Œä»¥æå‡ä¸´åºŠè¯Šæ–­çš„å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§åŠä¸å·¥ä½œæµç¨‹çš„å¥‘åˆåº¦ï¼Œå¹¶è®¾è®¡äº†å®šé‡è¯„ä¼°æŒ‡æ ‡ã€‚è¯¥æ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå±äºRAGæŠ€æœ¯åœ¨åŒ»ç–—è¯Šæ–­é¢†åŸŸçš„æ‰©å±•åº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.19952v1">When Words Can't Capture It All: Towards Video-Based User Complaint Text Generation with Multimodal Video Complaint Dataset</a></td><td><details><summary>å±•å¼€</summary>While there exists a lot of work on explainable complaint mining,
articulating user concerns through text or video remains a significant
challenge, often leaving issues unresolved. Users frequently struggle to
express their complaints clearly in text but can easily upload videos depicting
product defects (e.g., vague text such as `worst product' paired with a
5-second video depicting a broken headphone with the right earcup). This paper
formulates a new task in the field of complaint mining to aid the common users'
need to write an expressive complaint, which is Complaint Description from
Videos (CoD-V) (e.g., to help the above user articulate her complaint about the
defective right earcup). To this end, we introduce ComVID, a video complaint
dataset containing 1,175 complaint videos and the corresponding descriptions,
also annotated with the emotional state of the complainer. Additionally, we
present a new complaint retention (CR) evaluation metric that discriminates the
proposed (CoD-V) task against standard video summary generation and description
tasks. To strengthen this initiative, we introduce a multimodal
Retrieval-Augmented Generation (RAG) embedded VideoLLaMA2-7b model, designed to
generate complaints while accounting for the user's emotional state. We conduct
a comprehensive evaluation of several Video Language Models on several tasks
(pre-trained and fine-tuned versions) with a range of established evaluation
metrics, including METEOR, perplexity, and the Coleman-Liau readability score,
among others. Our study lays the foundation for a new research direction to
provide a platform for users to express complaints through video. Dataset and
resources are available at: https://github.com/sarmistha-D/CoD-V.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æŠ•è¯‰æŒ–æ˜ä»»åŠ¡â€”â€”è§†é¢‘æŠ•è¯‰æè¿°ï¼ˆCoD-Vï¼‰ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·é€šè¿‡è§†é¢‘è¡¨è¾¾æŠ•è¯‰å†…å®¹ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªåŒ…å«1175æ¡æŠ•è¯‰è§†é¢‘åŠå¯¹åº”æè¿°çš„æ•°æ®é›†ComVIDã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡CRï¼Œå¹¶å¼€å‘äº†ä¸€ç§åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„å¤šæ¨¡æ€æ¨¡å‹VideoLLaMA2-7bï¼Œç”¨äºç”Ÿæˆè€ƒè™‘ç”¨æˆ·æƒ…æ„ŸçŠ¶æ€çš„æŠ•è¯‰æè¿°ã€‚ç ”ç©¶é€šè¿‡å¤šç§è¯„ä¼°æŒ‡æ ‡å¯¹æ¨¡å‹æ€§èƒ½è¿›è¡Œäº†å…¨é¢éªŒè¯ï¼Œä¸ºè¯¥é¢†åŸŸçš„æ–°ç ”ç©¶æ–¹å‘å¥ å®šäº†åŸºç¡€ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.19931v1">Documentation Retrieval Improves Planning Language Generation</a></td><td><details><summary>å±•å¼€</summary>Certain strong LLMs have shown promise for zero-shot formal planning by
generating planning languages like PDDL. Yet, performance of most open-source
models under 50B parameters has been reported to be close to zero due to the
low-resource nature of these languages. We significantly improve their
performance via a series of lightweight pipelines that integrates documentation
retrieval with modular code generation and error refinement. With models like
Llama-4-Maverick, our best pipeline improves plan correctness from 0\% to over
80\% on the common BlocksWorld domain. However, while syntactic errors are
substantially reduced, semantic errors persist in more challenging domains,
revealing fundamental limitations in current models' reasoning
capabilities.\footnote{Our code and data can be found at
https://github.com/Nangxxxxx/PDDL-RAG</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§é€šè¿‡æ•´åˆæ–‡æ¡£æ£€ç´¢ã€æ¨¡å—åŒ–ä»£ç ç”Ÿæˆå’Œé”™è¯¯ä¿®æ­£çš„è½»é‡çº§æµç¨‹ï¼Œæ˜¾è‘—æå‡äº†ä¸­å°å‹å¼€æºLLMsåœ¨é›¶æ ·æœ¬å½¢å¼åŒ–è§„åˆ’ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ˆå¦‚ç”ŸæˆPDDLè§„åˆ’è¯­è¨€ï¼‰ï¼Œå°¤å…¶åœ¨BlocksWorldé¢†åŸŸå°†æ­£ç¡®ç‡ä»0%æå‡è‡³80%ä»¥ä¸Šï¼Œä½†æŒ‡å‡ºæ¨¡å‹åœ¨å¤æ‚é¢†åŸŸçš„è¯­ä¹‰æ¨ç†ä»å­˜åœ¨æ ¹æœ¬æ€§å±€é™ã€‚å…¶æ–¹æ³•æ ¸å¿ƒæ¶‰åŠæ£€ç´¢å¢å¼ºæŠ€æœ¯ï¼ˆä»£ç åº“æ ‡æ³¨äº†PDDL-RAGï¼‰ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-09-23
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2509.19218v1">HyKid: An Open MRI Dataset with Expert-Annotated Multi-Structure and Choroid Plexus in Pediatric Hydrocephalus</a></td><td><details><summary>å±•å¼€</summary>Evaluation of hydrocephalus in children is challenging, and the related
research is limited by a lack of publicly available, expert-annotated datasets,
particularly those with segmentation of the choroid plexus. To address this, we
present HyKid, an open-source dataset from 48 pediatric patients with
hydrocephalus. 3D MRIs were provided with 1mm isotropic resolution, which was
reconstructed from routine low-resolution images using a slice-to-volume
algorithm. Manually corrected segmentations of brain tissues, including white
matter, grey matter, lateral ventricle, external CSF, and the choroid plexus,
were provided by an experienced neurologist. Additionally, structured data was
extracted from clinical radiology reports using a Retrieval-Augmented
Generation framework. The strong correlation between choroid plexus volume and
total CSF volume provided a potential biomarker for hydrocephalus evaluation,
achieving excellent performance in a predictive model (AUC = 0.87). The
proposed HyKid dataset provided a high-quality benchmark for neuroimaging
algorithms development, and it revealed the choroid plexus-related features in
hydrocephalus assessments. Our datasets are publicly available at
https://www.synapse.org/Synapse:syn68544889.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†HyKidæ•°æ®é›†ï¼Œä¸€ä¸ªé’ˆå¯¹å„¿ç«¥è„‘ç§¯æ°´çš„å¼€æºæ•°æ®é›†ï¼ŒåŒ…å«é«˜åˆ†è¾¨ç‡3D MRIå›¾åƒå’Œä¸“å®¶æ‰‹åŠ¨æ ¡æ­£çš„åˆ†å‰²æ ‡æ³¨ã€‚ç ”ç©¶åˆ©ç”¨RAGæ¡†æ¶ä»ä¸´åºŠæ”¾å°„å­¦æŠ¥å‘Šä¸­æå–ç»“æ„åŒ–æ•°æ®ï¼Œå¹¶å‘ç°äº†è„‰ç»œä¸›ä½“ç§¯ä¸è„‘è„Šæ¶²æ€»é‡çš„ç›¸å…³æ€§å¯ä½œä¸ºè„‘ç§¯æ°´è¯„ä¼°çš„ç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œé¢„æµ‹æ¨¡å‹è¡¨ç°ä¼˜å¼‚ï¼ˆAUC=0.87ï¼‰ã€‚è¯¥æ•°æ®é›†ä¸ºç¥ç»å½±åƒç®—æ³•å¼€å‘æä¾›äº†é«˜è´¨é‡åŸºå‡†ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.19209v1">A Knowledge Graph and a Tripartite Evaluation Framework Make Retrieval-Augmented Generation Scalable and Transparent</a></td><td><details><summary>å±•å¼€</summary>Large Language Models (LLMs) have significantly enhanced conversational
Artificial Intelligence(AI) chatbots; however, domain-specific accuracy and the
avoidance of factual inconsistencies remain pressing challenges, particularly
for large datasets. Designing an effective chatbot with appropriate methods and
evaluating its effectiveness is among the challenges in this domain. This study
presents a Retrieval Augmented Generation (RAG) chatbot that harnesses a
knowledge graph and vector search retrieval to deliver precise, context-rich
responses in an exemplary use case from over high-volume engineering
project-related emails, thereby minimising the need for document chunking. A
central innovation of this work is the introduction of RAG Evaluation
(RAG-Eval), a novel chain-of-thought LLM-based tripartite evaluation framework
specifically developed to assess RAG applications. This framework operates in
parallel with the chatbot, jointly assessing the user's query, the retrieved
document, and the generated response, enabling a holistic evaluation across
multiple quality metrics like query relevance, factual accuracy, coverage,
coherence and fluency. The resulting scoring system is provided directly to
users as a confidence score (1 to 100%), enabling quick identification of
possible misaligned or incomplete answers. This proposed approach promotes
transparency and rapid verification by incorporating metadata email IDs,
timestamps into responses. Experimental comparisons against BERTScore and
G-EVAL for summarisation evaluation tasks confirm its effectiveness, and
empirical analysis also shows RAG-Eval reliably detects factual gaps and query
mismatches, thereby fostering trust in high demand, data centric environments.
These findings highlight a scalable path for developing accurate,
user-verifiable chatbots that bridge the gap between high-level conversational
fluency and factual accuracy.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„èŠå¤©æœºå™¨äººï¼Œç»“åˆçŸ¥è¯†å›¾è°±å’Œå‘é‡æœç´¢æ£€ç´¢æŠ€æœ¯ï¼Œä»å¤§è§„æ¨¡å·¥ç¨‹ç›¸å…³é‚®ä»¶æ•°æ®ä¸­ç”Ÿæˆç²¾å‡†ä¸”ä¸Šä¸‹æ–‡ä¸°å¯Œçš„å›ç­”ï¼Œå‡å°‘æ–‡æ¡£åˆ†å—çš„éœ€æ±‚ã€‚è®ºæ–‡è¿˜åˆ›æ–°åœ°å¼•å…¥äº†RAG-Evalï¼Œä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä¸‰æ–¹è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°RAGåº”ç”¨çš„æŸ¥è¯¢ç›¸å…³æ€§ã€äº‹å®å‡†ç¡®æ€§ã€è¦†ç›–èŒƒå›´ç­‰è´¨é‡æŒ‡æ ‡ï¼Œå¹¶é€šè¿‡ç½®ä¿¡åº¦åˆ†æ•°å’Œå…ƒæ•°æ®å¢å¼ºé€æ˜åº¦ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨é«˜æ•ˆæ€§å’Œå¯ä¿¡åº¦ä¸Šä¼˜äºBERTScoreå’ŒG-EVALã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.18868v1">Memory in Large Language Models: Mechanisms, Evaluation and Evolution</a></td><td><details><summary>å±•å¼€</summary>Under a unified operational definition, we define LLM memory as a persistent
state written during pretraining, finetuning, or inference that can later be
addressed and that stably influences outputs. We propose a four-part taxonomy
(parametric, contextual, external, procedural/episodic) and a memory quadruple
(location, persistence, write/access path, controllability). We link mechanism,
evaluation, and governance via the chain write -> read -> inhibit/update. To
avoid distorted comparisons across heterogeneous setups, we adopt a
three-setting protocol (parametric only, offline retrieval, online retrieval)
that decouples capability from information availability on the same data and
timeline. On this basis we build a layered evaluation: parametric (closed-book
recall, edit differential, memorization/privacy), contextual (position curves
and the mid-sequence drop), external (answer correctness vs snippet
attribution/faithfulness), and procedural/episodic (cross-session consistency
and timeline replay, E MARS+). The framework integrates temporal governance and
leakage auditing (freshness hits, outdated answers, refusal slices) and
uncertainty reporting via inter-rater agreement plus paired tests with
multiple-comparison correction. For updating and forgetting, we present DMM
Gov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),
and RAG to form an auditable loop covering admission thresholds, rollout,
monitoring, rollback, and change audits, with specs for timeliness, conflict
handling, and long-horizon consistency. Finally, we give four testable
propositions: minimum identifiability; a minimal evaluation card; causally
constrained editing with verifiable forgetting; and when retrieval with
small-window replay outperforms ultra-long-context reading. This yields a
reproducible, comparable, and governable coordinate system for research and
deployment.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªå…³äºLLMè®°å¿†çš„ç»Ÿä¸€æ“ä½œå®šä¹‰å’Œå››éƒ¨åˆ†åˆ†ç±»æ³•ï¼ˆå‚æ•°åŒ–ã€ä¸Šä¸‹æ–‡ã€å¤–éƒ¨ã€è¿‡ç¨‹/æƒ…æ™¯ï¼‰ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œå…¶ä¸­åŒ…æ‹¬å¤–éƒ¨è®°å¿†ï¼ˆä¸RAGç›¸å…³ï¼‰çš„è¯„ä¼°æ ‡å‡†ï¼Œå¦‚ç­”æ¡ˆæ­£ç¡®æ€§ä¸ç‰‡æ®µå½’å› /å¿ å®æ€§ã€‚è®ºæ–‡è¿˜è®¨è®ºäº†DMM Govæ¡†æ¶ï¼Œåè°ƒåŒ…æ‹¬RAGåœ¨å†…çš„å¤šç§æŠ€æœ¯å½¢æˆä¸€ä¸ªå¯å®¡è®¡çš„å¾ªç¯ï¼Œç”¨äºæ›´æ–°å’Œé—å¿˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.18667v1">TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Graph-based Retrieval-augmented generation (RAG) has become a widely studied
approach for improving the reasoning, accuracy, and factuality of Large
Language Models. However, many existing graph-based RAG systems overlook the
high cost associated with LLM token usage during graph construction, hindering
large-scale adoption. To address this, we propose TERAG, a simple yet effective
framework designed to build informative graphs at a significantly lower cost.
Inspired by HippoRAG, we incorporate Personalized PageRank (PPR) during the
retrieval phase, and we achieve at least 80% of the accuracy of widely used
graph-based RAG methods while consuming only 3%-11% of the output tokens.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºTERAGçš„ä½æˆæœ¬å›¾ç»“æ„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆä¸ªæ€§åŒ–PageRankï¼ˆPPRï¼‰ä¼˜åŒ–æ£€ç´¢é˜¶æ®µï¼Œå¤§å¹…å‡å°‘LLMå»ºå›¾æ—¶çš„tokenæ¶ˆè€—ï¼ˆé™è‡³3%-11%ï¼‰ï¼ŒåŒæ—¶ä¿æŒä¸»æµå›¾åŸºRAGæ–¹æ³•80%ä»¥ä¸Šçš„å‡†ç¡®æ€§ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-09-22
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2509.18054v1">A Knowledge Graph-based Retrieval-Augmented Generation Framework for Algorithm Selection in the Facility Layout Problem</a></td><td><details><summary>å±•å¼€</summary>Selecting a solution algorithm for the Facility Layout Problem (FLP), an
NP-hard optimization problem with a multiobjective trade-off, is a complex task
that requires deep expert knowledge. The performance of a given algorithm
depends on specific problem characteristics such as its scale, objectives, and
constraints. This creates a need for a data-driven recommendation method to
guide algorithm selection in automated design systems. This paper introduces a
new recommendation method to make such expertise accessible, based on a
Knowledge Graph-based Retrieval-Augmented Generation (KG RAG) framework. To
address this, a domain-specific knowledge graph is constructed from published
literature. The method then employs a multi-faceted retrieval mechanism to
gather relevant evidence from this knowledge graph using three distinct
approaches, which include a precise graph-based search, flexible vector-based
search, and high-level cluster-based search. The retrieved evidence is utilized
by a Large Language Model (LLM) to generate algorithm recommendations with
data-driven reasoning. The proposed KG-RAG method is compared against a
commercial LLM chatbot with access to the knowledge base as a table, across a
series of diverse, real-world FLP test cases. Based on recommendation accuracy
and reasoning capability, the proposed method performed significantly better
than the commercial LLM chatbot.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºçŸ¥è¯†å›¾è°±çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆKG-RAGï¼‰æ¡†æ¶ï¼Œç”¨äºä¸ºè®¾æ–½å¸ƒå±€é—®é¢˜ï¼ˆFLPï¼‰æ¨èåˆé€‚çš„ç®—æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºé¢†åŸŸç‰¹å®šçš„çŸ¥è¯†å›¾è°±ï¼Œç»“åˆå¤šæ–¹é¢çš„æ£€ç´¢æœºåˆ¶ï¼ˆåŒ…æ‹¬åŸºäºå›¾çš„ç²¾ç¡®æœç´¢ã€åŸºäºå‘é‡çš„çµæ´»æœç´¢å’ŒåŸºäºèšç±»çš„é«˜çº§æœç´¢ï¼‰ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆç®—æ³•æ¨èï¼Œå¹¶åœ¨çœŸå®FLPæ¡ˆä¾‹ä¸­éªŒè¯äº†å…¶ä¼˜äºå•†ç”¨LLMèŠå¤©æœºå™¨äººçš„æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.17788v1">One Agent to Serve All: a Lite-Adaptive Stylized AI Assistant for Millions of Multi-Style Official Accounts</a></td><td><details><summary>å±•å¼€</summary>Conversational agents deployed in industrial-scale official account platforms
must generate responses that are both contextually grounded and stylistically
aligned-requirements that existing methods struggle to meet. Chain-of-thought
(CoT) prompting induces significant latency due to multi-turn reasoning;
per-account fine-tuning is computationally prohibitive; and long prompt-based
methods degrade the model's ability to grasp injected context and style. In
this paper, we propose WeStar, a lite-adaptive framework for stylized
contextual question answering that scales to millions of official accounts.
WeStar combines context-grounded generation via RAG with style-aware generation
using Parametric RAG (PRAG), where LoRA modules are dynamically activated per
style cluster. Our contributions are fourfold: (1) We introduce WeStar, a
unified framework capable of serving large volumes of official accounts with
minimal overhead. (2) We propose a multi-dimensional, cluster-based parameter
sharing scheme that enables compact style representation while preserving
stylistic diversity. (3) We develop a style-enhanced Direct Preference
Optimization (SeDPO) method to optimize each style cluster's parameters for
improved generation quality. (4) Experiments on a large-scale industrial
dataset validate the effectiveness and efficiency of WeStar, underscoring its
pracitical value in real-world deployment.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†WeStaræ¡†æ¶ï¼Œç»“åˆRAGå’ŒParametric RAGï¼ˆPRAGï¼‰æŠ€æœ¯ï¼Œé€šè¿‡åŠ¨æ€æ¿€æ´»LoRAæ¨¡å—å®ç°é£æ ¼åŒ–ä¸Šä¸‹æ–‡é—®ç­”ï¼Œæ—¨åœ¨ä¸ºæµ·é‡å®˜æ–¹è´¦å·æä¾›ä½å»¶è¿Ÿã€é«˜é€‚åº”æ€§çš„ç”Ÿæˆè§£å†³æ–¹æ¡ˆã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.17671v1">Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications</a></td><td><details><summary>å±•å¼€</summary>The widespread adoption of Large Language Models (LLMs) has been hindered by
their tendency to hallucinate, generating plausible but factually incorrect
information. While Retrieval-Augmented Generation (RAG) systems attempt to
address this issue by grounding responses in external knowledge, hallucination
remains a persistent challenge, particularly for morphologically complex,
low-resource languages like Turkish. This paper introduces Turk-LettuceDetect,
the first suite of hallucination detection models specifically designed for
Turkish RAG applications. Building on the LettuceDetect framework, we formulate
hallucination detection as a token-level classification task and fine-tune
three distinct encoder architectures: a Turkish-specific ModernBERT,
TurkEmbed4STS, and multilingual EuroBERT. These models were trained on a
machine-translated version of the RAGTruth benchmark dataset containing 17,790
instances across question answering, data-to-text generation, and summarization
tasks. Our experimental results show that the ModernBERT-based model achieves
an F1-score of 0.7266 on the complete test set, with particularly strong
performance on structured tasks. The models maintain computational efficiency
while supporting long contexts up to 8,192 tokens, making them suitable for
real-time deployment. Comparative analysis reveals that while state-of-the-art
LLMs demonstrate high recall, they suffer from low precision due to
over-generation of hallucinated content, underscoring the necessity of
specialized detection mechanisms. By releasing our models and translated
dataset, this work addresses a critical gap in multilingual NLP and establishes
a foundation for developing more reliable and trustworthy AI applications for
Turkish and other languages.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡é’ˆå¯¹åœŸè€³å…¶è¯­ç­‰ä½èµ„æºè¯­è¨€ä¸­RAGç³»ç»Ÿçš„å¹»è§‰é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªåœŸè€³å…¶è¯­ä¸“ç”¨å¹»è§‰æ£€æµ‹æ¨¡å‹å¥—ä»¶Turk-LettuceDetectã€‚é€šè¿‡å¾®è°ƒä¸‰ç§ç¼–ç å™¨æ¶æ„å¹¶ä½¿ç”¨æœºå™¨ç¿»è¯‘çš„åŸºå‡†æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œé‡ç‚¹è§£å†³äº†é—®ç­”ã€æ•°æ®åˆ°æ–‡æœ¬ç”Ÿæˆå’Œæ‘˜è¦ä»»åŠ¡ä¸­çš„å¹»è§‰æ£€æµ‹é—®é¢˜ï¼Œå®éªŒè¡¨æ˜å…¶æ¨¡å‹åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶æœ‰æ•ˆæå‡äº†æ£€æµ‹æ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.17544v1">A Multimodal Conversational Assistant for the Characterization of Agricultural Plots from Geospatial Open Data</a></td><td><details><summary>å±•å¼€</summary>The increasing availability of open Earth Observation (EO) and agricultural
datasets holds great potential for supporting sustainable land management.
However, their high technical entry barrier limits accessibility for non-expert
users. This study presents an open-source conversational assistant that
integrates multimodal retrieval and large language models (LLMs) to enable
natural language interaction with heterogeneous agricultural and geospatial
data. The proposed architecture combines orthophotos, Sentinel-2 vegetation
indices, and user-provided documents through retrieval-augmented generation
(RAG), allowing the system to flexibly determine whether to rely on multimodal
evidence, textual knowledge, or both in formulating an answer. To assess
response quality, we adopt an LLM-as-a-judge methodology using Qwen3-32B in a
zero-shot, unsupervised setting, applying direct scoring in a multi-dimensional
quantitative evaluation framework. Preliminary results show that the system is
capable of generating clear, relevant, and context-aware responses to
agricultural queries, while remaining reproducible and scalable across
geographic regions. The primary contributions of this work include an
architecture for fusing multimodal EO and textual knowledge sources, a
demonstration of lowering the barrier to access specialized agricultural
information through natural language interaction, and an open and reproducible
design.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆå¤šæ¨¡æ€æ£€ç´¢ä¸å¤§è¯­è¨€æ¨¡å‹çš„å¼€æºå¯¹è¯åŠ©æ‰‹ï¼Œåˆ©ç”¨RAGæŠ€æœ¯æ•´åˆå†œä¸šä¸åœ°ç†ç©ºé—´æ•°æ®ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€äº¤äº’é™ä½éä¸“å®¶ç”¨æˆ·ä½¿ç”¨é—¨æ§›ï¼Œå¹¶é‡‡ç”¨LLMè¯„ä¼°æ–¹æ³•éªŒè¯å“åº”è´¨é‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.17486v1">AttnComp: Attention-Guided Adaptive Context Compression for Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented generation improves the factual accuracy of Large
Language Models (LLMs) by incorporating external context, but often suffers
from irrelevant retrieved content that hinders effectiveness. Context
compression addresses this issue by filtering out irrelevant information from
context before LLM generation. However, existing methods struggle to adaptively
adjust compression rates for different context, maintain low latency and
integrate information across multiple documents. To overcome these limitations,
We introduce AttnComp, an adaptive, efficient and context-aware compression
framework. By leveraging the attention mechanism of LLMs to identify relevant
information, AttnComp employs a Top-P compression algorithm to retain the
minimal set of documents whose cumulative attention weights exceeds a
predefined threshold. In addition to compression, AttnComp estimates response
confidence by assessing the overall relevance of the retrieved content,
enabling users to gauge response reliability. Experiments demonstrate that
AttnComp outperforms existing compression methods and uncompressed baselines,
achieving higher accuracy with substantial compression rates and lower latency.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºAttnCompçš„è‡ªé€‚åº”ã€é«˜æ•ˆä¸”ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å‹ç¼©æ¡†æ¶ï¼Œç”¨äºè§£å†³RAGä¸­æ£€ç´¢å†…å®¹å¯èƒ½æ— å…³å¯¼è‡´æ•ˆæœä¸‹é™çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶è¯†åˆ«ç›¸å…³ä¿¡æ¯ï¼Œå¹¶é€šè¿‡Top-På‹ç¼©ç®—æ³•ä¿ç•™å…³é”®æ–‡æ¡£ï¼ŒåŒæ—¶è¿˜èƒ½è¯„ä¼°å“åº”ç½®ä¿¡åº¦ä»¥æå‡å¯é æ€§ï¼Œå®éªŒè¯æ˜å…¶æ€§èƒ½ä¼˜äºç°æœ‰å‹ç¼©æ–¹æ³•å’Œæœªå‹ç¼©åŸºçº¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.17395v1">FinDebate: Multi-Agent Collaborative Intelligence for Financial Analysis</a></td><td><details><summary>å±•å¼€</summary>We introduce FinDebate, a multi-agent framework for financial analysis,
integrating collaborative debate with domain-specific Retrieval-Augmented
Generation (RAG). Five specialized agents, covering earnings, market,
sentiment, valuation, and risk, run in parallel to synthesize evidence into
multi-dimensional insights. To mitigate overconfidence and improve reliability,
we introduce a safe debate protocol that enables agents to challenge and refine
initial conclusions while preserving coherent recommendations. Experimental
results, based on both LLM-based and human evaluations, demonstrate the
framework's efficacy in producing high-quality analysis with calibrated
confidence levels and actionable investment strategies across multiple time
horizons.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªåä¸ºFinDebateçš„å¤šä»£ç†æ¡†æ¶ï¼Œç”¨äºé‡‘èåˆ†æï¼Œç»“åˆäº†åä½œè¾©è®ºå’Œç‰¹å®šé¢†åŸŸçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€‚äº”ä¸ªä¸“ä¸šä»£ç†å¹¶è¡Œå·¥ä½œï¼Œå°†è¯æ®åˆæˆä¸ºå¤šç»´åº¦çš„è§è§£ï¼Œå¹¶é€šè¿‡å®‰å…¨è¾©è®ºåè®®å‡å°‘è¿‡åº¦è‡ªä¿¡å¹¶æé«˜å¯é æ€§ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ¡†æ¶èƒ½ç”Ÿæˆé«˜è´¨é‡çš„åˆ†æå’Œå¯æ“ä½œçš„æŠ•èµ„ç­–ç•¥ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-09-21
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2509.17197v1">SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal Processing</a></td><td><details><summary>å±•å¼€</summary>Modern signal processing (SP) pipelines, whether model-based or data-driven,
often constrained by complex and fragmented workflow, rely heavily on expert
knowledge and manual engineering, and struggle with adaptability and
generalization under limited data. In contrast, Large Language Models (LLMs)
offer strong reasoning capabilities, broad general-purpose knowledge,
in-context learning, and cross-modal transfer abilities, positioning them as
powerful tools for automating and generalizing SP workflows. Motivated by these
potentials, we introduce SignalLLM, the first general-purpose LLM-based agent
framework for general SP tasks. Unlike prior LLM-based SP approaches that are
limited to narrow applications or tricky prompting, SignalLLM introduces a
principled, modular architecture. It decomposes high-level SP goals into
structured subtasks via in-context learning and domain-specific retrieval,
followed by hierarchical planning through adaptive retrieval-augmented
generation (RAG) and refinement; these subtasks are then executed through
prompt-based reasoning, cross-modal reasoning, code synthesis, model
invocation, or data-driven LLM-assisted modeling. Its generalizable design
enables the flexible selection of problem solving strategies across different
signal modalities, task types, and data conditions. We demonstrate the
versatility and effectiveness of SignalLLM through five representative tasks in
communication and sensing, such as radar target detection, human activity
recognition, and text compression. Experimental results show superior
performance over traditional and existing LLM-based methods, particularly in
few-shot and zero-shot settings.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†SignalLLMï¼Œä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é€šç”¨ä¿¡å·å¤„ç†ï¼ˆSPï¼‰ä»£ç†æ¡†æ¶ï¼Œå®ƒé€šè¿‡å¼•å…¥æ¨¡å—åŒ–æ¶æ„å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œå°†é«˜å±‚SPç›®æ ‡åˆ†è§£ä¸ºç»“æ„åŒ–çš„å­ä»»åŠ¡ï¼Œå¹¶ç»“åˆé¢†åŸŸç‰¹å®šæ£€ç´¢ã€åˆ†å±‚è§„åˆ’å’Œå¤šæ¨¡æ€æ¨ç†ï¼Œå®ç°äº†è·¨ä¿¡å·æ¨¡æ€å’Œä»»åŠ¡ç±»å‹çš„çµæ´»é—®é¢˜è§£å†³ã€‚å®éªŒè¯æ˜å…¶åœ¨å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬è®¾å®šä¸‹çš„ä¼˜è¶Šæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.17066v1">RALLM-POI: Retrieval-Augmented LLM for Zero-shot Next POI Recommendation with Geographical Reranking</a></td><td><details><summary>å±•å¼€</summary>Next point-of-interest (POI) recommendation predicts a user's next
destination from historical movements. Traditional models require intensive
training, while LLMs offer flexible and generalizable zero-shot solutions but
often generate generic or geographically irrelevant results due to missing
trajectory and spatial context. To address these issues, we propose RALLM-POI,
a framework that couples LLMs with retrieval-augmented generation and
self-rectification. We first propose a Historical Trajectory Retriever (HTR)
that retrieves relevant past trajectories to serve as contextual references,
which are then reranked by a Geographical Distance Reranker (GDR) for
prioritizing spatially relevant trajectories. Lastly, an Agentic LLM Rectifier
(ALR) is designed to refine outputs through self-reflection. Without additional
training, RALLM-POI achieves substantial accuracy gains across three real-world
Foursquare datasets, outperforming both conventional and LLM-based baselines.
Code is released at https://github.com/LKRcrocodile/RALLM-POI.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºRALLM-POIæ¡†æ¶ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œè‡ªçŸ«æ­£æŠ€æœ¯æ”¹è¿›åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸‹ä¸€ä¸ªå…´è¶£ç‚¹ï¼ˆPOIï¼‰æ¨èã€‚æ¡†æ¶åŒ…å«å†å²è½¨è¿¹æ£€ç´¢å™¨ï¼ˆHTRï¼‰ã€åœ°ç†è·ç¦»é‡æ’åºå™¨ï¼ˆGDRï¼‰å’ŒLLMä»£ç†çŸ«æ­£å™¨ï¼ˆALRï¼‰ï¼Œåˆ©ç”¨ç›¸å…³è½¨è¿¹ä½œä¸ºä¸Šä¸‹æ–‡è¾“å…¥LLMå¹¶è‡ªæˆ‘ä¼˜åŒ–è¾“å‡ºï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³æ˜¾è‘—æå‡æ¨èå‡†ç¡®æ€§ï¼Œåœ¨Foursquareæ•°æ®é›†ä¸Šè¶…è¶Šä¼ ç»Ÿå’ŒLLMåŸºçº¿æ–¹æ³•ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-09-20
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2509.16780v1">Comparing RAG and GraphRAG for Page-Level Retrieval Question Answering on Math Textbook</a></td><td><details><summary>å±•å¼€</summary>Technology-enhanced learning environments often help students retrieve
relevant learning content for questions arising during self-paced study. Large
language models (LLMs) have emerged as novel aids for information retrieval
during learning. While LLMs are effective for general-purpose
question-answering, they typically lack alignment with the domain knowledge of
specific course materials such as textbooks and slides. We investigate
Retrieval-Augmented Generation (RAG) and GraphRAG, a knowledge graph-enhanced
RAG approach, for page-level question answering in an undergraduate mathematics
textbook. While RAG has been effective for retrieving discrete, contextually
relevant passages, GraphRAG may excel in modeling interconnected concepts and
hierarchical knowledge structures. We curate a dataset of 477 question-answer
pairs, each tied to a distinct textbook page. We then compare the standard
embedding-based RAG methods to GraphRAG for evaluating both retrieval
accuracy-whether the correct page is retrieved-and generated answer quality via
F1 scores. Our findings show that embedding-based RAG achieves higher retrieval
accuracy and better F1 scores compared to GraphRAG, which tends to retrieve
excessive and sometimes irrelevant content due to its entity-based structure.
We also explored re-ranking the retrieved pages with LLM and observed mixed
results, including performance drop and hallucinations when dealing with larger
context windows. Overall, this study highlights both the promises and
challenges of page-level retrieval systems in educational contexts, emphasizing
the need for more refined retrieval methods to build reliable AI tutoring
solutions in providing reference page numbers.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’ŒçŸ¥è¯†å›¾è°±å¢å¼ºçš„GraphRAGæ–¹æ³•åœ¨æœ¬ç§‘æ•°å­¦æ•™ç§‘ä¹¦é¡µçº§é—®ç­”ä¸­çš„åº”ç”¨ï¼Œæ¯”è¾ƒäº†å®ƒä»¬åœ¨æ£€ç´¢å‡†ç¡®æ€§å’Œç”Ÿæˆç­”æ¡ˆè´¨é‡ä¸Šçš„è¡¨ç°ï¼Œå‘ç°åŸºäºåµŒå…¥çš„RAGä¼˜äºGraphRAGï¼Œå¹¶æ¢è®¨äº†æ•™è‚²åœºæ™¯ä¸­æ£€ç´¢ç³»ç»Ÿçš„æ½œåŠ›ä¸æŒ‘æˆ˜ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.16584v1">From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) have demonstrated promising performance on
medical benchmarks; however, their ability to perform medical calculations, a
crucial aspect of clinical decision-making, remains underexplored and poorly
evaluated. Existing benchmarks often assess only the final answer with a wide
numerical tolerance, overlooking systematic reasoning failures and potentially
causing serious clinical misjudgments. In this work, we revisit medical
calculation evaluation with a stronger focus on clinical trustworthiness.
First, we clean and restructure the MedCalc-Bench dataset and propose a new
step-by-step evaluation pipeline that independently assesses formula selection,
entity extraction, and arithmetic computation. Under this granular framework,
the accuracy of GPT-4o drops from 62.7% to 43.6%, revealing errors masked by
prior evaluations. Second, we introduce an automatic error analysis framework
that generates structured attribution for each failure mode. Human evaluation
confirms its alignment with expert judgment, enabling scalable and explainable
diagnostics. Finally, we propose a modular agentic pipeline, MedRaC, that
combines retrieval-augmented generation and Python-based code execution.
Without any fine-tuning, MedRaC improves the accuracy of different LLMs from
16.35% up to 53.19%. Our work highlights the limitations of current benchmark
practices and proposes a more clinically faithful methodology. By enabling
transparent and transferable reasoning evaluation, we move closer to making
LLM-based systems trustworthy for real-world medical applications.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»å­¦è®¡ç®—ä»»åŠ¡ä¸­çš„æ€§èƒ½é—®é¢˜ï¼Œæå‡ºæ”¹è¿›è¯„ä¼°æ–¹æ³•ï¼ˆMedCalc-Benchæ•°æ®é›†å’Œåˆ†æ­¥è¯„ä¼°æµç¨‹ï¼‰ï¼Œå‘ç°ç°æœ‰è¯„æµ‹æ©ç›–ç³»ç»Ÿæ€§é”™è¯¯ï¼ˆå¦‚GPT-4oå‡†ç¡®ç‡ä»62.7%é™è‡³43.6%ï¼‰ã€‚ä½œè€…å¼€å‘äº†è‡ªåŠ¨é”™è¯¯åˆ†ææ¡†æ¶ï¼Œå¹¶æå‡ºç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’ŒPythonä»£ç æ‰§è¡Œçš„æ¨¡å—åŒ–æµç¨‹MedRaCï¼Œæ˜¾è‘—æå‡ä¸åŒLLMçš„å‡†ç¡®ç‡ï¼ˆæœ€é«˜è¾¾53.19%ï¼‰ã€‚ç ”ç©¶å¼ºè°ƒä¸´åºŠå¯ä¿¡åº¦è¯„ä¼°çš„é‡è¦æ€§ï¼Œæ¨åŠ¨LLMåœ¨çœŸå®åŒ»ç–—åœºæ™¯çš„å¯é åº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.16508v1">Federated Learning with Ad-hoc Adapter Insertions: The Case of Soft-Embeddings for Training Classifier-as-Retriever</a></td><td><details><summary>å±•å¼€</summary>When existing retrieval-augmented generation (RAG) solutions are intended to
be used for new knowledge domains, it is necessary to update their encoders,
which are taken to be pretrained large language models (LLMs). However, fully
finetuning these large models is compute- and memory-intensive, and even
infeasible when deployed on resource-constrained edge devices. We propose a
novel encoder architecture in this work that addresses this limitation by using
a frozen small language model (SLM), which satisfies the memory constraints of
edge devices, and inserting a small adapter network before the transformer
blocks of the SLM. The trainable adapter takes the token embeddings of the new
corpus and learns to produce enhanced soft embeddings for it, while requiring
significantly less compute power to update than full fine-tuning. We further
propose a novel retrieval mechanism by attaching a classifier head to the SLM
encoder, which is trained to learn a similarity mapping of the input embeddings
to their corresponding documents. Finally, to enable the online fine-tuning of
both (i) the encoder soft embeddings and (ii) the classifier-as-retriever on
edge devices, we adopt federated learning (FL) and differential privacy (DP) to
achieve an efficient, privacy-preserving, and product-grade training solution.
We conduct a theoretical analysis of our methodology, establishing convergence
guarantees under mild assumptions on gradient variance when deployed for
general smooth nonconvex loss functions. Through extensive numerical
experiments, we demonstrate (i) the efficacy of obtaining soft embeddings to
enhance the encoder, (ii) training a classifier to improve the retriever, and
(iii) the role of FL in achieving speedup.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§é€‚ç”¨äºè¾¹ç¼˜è®¾å¤‡çš„æ–°å‹RAGç¼–ç å™¨æ¶æ„ï¼Œé‡‡ç”¨å†»ç»“çš„å°è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰å’Œé€‚é…å™¨ç½‘ç»œæ¥å‡å°‘è®¡ç®—å’Œå†…å­˜éœ€æ±‚ï¼ŒåŒæ—¶å¼•å…¥åŸºäºåˆ†ç±»å™¨çš„æ£€ç´¢æœºåˆ¶å’Œè”é‚¦å­¦ä¹ ï¼ˆFLï¼‰è¿›è¡Œéšç§ä¿æŠ¤å’Œé«˜æ•ˆåœ¨çº¿å¾®è°ƒï¼Œç†è®ºåˆ†æå’Œå®éªŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.16502v1">GRIL: Knowledge Graph Retrieval-Integrated Learning with Large Language Models</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has significantly mitigated the
hallucinations of Large Language Models (LLMs) by grounding the generation with
external knowledge. Recent extensions of RAG to graph-based retrieval offer a
promising direction, leveraging the structural knowledge for multi-hop
reasoning. However, existing graph RAG typically decouples retrieval and
reasoning processes, which prevents the retriever from adapting to the
reasoning needs of the LLM. They also struggle with scalability when performing
multi-hop expansion over large-scale graphs, or depend heavily on annotated
ground-truth entities, which are often unavailable in open-domain settings. To
address these challenges, we propose a novel graph retriever trained end-to-end
with LLM, which features an attention-based growing and pruning mechanism,
adaptively navigating multi-hop relevant entities while filtering out noise.
Within the extracted subgraph, structural knowledge and semantic features are
encoded via soft tokens and the verbalized graph, respectively, which are
infused into the LLM together, thereby enhancing its reasoning capability and
facilitating interactive joint training of the graph retriever and the LLM
reasoner. Experimental results across three QA benchmarks show that our
approach consistently achieves state-of-the-art performance, validating the
strength of joint graph-LLM optimization for complex reasoning tasks. Notably,
our framework eliminates the need for predefined ground-truth entities by
directly optimizing the retriever using LLM logits as implicit feedback, making
it especially effective in open-domain settings.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç«¯åˆ°ç«¯è®­ç»ƒæ–¹æ³•ï¼Œå°†åŸºäºå›¾çš„æ£€ç´¢å™¨ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è”åˆä¼˜åŒ–ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶åŠ¨æ€å¯¼èˆªå¤šè·³ç›¸å…³å®ä½“å¹¶è¿‡æ»¤å™ªå£°ï¼ŒåŒæ—¶èåˆç»“æ„çŸ¥è¯†å’Œè¯­ä¹‰ç‰¹å¾ä»¥å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†å¼€æ”¾é¢†åŸŸå¤æ‚é—®ç­”ä»»åŠ¡çš„è¡¨ç°ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-09-19
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2509.16112v1">CodeRAG: Finding Relevant and Necessary Knowledge for Retrieval-Augmented Repository-Level Code Completion</a></td><td><details><summary>å±•å¼€</summary>Repository-level code completion automatically predicts the unfinished code
based on the broader information from the repository. Recent strides in Code
Large Language Models (code LLMs) have spurred the development of
repository-level code completion methods, yielding promising results.
Nevertheless, they suffer from issues such as inappropriate query construction,
single-path code retrieval, and misalignment between code retriever and code
LLM. To address these problems, we introduce CodeRAG, a framework tailored to
identify relevant and necessary knowledge for retrieval-augmented
repository-level code completion. Its core components include log probability
guided query construction, multi-path code retrieval, and preference-aligned
BestFit reranking. Extensive experiments on benchmarks ReccEval and CCEval
demonstrate that CodeRAG significantly and consistently outperforms
state-of-the-art methods. The implementation of CodeRAG is available at
https://github.com/KDEGroup/CodeRAG.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºCodeRAGçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ä»“åº“çº§ä»£ç è¡¥å…¨æ–¹æ³•ä¸­å­˜åœ¨çš„é—®é¢˜ï¼Œå¦‚ä¸æ°å½“çš„æŸ¥è¯¢æ„å»ºã€å•ä¸€è·¯å¾„çš„ä»£ç æ£€ç´¢ä»¥åŠä»£ç æ£€ç´¢å™¨ä¸å¤§è¯­è¨€æ¨¡å‹ä¹‹é—´çš„ä¸å¯¹é½ã€‚CodeRAGé€šè¿‡æ¦‚ç‡å¼•å¯¼çš„æŸ¥è¯¢æ„å»ºã€å¤šè·¯å¾„ä»£ç æ£€ç´¢å’Œåå¥½å¯¹é½çš„BestFité‡æ’åºç­‰æ ¸å¿ƒç»„ä»¶ï¼Œæå‡äº†æ£€ç´¢å¢å¼ºçš„ä»“åº“çº§ä»£ç è¡¥å…¨çš„æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼ŒCodeRAGåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.15883v1">RACap: Relation-Aware Prompting for Lightweight Retrieval-Augmented Image Captioning</a></td><td><details><summary>å±•å¼€</summary>Recent retrieval-augmented image captioning methods incorporate external
knowledge to compensate for the limitations in comprehending complex scenes.
However, current approaches face challenges in relation modeling: (1) the
representation of semantic prompts is too coarse-grained to capture
fine-grained relationships; (2) these methods lack explicit modeling of image
objects and their semantic relationships. To address these limitations, we
propose RACap, a relation-aware retrieval-augmented model for image captioning,
which not only mines structured relation semantics from retrieval captions, but
also identifies heterogeneous objects from the image. RACap effectively
retrieves structured relation features that contain heterogeneous visual
information to enhance the semantic consistency and relational expressiveness.
Experimental results show that RACap, with only 10.8M trainable parameters,
achieves superior performance compared to previous lightweight captioning
models.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRACapçš„å…³ç³»æ„ŸçŸ¥æ£€ç´¢å¢å¼ºå›¾åƒæè¿°ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡ä»æ£€ç´¢åˆ°çš„æè¿°ä¸­æŒ–æ˜ç»“æ„åŒ–å…³ç³»è¯­ä¹‰å¹¶è¯†åˆ«å›¾åƒä¸­çš„å¼‚æ„å¯¹è±¡ï¼Œä»¥æå‡è¯­ä¹‰ä¸€è‡´æ€§å’Œå…³ç³»è¡¨è¾¾èƒ½åŠ›ï¼Œå®éªŒæ˜¾ç¤ºå…¶åœ¨è½»é‡çº§æ¨¡å‹ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.15577v1">Relevance to Utility: Process-Supervised Rewrite for RAG</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation systems often suffer from a gap between
optimizing retrieval relevance and generative utility: retrieved documents may
be topically relevant but still lack the content needed for effective reasoning
during generation. While existing "bridge" modules attempt to rewrite the
retrieved text for better generation, we show how they fail to capture true
document utility. In this work, we propose R2U, with a key distinction of
directly optimizing to maximize the probability of generating a correct answer
through process supervision. As such direct observation is expensive, we also
propose approximating an efficient distillation pipeline by scaling the
supervision from LLMs, which helps the smaller rewriter model generalize
better. We evaluate our method across multiple open-domain question-answering
benchmarks. The empirical results demonstrate consistent improvements over
strong bridging baselines.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºR2Uçš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³RAGç³»ç»Ÿä¸­æ£€ç´¢ç›¸å…³æ€§ä¸ç”Ÿæˆæ•ˆç”¨ä¹‹é—´çš„ä¸ä¸€è‡´é—®é¢˜ã€‚é€šè¿‡ç›´æ¥ä¼˜åŒ–ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆçš„æ¦‚ç‡ï¼Œå¹¶åˆ©ç”¨LLMçš„ç›‘ç£ä¿¡å·æ¥é«˜æ•ˆè®­ç»ƒè¾ƒå°çš„é‡å†™æ¨¡å‹ï¼Œè®ºæ–‡åœ¨å¤šä¸ªå¼€æ”¾åŸŸé—®ç­”åŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•çš„æ€§èƒ½ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-09-18
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2509.15211v1">What's the Best Way to Retrieve Slides? A Comparative Study of Multimodal, Caption-Based, and Hybrid Retrieval Techniques</a></td><td><details><summary>å±•å¼€</summary>Slide decks, serving as digital reports that bridge the gap between
presentation slides and written documents, are a prevalent medium for conveying
information in both academic and corporate settings. Their multimodal nature,
combining text, images, and charts, presents challenges for retrieval-augmented
generation systems, where the quality of retrieval directly impacts downstream
performance. Traditional approaches to slide retrieval often involve separate
indexing of modalities, which can increase complexity and lose contextual
information. This paper investigates various methodologies for effective slide
retrieval, including visual late-interaction embedding models like ColPali, the
use of visual rerankers, and hybrid retrieval techniques that combine dense
retrieval with BM25, further enhanced by textual rerankers and fusion methods
like Reciprocal Rank Fusion. A novel Vision-Language Models-based captioning
pipeline is also evaluated, demonstrating significantly reduced embedding
storage requirements compared to visual late-interaction techniques, alongside
comparable retrieval performance. Our analysis extends to the practical aspects
of these methods, evaluating their runtime performance and storage demands
alongside retrieval efficacy, thus offering practical guidance for the
selection and development of efficient and robust slide retrieval systems for
real-world applications.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¹»ç¯ç‰‡ï¼ˆåŒ…å«æ–‡æœ¬ã€å›¾åƒå’Œå›¾è¡¨ï¼‰çš„é«˜æ•ˆæ£€ç´¢æ–¹æ³•ï¼Œæ¢è®¨äº†è§†è§‰å»¶è¿Ÿäº¤äº’åµŒå…¥æ¨¡å‹ã€è§†è§‰é‡æ’åºå™¨ã€æ··åˆæ£€ç´¢æŠ€æœ¯ï¼ˆç»“åˆç¨ å¯†æ£€ç´¢ä¸BM25ï¼‰ç­‰æ–¹æ¡ˆï¼Œå¹¶æå‡ºåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ ‡é¢˜ç”Ÿæˆæµç¨‹ï¼Œåœ¨ä¿è¯æ£€ç´¢æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½å­˜å‚¨éœ€æ±‚ï¼Œä¸ºRAGç³»ç»Ÿä¸­å¹»ç¯ç‰‡æ£€ç´¢çš„å®é™…åº”ç”¨æä¾›æ•ˆèƒ½è¯„ä¼°ä¸å¼€å‘æŒ‡å¯¼ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.15159v1">AIP: Subverting Retrieval-Augmented Generation via Adversarial Instructional Prompt</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
retrieving relevant documents from external sources to improve factual accuracy
and verifiability. However, this reliance introduces new attack surfaces within
the retrieval pipeline, beyond the LLM itself. While prior RAG attacks have
exposed such vulnerabilities, they largely rely on manipulating user queries,
which is often infeasible in practice due to fixed or protected user inputs.
This narrow focus overlooks a more realistic and stealthy vector: instructional
prompts, which are widely reused, publicly shared, and rarely audited. Their
implicit trust makes them a compelling target for adversaries to manipulate RAG
behavior covertly.
  We introduce a novel attack for Adversarial Instructional Prompt (AIP) that
exploits adversarial instructional prompts to manipulate RAG outputs by subtly
altering retrieval behavior. By shifting the attack surface to the
instructional prompts, AIP reveals how trusted yet seemingly benign interface
components can be weaponized to degrade system integrity. The attack is crafted
to achieve three goals: (1) naturalness, to evade user detection; (2) utility,
to encourage use of prompts; and (3) robustness, to remain effective across
diverse query variations. We propose a diverse query generation strategy that
simulates realistic linguistic variation in user queries, enabling the
discovery of prompts that generalize across paraphrases and rephrasings.
Building on this, a genetic algorithm-based joint optimization is developed to
evolve adversarial prompts by balancing attack success, clean-task utility, and
stealthiness. Experimental results show that AIP achieves up to 95.23% ASR
while preserving benign functionality. These findings uncover a critical and
previously overlooked vulnerability in RAG systems, emphasizing the need to
reassess the shared instructional prompts.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†RAGç³»ç»Ÿä¸­çš„æ–°å‹æ”»å‡»æ–¹å¼Adversarial Instructional Prompt (AIP)ï¼Œé€šè¿‡æ“çºµå¹¿æ³›å¤ç”¨ä¸”æœªè¢«å®¡è®¡çš„æŒ‡ä»¤æç¤ºï¼ˆè€Œéç›´æ¥ç¯¡æ”¹ç”¨æˆ·æŸ¥è¯¢ï¼‰ï¼Œéšç§˜åœ°æ”¹å˜æ£€ç´¢è¡Œä¸ºä»¥æ“æ§è¾“å‡ºã€‚ç ”ç©¶æå‡ºåŸºäºç”Ÿæˆå¤šæ ·æŸ¥è¯¢å’Œé—ä¼ ç®—æ³•çš„è”åˆä¼˜åŒ–æ–¹æ³•ï¼Œæ­ç¤ºRAGä¸­åŸºäºæŒ‡ä»¤æç¤ºçš„å®‰å…¨æ¼æ´ï¼Œå®éªŒæ˜¾ç¤ºAIPæ”»å‡»æˆåŠŸç‡é«˜è¾¾95.23%ä¸”ä¿æŒæ­£å¸¸åŠŸèƒ½ï¼Œå¼ºè°ƒäº†é‡æ–°è¯„ä¼°å…±äº«æç¤ºé£é™©çš„å¿…è¦æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.14956v1">Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems</a></td><td><details><summary>å±•å¼€</summary>This paper proposes a novel architectural framework aimed at enhancing
security and reliability in multi-agent systems (MAS). A central component of
this framework is a network of Sentinel Agents, functioning as a distributed
security layer that integrates techniques such as semantic analysis via large
language models (LLMs), behavioral analytics, retrieval-augmented verification,
and cross-agent anomaly detection. Such agents can potentially oversee
inter-agent communications, identify potential threats, enforce privacy and
access controls, and maintain comprehensive audit records. Complementary to the
idea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator
Agent supervises policy implementation, and manages agent participation. In
addition, the Coordinator also ingests alerts from Sentinel Agents. Based on
these alerts, it can adapt policies, isolate or quarantine misbehaving agents,
and contain threats to maintain the integrity of the MAS ecosystem. This
dual-layered security approach, combining the continuous monitoring of Sentinel
Agents with the governance functions of Coordinator Agents, supports dynamic
and adaptive defense mechanisms against a range of threats, including prompt
injection, collusive agent behavior, hallucinations generated by LLMs, privacy
breaches, and coordinated multi-agent attacks. In addition to the architectural
design, we present a simulation study where 162 synthetic attacks of different
families (prompt injection, hallucination, and data exfiltration) were injected
into a multi-agent conversational environment. The Sentinel Agents successfully
detected the attack attempts, confirming the practical feasibility of the
proposed monitoring approach. The framework also offers enhanced system
observability, supports regulatory compliance, and enables policy evolution
over time.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å¢å¼ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰å®‰å…¨æ€§å’Œå¯é æ€§çš„æ–°å‹æ¶æ„æ¡†æ¶ï¼Œå…¶ä¸­åŒ…å«åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œè¯­ä¹‰åˆ†æã€æ£€ç´¢å¢å¼ºéªŒè¯ç­‰æŠ€æœ¯ã€‚Sentinel Agentsä½œä¸ºåˆ†å¸ƒå¼å®‰å…¨å±‚ç›‘æ§é€šä¿¡å¹¶è¯†åˆ«å¨èƒï¼ŒCoordinator Agentåˆ™å®æ–½ç­–ç•¥ç®¡ç†å’Œå¨èƒå“åº”ï¼Œå¹¶é€šè¿‡ä»¿çœŸéªŒè¯äº†è¯¥æ¡†æ¶å¯¹æŠ—å¤šç§æ”»å‡»ï¼ˆå¦‚æç¤ºæ³¨å…¥ã€å¹»è§‰ç”Ÿæˆï¼‰çš„æœ‰æ•ˆæ€§ã€‚å…¶æ£€ç´¢å¢å¼ºéªŒè¯ï¼ˆretrieval-augmented verificationï¼‰æŠ€æœ¯æ˜ç¡®ä½“ç°äº†RAGçš„åº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.14750v1">Enhancing Retrieval Augmentation via Adversarial Collaboration</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented Generation (RAG) is a prevalent approach for
domain-specific LLMs, yet it is often plagued by "Retrieval Hallucinations"--a
phenomenon where fine-tuned models fail to recognize and act upon poor-quality
retrieved documents, thus undermining performance. To address this, we propose
the Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two
heterogeneous agents: a generalist Detector that identifies knowledge gaps, and
a domain-specialized Resolver that provides precise solutions. Guided by a
moderator, these agents engage in an adversarial collaboration, where the
Detector's persistent questioning challenges the Resolver's expertise. This
dynamic process allows for iterative problem dissection and refined knowledge
retrieval. Extensive experiments show that AC-RAG significantly improves
retrieval accuracy and outperforms state-of-the-art RAG methods across various
vertical domains.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºä¸€ç§åä¸ºAC-RAGçš„æ–°æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥å¯¹æŠ—æ€§åä½œæœºåˆ¶ï¼ˆåŒ…å«é€šç”¨æ£€æµ‹å™¨å’Œé¢†åŸŸä¸“å®¶è§£æå™¨ä¸¤ä¸ªå¼‚æ„ä»£ç†ï¼‰ï¼Œæœ‰æ•ˆè§£å†³RAGä¸­å­˜åœ¨çš„"æ£€ç´¢å¹»è§‰"é—®é¢˜ï¼Œå³æ¨¡å‹æ— æ³•è¯†åˆ«ä½è´¨é‡æ£€ç´¢æ–‡æ¡£çš„ç¼ºé™·ã€‚å®éªŒè¡¨æ˜AC-RAGåœ¨æ£€ç´¢å‡†ç¡®æ€§å’Œå‚ç›´é¢†åŸŸæ€§èƒ½ä¸Šè¶…è¶Šç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.14623v1">Automating Modelica Module Generation Using Large Language Models: A Case Study on Building Control Description Language</a></td><td><details><summary>å±•å¼€</summary>Dynamic energy systems and controls require advanced modeling frameworks to
design and test supervisory and fault tolerant strategies. Modelica is a widely
used equation based language, but developing control modules is labor intensive
and requires specialized expertise. This paper examines the use of large
language models (LLMs) to automate the generation of Control Description
Language modules in the Building Modelica Library as a case study. We developed
a structured workflow that combines standardized prompt scaffolds, library
aware grounding, automated compilation with OpenModelica, and human in the loop
evaluation. Experiments were carried out on four basic logic tasks (And, Or,
Not, and Switch) and five control modules (chiller enable/disable, bypass valve
control, cooling tower fan speed, plant requests, and relief damper control).
The results showed that GPT 4o failed to produce executable Modelica code in
zero shot mode, while Claude Sonnet 4 achieved up to full success for basic
logic blocks with carefully engineered prompts. For control modules, success
rates reached 83 percent, and failed outputs required medium level human repair
(estimated one to eight hours). Retrieval augmented generation often produced
mismatches in module selection (for example, And retrieved as Or), while a
deterministic hard rule search strategy avoided these errors. Human evaluation
also outperformed AI evaluation, since current LLMs cannot assess simulation
results or validate behavioral correctness. Despite these limitations, the LLM
assisted workflow reduced the average development time from 10 to 20 hours down
to 4 to 6 hours per module, corresponding to 40 to 60 percent time savings.
These results highlight both the potential and current limitations of LLM
assisted Modelica generation, and point to future research in pre simulation
validation, stronger grounding, and closed loop evaluation.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯è‡ªåŠ¨åŒ–ç”ŸæˆModelicaæ§åˆ¶æ¨¡å—çš„æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆæ ‡å‡†åŒ–æç¤ºæ¡†æ¶ã€åº“æ„ŸçŸ¥åŸºç¡€ã€è‡ªåŠ¨ç¼–è¯‘å’Œäººå·¥è¯„ä¼°ï¼Œæ˜¾è‘—å‡å°‘äº†å¼€å‘æ—¶é—´ï¼ŒåŒæ—¶æŒ‡å‡ºäº†RAGåœ¨æ¨¡å—é€‰æ‹©ä¸Šçš„å±€é™æ€§ä»¥åŠæœªæ¥æ”¹è¿›æ–¹å‘ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.14622v1">Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection</a></td><td><details><summary>å±•å¼€</summary>With the deployment of Large Language Models (LLMs) in interactive
applications, online malicious intent detection has become increasingly
critical. However, existing approaches fall short of handling diverse and
complex user queries in real time. To address these challenges, we introduce
ADRAG (Adversarial Distilled Retrieval-Augmented Guard), a two-stage framework
for robust and efficient online malicious intent detection. In the training
stage, a high-capacity teacher model is trained on adversarially perturbed,
retrieval-augmented inputs to learn robust decision boundaries over diverse and
complex user queries. In the inference stage, a distillation scheduler
transfers the teacher's knowledge into a compact student model, with a
continually updated knowledge base collected online. At deployment, the compact
student model leverages top-K similar safety exemplars retrieved from the
online-updated knowledge base to enable both online and real-time malicious
query detection. Evaluations across ten safety benchmarks demonstrate that
ADRAG, with a 149M-parameter model, achieves 98.5% of WildGuard-7B's
performance, surpasses GPT-4 by 3.3% and Llama-Guard-3-8B by 9.5% on
out-of-distribution detection, while simultaneously delivering up to 5.6x lower
latency at 300 queries per second (QPS) in real-time applications.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ADRAGï¼ˆAdversarial Distilled Retrieval-Augmented Guardï¼‰ï¼Œä¸€ç§ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œå¯¹æŠ—è’¸é¦çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºå®æ—¶åœ¨çº¿æ¶æ„æ„å›¾æ£€æµ‹ã€‚é€šè¿‡è®­ç»ƒé˜¶æ®µåˆ©ç”¨æ£€ç´¢å¢å¼ºçš„å¯¹æŠ—æ‰°åŠ¨è¾“å…¥è®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µå°†çŸ¥è¯†è’¸é¦åˆ°è½»é‡çº§å­¦ç”Ÿæ¨¡å‹ä¸­ï¼Œå…¶åœ¨çº¿æ›´æ–°çš„çŸ¥è¯†åº“æ”¯æŒå®æ—¶æ£€ç´¢Top-Kç›¸ä¼¼å®‰å…¨ç¤ºä¾‹ï¼Œæ˜¾è‘—æå‡äº†æ¶æ„æŸ¥è¯¢æ£€æµ‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.14608v1">Enterprise AI Must Enforce Participant-Aware Access Control</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) are increasingly deployed in enterprise settings
where they interact with multiple users and are trained or fine-tuned on
sensitive internal data. While fine-tuning enhances performance by
internalizing domain knowledge, it also introduces a critical security risk:
leakage of confidential training data to unauthorized users. These risks are
exacerbated when LLMs are combined with Retrieval-Augmented Generation (RAG)
pipelines that dynamically fetch contextual documents at inference time.
  We demonstrate data exfiltration attacks on AI assistants where adversaries
can exploit current fine-tuning and RAG architectures to leak sensitive
information by leveraging the lack of access control enforcement. We show that
existing defenses, including prompt sanitization, output filtering, system
isolation, and training-level privacy mechanisms, are fundamentally
probabilistic and fail to offer robust protection against such attacks.
  We take the position that only a deterministic and rigorous enforcement of
fine-grained access control during both fine-tuning and RAG-based inference can
reliably prevent the leakage of sensitive data to unauthorized recipients.
  We introduce a framework centered on the principle that any content used in
training, retrieval, or generation by an LLM is explicitly authorized for
\emph{all users involved in the interaction}. Our approach offers a simple yet
powerful paradigm shift for building secure multi-user LLM systems that are
grounded in classical access control but adapted to the unique challenges of
modern AI workflows. Our solution has been deployed in Microsoft Copilot
Tuning, a product offering that enables organizations to fine-tune models using
their own enterprise-specific data.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æ¢è®¨äº†åœ¨ä¼ä¸šç¯å¢ƒä¸­éƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç®¡é“æ—¶é¢ä¸´çš„æ•°æ®å®‰å…¨é£é™©ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç»†ç²’åº¦è®¿é—®æ§åˆ¶çš„æ¡†æ¶ï¼Œä»¥é˜²æ­¢æ•æ„Ÿä¿¡æ¯æ³„éœ²ç»™æœªç»æˆæƒçš„ç”¨æˆ·ï¼Œå¹¶å·²åœ¨Microsoft Copilot Tuningä¸­éƒ¨ç½²åº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.14507v1">DeKeyNLU: Enhancing Natural Language to SQL Generation through Task Decomposition and Keyword Extraction</a></td><td><details><summary>å±•å¼€</summary>Natural Language to SQL (NL2SQL) provides a new model-centric paradigm that
simplifies database access for non-technical users by converting natural
language queries into SQL commands. Recent advancements, particularly those
integrating Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)
reasoning, have made significant strides in enhancing NL2SQL performance.
However, challenges such as inaccurate task decomposition and keyword
extraction by LLMs remain major bottlenecks, often leading to errors in SQL
generation. While existing datasets aim to mitigate these issues by fine-tuning
models, they struggle with over-fragmentation of tasks and lack of
domain-specific keyword annotations, limiting their effectiveness. To address
these limitations, we present DeKeyNLU, a novel dataset which contains 1,500
meticulously annotated QA pairs aimed at refining task decomposition and
enhancing keyword extraction precision for the RAG pipeline. Fine-tuned with
DeKeyNLU, we propose DeKeySQL, a RAG-based NL2SQL pipeline that employs three
distinct modules for user question understanding, entity retrieval, and
generation to improve SQL generation accuracy. We benchmarked multiple model
configurations within DeKeySQL RAG pipeline. Experimental results demonstrate
that fine-tuning with DeKeyNLU significantly improves SQL generation accuracy
on both BIRD (62.31% to 69.10%) and Spider (84.2% to 88.7%) dev datasets.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºDeKeyNLUæ•°æ®é›†å’ŒDeKeySQLç®¡é“ï¼Œé€šè¿‡æ”¹è¿›ä»»åŠ¡åˆ†è§£å’Œå…³é”®è¯æå–å¢å¼ºRAGåœ¨è‡ªç„¶è¯­è¨€è½¬SQLï¼ˆNL2SQLï¼‰ä¸­çš„æ€§èƒ½ï¼Œå®éªŒæ˜¾ç¤ºå…¶æ˜¾è‘—æå‡äº†BIRDå’ŒSpideræ•°æ®é›†ä¸Šçš„SQLç”Ÿæˆå‡†ç¡®ç‡ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-09-17
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2509.14436v1">When Content is Goliath and Algorithm is David: The Style and Semantic Effects of Generative Search Engine</a></td><td><details><summary>å±•å¼€</summary>Generative search engines (GEs) leverage large language models (LLMs) to
deliver AI-generated summaries with website citations, establishing novel
traffic acquisition channels while fundamentally altering the search engine
optimization landscape. To investigate the distinctive characteristics of GEs,
we collect data through interactions with Google's generative and conventional
search platforms, compiling a dataset of approximately ten thousand websites
across both channels. Our empirical analysis reveals that GEs exhibit
preferences for citing content characterized by significantly higher
predictability for underlying LLMs and greater semantic similarity among
selected sources. Through controlled experiments utilizing retrieval augmented
generation (RAG) APIs, we demonstrate that these citation preferences emerge
from intrinsic LLM tendencies to favor content aligned with their generative
expression patterns. Motivated by applications of LLMs to optimize website
content, we conduct additional experimentation to explore how LLM-based content
polishing by website proprietors alters AI summaries, finding that such
polishing paradoxically enhances information diversity within AI summaries.
Finally, to assess the user-end impact of LLM-induced information increases, we
design a generative search engine and recruit Prolific participants to conduct
a randomized controlled experiment involving an information-seeking and writing
task. We find that higher-educated users exhibit minimal changes in their final
outputs' information diversity but demonstrate significantly reduced task
completion time when original sites undergo polishing. Conversely,
lower-educated users primarily benefit through enhanced information density in
their task outputs while maintaining similar completion times across
experimental groups.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ç ”ç©¶ç”Ÿæˆå¼æœç´¢å¼•æ“ï¼ˆGEsï¼‰çš„ç‰¹ç‚¹åŠå…¶å¼•ç”¨åå¥½ï¼Œå‘ç°GEså€¾å‘äºå¼•ç”¨ä¸åº•å±‚LLMç”Ÿæˆè¡¨è¾¾æ¨¡å¼ä¸€è‡´çš„å†…å®¹ï¼Œå¹¶é€šè¿‡RAG APIå®éªŒéªŒè¯äº†è¿™ä¸€åå¥½æºè‡ªLLMçš„å†…åœ¨å€¾å‘ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢è®¨äº†ç½‘ç«™æ‰€æœ‰è€…é€šè¿‡LLMä¼˜åŒ–å†…å®¹å¯¹AIæ‘˜è¦çš„å½±å“ï¼Œå¹¶è¯„ä¼°äº†ä¸åŒæ•™è‚²èƒŒæ™¯ç”¨æˆ·åœ¨ä½¿ç”¨GEsæ—¶çš„è¡¨ç°å·®å¼‚ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.14435v1">Causal-Counterfactual RAG: The Integration of Causal-Counterfactual Reasoning into RAG</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) have transformed natural language processing
(NLP), enabling diverse applications by integrating large-scale pre-trained
knowledge. However, their static knowledge limits dynamic reasoning over
external information, especially in knowledge-intensive domains.
Retrieval-Augmented Generation (RAG) addresses this challenge by combining
retrieval mechanisms with generative modeling to improve contextual
understanding. Traditional RAG systems suffer from disrupted contextual
integrity due to text chunking and over-reliance on semantic similarity for
retrieval, often resulting in shallow and less accurate responses. We propose
Causal-Counterfactual RAG, a novel framework that integrates explicit causal
graphs representing cause-effect relationships into the retrieval process and
incorporates counterfactual reasoning grounded on the causal structure. Unlike
conventional methods, our framework evaluates not only direct causal evidence
but also the counterfactuality of associated causes, combining results from
both to generate more robust, accurate, and interpretable answers. By
leveraging causal pathways and associated hypothetical scenarios,
Causal-Counterfactual RAG preserves contextual coherence, reduces
hallucination, and enhances reasoning fidelity.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºCausal-Counterfactual RAGçš„æ–°æ¡†æ¶ï¼Œé€šè¿‡å°†æ˜¾å¼å› æœå›¾æ•´åˆåˆ°æ£€ç´¢è¿‡ç¨‹ä¸­å¹¶å¼•å…¥åŸºäºå› æœç»“æ„çš„åäº‹å®æ¨ç†ï¼Œè§£å†³äº†ä¼ ç»ŸRAGç³»ç»Ÿå› æ–‡æœ¬åˆ†å—å’Œè¿‡åº¦ä¾èµ–è¯­ä¹‰ç›¸ä¼¼æ€§è€Œå¯¼è‡´çš„ä¸Šä¸‹æ–‡ä¸è¿è´¯å’Œå›ç­”æµ…æ˜¾çš„é—®é¢˜ï¼Œä»è€Œç”Ÿæˆæ›´å‡†ç¡®ã€é²æ£’ä¸”å¯è§£é‡Šçš„ç­”æ¡ˆã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.13978v1">LLM Agents for Interactive Workflow Provenance: Reference Architecture and Evaluation Methodology</a></td><td><details><summary>å±•å¼€</summary>Modern scientific discovery increasingly relies on workflows that process
data across the Edge, Cloud, and High Performance Computing (HPC) continuum.
Comprehensive and in-depth analyses of these data are critical for hypothesis
validation, anomaly detection, reproducibility, and impactful findings.
Although workflow provenance techniques support such analyses, at large scale,
the provenance data become complex and difficult to analyze. Existing systems
depend on custom scripts, structured queries, or static dashboards, limiting
data interaction. In this work, we introduce an evaluation methodology,
reference architecture, and open-source implementation that leverages
interactive Large Language Model (LLM) agents for runtime data analysis. Our
approach uses a lightweight, metadata-driven design that translates natural
language into structured provenance queries. Evaluations across LLaMA, GPT,
Gemini, and Claude, covering diverse query classes and a real-world chemistry
workflow, show that modular design, prompt tuning, and Retrieval-Augmented
Generation (RAG) enable accurate and insightful LLM agent responses beyond
recorded provenance.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨äº¤äº’å¼å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†è¿›è¡Œè¿è¡Œæ—¶æ•°æ®åˆ†æçš„æ–¹æ³•ï¼Œé‡‡ç”¨è½»é‡çº§ã€ä»¥å…ƒæ•°æ®é©±åŠ¨çš„è®¾è®¡å°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºç»“æ„åŒ–çš„æº¯æºæŸ¥è¯¢ï¼Œå¹¶é€šè¿‡å¯¹æ¯”å®éªŒï¼ˆæ¶µç›–å¤šç§LLMæ¨¡å‹åŠå®é™…åŒ–å­¦å·¥ä½œæµï¼‰è¯æ˜ï¼Œå…¶æ¨¡å—åŒ–è®¾è®¡ã€æç¤ºè°ƒä¼˜åŠæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯èƒ½æ˜¾è‘—æå‡LLMä»£ç†å“åº”çš„å‡†ç¡®æ€§å’Œæ´å¯ŸåŠ›ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿè®°å½•çš„æº¯æºæ•°æ®èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.13930v1">Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG</a></td><td><details><summary>å±•å¼€</summary>Multilingual Retrieval-Augmented Generation (mRAG) systems enable language
models to answer knowledge-intensive queries with citation-supported responses
across languages. While such systems have been proposed, an open questions is
whether the mixture of different document languages impacts generation and
citation in unintended ways. To investigate, we introduce a controlled
methodology using model internals to measure language preference while holding
other factors such as document relevance constant. Across eight languages and
six open-weight models, we find that models preferentially cite English sources
when queries are in English, with this bias amplified for lower-resource
languages and for documents positioned mid-context. Crucially, we find that
models sometimes trade-off document relevance for language preference,
indicating that citation choices are not always driven by informativeness
alone. Our findings shed light on how language models leverage multilingual
context and influence citation behavior.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ç ”ç©¶å¤šè¯­è¨€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆmRAGï¼‰ç³»ç»Ÿä¸­è¯­è¨€åå¥½å¯¹ç”Ÿæˆå’Œå¼•ç”¨çš„å½±å“ï¼Œå‘ç°æ¨¡å‹å€¾å‘äºå¼•ç”¨è‹±æ–‡æ¥æºï¼Œä¸”å¯èƒ½ç‰ºç‰²æ–‡æ¡£ç›¸å…³æ€§è€Œé€‰æ‹©è¯­è¨€åå¥½ï¼Œæ­ç¤ºäº†è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€è¯­å¢ƒä¸­çš„å¼•ç”¨è¡Œä¸ºç‰¹ç‚¹ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.13772v1">Who Taught the Lie? Responsibility Attribution for Poisoned Knowledge in Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) integrates external knowledge into large
language models to improve response quality. However, recent work has shown
that RAG systems are highly vulnerable to poisoning attacks, where malicious
texts are inserted into the knowledge database to influence model outputs.
While several defenses have been proposed, they are often circumvented by more
adaptive or sophisticated attacks.
  This paper presents RAGOrigin, a black-box responsibility attribution
framework designed to identify which texts in the knowledge database are
responsible for misleading or incorrect generations. Our method constructs a
focused attribution scope tailored to each misgeneration event and assigns a
responsibility score to each candidate text by evaluating its retrieval
ranking, semantic relevance, and influence on the generated response. The
system then isolates poisoned texts using an unsupervised clustering method. We
evaluate RAGOrigin across seven datasets and fifteen poisoning attacks,
including newly developed adaptive poisoning strategies and multi-attacker
scenarios. Our approach outperforms existing baselines in identifying poisoned
content and remains robust under dynamic and noisy conditions. These results
suggest that RAGOrigin provides a practical and effective solution for tracing
the origins of corrupted knowledge in RAG systems.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºRAGOriginæ¡†æ¶ï¼Œé’ˆå¯¹RAGç³»ç»Ÿä¸­çŸ¥è¯†åº“ä¸­æ¯’æ”»å‡»å¯¼è‡´é”™è¯¯ç”Ÿæˆçš„é—®é¢˜ï¼Œé€šè¿‡é»‘ç›’è´£ä»»æº¯æºæ–¹æ³•åˆ†ææ£€ç´¢æ’åºã€è¯­ä¹‰ç›¸å…³æ€§å’Œç”Ÿæˆå“åº”å½±å“ï¼Œè¯†åˆ«å’Œéš”ç¦»æ¶æ„æ–‡æœ¬ï¼Œå¹¶åœ¨å¤šæ•°æ®é›†å’Œæ”»å‡»åœºæ™¯ä¸‹éªŒè¯å…¶ä¼˜äºç°æœ‰åŸºçº¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.13702v1">DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models</a></td><td><details><summary>å±•å¼€</summary>Large Language Model (LLM) hallucination is a significant barrier to their
reliable deployment. Current methods like Retrieval-Augmented Generation (RAG)
are often reactive. We introduce **Dynamic Self-reinforcing Calibration for
Hallucination Suppression (DSCC-HS)**, a novel, proactive framework that
intervenes during autoregressive decoding. Inspired by dual-process cognitive
theory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a
Factual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During
inference, these proxies dynamically steer a large target model by injecting a
real-time steering vector, which is the difference between FAP and HDP logits,
at each decoding step. This plug-and-play approach requires no modification to
the target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS
achieves state-of-the-art performance. On TruthfulQA, it reached a 99.2%
Factual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained
the highest FActScore of 46.50. These results validate DSCC-HS as a principled
and efficient solution for enhancing LLM factuality.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDSCC-HSçš„æ–°å‹ä¸»åŠ¨å¼æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è‡ªæˆ‘å¼ºåŒ–æ ¡å‡†æ¥æŠ‘åˆ¶LLMçš„å¹»è§‰é—®é¢˜ï¼Œé‡‡ç”¨åŒä»£ç†æ¨¡å‹ï¼ˆFAPå’ŒHDPï¼‰åœ¨è‡ªå›å½’è§£ç è¿‡ç¨‹ä¸­å®æ—¶ä¿®æ­£ç›®æ ‡æ¨¡å‹çš„è¾“å‡ºã€‚å°½ç®¡å±äºRAGç›¸å…³ç ”ç©¶ï¼ˆæåˆ°RAGä½œä¸ºç°æœ‰æ–¹æ³•å¯¹æ¯”ï¼‰ï¼Œä½†å…¶æ ¸å¿ƒåˆ›æ–°ç‚¹åœ¨äºä¸ä¾èµ–å¤–éƒ¨æ£€ç´¢çš„ä¸»åŠ¨å¹²é¢„æœºåˆ¶ï¼Œå®éªŒè¯æ˜åœ¨TruthfulQAå’ŒBioGENåŸºå‡†ä¸­æ˜¾è‘—æå‡äº†ç”Ÿæˆå†…å®¹çš„çœŸå®æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.13683v1">Improving Context Fidelity via Native Retrieval-Augmented Reasoning</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) often struggle with context fidelity, producing
inconsistent answers when responding to questions based on provided
information. Existing approaches either rely on expensive supervised
fine-tuning to generate evidence post-answer or train models to perform web
searches without necessarily improving utilization of the given context. We
propose CARE, a novel native retrieval-augmented reasoning framework that
teaches LLMs to explicitly integrate in-context evidence within their reasoning
process with the model's own retrieval capabilities. Our method requires
limited labeled evidence data while significantly enhancing both retrieval
accuracy and answer generation performance through strategically retrieved
in-context tokens in the reasoning chain. Extensive experiments on multiple
real-world and counterfactual QA benchmarks demonstrate that our approach
substantially outperforms supervised fine-tuning, traditional
retrieval-augmented generation methods, and external retrieval solutions. This
work represents a fundamental advancement in making LLMs more accurate,
reliable, and efficient for knowledge-intensive tasks.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†CAREæ¡†æ¶ï¼Œé€šè¿‡è®©å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ˜¾å¼æ•´åˆä¸Šä¸‹æ–‡è¯æ®å¹¶ç»“åˆè‡ªèº«æ£€ç´¢èƒ½åŠ›ï¼Œæ”¹è¿›äº†ä¼ ç»Ÿæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†æ£€ç´¢å‡†ç¡®æ€§å’Œç­”æ¡ˆç”Ÿæˆæ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šé¡¹QAåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç›‘ç£å¾®è°ƒå’Œå¤–éƒ¨æ£€ç´¢æ–¹æ¡ˆï¼Œå¢å¼ºäº†LLMsåœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.13626v1">Mind the Gap: Aligning Knowledge Bases with User Needs to Enhance Mental Health Retrieval</a></td><td><details><summary>å±•å¼€</summary>Access to reliable mental health information is vital for early help-seeking,
yet expanding knowledge bases is resource-intensive and often misaligned with
user needs. This results in poor performance of retrieval systems when
presented concerns are not covered or expressed in informal or contextualized
language. We present an AI-based gap-informed framework for corpus augmentation
that authentically identifies underrepresented topics (gaps) by overlaying
naturalistic user data such as forum posts in order to prioritize expansions
based on coverage and usefulness. In a case study, we compare Directed
(gap-informed augmentations) with Non-Directed augmentation (random additions),
evaluating the relevance and usefulness of retrieved information across four
retrieval-augmented generation (RAG) pipelines. Directed augmentation achieved
near-optimal performance with modest expansions--requiring only a 42% increase
for Query Transformation, 74% for Reranking and Hierarchical, and 318% for
Baseline--to reach ~95% of the performance of an exhaustive reference corpus.
In contrast, Non-Directed augmentation required substantially larger and thus
practically infeasible expansions to achieve comparable performance (232%,
318%, 403%, and 763%, respectively). These results show that strategically
targeted corpus growth can reduce content creation demands while sustaining
high retrieval and provision quality, offering a scalable approach for building
trusted health information repositories and supporting generative AI
applications in high-stakes domains.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºAIçš„æ¡†æ¶ï¼Œé€šè¿‡è¯†åˆ«æœªå……åˆ†è¦†ç›–çš„ä¸»é¢˜ï¼ˆç¼ºå£ï¼‰æ¥å¢å¼ºè¯­æ–™åº“ï¼Œå¹¶è¯„ä¼°äº†å…¶åœ¨å››ç§æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç®¡é“ä¸­çš„æ•ˆæœï¼Œç»“æœæ˜¾ç¤ºå®šå‘å¢å¼ºèƒ½ä»¥è¾ƒå°çš„æ‰©å±•è¾¾åˆ°æ¥è¿‘æœ€ä¼˜çš„æ£€ç´¢æ€§èƒ½ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-09-16
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2509.12765v1">InfoGain-RAG: Boosting Retrieval-Augmented Generation via Document Information Gain-based Reranking and Filtering</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) has emerged as a promising approach to
address key limitations of Large Language Models (LLMs), such as hallucination,
outdated knowledge, and lacking reference. However, current RAG frameworks
often struggle with identifying whether retrieved documents meaningfully
contribute to answer generation. This shortcoming makes it difficult to filter
out irrelevant or even misleading content, which notably impacts the final
performance. In this paper, we propose Document Information Gain (DIG), a novel
metric designed to quantify the contribution of retrieved documents to correct
answer generation. DIG measures a document's value by computing the difference
of LLM's generation confidence with and without the document augmented.
Further, we introduce InfoGain-RAG, a framework that leverages DIG scores to
train a specialized reranker, which prioritizes each retrieved document from
exact distinguishing and accurate sorting perspectives. This approach can
effectively filter out irrelevant documents and select the most valuable ones
for better answer generation. Extensive experiments across various models and
benchmarks demonstrate that InfoGain-RAG can significantly outperform existing
approaches, on both single and multiple retrievers paradigm. Specifically on
NaturalQA, it achieves the improvements of 17.9%, 4.5%, 12.5% in exact match
accuracy against naive RAG, self-reflective RAG and modern ranking-based RAG
respectively, and even an average of 15.3% increment on advanced proprietary
model GPT-4o across all datasets. These results demonstrate the feasibility of
InfoGain-RAG as it can offer a reliable solution for RAG in multiple
applications.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œæ–‡æ¡£ä¿¡æ¯å¢ç›Šï¼ˆDIGï¼‰â€çš„æ–°æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£å¯¹ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆçš„è´¡çŒ®ï¼Œå¹¶è¿›ä¸€æ­¥ä»‹ç»äº†åŸºäºDIGçš„InfoGain-RAGæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡è®­ç»ƒä¸“é—¨çš„é‡æ–°æ’åºæ¨¡å‹æ¥ä¼˜å…ˆé€‰æ‹©æœ€æœ‰ä»·å€¼çš„æ–‡æ¡£ï¼Œæ˜¾è‘—æå‡äº†RAGçš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.12743v1">Zero-shot Graph Reasoning via Retrieval Augmented Framework with LLMs</a></td><td><details><summary>å±•å¼€</summary>We propose a new, training-free method, Graph Reasoning via Retrieval
Augmented Framework (GRRAF), that harnesses retrieval-augmented generation
(RAG) alongside the code-generation capabilities of large language models
(LLMs) to address a wide range of graph reasoning tasks. In GRRAF, the target
graph is stored in a graph database, and the LLM is prompted to generate
executable code queries that retrieve the necessary information. This approach
circumvents the limitations of existing methods that require extensive
finetuning or depend on predefined algorithms, and it incorporates an error
feedback loop with a time-out mechanism to ensure both correctness and
efficiency. Experimental evaluations on the GraphInstruct dataset reveal that
GRRAF achieves 100% accuracy on most graph reasoning tasks, including cycle
detection, bipartite graph checks, shortest path computation, and maximum flow,
while maintaining consistent token costs regardless of graph sizes. Imperfect
but still very high performance is observed on subgraph matching. Notably,
GRRAF scales effectively to large graphs with up to 10,000 nodes.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºGRRAFçš„æ–°å‹å…è®­ç»ƒæ–¹æ³•ï¼Œåˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä»£ç ç”Ÿæˆèƒ½åŠ›æ¥è§£å†³å¹¿æ³›çš„å›¾æ¨ç†ä»»åŠ¡ã€‚GRRAFé€šè¿‡å°†ç›®æ ‡å›¾å­˜å‚¨åœ¨å›¾å½¢æ•°æ®åº“ä¸­ï¼Œå¹¶æç¤ºLLMç”Ÿæˆå¯æ‰§è¡Œçš„ä»£ç æŸ¥è¯¢æ¥æ£€ç´¢å¿…è¦ä¿¡æ¯ï¼Œä»è€Œé¿å…äº†ç°æœ‰æ–¹æ³•éœ€è¦å¤§é‡å¾®è°ƒæˆ–ä¾èµ–é¢„å®šä¹‰ç®—æ³•çš„é™åˆ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGRRAFåœ¨å¤§å¤šæ•°å›¾æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†100%çš„å‡†ç¡®ç‡ï¼Œå¹¶èƒ½æœ‰æ•ˆæ‰©å±•åˆ°åŒ…å«å¤šè¾¾10,000ä¸ªèŠ‚ç‚¹çš„å¤§å‹å›¾ä¸­ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.12653v1">Beyond Artificial Misalignment: Detecting and Grounding Semantic-Coordinated Multimodal Manipulations</a></td><td><details><summary>å±•å¼€</summary>The detection and grounding of manipulated content in multimodal data has
emerged as a critical challenge in media forensics. While existing benchmarks
demonstrate technical progress, they suffer from misalignment artifacts that
poorly reflect real-world manipulation patterns: practical attacks typically
maintain semantic consistency across modalities, whereas current datasets
artificially disrupt cross-modal alignment, creating easily detectable
anomalies. To bridge this gap, we pioneer the detection of
semantically-coordinated manipulations where visual edits are systematically
paired with semantically consistent textual descriptions. Our approach begins
with constructing the first Semantic-Aligned Multimodal Manipulation (SAMM)
dataset, generated through a two-stage pipeline: 1) applying state-of-the-art
image manipulations, followed by 2) generation of contextually-plausible
textual narratives that reinforce the visual deception. Building on this
foundation, we propose a Retrieval-Augmented Manipulation Detection and
Grounding (RamDG) framework. RamDG commences by harnessing external knowledge
repositories to retrieve contextual evidence, which serves as the auxiliary
texts and encoded together with the inputs through our image forgery grounding
and deep manipulation detection modules to trace all manipulations. Extensive
experiments demonstrate our framework significantly outperforms existing
methods, achieving 2.06\% higher detection accuracy on SAMM compared to
state-of-the-art approaches. The dataset and code are publicly available at
https://github.com/shen8424/SAMM-RamDG-CAP.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRAMDGçš„æ£€ç´¢å¢å¼ºå¤šæ¨¡æ€ç¯¡æ”¹æ£€æµ‹ä¸å®šä½æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºè¯­ä¹‰å¯¹é½çš„å¤šæ¨¡æ€ç¯¡æ”¹æ•°æ®é›†ï¼ˆSAMMï¼‰å¹¶åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†åº“æ£€ç´¢è¾…åŠ©è¯æ®ï¼Œæ˜¾è‘—æå‡äº†ç¯¡æ”¹æ£€æµ‹çš„å‡†ç¡®ç‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.12589v1">Redefining CX with Agentic AI: Minerva CQ Case Study</a></td><td><details><summary>å±•å¼€</summary>Despite advances in AI for contact centers, customer experience (CX)
continues to suffer from high average handling time (AHT), low first-call
resolution, and poor customer satisfaction (CSAT). A key driver is the
cognitive load on agents, who must navigate fragmented systems, troubleshoot
manually, and frequently place customers on hold. Existing AI-powered
agent-assist tools are often reactive driven by static rules, simple prompting,
or retrieval-augmented generation (RAG) without deeper contextual reasoning. We
introduce Agentic AI goal-driven, autonomous, tool-using systems that
proactively support agents in real time. Unlike conventional approaches,
Agentic AI identifies customer intent, triggers modular workflows, maintains
evolving context, and adapts dynamically to conversation state. This paper
presents a case study of Minerva CQ, a real-time Agent Assist product deployed
in voice-based customer support. Minerva CQ integrates real-time transcription,
intent and sentiment detection, entity recognition, contextual retrieval,
dynamic customer profiling, and partial conversational summaries enabling
proactive workflows and continuous context-building. Deployed in live
production, Minerva CQ acts as an AI co-pilot, delivering measurable
improvements in agent efficiency and customer experience across multiple
deployments.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†Agentic AIåœ¨å®¢æœä¸­å¿ƒçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯Minerva CQäº§å“ï¼Œå®ƒç»“åˆäº†å®æ—¶è½¬å½•ã€æ„å›¾è¯†åˆ«å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­‰æŠ€æœ¯ï¼Œé€šè¿‡åŠ¨æ€ä¸Šä¸‹æ–‡å’Œå·¥ä½œæµæå‡å®¢æœä»£ç†æ•ˆç‡åŠå®¢æˆ·ä½“éªŒã€‚å°½ç®¡RAGæ˜¯ç°æœ‰æŠ€æœ¯ä¹‹ä¸€ï¼Œä½†æ–‡ç« é‡ç‚¹å¼ºè°ƒå…¶è¶…è¶Šä¼ ç»ŸRAGçš„è‡ªä¸»æ€§å’Œå®æ—¶æ€§èƒ½åŠ›ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-09-15
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2509.12382v1">LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>The evaluation bottleneck in recommendation systems has become particularly
acute with the rise of Generative AI, where traditional metrics fall short of
capturing nuanced quality dimensions that matter in specialized domains like
legal research. Can we trust Large Language Models to serve as reliable judges
of their own kind? This paper investigates LLM-as-a-Judge as a principled
approach to evaluating Retrieval-Augmented Generation systems in legal
contexts, where the stakes of recommendation quality are exceptionally high.
  We tackle two fundamental questions that determine practical viability: which
inter-rater reliability metrics best capture the alignment between LLM and
human assessments, and how do we conduct statistically sound comparisons
between competing systems? Through systematic experimentation, we discover that
traditional agreement metrics like Krippendorff's alpha can be misleading in
the skewed distributions typical of AI system evaluations. Instead, Gwet's AC2
and rank correlation coefficients emerge as more robust indicators for judge
selection, while the Wilcoxon Signed-Rank Test with Benjamini-Hochberg
corrections provides the statistical rigor needed for reliable system
comparisons.
  Our findings suggest a path toward scalable, cost-effective evaluation that
maintains the precision demanded by legal applications, transforming what was
once a human-intensive bottleneck into an automated, yet statistically
principled, evaluation framework.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åœ¨æ¨èç³»ç»Ÿä¸­åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯„ä¼°å·¥å…·çš„å¯è¡Œæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ³•å¾‹æ£€ç´¢ä¸ç”Ÿæˆï¼ˆRAGï¼‰é¢†åŸŸã€‚ç ”ç©¶é‡ç‚¹å…³æ³¨å¦‚ä½•é€‰æ‹©å¯ä¿¡çš„æŒ‡æ ‡ï¼ˆå¦‚Gwet's AC2å’Œç§©ç›¸å…³ç³»æ•°ï¼‰å’Œç»Ÿè®¡æ–¹æ³•ï¼ˆå¦‚Wilcoxon Signed-Rank Testï¼‰æ¥å¯¹é½LLMä¸äººç±»è¯„ä¼°ç»“æœï¼Œä»è€Œä¸ºé«˜é£é™©çš„RAGç³»ç»Ÿæä¾›å¯æ‰©å±•ä¸”ç²¾å‡†çš„è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.12168v1">RAGs to Riches: RAG-like Few-shot Learning for Large Language Model Role-playing</a></td><td><details><summary>å±•å¼€</summary>Role-playing Large language models (LLMs) are increasingly deployed in
high-stakes domains such as healthcare, education, and governance, where
failures can directly impact user trust and well-being. A cost effective
paradigm for LLM role-playing is few-shot learning, but existing approaches
often cause models to break character in unexpected and potentially harmful
ways, especially when interacting with hostile users. Inspired by
Retrieval-Augmented Generation (RAG), we reformulate LLM role-playing into a
text retrieval problem and propose a new prompting framework called
RAGs-to-Riches, which leverages curated reference demonstrations to condition
LLM responses. We evaluate our framework with LLM-as-a-judge preference voting
and introduce two novel token-level ROUGE metrics: Intersection over Output
(IOO) to quantity how much an LLM improvises and Intersection over References
(IOR) to measure few-shot demonstrations utilization rate during the evaluation
tasks. When simulating interactions with a hostile user, our prompting strategy
incorporates in its responses during inference an average of 35% more tokens
from the reference demonstrations. As a result, across 453 role-playing
interactions, our models are consistently judged as being more authentic, and
remain in-character more often than zero-shot and in-context Learning (ICL)
methods. Our method presents a scalable strategy for building robust,
human-aligned LLM role-playing frameworks.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºRAGs-to-Richesçš„æç¤ºæ¡†æ¶ï¼Œå°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§’è‰²æ‰®æ¼”é‡æ–°æ„å»ºä¸ºæ–‡æœ¬æ£€ç´¢é—®é¢˜ï¼Œé€šè¿‡åˆ©ç”¨ç²¾å¿ƒç­–åˆ’çš„å‚è€ƒæ¼”ç¤ºæ¥è°ƒèŠ‚LLMçš„å“åº”ã€‚è¯¥æ¡†æ¶åœ¨å¯¹æŠ—æ€§ç”¨æˆ·äº’åŠ¨ä¸­è¡¨ç°æ›´ä¼˜ï¼Œèƒ½æ›´æœ‰æ•ˆåœ°åˆ©ç”¨å‚è€ƒæ¼”ç¤ºï¼Œæé«˜è§’è‰²çš„çœŸå®æ€§å’Œä¸€è‡´æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.12086v1">SAQ: Pushing the Limits of Vector Quantization through Code Adjustment and Dimension Segmentation</a></td><td><details><summary>å±•å¼€</summary>Approximate Nearest Neighbor Search (ANNS) plays a critical role in
applications such as search engines, recommender systems, and RAG for LLMs.
Vector quantization (VQ), a crucial technique for ANNS, is commonly used to
reduce space overhead and accelerate distance computations. However, despite
significant research advances, state-of-the-art VQ methods still face
challenges in balancing encoding efficiency and quantization accuracy. To
address these limitations, we propose a novel VQ method called SAQ. To improve
accuracy, SAQ employs a new dimension segmentation technique to strategically
partition PCA-projected vectors into segments along their dimensions. By
prioritizing leading dimension segments with larger magnitudes, SAQ allocates
more bits to high-impact segments, optimizing the use of the available space
quota. An efficient dynamic programming algorithm is developed to optimize
dimension segmentation and bit allocation, ensuring minimal quantization error.
To speed up vector encoding, SAQ devises a code adjustment technique to first
quantize each dimension independently and then progressively refine quantized
vectors using a coordinate-descent-like approach to avoid exhaustive
enumeration. Extensive experiments demonstrate SAQ's superiority over classical
methods (e.g., PQ, PCA) and recent state-of-the-art approaches (e.g., LVQ,
Extended RabitQ). SAQ achieves up to 80% reduction in quantization error and
accelerates encoding speed by over 80x compared to Extended RabitQ.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSAQçš„æ–°å‹å‘é‡é‡åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹è¿›è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ï¼ˆANNSï¼‰ä¸­çš„ç¼–ç æ•ˆç‡å’Œé‡åŒ–ç²¾åº¦å¹³è¡¡é—®é¢˜ï¼Œé€šè¿‡ç»´åº¦åˆ†å‰²å’ŒåŠ¨æ€ç¼–ç¨‹ä¼˜åŒ–æŠ€æœ¯æ˜¾è‘—é™ä½é‡åŒ–è¯¯å·®å¹¶åŠ é€Ÿç¼–ç é€Ÿåº¦ï¼Œç›´æ¥å…³è”å¹¶ä¼˜åŒ–äº†RAGæŠ€æœ¯ä¸­æ£€ç´¢ç¯èŠ‚çš„æ ¸å¿ƒæ€§èƒ½ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.12042v1">FinGEAR: Financial Mapping-Guided Enhanced Answer Retrieval</a></td><td><details><summary>å±•å¼€</summary>Financial disclosures such as 10-K filings present challenging retrieval
problems due to their length, regulatory section hierarchy, and domain-specific
language, which standard retrieval-augmented generation (RAG) models underuse.
We introduce FinGEAR (Financial Mapping-Guided Enhanced Answer Retrieval), a
retrieval framework tailored to financial documents. FinGEAR combines a finance
lexicon for Item-level guidance (FLAM), dual hierarchical indices for
within-Item search (Summary Tree and Question Tree), and a two-stage
cross-encoder reranker. This design aligns retrieval with disclosure structure
and terminology, enabling fine-grained, query-aware context selection.
Evaluated on full 10-Ks with queries aligned to the FinQA dataset, FinGEAR
delivers consistent gains in precision, recall, F1, and relevancy, improving F1
by up to 56.7% over flat RAG, 12.5% over graph-based RAGs, and 217.6% over
prior tree-based systems, while also increasing downstream answer accuracy with
a fixed reader. By jointly modeling section hierarchy and domain lexicon
signals, FinGEAR improves retrieval fidelity and provides a practical
foundation for high-stakes financial analysis.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†FinGEARï¼Œä¸€ä¸ªé’ˆå¯¹é‡‘èæ–‡æ¡£ï¼ˆå¦‚10-Kæ–‡ä»¶ï¼‰ä¼˜åŒ–çš„æ£€ç´¢æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆé‡‘èè¯æ±‡è¡¨ï¼ˆFLAMï¼‰ã€åŒé‡å±‚æ¬¡ç´¢å¼•å’Œä¸¤é˜¶æ®µäº¤å‰ç¼–ç å™¨é‡æ’å™¨ï¼Œæ”¹è¿›äº†ä¼ ç»ŸRAGæ¨¡å‹åœ¨é‡‘èé¢†åŸŸçš„æ£€ç´¢æ•ˆæœï¼Œæ˜¾è‘—æå‡äº†ç²¾ç¡®ç‡ã€å¬å›ç‡å’Œä¸‹æ¸¸ç­”æ¡ˆå‡†ç¡®æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.11947v1">A GPU-Accelerated RAG-Based Telegram Assistant for Supporting Parallel Processing Students</a></td><td><details><summary>å±•å¼€</summary>This project addresses a critical pedagogical need: offering students
continuous, on-demand academic assistance beyond conventional reception hours.
I present a domain-specific Retrieval-Augmented Generation (RAG) system powered
by a quantized Mistral-7B Instruct model and deployed as a Telegram bot. The
assistant enhances learning by delivering real-time, personalized responses
aligned with the "Introduction to Parallel Processing" course materials. GPU
acceleration significantly improves inference latency, enabling practical
deployment on consumer hardware. This approach demonstrates how consumer GPUs
can enable affordable, private, and effective AI tutoring for HPC education.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªé¢å‘æ•™è‚²é¢†åŸŸçš„RAGç³»ç»Ÿï¼ŒåŸºäºé‡åŒ–ç‰ˆMistral-7B Instructæ¨¡å‹æ„å»ºï¼Œé€šè¿‡Telegramæœºå™¨äººæä¾›å¹¶è¡Œå¤„ç†è¯¾ç¨‹çš„å®æ—¶ä¸ªæ€§åŒ–å­¦ä¹ æ”¯æŒï¼Œåˆ©ç”¨GPUåŠ é€Ÿå®ç°æ¶ˆè´¹çº§ç¡¬ä»¶éƒ¨ç½²ï¼Œå±•ç¤ºäº†ä½æˆæœ¬é«˜æ•ˆçš„AIè¾…å¯¼æ–¹æ¡ˆã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.11937v1">MMORE: Massive Multimodal Open RAG & Extraction</a></td><td><details><summary>å±•å¼€</summary>We introduce MMORE, an open-source pipeline for Massive Multimodal Open
RetrievalAugmented Generation and Extraction, designed to ingest, transform,
and retrieve knowledge from heterogeneous document formats at scale. MMORE
supports more than fifteen file types, including text, tables, images, emails,
audio, and video, and processes them into a unified format to enable downstream
applications for LLMs. The architecture offers modular, distributed processing,
enabling scalable parallelization across CPUs and GPUs. On processing
benchmarks, MMORE demonstrates a 3.8-fold speedup over single-node baselines
and 40% higher accuracy than Docling on scanned PDFs. The pipeline integrates
hybrid dense-sparse retrieval and supports both interactive APIs and batch RAG
endpoints. Evaluated on PubMedQA, MMORE-augmented medical LLMs improve
biomedical QA accuracy with increasing retrieval depth. MMORE provides a
robust, extensible foundation for deploying task-agnostic RAG systems on
diverse, real-world multimodal data. The codebase is available at
https://github.com/swiss-ai/mmore.</details></td><td><details><summary>å±•å¼€</summary>MMOREæ˜¯ä¸€ä¸ªå¼€æºçš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿï¼Œæ”¯æŒå¤„ç†å¤šç§æ–‡æ¡£æ ¼å¼ï¼ˆå¦‚æ–‡æœ¬ã€è¡¨æ ¼ã€å›¾åƒç­‰ï¼‰ï¼Œå¹¶å°†å…¶ç»Ÿä¸€å¤„ç†ä»¥ä¾›å¤§è¯­è¨€æ¨¡å‹ä½¿ç”¨ã€‚è¯¥ç³»ç»Ÿé€šè¿‡åˆ†å¸ƒå¼å¤„ç†æé«˜äº†æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œé›†æˆäº†æ··åˆæ£€ç´¢æ–¹æ³•ï¼Œå¹¶åœ¨åŒ»ç–—QAä»»åŠ¡ä¸­å±•ç°äº†æ€§èƒ½æå‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.11687v1">A Dynamic Knowledge Update-Driven Model with Large Language Models for Fake News Detection</a></td><td><details><summary>å±•å¼€</summary>As the Internet and social media evolve rapidly, distinguishing credible news
from a vast amount of complex information poses a significant challenge. Due to
the suddenness and instability of news events, the authenticity labels of news
can potentially shift as events develop, making it crucial for fake news
detection to obtain the latest event updates. Existing methods employ
retrieval-augmented generation to fill knowledge gaps, but they suffer from
issues such as insufficient credibility of retrieved content and interference
from noisy information. We propose a dynamic knowledge update-driven model for
fake news detection (DYNAMO), which leverages knowledge graphs to achieve
continuous updating of new knowledge and integrates with large language models
to fulfill dual functions: news authenticity detection and verification of new
knowledge correctness, solving the two key problems of ensuring the
authenticity of new knowledge and deeply mining news semantics. Specifically,
we first construct a news-domain-specific knowledge graph. Then, we use Monte
Carlo Tree Search to decompose complex news and verify them step by step.
Finally, we extract and update new knowledge from verified real news texts and
reasoning paths. Experimental results demonstrate that DYNAMO achieves the best
performance on two real-world datasets.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDYNAMOçš„å‡æ–°é—»æ£€æµ‹æ¨¡å‹ï¼Œé€šè¿‡ç»“åˆçŸ¥è¯†å›¾è°±çš„åŠ¨æ€æ›´æ–°ä¸å¤§è¯­è¨€æ¨¡å‹ï¼Œè§£å†³äº†ç°æœ‰æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ä¸­æ£€ç´¢å†…å®¹å¯ä¿¡åº¦ä¸è¶³å’Œå™ªå£°å¹²æ‰°çš„é—®é¢˜ã€‚æ¨¡å‹åˆ©ç”¨æ–°é—»é¢†åŸŸç‰¹å®šçš„çŸ¥è¯†å›¾è°±ï¼Œé€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢é€æ­¥åˆ†è§£å’ŒéªŒè¯å¤æ‚æ–°é—»ï¼ŒåŒæ—¶ä»å·²éªŒè¯çš„çœŸå®æ–°é—»ä¸­æå–å’Œæ›´æ–°çŸ¥è¯†ï¼Œå®ç°äº†æ–°é—»çœŸå®æ€§æ£€æµ‹ä¸æ–°çŸ¥è¯†æ­£ç¡®æ€§éªŒè¯çš„åŒé‡åŠŸèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜DYNAMOåœ¨ä¸¤ä¸ªçœŸå®æ•°æ®é›†ä¸Šè¡¨ç°æœ€ä½³ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.11645v1">Adapting and Evaluating Multimodal Large Language Models for Adolescent Idiopathic Scoliosis Self-Management: A Divide and Conquer Framework</a></td><td><details><summary>å±•å¼€</summary>This study presents the first comprehensive evaluation of Multimodal Large
Language Models (MLLMs) for Adolescent Idiopathic Scoliosis (AIS)
self-management. We constructed a database of approximately 3,000
anteroposterior X-rays with diagnostic texts and evaluated five MLLMs through a
`Divide and Conquer' framework consisting of a visual question-answering task,
a domain knowledge assessment task, and a patient education counseling
assessment task. Our investigation revealed limitations of MLLMs' ability in
interpreting complex spinal radiographs and comprehending AIS care knowledge.
To address these, we pioneered enhancing MLLMs with spinal keypoint prompting
and compiled an AIS knowledge base for retrieval augmented generation (RAG),
respectively. Results showed varying effectiveness of visual prompting across
different architectures, while RAG substantially improved models' performances
on the knowledge assessment task. Our findings indicate current MLLMs are far
from capable in realizing personalized assistant in AIS care. The greatest
challenge lies in their abilities to obtain accurate detections of spinal
deformity locations (best accuracy: 0.55) and directions (best accuracy: 0.13).</details></td><td><details><summary>å±•å¼€</summary>è¯¥ç ”ç©¶è¯„ä¼°äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨é’å°‘å¹´ç‰¹å‘æ€§è„ŠæŸ±ä¾§å‡¸(AIS)è‡ªæˆ‘ç®¡ç†ä¸­çš„åº”ç”¨ï¼Œå‘ç°æ¨¡å‹åœ¨è§£è¯»å¤æ‚è„ŠæŸ±Xå…‰ç‰‡å’Œç†è§£AISæŠ¤ç†çŸ¥è¯†æ–¹é¢å­˜åœ¨å±€é™ï¼Œå¹¶é€šè¿‡å¼•å…¥è„ŠæŸ±å…³é”®ç‚¹æç¤ºå’Œæ„å»ºAISçŸ¥è¯†åº“ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯æ¥æå‡æ¨¡å‹æ€§èƒ½ï¼Œç»“æœæ˜¾ç¤ºRAGæ˜¾è‘—æ”¹å–„äº†æ¨¡å‹çš„çŸ¥è¯†è¯„ä¼°ä»»åŠ¡è¡¨ç°ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.14267v1">Graph-Enhanced Retrieval-Augmented Question Answering for E-Commerce Customer Support</a></td><td><details><summary>å±•å¼€</summary>E-Commerce customer support requires quick and accurate answers grounded in
product data and past support cases. This paper develops a novel
retrieval-augmented generation (RAG) framework that uses knowledge graphs (KGs)
to improve the relevance of the answer and the factual grounding. We examine
recent advances in knowledge-augmented RAG and chatbots based on large language
models (LLM) in customer support, including Microsoft's GraphRAG and hybrid
retrieval architectures. We then propose a new answer synthesis algorithm that
combines structured subgraphs from a domain-specific KG with text documents
retrieved from support archives, producing more coherent and grounded
responses. We detail the architecture and knowledge flow of our system, provide
comprehensive experimental evaluation, and justify its design in real-time
support settings. Our implementation demonstrates 23\% improvement in factual
accuracy and 89\% user satisfaction in e-Commerce QA scenarios.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºçŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰çš„RAGæ¡†æ¶ï¼Œæ—¨åœ¨æå‡ç”µå­å•†åŠ¡å®¢æœå›ç­”çš„ç›¸å…³æ€§å’Œäº‹å®ä¾æ®ï¼Œé€šè¿‡ç»“åˆç»“æ„åŒ–å­å›¾å’Œæ–‡æœ¬æ£€ç´¢ç”Ÿæˆæ›´è¿è´¯çš„å“åº”ï¼Œå®éªŒè¡¨æ˜å…¶å®ç°23%çš„äº‹å®å‡†ç¡®æ€§æå‡å’Œ89%çš„ç”¨æˆ·æ»¡æ„åº¦ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.11552v2">HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with Hierarchical Chunking</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) enhances the response capabilities of
language models by integrating external knowledge sources. However, document
chunking as an important part of RAG system often lacks effective evaluation
tools. This paper first analyzes why existing RAG evaluation benchmarks are
inadequate for assessing document chunking quality, specifically due to
evidence sparsity. Based on this conclusion, we propose HiCBench, which
includes manually annotated multi-level document chunking points, synthesized
evidence-dense quetion answer(QA) pairs, and their corresponding evidence
sources. Additionally, we introduce the HiChunk framework, a multi-level
document structuring framework based on fine-tuned LLMs, combined with the
Auto-Merge retrieval algorithm to improve retrieval quality. Experiments
demonstrate that HiCBench effectively evaluates the impact of different
chunking methods across the entire RAG pipeline. Moreover, HiChunk achieves
better chunking quality within reasonable time consumption, thereby enhancing
the overall performance of RAG systems.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡èšç„¦äºRAGç³»ç»Ÿä¸­æ–‡æ¡£åˆ†å—ï¼ˆchunkingï¼‰è¯„ä¼°çš„ä¸è¶³ï¼Œæå‡ºå¸¦æœ‰æ‰‹åŠ¨æ ‡æ³¨å¤šçº§åˆ†å—ç‚¹çš„è¯„ä¼°åŸºå‡†HiCBenchå’Œè¯æ®å¯†é›†å‹QAæ•°æ®é›†ï¼ŒåŒæ—¶è®¾è®¡äº†åŸºäºå¾®è°ƒLLMsçš„å¤šçº§æ–‡æ¡£ç»“æ„åŒ–æ¡†æ¶HiChunkåŠAuto-Mergeæ£€ç´¢ç®—æ³•ï¼Œå®éªŒè¯æ˜å…¶èƒ½æœ‰æ•ˆæå‡åˆ†å—è´¨é‡å’ŒRAGæ•´ä½“æ€§èƒ½ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-09-14
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2509.11376v1">Intelligent Reservoir Decision Support: An Integrated Framework Combining Large Language Models, Advanced Prompt Engineering, and Multimodal Data Fusion for Real-Time Petroleum Operations</a></td><td><details><summary>å±•å¼€</summary>The petroleum industry faces unprecedented challenges in reservoir
management, requiring rapid integration of complex multimodal datasets for
real-time decision support. This study presents a novel integrated framework
combining state-of-the-art large language models (GPT-4o, Claude 4 Sonnet,
Gemini 2.5 Pro) with advanced prompt engineering techniques and multimodal data
fusion for comprehensive reservoir analysis. The framework implements
domain-specific retrieval-augmented generation (RAG) with over 50,000 petroleum
engineering documents, chain-of-thought reasoning, and few-shot learning for
rapid field adaptation. Multimodal integration processes seismic
interpretations, well logs, and production data through specialized AI models
with vision transformers. Field validation across 15 diverse reservoir
environments demonstrates exceptional performance: 94.2% reservoir
characterization accuracy, 87.6% production forecasting precision, and 91.4%
well placement optimization success rate. The system achieves sub-second
response times while maintaining 96.2% safety reliability with no high-risk
incidents during evaluation. Economic analysis reveals 62-78% cost reductions
(mean 72%) relative to traditional methods with 8-month payback period.
Few-shot learning reduces field adaptation time by 72%, while automated prompt
optimization achieves 89% improvement in reasoning quality. The framework
processed real-time data streams with 96.2% anomaly detection accuracy and
reduced environmental incidents by 45%. We provide detailed experimental
protocols, baseline comparisons, ablation studies, and statistical significance
testing to ensure reproducibility. This research demonstrates practical
integration of cutting-edge AI technologies with petroleum domain expertise for
enhanced operational efficiency, safety, and economic performance.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ã€å¤šæ¨¡æ€æ•°æ®èåˆå’Œé¢†åŸŸç‰¹å®šæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯çš„é›†æˆæ¡†æ¶ï¼Œç”¨äºçŸ³æ²¹è¡Œä¸šçš„å‚¨å±‚ç®¡ç†ã€‚é€šè¿‡æ•´åˆè¶…è¿‡50,000ä»½çŸ³æ²¹å·¥ç¨‹æ–‡æ¡£çš„RAGç³»ç»Ÿã€å¤šæ¨¡æ€æ•°æ®å¤„ç†ï¼ˆå¦‚åœ°éœ‡è§£é‡Šã€æµ‹äº•æ•°æ®å’Œç”Ÿäº§æ•°æ®ï¼‰ä»¥åŠé“¾å¼æ¨ç†å’Œå°‘æ ·æœ¬å­¦ä¹ ï¼Œæ˜¾è‘—æé«˜äº†å‚¨å±‚è¡¨å¾ã€äº§é‡é¢„æµ‹å’Œäº•ä½ä¼˜åŒ–çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼ŒåŒæ—¶é™ä½äº†æˆæœ¬å’Œå®‰å…¨é£é™©ã€‚å®è¯ç»“æœè¡¨æ˜è¯¥ç³»ç»Ÿåœ¨å¤šä¸ªæ€§èƒ½æŒ‡æ ‡ä¸Šè¡¨ç°å“è¶Šã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.14265v1">Evolution of Kernels: Automated RISC-V Kernel Optimization with Large Language Models</a></td><td><details><summary>å±•å¼€</summary>Automated kernel design is critical for overcoming software ecosystem
barriers in emerging hardware platforms like RISC-V. While large language
models (LLMs) have shown promise for automated kernel optimization,
demonstrating success in CUDA domains with comprehensive technical documents
and mature codebases, their effectiveness remains unproven for reference-scarce
domains like RISC-V. We present Evolution of Kernels (EoK), a novel LLM-based
evolutionary program search framework that automates kernel design for domains
with limited reference material. EoK mitigates reference scarcity by mining and
formalizing reusable optimization ideas (general design principles + actionable
thoughts) from established kernel libraries' development histories; it then
guides parallel LLM explorations using these ideas, enriched via
Retrieval-Augmented Generation (RAG) with RISC-V-specific context, prioritizing
historically effective techniques. Empirically, EoK achieves a median 1.27x
speedup, surpassing human experts on all 80 evaluated kernel design tasks and
improving upon prior LLM-based automated kernel design methods by 20%. These
results underscore the viability of incorporating human experience into
emerging domains and highlight the immense potential of LLM-based automated
kernel optimization.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºEoKï¼ˆEvolution of Kernelsï¼‰çš„åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è¿›åŒ–ç¨‹åºæœç´¢æ¡†æ¶ï¼Œç”¨äºåœ¨RISC-Vç­‰å‚è€ƒèµ„æºç¨€ç¼ºçš„é¢†åŸŸè‡ªåŠ¨åŒ–å†…æ ¸è®¾è®¡ã€‚EoKé€šè¿‡ä»å·²æœ‰å†…æ ¸åº“çš„å¼€å‘å†å²ä¸­æŒ–æ˜å’Œå½¢å¼åŒ–å¯é‡ç”¨çš„ä¼˜åŒ–æ€æƒ³ï¼Œå¹¶åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç»“åˆRISC-Vç‰¹å®šä¸Šä¸‹æ–‡æ¥æŒ‡å¯¼å¹¶è¡Œçš„å¤§è¯­è¨€æ¨¡å‹æ¢ç´¢ï¼Œä»è€Œåœ¨80é¡¹å†…æ ¸è®¾è®¡ä»»åŠ¡ä¸­å®ç°äº†ä¸­ä½æ•°1.27å€çš„åŠ é€Ÿï¼Œè¶…è¶Šäººç±»ä¸“å®¶å’Œå…ˆå‰åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ã€‚</details></td></tr></tbody></table>
